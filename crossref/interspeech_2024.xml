<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2024</doi_batch_id>
		<timestamp>1731097360166968</timestamp>
		<depositor>
			<depositor_name>Sébastien Le Maguer</depositor_name>
			<email_address>sebastien.lemaguer@helsinki.fi</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant>
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>Interspeech 2024</conference_name>
				<conference_acronym>interspeech_2024</conference_acronym>
				<conference_date>1-5 September 2024</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>Interspeech 2024</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2024</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2024</doi>
					<timestamp>1731097360166968</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2024/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruizhe</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>SA-MF: A Novel Self-Attention Mechanism for Multifeature Fusion in Speech Enhancement Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3365</first_page>
						<last_page>3369</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-4</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiqi</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shugong</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2415</first_page>
						<last_page>2419</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-10</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ai24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye-Xin</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Yan</given_name>
<surname>Sheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>MultiStage Speech Bandwidth Extension with Flexible Sampling Rate Control</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2270</first_page>
						<last_page>2274</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-11</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olympia</given_name>
<surname>Simantiraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name>
					</contributors>
					<titles><title>Listeners' F0 preferences in quiet and stationary noise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4224</first_page>
						<last_page>4228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-14</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/simantiraki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eike</given_name>
<surname>Schneiders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henry</given_name>
<surname>Card</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tina</given_name>
<surname>Seabrooke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beatrice</given_name>
<surname>Pakenham-Walsh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tayyaba</given_name>
<surname>Azim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucy</given_name>
<surname>Valls-Reed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Vigneswaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John Robert</given_name>
<surname>Bautista</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arya</given_name>
<surname>Farahi</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Acute Pain Levels Implicitly from Vocal Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1460</first_page>
						<last_page>1464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-15</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/williams24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingxing</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>G2PA: G2P with Aligned Audio for Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2800</first_page>
						<last_page>2804</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-19</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sizhou</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yibo</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiadi</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuelong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Textual-Driven Adversarial Purification for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>527</first_page>
						<last_page>531</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-21</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thien-Phuc</given_name>
<surname>Doan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Nguyen-Vu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kihun</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Souhwan</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>Balance, Multiple Augmentation, and Re-synthesis: A Triad Training Strategy for Enhanced Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2105</first_page>
						<last_page>2109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-24</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/doan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Keizer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama</given_name>
<surname>Doddipatla</surname>
</person_name>
					</contributors>
					<titles><title>Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1330</first_page>
						<last_page>1334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-26</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Niizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasunori</given_name>
<surname>Ohishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noboru</given_name>
<surname>Harada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masahiro</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunsuke</given_name>
<surname>Tsubaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Imoto</surname>
</person_name>
					</contributors>
					<titles><title>M2D-CLAP: Masked Modeling Duo Meets CLAP for Learning General-purpose Audio-Language Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>57</first_page>
						<last_page>61</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-29</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas M.</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Kawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Neu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Sperl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Böttinger</surname>
</person_name>
					</contributors>
					<titles><title>A New Approach to Voice Authenticity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2245</first_page>
						<last_page>2249</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-31</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/muller24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Kuhn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Verena</given_name>
<surname>Kersken</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gottfried</given_name>
<surname>Zimmermann</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4543</first_page>
						<last_page>4547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-32</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kuhn24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueqian</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyi</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keyi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuning</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifeng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1880</first_page>
						<last_page>1884</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-33</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Cappellazzo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Falavigna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3315</first_page>
						<last_page>3319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-38</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cappellazzo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogier</given_name>
<surname>van Dalen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sourav</given_name>
<surname>Bhattacharya</surname>
</person_name>
					</contributors>
					<titles><title>SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3460</first_page>
						<last_page>3464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-40</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/parcollet24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soham</given_name>
<surname>Deshmukh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>Domain Adaptation for Contrastive Audio-Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1680</first_page>
						<last_page>1684</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-41</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/deshmukh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Pešán</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vojtěch</given_name>
<surname>Juřík</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Karafiát</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>BESST Dataset: A Multimodal Resource for Speech-based Stress Detection and Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1355</first_page>
						<last_page>1359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-42</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pesan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Bentum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Lentz</surname>
</person_name>
					</contributors>
					<titles><title>The Processing of Stress in End-to-End Automatic Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2350</first_page>
						<last_page>2354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-44</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bentum24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhouyuan</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>AdaRA: Adaptive Rank Allocation of Residual Adapters for Speech Foundation Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2375</first_page>
						<last_page>2379</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-45</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinlei</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles Patrick</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>HybridVC: Efficient Voice Style Conversion with Text and Audio Prompts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4368</first_page>
						<last_page>4372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-46</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/niu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitriy</given_name>
<surname>Serdyuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengjian</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>USM RNN-T model weights binarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4508</first_page>
						<last_page>4512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-47</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rybakov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji-Sang</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeongrae</given_name>
<surname>Noh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonseok</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Insoo</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>X-Singer: Code-Mixed Singing Voice Synthesis via Cross-Lingual Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1885</first_page>
						<last_page>1889</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-49</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hwang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3325</first_page>
						<last_page>3329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-51</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianrui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yungang</given_name>
<surname>Jia</surname>
</person_name>
					</contributors>
					<titles><title>VoiCor: A Residual Iterative Voice Correction Framework for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4858</first_page>
						<last_page>4862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-53</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heeseung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-gil</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiheum</given_name>
<surname>Yeom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Che Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungwon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungroh</given_name>
<surname>Yoon</surname>
</person_name>
					</contributors>
					<titles><title>VoiceTailor: Lightweight Plug-In Adapter for Diffusion-Based Personalized Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4413</first_page>
						<last_page>4417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-63</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jizhong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1135</first_page>
						<last_page>1139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-65</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Learning Pronunciation from Other Accents via Pronunciation Knowledge Transfer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2805</first_page>
						<last_page>2809</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-66</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sun24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masaya</given_name>
<surname>Ohagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoya</given_name>
<surname>Mizumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsumasa</given_name>
<surname>Yoshikawa</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of look-ahead techniques to improve response time in spoken dialogue system</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3580</first_page>
						<last_page>3584</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-67</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ohagi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Si</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bruce Xiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yitian</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-yi</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Cheung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuoming</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic changes in speech prosody produced by children with autism after robot-assisted speech training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2480</first_page>
						<last_page>2484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-68</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenbin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjay</given_name>
<surname>Jha</surname>
</person_name>
					</contributors>
					<titles><title>GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1365</first_page>
						<last_page>1369</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-70</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaden</given_name>
<surname>Pieper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Voran</surname>
</person_name>
					</contributors>
					<titles><title>AlignNet: Learning dataset score alignment functions to enable better training of speech quality estimators</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>82</first_page>
						<last_page>86</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-74</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pieper24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji Won</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beom Jun</given_name>
<surname>Woo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2400</first_page>
						<last_page>2404</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-80</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yoon24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hideyuki</given_name>
<surname>Oiso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Matsunaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Kakizaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiki</given_name>
<surname>Miyagawa</surname>
</person_name>
					</contributors>
					<titles><title>Prompt Tuning for Audio Deepfake Detection: Computationally Efficient Test-time Domain Adaptation with Limited Target Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2710</first_page>
						<last_page>2714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-81</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/oiso24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bunlong</given_name>
<surname>Lay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>An Analysis of the Variance of Diffusion-based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2205</first_page>
						<last_page>2209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-85</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lay24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Christ</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Friederike</given_name>
<surname>Hawighorst</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ann-Kathrin</given_name>
<surname>Schill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelo</given_name>
<surname>Boutalikakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorenz</given_name>
<surname>Graf-Vlachy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>König</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>This Paper Had the Smartest Reviewers - Flattery Detection Utilising an Audio-Textual Transformer-Based Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3530</first_page>
						<last_page>3534</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-87</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/christ24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengwei</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangjun</given_name>
<surname>Kuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name>
					</contributors>
					<titles><title>LibriheavyMix: A 20,000-Hour Dataset for Single-Channel Reverberant Multi-Talker Speech Separation, ASR and Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>702</first_page>
						<last_page>706</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-90</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Mayr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Fendler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Pokorny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Berghaus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Sustained Vowels for Pre- vs Post-Treatment COPD Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1410</first_page>
						<last_page>1414</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-96</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/triantafyllopoulos24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Rampp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Milling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1585</first_page>
						<last_page>1589</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-97</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/triantafyllopoulos24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3729</first_page>
						<last_page>3733</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-98</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/triantafyllopoulos24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuyu</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biqiao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mumin</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaojun</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Query-by-Example Keyword Spotting Using Spectral-Temporal Graph Attentive Pooling and Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>322</first_page>
						<last_page>326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-100</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniela A.</given_name>
<surname>Wiepert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rene L.</given_name>
<surname>Utianski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph R.</given_name>
<surname>Duffy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John L.</given_name>
<surname>Stricker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leland R.</given_name>
<surname>Barnard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David T.</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Botha</surname>
</person_name>
					</contributors>
					<titles><title>Speech foundation models in healthcare: Effect of layer selection on pathological speech feature prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4618</first_page>
						<last_page>4622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-102</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wiepert24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengxin</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanqing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shimin</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Using Large Language Model for End-to-End Chinese ASR and NER</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>822</first_page>
						<last_page>826</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-103</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinglu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaosong</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengxin</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miaomiao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daimeng</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shimin</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>A Multitask Training Approach to Enhance Whisper with Open-Vocabulary Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1260</first_page>
						<last_page>1264</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-104</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucheng</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name>
					</contributors>
					<titles><title>LungAdapter: Efficient Adapting Audio Spectrogram Transformer for Lung Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4738</first_page>
						<last_page>4742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-106</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xiao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Su</surname>
</person_name>
					</contributors>
					<titles><title>Low Complexity Echo Delay Estimator Based on Binarized Feature Matching</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>157</first_page>
						<last_page>161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-107</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Xin</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Hang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Yan</given_name>
<surname>Sheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui-Chen</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>A Low-Bitrate Neural Audio Codec Framework with Bandwidth Reduction and Recovery for High-Sampling-Rate Waveforms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1765</first_page>
						<last_page>1769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-108</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ai24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shintaro</given_name>
<surname>Doan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minami</given_name>
<surname>Ishikawa</surname>
</person_name>
					</contributors>
					<titles><title>Signal processing algorithm effective for sound quality of hearing loss simulators </title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>882</first_page>
						<last_page>886</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-111</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/irino24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hualei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifang</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangdong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Language Model Capabilities for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4803</first_page>
						<last_page>4807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-112</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Hassan</given_name>
<surname>Vali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>Privacy PORCUPINE: Anonymization of Speaker Attributes Using Occurrence Normalization for Space-Filling Vector Quantization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2230</first_page>
						<last_page>2234</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-117</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vali24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3185</first_page>
						<last_page>3189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-119</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhas</given_name>
<surname>BN</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Rebar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saeed</given_name>
<surname>Abdullah</surname>
</person_name>
					</contributors>
					<titles><title>Speaking of Health: Leveraging Large Language Models to assess Exercise Motivation and Behavior of Rehabilitation Patients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3155</first_page>
						<last_page>3159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-121</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bn24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Wen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>MultiPA: A Multi-task Speech Pronunciation Assessment Model for Open Response Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>297</first_page>
						<last_page>301</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-123</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Candy Olivia</given_name>
<surname>Mawalim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Okada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name>
					</contributors>
					<titles><title>Are Recent Deep Learning-Based Speech Enhancement Methods Ready to Confront Real-World Noisy Environments?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1735</first_page>
						<last_page>1739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-129</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mawalim24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Developing an End-to-End Framework for Predicting the Social Communication Severity Scores of Children with Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1430</first_page>
						<last_page>1434</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-131</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mun24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mewlude</given_name>
<surname>Nijat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Askar</given_name>
<surname>Hamdulla</surname>
</person_name>
					</contributors>
					<titles><title>UY/CH-CHILD -- A Public Chinese L2 Speech Database of Uyghur Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2585</first_page>
						<last_page>2589</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-135</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nijat24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steven</given_name>
<surname>Vander Eeckt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Online Continual Learning for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2845</first_page>
						<last_page>2849</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-136</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vandereeckt24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name>
					</contributors>
					<titles><title>Universal Score-based Speech Enhancement with High Content Preservation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1165</first_page>
						<last_page>1169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-138</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/scheibler24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunil</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungeun</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>FVTTS : Face Based Voice Synthesis for Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4953</first_page>
						<last_page>4957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-140</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maoshen</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonggang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changchun</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Spatial Acoustic Enhancement Using Unbiased Relative Harmonic Coefficients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3265</first_page>
						<last_page>3269</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-141</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangjian</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gebhard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>DB3V: A Dialect Dominated Dataset of Bird Vocalisation for Cross-corpus Bird Species Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>127</first_page>
						<last_page>131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-143</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jing24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Da</given_name>
<surname>Mu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhicheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haobo</given_name>
<surname>Yue</surname>
</person_name>
					</contributors>
					<titles><title>MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>92</first_page>
						<last_page>96</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-145</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minghan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Guo</surname>
</person_name>
					</contributors>
					<titles><title>RASU: Retrieval Augmented Speech Understanding through Generative Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3510</first_page>
						<last_page>3514</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-148</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julius</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steven</given_name>
<surname>Krenn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Welker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bunlong</given_name>
<surname>Lay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Richard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4873</first_page>
						<last_page>4877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-153</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/richter24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Despotovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abir</given_name>
<surname>Elbéji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr V.</given_name>
<surname>Nazarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guy</given_name>
<surname>Fagherazzi</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Fusion for Vocal Biomarkers Using Vector Cross-Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1435</first_page>
						<last_page>1439</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-156</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/despotovic24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nauman</given_name>
<surname>Dawalatabad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Vilela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Placek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Tracey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yishu</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>Premasiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fernando</given_name>
<surname>Vieira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal Speech Transformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2000</first_page>
						<last_page>2004</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-158</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujia</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia</surname>
</person_name>
					</contributors>
					<titles><title>Bridging Child-Centered Speech Language Identification and Language Diarization via Phonetics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5148</first_page>
						<last_page>5152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-159</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>On the Usefulness of Speaker Embeddings for Speaker Retrieval in the Wild: A Comparative Study of x-vector and ECAPA-TDNN Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3774</first_page>
						<last_page>3778</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-161</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/loweimi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengxiao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nakamasa</given_name>
<surname>Inoue</surname>
</person_name>
					</contributors>
					<titles><title>Locally Aligned Rectified Flow Model for Speech Enhancement Towards Single-Step Diffusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2195</first_page>
						<last_page>2199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-162</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeehye</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeji</given_name>
<surname>Seo</surname>
</person_name>
					</contributors>
					<titles><title>Online Knowledge Distillation of Decoder-Only Large Language Models for Efficient Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2890</first_page>
						<last_page>2894</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-163</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Neural Network Augmented Kalman Filter for Robust Acoustic Howling Suppression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1715</first_page>
						<last_page>1719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-166</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name>
					</contributors>
					<titles><title>Frame-Wise Breath Detection with Self-Training: An Exploration of Enhancing Breath Naturalness in Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4928</first_page>
						<last_page>4932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-168</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Suda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aya</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name>
					</contributors>
					<titles><title>Who Finds This Voice Attractive? A Large-Scale Experiment Using In-the-Wild Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3165</first_page>
						<last_page>3169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-173</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/suda24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mayank Kumar</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weihsiang</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Mitsufuji</surname>
</person_name>
					</contributors>
					<titles><title>SilentCipher: Deep Audio Watermarking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2235</first_page>
						<last_page>2239</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-174</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/singh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun Myung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kangwook</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>One-class learning with adaptive centroid shift for audio deepfake detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4853</first_page>
						<last_page>4857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-177</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Constantijn</given_name>
<surname>Kaland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeremy</given_name>
<surname>Steffman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Cole</surname>
</person_name>
					</contributors>
					<titles><title>K-means and hierarchical clustering of f0 contours</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1520</first_page>
						<last_page>1524</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-181</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kaland24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Constantijn</given_name>
<surname>Kaland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Lialiou</surname>
</person_name>
					</contributors>
					<titles><title>Quantity-sensitivity affects recall performance of word stress</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4238</first_page>
						<last_page>4242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-182</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kaland24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Loukas</given_name>
<surname>Ilias</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitris</given_name>
<surname>Askounis</surname>
</person_name>
					</contributors>
					<titles><title>A Cross-Attention Layer coupled with Multimodal Fusion Methods for Recognizing Depression from Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>912</first_page>
						<last_page>916</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-188</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ilias24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Paissan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabetta</given_name>
<surname>Farella</surname>
</person_name>
					</contributors>
					<titles><title>tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1685</first_page>
						<last_page>1689</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-193</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/paissan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>TSE-PI: Target Sound Extraction under Reverberant Environments with Pitch Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>602</first_page>
						<last_page>606</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-197</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin Yuen</given_name>
<surname>Kwok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Continual Learning Optimizations for Auto-regressive Decoder of Multilingual ASR systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1225</first_page>
						<last_page>1229</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-205</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kwok24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Gengembre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Le Blouch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cédric</given_name>
<surname>Gendrot</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling prosody and timbre embeddings via voice conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2765</first_page>
						<last_page>2769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-207</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gengembre24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masato</given_name>
<surname>Murata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name>
					</contributors>
					<titles><title>An Attribute Interpolation Method in Speech Synthesis by Model Merging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3380</first_page>
						<last_page>3384</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-208</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/murata24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiling</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evan</given_name>
<surname>Clark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hank</given_name>
<surname>Liao</surname>
</person_name>
					</contributors>
					<titles><title>DiarizationLM: Speaker Diarization Post-Processing with Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3754</first_page>
						<last_page>3758</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-209</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pawel</given_name>
<surname>Bujnowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bartlomiej</given_name>
<surname>Kuzma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bartlomiej</given_name>
<surname>Paziewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacek</given_name>
<surname>Rutkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Marhula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuzanna</given_name>
<surname>Bordzicka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Andruszkiewicz</surname>
</person_name>
					</contributors>
					<titles><title>SAMSEMO: New dataset for multilingual and multimodal emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2925</first_page>
						<last_page>2929</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-212</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bujnowski24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyeonuk</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Hu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deokki</given_name>
<surname>Min</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong-Hwa</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Diversifying and Expanding Frequency-Adaptive Convolution Kernels for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>97</first_page>
						<last_page>101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-216</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nam24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihwan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Kommineni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kleanthis</given_name>
<surname>Avramidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Toward Fully-End-to-End Listened Speech Decoding from EEG Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1500</first_page>
						<last_page>1504</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-223</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuochao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qirui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malek</given_name>
<surname>Itani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emre Sefik</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shyamnath</given_name>
<surname>Gollakota</surname>
</person_name>
					</contributors>
					<titles><title>Target conversation extraction: Source separation using turn-taking dynamics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3550</first_page>
						<last_page>3554</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-225</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngmoon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungjin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Young</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Roh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang Woo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Relational Proxy Loss for Audio-Text based Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>327</first_page>
						<last_page>331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-229</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jung24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaejun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoori</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Injune</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Hear Your Face: Face-based voice conversion with F0 estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4378</first_page>
						<last_page>4382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-232</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunik</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daehyun</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungjun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>RepTor: Re-parameterizable Temporal Convolution for Keyword Spotting via Differentiable Kernel Search</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4518</first_page>
						<last_page>4522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-233</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/park24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seonwoo</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Speech Production Skills for Children with Cochlear Implants Using Wav2Vec2.0 Acoustic Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>862</first_page>
						<last_page>866</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-234</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hokuto</given_name>
<surname>Munakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Terashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name>
					</contributors>
					<titles><title>Song Data Cleansing for End-to-End Neural Singer Diarization Using Neural Analysis and Synthesis Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1665</first_page>
						<last_page>1669</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-235</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/munakata24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Audio Transformers for Online Audio Tagging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1145</first_page>
						<last_page>1149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-242</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dinkel24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaixun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongfeng</given_name>
<surname>Quan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weihong</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoqing</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>SEQ-former: A context-enhanced and efficient automatic speech recognition framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>212</first_page>
						<last_page>216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-243</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name>
					</contributors>
					<titles><title>Audio Fingerprinting with Holographic Reduced Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>62</first_page>
						<last_page>66</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-245</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fujita24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Scaling up masked audio encoder learning for general audio classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>547</first_page>
						<last_page>551</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-246</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dinkel24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas M.</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Sperl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Böttinger</surname>
</person_name>
					</contributors>
					<titles><title>Harder or Different? Understanding Generalization of Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2705</first_page>
						<last_page>2709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-247</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/muller24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shihao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Na</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rilin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion with Singer Guidance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2770</first_page>
						<last_page>2774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-250</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haochen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhentao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenting</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Spoofing Speech Detection by Modeling Local Spectro-Temporal and Long-term Dependency</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>507</first_page>
						<last_page>511</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-251</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haochen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuhai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Adapter Learning from Pre-trained Model for Robust Spoof Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2095</first_page>
						<last_page>2099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-253</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuankun</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaopeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haonnan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm with Real Emphasis and Fake Dispersion Strategy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4833</first_page>
						<last_page>4837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-254</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xie24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui-Peng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Xin</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>BiVocoder: A Bidirectional Neural Vocoder Integrating Feature Extraction and Waveform Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3894</first_page>
						<last_page>3898</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-255</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/du24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinsuke</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Efficient and Robust Long-Form Speech Recognition with Hybrid H3-Conformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2895</first_page>
						<last_page>2899</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-258</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/honda24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuaishuai</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunfei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinkang</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and Non-streaming Code-Switching ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3999</first_page>
						<last_page>4003</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-259</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ye24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haochen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuoliang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tune Pre-Trained Models with Multi-Level Feature Fusion for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2110</first_page>
						<last_page>2114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-260</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/peng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tongtao</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutao</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>A Small and Fast BERT for Chinese Medical Punctuation Restoration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4533</first_page>
						<last_page>4537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-263</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ling24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qian</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyue</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingze</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhefeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baoxing</given_name>
<surname>Huai</surname>
</person_name>
					</contributors>
					<titles><title>MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1845</first_page>
						<last_page>1849</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-266</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Malo</given_name>
<surname>Maisonneuve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Fredouille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muriel</given_name>
<surname>Lalain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alain</given_name>
<surname>Ghio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name>
					</contributors>
					<titles><title>Towards objective and interpretable speech disorder assessment: a comparative analysis of CNN and transformer-based models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1970</first_page>
						<last_page>1974</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-267</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/maisonneuve24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Setoguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiko</given_name>
<surname>Arimoto</surname>
</person_name>
					</contributors>
					<titles><title>Acoustical analysis of the initial phones in speech-laugh</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3170</first_page>
						<last_page>3174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-268</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/setoguchi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Dual-path Adaptation of Pretrained Feature Extraction Module for Robust Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2850</first_page>
						<last_page>2854</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-270</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thanh Lan</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Weber</surname>
</person_name>
					</contributors>
					<titles><title>Ethnolinguistic Identification of Vietnamese-German Heritage Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4209</first_page>
						<last_page>4213</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-274</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/truong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nafiseh</given_name>
<surname>Taghva</surname>
</person_name>
					</contributors>
					<titles><title>How rhythm metrics are linked to produced and perceived speaker charisma</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1065</first_page>
						<last_page>1069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-277</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/niebuhr24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Filip</given_name>
<surname>Packań</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2635</first_page>
						<last_page>2639</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-280</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/amiriparian24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiuping</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>SimuSOE: A Simulated Snoring Dataset for Obstructive Sleep Apnea-Hypopnea Syndrome Evaluation during Wakefulness</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>142</first_page>
						<last_page>146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-283</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoval</given_name>
<surname>Messica</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name>
					</contributors>
					<titles><title>NAST: Noise Aware Speech Tokenization for Speech Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4169</first_page>
						<last_page>4173</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-288</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/messica24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiara</given_name>
<surname>Riegger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tina</given_name>
<surname>Bögel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Walkden</surname>
</person_name>
					</contributors>
					<titles><title>The prosody of the verbal prefix ge-: historical and experimental evidence</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2065</first_page>
						<last_page>2069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-289</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/riegger24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kexu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanxin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengchen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Shao</surname>
</person_name>
					</contributors>
					<titles><title>Speech Formants Integration for Generalized Detection of Synthetic Speech Spoofing Attacks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2100</first_page>
						<last_page>2104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-292</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-efficient Fine-tuning of Speaker-Aware Dynamic Prompts for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2675</first_page>
						<last_page>2679</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-295</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>792</first_page>
						<last_page>796</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-298</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yusuf24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis</given_name>
<surname>Abel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Colotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Ouni</surname>
</person_name>
					</contributors>
					<titles><title>Towards realtime co-speech gestures synthesis using STARGATE</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4893</first_page>
						<last_page>4897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-302</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/abel24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jalal</given_name>
<surname>Al-Tamimi</surname>
</person_name>
					</contributors>
					<titles><title>Impact of the tonal factor on diphthong realizations in Standard Mandarin with Generalized Additive Mixed Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1555</first_page>
						<last_page>1559</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-303</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yehoshua</given_name>
<surname>Dissen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiry</given_name>
<surname>Yonash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5008</first_page>
						<last_page>5012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-306</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dissen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cecile</given_name>
<surname>Fougeron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Meunier</surname>
</person_name>
					</contributors>
					<titles><title>Do Speaker-dependent Vowel Characteristics depend on Speech Style?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3669</first_page>
						<last_page>3673</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-310</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/audibert24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Riyansha</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parinita</given_name>
<surname>Nema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinod K</given_name>
<surname>Kurmi</surname>
</person_name>
					</contributors>
					<titles><title>Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5023</first_page>
						<last_page>5027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-312</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/singh24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amit</given_name>
<surname>Roth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnon</given_name>
<surname>Turetzky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name>
					</contributors>
					<titles><title>A Language Modeling Approach to Diacritic-Free Hebrew TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2775</first_page>
						<last_page>2779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-313</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/roth24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Veranika</given_name>
<surname>Boukun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jakob</given_name>
<surname>Drefs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörg</given_name>
<surname>Lücke</surname>
</person_name>
					</contributors>
					<titles><title>Blind Zero-Shot Audio Restoration: A Variational Autoencoder Approach for Denoising and Inpainting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4823</first_page>
						<last_page>4827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-314</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/boukun24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Austin</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret E. L.</given_name>
<surname>Renwick</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Italian Vowel Variation with the Recurrent Neural Network Phonet</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3679</first_page>
						<last_page>3683</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-317</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jones24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harsha Veena</given_name>
<surname>Tadavarthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Austin</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret E. L.</given_name>
<surname>Renwick</surname>
</person_name>
					</contributors>
					<titles><title>Phonological Feature Detection for US English using the Phonet Library</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1515</first_page>
						<last_page>1519</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-318</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tadavarthy24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rouditchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonid</given_name>
<surname>Karlinsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hilde</given_name>
<surname>Kuehne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogerio</given_name>
<surname>Feris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2420</first_page>
						<last_page>2424</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-322</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rouditchenko24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soham</given_name>
<surname>Deshmukh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dareen</given_name>
<surname>Alharthi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elizalde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannes</given_name>
<surname>Gamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahmoud</given_name>
<surname>Al Ismail</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaming</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>PAM: Prompting Audio-Language Models for Audio Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3320</first_page>
						<last_page>3324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-325</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/deshmukh24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Jen</given_name>
<surname>Shih</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Interface Design for Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2504</first_page>
						<last_page>2508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-326</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shih24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Hai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Noise-robust Speech Separation with Fast Generative Correction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2165</first_page>
						<last_page>2169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-327</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhita</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Jouaiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnab</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Siegert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stober</surname>
</person_name>
					</contributors>
					<titles><title>Anonymising Elderly and Pathological Speech: Voice Conversion Using DDSP and Query-by-Example</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4438</first_page>
						<last_page>4442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-328</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ghosh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kunal</given_name>
<surname>Dhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin Rao</given_name>
<surname>Koluguri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ante</given_name>
<surname>Jukić</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Langman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2574</first_page>
						<last_page>2578</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-330</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dhawan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vidya</given_name>
<surname>Srinivas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malek</given_name>
<surname>Itani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuochao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emre Sefik</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shyamnath</given_name>
<surname>Gollakota</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge boosting during low-latency inference</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4338</first_page>
						<last_page>4342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-331</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/srinivas24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyuhong</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkyu</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunjae</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Adapter for Parameter-Efficient ASR Encoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2380</first_page>
						<last_page>2384</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-334</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shim24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paarth</given_name>
<surname>Neekhara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shehzeen</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhankar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3425</first_page>
						<last_page>3429</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-335</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/neekhara24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongmei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuelong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Graph Attention Based Multi-Channel U-Net for Speech Dereverberation With Ad-Hoc Microphone Arrays</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>617</first_page>
						<last_page>621</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-336</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Towards Audio Codec-based Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2190</first_page>
						<last_page>2194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-337</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yip24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nigel G.</given_name>
<surname>Ward</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andres</given_name>
<surname>Segura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Ceballos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divette</given_name>
<surname>Marco</surname>
</person_name>
					</contributors>
					<titles><title>Towards a General-Purpose Model of Perceived Pragmatic Similarity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4918</first_page>
						<last_page>4922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-339</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ward24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeongseon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name>
					</contributors>
					<titles><title>Audio-conditioned phonemic and prosodic annotation for building text-to-speech models from unlabeled speech data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2795</first_page>
						<last_page>2799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-342</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shirahata24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingjing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Beck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4563</first_page>
						<last_page>4567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-343</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuhai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haochen</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Boosting the Transferability of Adversarial Examples with Gradient-Aligned Ensemble Attack for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>532</first_page>
						<last_page>536</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-346</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Run</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haozhe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anushka</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linda</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divya</given_name>
<surname>Tadimeti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Empathy in Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1080</first_page>
						<last_page>1084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-347</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhipeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianfeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tze Yuang</given_name>
<surname>Chong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Dual-Pipeline with Low-Rank Adaptation for New Language Integration in Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>787</first_page>
						<last_page>791</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-348</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/khassanov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takatomo</given_name>
<surname>Kano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name>
					</contributors>
					<titles><title>Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1945</first_page>
						<last_page>1949</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-349</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/matsuura24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zezhong</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzhi</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>W-GVKT: Within-Global-View Knowledge Transfer for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3779</first_page>
						<last_page>3783</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-354</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jin24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Lambropoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frantz</given_name>
<surname>Clermont</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunichi</given_name>
<surname>Ishihara</surname>
</person_name>
					</contributors>
					<titles><title>The sub-band cepstrum as a tool for locating local spectral regions of phonetic sensitivity: A first attempt with multi-speaker vowel data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1535</first_page>
						<last_page>1539</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-357</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lambropoulos24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Hieu</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Enhanced Language Modeling for Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3440</first_page>
						<last_page>3444</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-359</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zezhong</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzhi</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Learning with Multi-Head Multi-Mode Knowledge Distillation for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4723</first_page>
						<last_page>4727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-360</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jin24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju-ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minjae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised speaker verification with relational mask prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2655</first_page>
						<last_page>2659</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-362</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eungbeom</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hantae</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4588</first_page>
						<last_page>4592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-363</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yao</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaqian</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenguang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fulin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>CEC: A Noisy Label Detection Method for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3784</first_page>
						<last_page>3788</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-364</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>En-Lun</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Hsun</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeih-weih</given_name>
<surname>Hung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Chieh</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berlin</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Conditional Sinc-Extractor for Personal VAD</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2115</first_page>
						<last_page>2119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-365</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>PARAN: Variational Autoencoder-based End-to-End Articulation-to-Speech System for Speech Intelligibility</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2475</first_page>
						<last_page>2479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-366</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/um24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingze</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengqiang</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Copy-Synthesis Anti-Spoofing Training Method with Rhythm and Speaker Perturbation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>512</first_page>
						<last_page>516</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-367</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin Yuen</given_name>
<surname>Kwok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenhui</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Investigating ASR Error Correction with Large Language Model and Multilingual 1-best Hypotheses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1315</first_page>
						<last_page>1319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-368</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yip Keng</given_name>
<surname>Kan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>VoiceDefense: Protecting Automatic Speaker Verification Models Against Black-box Adversarial Attacks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>517</first_page>
						<last_page>521</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-372</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>HyunJung</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muyeol</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohan</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minkyu</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seonhui</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>SangHun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Spoken-to-written text conversion with Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2410</first_page>
						<last_page>2414</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-376</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/choi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiwon</given_name>
<surname>Suh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Injae</given_name>
<surname>Na</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woohwan</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1255</first_page>
						<last_page>1259</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-377</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/suh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rubing</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhen</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongkun</given_name>
<surname>Sun</surname>
</person_name>
					</contributors>
					<titles><title>FA-GAN: Artifacts-free and Phase-aware High-fidelity GAN-based Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3884</first_page>
						<last_page>3888</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-380</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shen24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heyang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanfeng</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1495</first_page>
						<last_page>1499</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-382</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/feng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke-Han</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>HypR: A comprehensive study for ASR hypothesis revising with a reference corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3495</first_page>
						<last_page>3499</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-385</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuto</given_name>
<surname>Igarashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>SRC4VC: Smartphone-Recorded Corpus for Voice Conversion Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1825</first_page>
						<last_page>1829</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-388</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/saito24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Svirsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uri</given_name>
<surname>Shaham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ofir</given_name>
<surname>Lindenbaum</surname>
</person_name>
					</contributors>
					<titles><title>Sparse Binarization for Fast Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3010</first_page>
						<last_page>3014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-389</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/svirsky24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aviv</given_name>
<surname>Shamsian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aviv</given_name>
<surname>Navon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neta</given_name>
<surname>Glazer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gill</given_name>
<surname>Hetz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Keyword-Guided Adaptation of Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>732</first_page>
						<last_page>736</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-391</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shamsian24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guillem</given_name>
<surname>Bonafos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clara</given_name>
<surname>Bourot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Pudlo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Freyermuth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurence</given_name>
<surname>Reboul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Tronçon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Rey</surname>
</person_name>
					</contributors>
					<titles><title>Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3605</first_page>
						<last_page>3609</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-394</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bonafos24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Asad</given_name>
<surname>Ullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Ragano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Hines</surname>
</person_name>
					</contributors>
					<titles><title>Reduce, Reuse, Recycle: Is Perturbed Data Better than Other Language Augmentation for Low Resource Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>77</first_page>
						<last_page>81</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-396</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ullah24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deok-Hyeon</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Seok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-Bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1810</first_page>
						<last_page>1814</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-398</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cho24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linkai</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>Variable Segment Length and Domain-Adapted Feature Optimization for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3744</first_page>
						<last_page>3748</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-399</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marie</given_name>
<surname>Kunešová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Lehečka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef</given_name>
<surname>Michálek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindrich</given_name>
<surname>Matousek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švec</surname>
</person_name>
					</contributors>
					<titles><title>Zero-shot Out-of-domain is No Joke: Lessons Learned in the VoiceMOS 2023 MOS Prediction Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4913</first_page>
						<last_page>4917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-400</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kunesova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eros</given_name>
<surname>Rosello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel M.</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iván</given_name>
<surname>López-Espejo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio M.</given_name>
<surname>Peinado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan M.</given_name>
<surname>Martín-Doñas</surname>
</person_name>
					</contributors>
					<titles><title>Anti-spoofing Ensembling Model: Dynamic Weight Allocation in Ensemble Models for Improved Voice Biometrics Security</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>497</first_page>
						<last_page>501</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-403</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rosello24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Towards Effective and Efficient Non-autoregressive Decoding Using Block-based Attention Mask</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>262</first_page>
						<last_page>266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-404</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuxin</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xusheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1670</first_page>
						<last_page>1674</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-405</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Shinayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name>
					</contributors>
					<titles><title>SpeakerBeam-SS: Real-time Target Speaker Extraction with Lightweight Conv-TasNet and State Space Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5033</first_page>
						<last_page>5037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-413</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sato24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Monaghan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Sebastian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicky</given_name>
<surname>Chong-White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vicky</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijayalakshmi</given_name>
<surname>Easwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Padraig</given_name>
<surname>Kitterick</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Hearing Loss from Children's Speech using wav2vec 2.0 Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>892</first_page>
						<last_page>896</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-414</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/monaghan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iuliia</given_name>
<surname>Zaitova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Stenger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tania</given_name>
<surname>Avgustinova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Linguistic Intelligibility of Non-Compositional Expressions in Spoken Context</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4189</first_page>
						<last_page>4193</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-416</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zaitova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arnon</given_name>
<surname>Turetzky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Or</given_name>
<surname>Tal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yael</given_name>
<surname>Segal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yehoshua</given_name>
<surname>Dissen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ella</given_name>
<surname>Zeldes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amit</given_name>
<surname>Roth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eyal</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosi</given_name>
<surname>Shrem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bronya R.</given_name>
<surname>Chernyak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olga</given_name>
<surname>Seleznova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name>
					</contributors>
					<titles><title>HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1360</first_page>
						<last_page>1364</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-417</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/turetzky24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Yuen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>Towards a better understanding of receptive multilingualism: listening conditions and priming effects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>12</first_page>
						<last_page>16</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-418</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xue24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jizhong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Bridging Language Gaps in Audio-Text Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1675</first_page>
						<last_page>1679</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-420</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul-Gauthier</given_name>
<surname>Noé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting and Improving Scoring Fusion for Spoofing-aware Speaker Verification Using Compositional Data Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1110</first_page>
						<last_page>1114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-422</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miseul</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youna</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Seok</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>Speak in the Scene: Diffusion-based Acoustic Scene Transfer toward Immersive Speech Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4883</first_page>
						<last_page>4887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-425</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haiyang</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fulin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge in Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4703</first_page>
						<last_page>4707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-427</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sun24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun Kyung</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manami</given_name>
<surname>Hirayama</surname>
</person_name>
					</contributors>
					<titles><title>Acquisition of high vowel devoicing in Japanese: A production experiment with three and four year olds</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4180</first_page>
						<last_page>4183</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-428</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hwang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rotem</given_name>
<surname>Rousso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eyal</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name>
					</contributors>
					<titles><title>Tradition or Innovation: A Comparison of Modern ASR Methods for Forced Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1525</first_page>
						<last_page>1529</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-429</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rousso24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Young Jin</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangha</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonghyun</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kee-Eung</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>867</first_page>
						<last_page>871</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-432</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ahn24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengrun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangjin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wang</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yahui</given_name>
<surname>Shan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling Age and Identity with a Mutual Information Minimization for Cross-Age Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3789</first_page>
						<last_page>3793</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-434</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naotaka</given_name>
<surname>Kawata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>SOMSRED: Sequential Output Modeling for Joint Multi-talker Overlapped Speech Recognition and Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1660</first_page>
						<last_page>1664</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-436</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/makishima24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xihang</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lixian</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zikai</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiharu</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Study Selectively: An Adaptive Knowledge Distillation based on a Voting Network for Heart Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>137</first_page>
						<last_page>141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-439</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/qiu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taichi</given_name>
<surname>Asami</surname>
</person_name>
					</contributors>
					<titles><title>Boosting Hybrid Autoregressive Transducer-based ASR with Internal Acoustic Model Training and Dual Blank Thresholding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3465</first_page>
						<last_page>3469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-442</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/moriya24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>I-Ting</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Recognition Using Curriculum Learning and Articulatory Feature Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1300</first_page>
						<last_page>1304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-444</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hsieh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthijs</given_name>
<surname>Van keirsbilck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Keller</surname>
</person_name>
					</contributors>
					<titles><title>Conformer without Convolutions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3475</first_page>
						<last_page>3479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-445</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vankeirsbilck24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tzu-Quan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsi-Che</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy T.</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>On the social bias of speech self-supervised models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4638</first_page>
						<last_page>4642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-454</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke-Han</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chiang Frank</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4159</first_page>
						<last_page>4163</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-457</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lu24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ching-Yu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreya G.</given_name>
<surname>Upadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>RW-VoiceShield: Raw Waveform-based Adversarial Attack on One-shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2730</first_page>
						<last_page>2734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-458</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinwei</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name>
					</contributors>
					<titles><title>A Framework for Phoneme-Level Pronunciation Assessment Using CTC</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>302</first_page>
						<last_page>306</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-459</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woan-Shiuan</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Group versus Individual Fairness in Perceptually Fair Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3205</first_page>
						<last_page>3209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-461</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chien24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Lenglet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Perrotin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerard</given_name>
<surname>Bailly</surname>
</person_name>
					</contributors>
					<titles><title>FastLips: an End-to-End Audiovisual Text-to-Speech System with Lip Features Prediction for Virtual Avatars</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3450</first_page>
						<last_page>3454</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-462</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lenglet24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yin-Tse</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreya G.</given_name>
<surname>Upadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>SWiBE: A Parameterized Stochastic Diffusion Process for Noise-Robust Bandwidth Expansion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2265</first_page>
						<last_page>2269</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-463</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joun Yeop</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeonghun</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3445</first_page>
						<last_page>3449</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-465</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreya G.</given_name>
<surname>Upadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4693</first_page>
						<last_page>4697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-469</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/upadhyay24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Tung</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Po</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Cluster-based Personalized Federated Learning Strategy for End-to-End ASR of Dementia Patients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2450</first_page>
						<last_page>2454</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-470</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hsu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hsing-Hang</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woan-Shiuan</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>An Inter-Speaker Fairness-Aware Speech Emotion Regression Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3190</first_page>
						<last_page>3194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-471</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Lehečka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef V.</given_name>
<surname>Psutka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lubos</given_name>
<surname>Smidl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Ircing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef</given_name>
<surname>Psutka</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1285</first_page>
						<last_page>1289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-472</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lehecka24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dirk Eike</given_name>
<surname>Hoffner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jana</given_name>
<surname>Roßbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>Joint prediction of subjective listening effort and speech intelligibility based on end-to-end learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4214</first_page>
						<last_page>4218</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-473</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hoffner24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamas</given_name>
<surname>Grosz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>What happens in continued pre-training? Analysis of self-supervised speech models with continued pre-training for colloquial Finnish ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5043</first_page>
						<last_page>5047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-476</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/getman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fredrik</given_name>
<surname>Cumlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Ungureanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandan</given_name>
<surname>K. A. Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Schüldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saikat</given_name>
<surname>Chatterjee</surname>
</person_name>
					</contributors>
					<titles><title>DNSMOS Pro: A Reduced-Size DNN for Probabilistic MOS of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4818</first_page>
						<last_page>4822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-478</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cumlin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamas</given_name>
<surname>Grosz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katri</given_name>
<surname>Hiovain-Asikainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Exploring adaptation techniques of large speech foundation models for low-resource ASR: a case study on Northern Sámi</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2539</first_page>
						<last_page>2543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-479</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/getman24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hoan My</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Guennec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philippe</given_name>
<surname>Martin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aghilas</given_name>
<surname>Sini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Lolive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Delhay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-François</given_name>
<surname>Marteau</surname>
</person_name>
					</contributors>
					<titles><title>Spoofed Speech Detection with a Focus on Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2080</first_page>
						<last_page>2084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-481</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tran24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Can Modelling Inter-Rater Ambiguity Lead To Noise-Robust Continuous Emotion Predictions?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3714</first_page>
						<last_page>3718</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-482</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Improving child speech recognition with augmented child-like speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5183</first_page>
						<last_page>5187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-485</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victor</given_name>
<surname>Miara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theo</given_name>
<surname>Lepage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reda</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Towards Supervised Performance on Speaker Verification with Self-Supervised Learning by Leveraging Large-Scale ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2660</first_page>
						<last_page>2664</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-486</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/miara24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guanrou</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>MaLa-ASR: Multimedia-Assisted LLM-Based ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2405</first_page>
						<last_page>2409</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-488</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Macaire</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chloé</given_name>
<surname>Dion</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Didier</given_name>
<surname>Schwab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Lecouteux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuelle</given_name>
<surname>Esperança-Rodier</surname>
</person_name>
					</contributors>
					<titles><title>Towards Speech-to-Pictograms Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>857</first_page>
						<last_page>861</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-490</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/macaire24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>June-Woo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miika</given_name>
<surname>Toikkanen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yera</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seoung-Eun</given_name>
<surname>Moon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ho-Young</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1690</first_page>
						<last_page>1694</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-492</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Bürger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabrina</given_name>
<surname>Milkus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Hohmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Muderlak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Schottdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Musil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name>
					</contributors>
					<titles><title>Real-world PTSD Recognition: A Cross-corpus and Cross-linguistic Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>487</first_page>
						<last_page>491</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-493</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kathan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangdong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youshan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Diffusion Gaussian Mixture Audio Denoise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2200</first_page>
						<last_page>2204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-494</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Graave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Lohrenz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>Mixed Children/Adult/Childrenized Fine-Tuning for Children’s ASR: How to Reduce Age Mismatch and Speaking Style Mismatch</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5188</first_page>
						<last_page>5192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-499</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/graave24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogier</given_name>
<surname>van Dalen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sourav</given_name>
<surname>Bhattacharya</surname>
</person_name>
					</contributors>
					<titles><title>Linear-Complexity Self-Supervised Learning for Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3480</first_page>
						<last_page>3484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-500</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>DiffVC+: Improving Diffusion-based Voice Conversion for Speaker Anonymization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4453</first_page>
						<last_page>4457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-502</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Blumenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Graave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Lohrenz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siegfried</given_name>
<surname>Kunzmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>Interleaved Audio/Audiovisual Transfer Learning for AV-ASR in Low-Resourced Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2524</first_page>
						<last_page>2528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-503</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arnav</given_name>
<surname>Kundu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prateeth</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyanka</given_name>
<surname>Padmanabhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devang</given_name>
<surname>Naik</surname>
</person_name>
					</contributors>
					<titles><title>RepCNN: Micro-sized, Mighty Models for Wakeword Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3470</first_page>
						<last_page>3474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-505</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kundu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Théodor</given_name>
<surname>Lemerle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Obin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Roebel</surname>
</person_name>
					</contributors>
					<titles><title>Small-E: Small Language Model with Linear Attention for Efficient Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3420</first_page>
						<last_page>3424</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-508</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lemerle24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji-Hun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Hong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mun-Hak</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Whisper Multilingual Downstream Task Tuning Using Task Vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2385</first_page>
						<last_page>2389</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-513</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luis Felipe</given_name>
<surname>Parra-Gallego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tilak</given_name>
<surname>Purohit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Vlasenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Cross-transfer Knowledge between Speech and Text Encoders to Evaluate Customer Satisfaction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>477</first_page>
						<last_page>481</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-514</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/parragallego24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Honglie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Mira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>RT-LA-VocE: Real-Time Low-SNR Audio-Visual Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2215</first_page>
						<last_page>2219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-516</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mara</given_name>
<surname>Barberis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pieter</given_name>
<surname>De Clercq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bastiaan</given_name>
<surname>Tamm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maaike</given_name>
<surname>Vandermosten</surname>
</person_name>
					</contributors>
					<titles><title>Automatic recognition and detection of aphasic natural speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1990</first_page>
						<last_page>1994</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-517</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/barberis24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moreno</given_name>
<surname>La Quatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Francesca</given_name>
<surname>Turco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Foundation Models and Speech Enhancement for Parkinson's Disease Detection from Speech in Real-World Operative Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1405</first_page>
						<last_page>1409</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-522</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/laquatra24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gayle</given_name>
<surname>DeDe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christos</given_name>
<surname>Salis</surname>
</person_name>
					</contributors>
					<titles><title>Prosody of speech production in latent post-stroke aphasia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5108</first_page>
						<last_page>5112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-524</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Maselli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Véronique</given_name>
<surname>Delvaux</surname>
</person_name>
					</contributors>
					<titles><title>Aerodynamics of Sakata labial-velar oral stops</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3140</first_page>
						<last_page>3144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-525</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/maselli24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jongsuk</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiwon</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junmo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1915</first_page>
						<last_page>1919</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-526</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Taherian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vahid</given_name>
<surname>Ahmadi Kalkhorani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Towards Explainable Monaural Speaker Separation with Auditory-based Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>572</first_page>
						<last_page>576</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-530</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/taherian24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anith</given_name>
<surname>Selvakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Homa</given_name>
<surname>Fashandi</surname>
</person_name>
					</contributors>
					<titles><title>Getting More for Less: Using Weak Labels and AV-Mixup for Robust Audio-Visual Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4728</first_page>
						<last_page>4732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-531</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/selvakumar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avihu</given_name>
<surname>Dekel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raul</given_name>
<surname>Fernandez</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Benefits of Tokenization of Discrete Acoustic Units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2780</first_page>
						<last_page>2784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-533</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dekel24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junzuo</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chu Yuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>TraceableSpeech: Towards Proactively Traceable Text-to-Speech with Watermarking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2250</first_page>
						<last_page>2254</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-534</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhou24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Chochlakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandrashekhar</given_name>
<surname>Lavania</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Mathur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Tackling Missing Modalities in Audio-Visual Representation Learning Using Masked Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4678</first_page>
						<last_page>4682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-535</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chochlakis24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Om</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Rafidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Efficiently Train ASR Models that Memorize Less and Perform Better with Per-core Clipping</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1320</first_page>
						<last_page>1324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-536</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zilong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>MM-NodeFormer: Node Transformer Multimodal Fusion for Emotion Recognition in Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4069</first_page>
						<last_page>4073</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-538</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji Sub</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Utilizing Adaptive Global Response Normalization and Cluster-Based Pseudo Labels for Zero-Shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2740</first_page>
						<last_page>2744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-539</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/um24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karrie</given_name>
<surname>Karahalios</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Child Vocalization Classification with  Phonetically-Tuned Embeddings for Assisting Autism Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5163</first_page>
						<last_page>5167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-540</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Plantinga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pheobe</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swaroop</given_name>
<surname>Gadiyaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abenezer</given_name>
<surname>Girma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Emami</surname>
</person_name>
					</contributors>
					<titles><title>Efficient SQA from Long Audio Contexts: A Policy-driven Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1350</first_page>
						<last_page>1354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-543</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/johnson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li-Fang</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Holliday</surname>
</person_name>
					</contributors>
					<titles><title>Voice Quality Variation in AAE: An Additional Challenge for Addressing Bias in ASR Models?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3080</first_page>
						<last_page>3084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-544</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lai24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiran</given_name>
<surname>Aziz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shmuel</given_name>
<surname>Peleg</surname>
</person_name>
					</contributors>
					<titles><title>Audio Enhancement from Multiple Crowdsourced Recordings: A Simple and Effective Baseline</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3355</first_page>
						<last_page>3359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-545</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/aziz24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Confidence Estimation for Automatic Detection of Depression and Alzheimer’s Disease Based on Clinical Interviews</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3160</first_page>
						<last_page>3164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-546</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikentii</given_name>
<surname>Pankov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valeria</given_name>
<surname>Pronina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kuzmin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maksim</given_name>
<surname>Borisov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Usoltsev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingshan</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Golubkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolai</given_name>
<surname>Ermolenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandra</given_name>
<surname>Shirshova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulia</given_name>
<surname>Matveeva</surname>
</person_name>
					</contributors>
					<titles><title>DINO-VITS: Data-Efficient Zero-Shot TTS with Self-Supervised Speaker Verification Loss for Noise Robustness</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>697</first_page>
						<last_page>701</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-549</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pankov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Electra</given_name>
<surname>Wallington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Characterizing code-switching: Applying Linguistic Principles for Metric Assessment and Development</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>7</first_page>
						<last_page>11</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-551</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sean</given_name>
<surname>Robertson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerald</given_name>
<surname>Penn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Dunbar</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying the Role of Textual Predictability in Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4029</first_page>
						<last_page>4033</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-552</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/robertson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiyi</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingbin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingyu</given_name>
<surname>Na</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing CTC-based speech recognition with diverse modeling units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4583</first_page>
						<last_page>4587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-555</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/han24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiqi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diamantino</given_name>
<surname>Caseiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kandarp</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pat</given_name>
<surname>Rondon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Zadrazil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lillian</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing Large-Scale Context Retrieval for End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4573</first_page>
						<last_page>4577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-558</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiling</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hank</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>On the Success and Limitations of Auxiliary Network Based Word-Level End-to-End Neural Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>32</first_page>
						<last_page>36</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-561</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chung-Ming</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apoorv</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name>
					</contributors>
					<titles><title>Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3410</first_page>
						<last_page>3414</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-562</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chien24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankit</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the limits of decoder-only models trained on public speech recognition corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>252</first_page>
						<last_page>256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-565</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gupta24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woo Hyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Vishnubhotla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudolf</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yogesh</given_name>
<surname>Virkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghuveer</given_name>
<surname>Peri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>SWAN: SubWord Alignment Network for HMM-free word timing estimation in end-to-end automatic speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2910</first_page>
						<last_page>2914</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-569</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Frank</given_name>
<surname>Seide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morrie</given_name>
<surname>Doulaty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junteng</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Speech ReaLLM – Real-time Speech Recognition with Multimodal Language Models by Teaching the Flow of Time</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1900</first_page>
						<last_page>1904</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-571</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/seide24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan Vu</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kota</given_name>
<surname>Dohi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohei</given_name>
<surname>Kawaguchi</surname>
</person_name>
					</contributors>
					<titles><title>Stream-based Active Learning for Anomalous Sound Detection in Machine Condition Monitoring</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>102</first_page>
						<last_page>106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-573</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ho24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Virat</given_name>
<surname>Shejwalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Om</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying Unintended Memorization in BEST-RQ ASR Encoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2905</first_page>
						<last_page>2909</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-574</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shejwalkar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taylor</given_name>
<surname>Berg-Kirkpatrick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shlomo</given_name>
<surname>Dubnov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2180</first_page>
						<last_page>2184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-576</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tien-Ju</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>Contemplative Mechanism for Speech Recognition: Speech Encoders can Think</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3455</first_page>
						<last_page>3459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-577</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pan-Pan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimmy</given_name>
<surname>Tobin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Tomanek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>MacDonald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katie</given_name>
<surname>Seaver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Cave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marilyn</given_name>
<surname>Ladewig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rus</given_name>
<surname>Heywood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>Learnings from curating a trustworthy, well-annotated, and useful dataset of disordered English speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2490</first_page>
						<last_page>2493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-578</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jiang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ante</given_name>
<surname>Jukić</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roman</given_name>
<surname>Korostik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Schrödinger Bridge for Generative Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1175</first_page>
						<last_page>1179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-579</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jukic24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naijun</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xucheng</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziqing</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Huan</surname>
</person_name>
					</contributors>
					<titles><title>An efficient text augmentation approach for contextualized Mandarin speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1310</first_page>
						<last_page>1314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-583</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zheng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiafeng</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4838</first_page>
						<last_page>4842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-587</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zejiang</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Goeric</given_name>
<surname>Huybrechts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshu</given_name>
<surname>Bhatia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting Convolution-free Transformer for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4568</first_page>
						<last_page>4572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-588</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Houjian</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaoran</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos Toshinori</given_name>
<surname>Ishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Ishiguro</surname>
</person_name>
					</contributors>
					<titles><title>X-E-Speech: Joint Training Framework of Non-Autoregressive Cross-lingual Emotional Text-to-Speech and Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4983</first_page>
						<last_page>4987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-589</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guo24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haici</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minje</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1170</first_page>
						<last_page>1174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-590</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoqing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjie</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Low-Complexity Acoustic Scene Classification Using Parallel Attention-Convolution Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>567</first_page>
						<last_page>571</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-591</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuxin</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xusheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Audio-text Retrieval with Transformer-based Hierarchical Alignment and Disentangled Cross-modal Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1140</first_page>
						<last_page>1144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-593</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xin24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huai-Zhe</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Ping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan-Yun</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-Ruei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Bilingual and Code-switching TTS Enhanced with Denoising Diffusion Model and GAN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4938</first_page>
						<last_page>4942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-600</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiu-Hsiang</given_name>
<surname>Liou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-Cheng</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Ping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tzu-Chieh</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Li</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Han</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsiang-Feng</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Yu</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing ECAPA-TDNN with Feature Processing Module and Attention Mechanism for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2120</first_page>
						<last_page>2124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-601</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>HengYu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kangdi</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoci</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenhua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Refining Self-supervised Learnt Speech Representation using Brain Activations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1480</first_page>
						<last_page>1484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-604</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianteng</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>SparseWAV: Fast and Accurate One-Shot Unstructured Pruning for Large Speech Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4498</first_page>
						<last_page>4502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-607</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>ClariTTS: Feature-ratio Normalization and Duration Stabilization for Code-mixed Multi-speaker Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3400</first_page>
						<last_page>3404</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-608</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junhui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youshan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Complex Image-Generative Diffusion Transformer for Audio Denoising</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2220</first_page>
						<last_page>2224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-611</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aijun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Effect of Complex Boundary Tones on Tone Identification: An Experimental Study with Mandarin-speaking Preschool Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4204</first_page>
						<last_page>4208</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-614</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nameer</given_name>
<surname>Hirschkind</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eloi</given_name>
<surname>DuBois</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dao</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Thiebaut</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Sinclair</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyle</given_name>
<surname>Spence</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Abrams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgan</given_name>
<surname>McGuire</surname>
</person_name>
					</contributors>
					<titles><title>Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>837</first_page>
						<last_page>841</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-616</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hirschkind24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Nakagome</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Hentschel</surname>
</person_name>
					</contributors>
					<titles><title>InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>207</first_page>
						<last_page>211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-619</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nakagome24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haixin</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaen</given_name>
<surname>Liang</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Speech Distortion and Artifacts for Speech Enhancement by Loss Function</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1730</first_page>
						<last_page>1734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-620</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tzu-Quan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4513</first_page>
						<last_page>4517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-626</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guanlin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Cascaded Transfer Learning Strategy for Cross-Domain Alzheimer's  Disease Recognition through Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>907</first_page>
						<last_page>911</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-627</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keiko</given_name>
<surname>Ochi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divesh</given_name>
<surname>Lala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Entrainment Analysis and Prosody Prediction of Subsequent Interlocutor’s Backchannels in Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>462</first_page>
						<last_page>466</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-628</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ochi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cem</given_name>
<surname>Subakan</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme Discretized Saliency Maps for Explainable Detection of AI-Generated Voice</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3295</first_page>
						<last_page>3299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-632</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gupta24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Factor-Conditioned Speaking-Style Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>782</first_page>
						<last_page>786</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-633</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ando24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Speech Recognition Performance Towards Large Language Model Based Voice Assistants</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4099</first_page>
						<last_page>4103</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-635</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Paissan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Della Libera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhepei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paris</given_name>
<surname>Smaragdis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cem</given_name>
<surname>Subakan</surname>
</person_name>
					</contributors>
					<titles><title>Audio Editing with Non-Rigid Text Prompts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3290</first_page>
						<last_page>3294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-636</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/paissan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shilin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haixin</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name>
					</contributors>
					<titles><title>QMixCAT: Unsupervised Speech Enhancement Using Quality-guided Signal Mixing and Competitive Alternating Model Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>642</first_page>
						<last_page>646</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-639</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dehao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kexin</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malu</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Global-Local Convolution with Spiking Neural Networks for Energy-efficient Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4523</first_page>
						<last_page>4527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-642</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuexuan</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet-Anh</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Romain</given_name>
<surname>Hennequin</surname>
</person_name>
					</contributors>
					<titles><title>STraDa: A Singer Traits Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1370</first_page>
						<last_page>1374</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-644</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junseok</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youkyum</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeunju</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doyeop</given_name>
<surname>Kwak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seongkyu</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>VoxSim: A perceptual voice similarity dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2580</first_page>
						<last_page>2584</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-646</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ahn24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Mai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weidong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>DropFormer: A Dynamic Noise-Dropping Transformer for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2645</first_page>
						<last_page>2649</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-651</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mai24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>742</first_page>
						<last_page>746</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-653</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Duc-Tuan</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruijie</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuan</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hieu-Thi</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Temporal-Channel Modeling in Multi-head Self-Attention for Synthetic Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>537</first_page>
						<last_page>541</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-659</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/truong24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenggang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinjiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Deep Echo Path Modeling for Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>612</first_page>
						<last_page>616</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-662</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Hung</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Tsung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>USD-AC: Unsupervised Speech Disentanglement for Accent Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4388</first_page>
						<last_page>4392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-664</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuqin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Pre-trained Speech Model for Articulatory Feature Extraction in Dysarthric Speech Using ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4598</first_page>
						<last_page>4602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-665</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Key Acoustic Cues for the Realization of Metrical Prominence in Tone Languages: A Cross-Dialect Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3130</first_page>
						<last_page>3134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-666</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Härkönen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel J.</given_name>
<surname>Broughton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lahiru</given_name>
<surname>Samarakoon</surname>
</person_name>
					</contributors>
					<titles><title>EEND-M2F: Masked-attention mask transformers for speaker diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>37</first_page>
						<last_page>41</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-668</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/harkonen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongheon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung-Woo</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>DeFTAN-AA: Array Geometry Agnostic Multichannel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3360</first_page>
						<last_page>3364</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-669</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neil</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shirish</given_name>
<surname>Karande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Gandhi</surname>
</person_name>
					</contributors>
					<titles><title>Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2470</first_page>
						<last_page>2474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-672</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shah24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pin-Yen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name>
					</contributors>
					<titles><title>Modality Translation Learning for Joint Speech-Text Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>772</first_page>
						<last_page>776</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-675</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Semin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeonghun</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeonseung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byoung Jin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>MakeSinger: A Semi-Supervised Training Method for Data-Efficient Singing Voice Synthesis via Classifier-free Diffusion Guidance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1865</first_page>
						<last_page>1869</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-678</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanyu</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiquan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Binaural Selective Attention Model for Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4323</first_page>
						<last_page>4327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-683</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meng24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liuxian</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruobing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiharu</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4293</first_page>
						<last_page>4297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-685</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ma24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojun</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cedar</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Sentence Type Effects on the Lombard Effect and Intelligibility Enhancement: A Comparative Study of Natural and Grid Sentences</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3864</first_page>
						<last_page>3868</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-691</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masaya</given_name>
<surname>Kawamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Hasumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name>
					</contributors>
					<titles><title>LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1850</first_page>
						<last_page>1854</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-692</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kawamura24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunbo</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siding</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name>
					</contributors>
					<titles><title>RawBMamba: End-to-End Bidirectional State Space Model for Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2720</first_page>
						<last_page>2724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-698</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xujiang</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas Fang</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>A Joint Noise Disentanglement and Adversarial Training Framework for Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>707</first_page>
						<last_page>711</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-700</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xing24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chaeyoung</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>FlowAVSE: Efficient Audio-Visual Speech Enhancement with Conditional Flow Matching</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2210</first_page>
						<last_page>2214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-701</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jung24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Rapid Language Adaptation for Multilingual E2E Speech Recognition Using Encoder Prompting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2900</first_page>
						<last_page>2904</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-702</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kashiwagi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4503</first_page>
						<last_page>4507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-703</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Decoder-only Architecture for Streaming End-to-end Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4463</first_page>
						<last_page>4467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-705</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tsunoo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sichen</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngmoon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungjin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Roh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changwoo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoonyoung</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>332</first_page>
						<last_page>336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-706</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jin24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Kurihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanori</given_name>
<surname>Sano</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Japanese Text-to-Speech Accuracy with a Novel Combination Transformer-BERT-based G2P: Integrating Pronunciation Dictionaries and Accent Sandhi</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2790</first_page>
						<last_page>2794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-708</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kurihara24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiahao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Motion Based Audio-Visual Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3834</first_page>
						<last_page>3838</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-709</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Le</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongxiu</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>Residual Speaker Representation for One-Shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2760</first_page>
						<last_page>2764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-710</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>802</first_page>
						<last_page>806</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-712</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/futami24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based Pre-training for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>557</first_page>
						<last_page>561</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-714</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cai24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuliya</given_name>
<surname>Korotkova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Kalinovskiy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Vakhrusheva</surname>
</person_name>
					</contributors>
					<titles><title>Word-level Text Markup for Prosody Control in Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2280</first_page>
						<last_page>2284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-715</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/korotkova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryandhimas E.</given_name>
<surname>Zezario</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiou-Shann</given_name>
<surname>Fuh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Non-Intrusive Speech Intelligibility Prediction for Hearing Aids using Whisper and Metadata</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3844</first_page>
						<last_page>3848</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-716</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zezario24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lue</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Tager-Flusberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5193</first_page>
						<last_page>5197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-717</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mario</given_name>
<surname>Zusag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurin</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernhad</given_name>
<surname>Thallinger</surname>
</person_name>
					</contributors>
					<titles><title>CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1265</first_page>
						<last_page>1269</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-731</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zusag24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrés</given_name>
<surname>Piñeiro-Martín</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carmen</given_name>
<surname>García-Mateo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Docio-Fernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>María del Carmen</given_name>
<surname>López-Pérez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georg</given_name>
<surname>Rehm</surname>
</person_name>
					</contributors>
					<titles><title>Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1235</first_page>
						<last_page>1239</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-734</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pineiromartin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiran</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haolin</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pei</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongshu</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Auditory Attention Decoding in Four-Talker Environment with EEG</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>432</first_page>
						<last_page>436</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-739</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunbo</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Lv</surname>
</person_name>
					</contributors>
					<titles><title>Frequency-mix Knowledge Distillation for Fake Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2240</first_page>
						<last_page>2244</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-740</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yafeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luyao</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>ERes2NetV2: Boosting Short-Duration Speaker Verification Performance with Computational Efficiency</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3245</first_page>
						<last_page>3249</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-742</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anika A.</given_name>
<surname>Spiesberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Semertzidou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caterina</given_name>
<surname>Gawrilow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tilman</given_name>
<surname>Reinelt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang A.</given_name>
<surname>Rauch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>“So . . . my child . . . ” – How Child ADHD Influences the Way Parents Talk</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2010</first_page>
						<last_page>2014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-744</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/spiesberger24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lifeng</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuting</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqi</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Denoising: A Novel Training Paradigm for Enhancing Speech-Image Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4064</first_page>
						<last_page>4068</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-745</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhou24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Malin</given_name>
<surname>Svensson Lundmark</surname>
</person_name>
					</contributors>
					<titles><title>Magnitude and timing of acceleration peaks in stressed and unstressed syllables</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2630</first_page>
						<last_page>2634</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-746</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/svenssonlundmark24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruizhe</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahsa</given_name>
<surname>Yarmohammadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name>
					</contributors>
					<titles><title>Improving Neural Biasing for Contextual Speech Recognition by Early Context Injection and Text Perturbation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>752</first_page>
						<last_page>756</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-749</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinsheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>SCDNet: Self-supervised Learning Feature based Speaker Change Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4718</first_page>
						<last_page>4722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-752</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijie</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyu</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>ASA: An Auditory Spatial Attention Dataset with Multiple Speaking Locations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>437</first_page>
						<last_page>441</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-753</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghe</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feilong</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Efficient Adapter Based on Pre-trained Models for Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>357</first_page>
						<last_page>361</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-759</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tin Mei</given_name>
<surname>Lun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Voskoboinik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ragheb</given_name>
<surname>Al-Ghezi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamas</given_name>
<surname>Grosz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Oversampling, Augmentation and Curriculum Learning for Speaking Assessment with Limited Training Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4019</first_page>
						<last_page>4023</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-760</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lun24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinjiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>SDAEC: Signal Decoupling for Advancing Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>172</first_page>
						<last_page>176</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-763</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>YongKang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>YueXian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>AFL-Net: Integrating Audio, Facial, and Lip Modalities with a Two-step Cross-attention for Robust Speaker Diarization in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>42</first_page>
						<last_page>46</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-764</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Genshun</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingzhi</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongfu</given_name>
<surname>Ye</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Transducer Based on Frame-Level Criterion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>247</first_page>
						<last_page>251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-768</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Kanagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>Pre-training Neural Transducer-based Streaming Voice Conversion for Faster Convergence and Alignment-free Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2755</first_page>
						<last_page>2759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-771</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kanagawa24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Improving Multilingual Text-to-Speech with Mixture-of-Language-Experts and Accent Disentanglement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4968</first_page>
						<last_page>4972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-775</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Boyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Pu</surname>
</person_name>
					</contributors>
					<titles><title>Transferable speech-to-text large language model alignment module</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3005</first_page>
						<last_page>3009</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-777</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christina</given_name>
<surname>Tånnander</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sébastien</given_name>
<surname>Le Maguer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name>
					</contributors>
					<titles><title>Assessing the impact of contextual framing on subjective TTS quality</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1205</first_page>
						<last_page>1209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-781</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/edlund24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kexin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Ishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryoko</given_name>
<surname>Hayashi</surname>
</person_name>
					</contributors>
					<titles><title>A multimodal analysis of different types of laughter expression in conversational dialogues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4673</first_page>
						<last_page>4677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-782</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shunsuke</given_name>
<surname>Kando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Miyao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Naradowsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name>
					</contributors>
					<titles><title>Textless Dependency Parsing by Labeled Sequence Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1340</first_page>
						<last_page>1344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-783</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kando24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zugang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinghong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianbing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Streamlining Speech Enhancement DNNs: an Automated Pruning Method Based on Dependency Graph with Advanced Regularized Loss Strategies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>662</first_page>
						<last_page>666</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-785</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingru</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Ao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liqun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>597</first_page>
						<last_page>601</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-787</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingjie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hezhao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhisheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiquan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1580</first_page>
						<last_page>1584</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-788</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ma24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baochen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linfeng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Text-aware Speech Separation for Multi-talker Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>337</first_page>
						<last_page>341</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-789</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaesong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soyoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanbyul</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Audio Segmentation for Long-form Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>847</first_page>
						<last_page>851</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-790</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Lebourdais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Théo</given_name>
<surname>Mariotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Almudévar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Tahon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name>
					</contributors>
					<titles><title>Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4753</first_page>
						<last_page>4757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-791</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lebourdais24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youhai</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongmin</given_name>
<surname>Qi</surname>
</person_name>
					</contributors>
					<titles><title>PLDNet: PLD-Guided Lightweight Deep Network Boosted by Efﬁcient Attention for Handheld Dual-Microphone Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3370</first_page>
						<last_page>3374</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-801</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhou24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martina</given_name>
<surname>Valente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabio</given_name>
<surname>Brugnara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Morrone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enrico</given_name>
<surname>Zovato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Badino</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1645</first_page>
						<last_page>1649</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-802</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/valente24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Zero-shot Text-to-Speech with Mixture of Adapters</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>692</first_page>
						<last_page>696</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-803</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fujita24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junzhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Improved Factorized Neural Transducer Model For Text-only Domain Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>767</first_page>
						<last_page>771</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-812</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Hoscilowicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Wiacek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Chojnacki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Cieslak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leszek</given_name>
<surname>Michon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artur</given_name>
<surname>Janicki</surname>
</person_name>
					</contributors>
					<titles><title>Non-Linear Inference Time Intervention: Improving LLM Truthfulness</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4094</first_page>
						<last_page>4098</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-819</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hoscilowicz24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Simko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Turk</surname>
</person_name>
					</contributors>
					<titles><title>A data-driven model of acoustic speech intelligibility for optimization-based models of speech production</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3610</first_page>
						<last_page>3614</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-822</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/elie24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinghong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zugang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianbing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Niu</surname>
</person_name>
					</contributors>
					<titles><title>TD-PLC: A Semantic-Aware Speech Encoding for Improved Packet Loss Concealment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1745</first_page>
						<last_page>1749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-823</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Almudévar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Théo</given_name>
<surname>Mariotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Tahon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis</given_name>
<surname>Vicente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Predefined Prototypes for Intra-Class Separation and Disentanglement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3809</first_page>
						<last_page>3813</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-825</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/almudevar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tina</given_name>
<surname>Raissi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Lüscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Effect of Label Topology and Training Criterion on ASR Performance and Alignment Quality</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3899</first_page>
						<last_page>3903</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-830</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/raissi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kira</given_name>
<surname>Tulchynska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvanus</given_name>
<surname>Job</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alena</given_name>
<surname>Witzlack-Makarevich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic marking of syntactic boundaries in Khoekhoe</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3684</first_page>
						<last_page>3688</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-833</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tulchynska24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Premanand</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamini</given_name>
<surname>Sabu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M. Ali Basha</given_name>
<surname>Shaik</surname>
</person_name>
					</contributors>
					<titles><title>Multi-mic Echo Cancellation Coalesced with Beamforming for Real World Adverse Acoustic Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>147</first_page>
						<last_page>151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-834</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nayak24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martina</given_name>
<surname>Di Bratto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Di Maro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Origlia</surname>
</person_name>
					</contributors>
					<titles><title>On the Use of Plausible Arguments in Explainable Conversational AI</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4054</first_page>
						<last_page>4058</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-839</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dibratto24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaoyao</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proctor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luping</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rijul</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharinda</given_name>
<surname>Piyadasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia</given_name>
<surname>Gully</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Towards Speech Classification from Acoustic and Vocal Tract data in Real-time MRI</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1345</first_page>
						<last_page>1349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-840</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yue24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thanapat</given_name>
<surname>Trachu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chawan</given_name>
<surname>Piansaddhayanon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Thunder : Unified Regression-Diffusion Speech Enhancement with a Single Reverse Step using Brownian Bridge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1180</first_page>
						<last_page>1184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-841</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/trachu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Paturi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>AG-LSEC: Audio Grounded Lexical Speaker Error Correction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1650</first_page>
						<last_page>1654</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-845</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/paturi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng-Chieh</given_name>
<surname>Chiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Hua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jih-Kang</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Learnable Layer Selection and Model Fusion for Speech Self-Supervised Learning Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3914</first_page>
						<last_page>3918</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-849</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chiu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yicong</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Tian</surname>
</person_name>
					</contributors>
					<titles><title>Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2025</first_page>
						<last_page>2029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-852</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jiang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Premanand</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M. Ali Basha</given_name>
<surname>Shaik</surname>
</person_name>
					</contributors>
					<titles><title>Elucidating Clock-drift Using Real-world Audios In Wireless Mode For Time-offset Insensitive End-to-End Asynchronous Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>637</first_page>
						<last_page>641</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-854</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nayak24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaochen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjian</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaguan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoquan</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>DualPure: An Efficient Adversarial Purification Method for Speech Command Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1280</first_page>
						<last_page>1284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-855</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Flynn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name>
					</contributors>
					<titles><title>Self-Train Before You Transcribe</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2840</first_page>
						<last_page>2844</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-858</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/flynn24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bulat</given_name>
<surname>Khaertdinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Jeruis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Annanda</given_name>
<surname>Sousa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enrique</given_name>
<surname>Hortal</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4708</first_page>
						<last_page>4712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-860</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/khaertdinov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengxu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanli</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sujie</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Biasing with Confidence-based Homophone Detector for Mandarin End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>747</first_page>
						<last_page>751</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-869</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Flynn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name>
					</contributors>
					<titles><title>How Much Context Does My Attention-Based ASR System Need?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>217</first_page>
						<last_page>221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-870</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/flynn24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Fagniart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brigitte</given_name>
<surname>Charlier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Véronique</given_name>
<surname>Delvaux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernard</given_name>
<surname>Harmegnies</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Huberlant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myriam</given_name>
<surname>Piccaluga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathy</given_name>
<surname>Huet</surname>
</person_name>
					</contributors>
					<titles><title>Production of fricative consonants in French-speaking children with cochlear implants and typical hearing: acoustic and phonological analyses.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>877</first_page>
						<last_page>881</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-871</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fagniart24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxiao</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianxing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Voice Wake-Up for Dysarthria: Mandarin Dysarthria Speech Corpus Release and Customized System Design</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2465</first_page>
						<last_page>2469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-879</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gao24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeremy</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Applying Reinforcement Learning and Multi-Generators for Stage Transition in an Emotional Support Dialogue System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3545</first_page>
						<last_page>3549</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-882</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bingsong</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengping</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>SPA-SVC: Self-supervised Pitch Augmentation for Singing Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4353</first_page>
						<last_page>4357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-888</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bai24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Song</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongbin</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuezhi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglu</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1245</first_page>
						<last_page>1249</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-890</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheshu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianheng</given_name>
<surname>Zhuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shixiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>LoRA-Whisper: Parameter-Efficient and Extensible Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3934</first_page>
						<last_page>3938</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-892</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/song24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Na</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Schnack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amalia</given_name>
<surname>Arvaniti</surname>
</person_name>
					</contributors>
					<titles><title>Automatic pitch accent classification through image classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2050</first_page>
						<last_page>2054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-895</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-lin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Xuan</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian-tao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao-yu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>DB-PMAE: Dual-Branch Prototypical Masked AutoEncoder with locality for domain robust speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2150</first_page>
						<last_page>2154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-897</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xie24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Translating speech with just images</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>387</first_page>
						<last_page>391</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-903</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/oneata24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Per E</given_name>
<surname>Kummervold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>de la Rosa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Freddy</given_name>
<surname>Wetjen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rolv-Arild</given_name>
<surname>Braaten</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Per Erik</given_name>
<surname>Solberg</surname>
</person_name>
					</contributors>
					<titles><title>Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3984</first_page>
						<last_page>3988</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-907</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kummervold24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Ortiz-Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Garcia-Rodriguez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Tomás</surname>
</person_name>
					</contributors>
					<titles><title>Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>952</first_page>
						<last_page>956</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-914</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ortizperez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Théo</given_name>
<surname>Mariotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvio</given_name>
<surname>Montrésor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Hugh</given_name>
<surname>Thomas</surname>
</person_name>
					</contributors>
					<titles><title>ASoBO: Attentive Beamformer Selection for Distant Speaker Diarization in Meetings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1620</first_page>
						<last_page>1624</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-917</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mariotte24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongfei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lezhi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qisheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaomei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Bin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5098</first_page>
						<last_page>5102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-918</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sumit</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupayan</given_name>
<surname>Chakraborty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>Reinforcement Learning based Data Augmentation for Noise Robust Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1040</first_page>
						<last_page>1044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-921</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ranjan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bilal</given_name>
<surname>Rahou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name>
					</contributors>
					<titles><title>Multi-latency look-ahead for streaming speaker segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1610</first_page>
						<last_page>1614</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-923</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rahou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Kanagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation from Self-Supervised Representation Learning Model with Discrete Speech Units for Any-to-Any Streaming Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4393</first_page>
						<last_page>4397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-924</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kanagawa24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruizhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiushi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Noise-aware Speech Enhancement using Diffusion Probabilistic Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2225</first_page>
						<last_page>2229</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-929</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hu24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanzhong</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaoxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojun</given_name>
<surname>Fei</surname>
</person_name>
					</contributors>
					<titles><title>Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End Multi-Accent Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2395</first_page>
						<last_page>2399</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-930</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianci</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahui</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haifeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Mo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Unified Audio Visual Cues for Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4343</first_page>
						<last_page>4347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-934</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiguang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqin</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>Uncertainty-Aware Mean Opinion Score Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1215</first_page>
						<last_page>1219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-937</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marcely</given_name>
<surname>Zanon Boito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivek</given_name>
<surname>Iyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Lagos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioan</given_name>
<surname>Calapodescu</surname>
</person_name>
					</contributors>
					<titles><title>mHuBERT-147: A Compact Multilingual HuBERT Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3939</first_page>
						<last_page>3943</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-938</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zanonboito24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seung-bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chan-yeong</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-seo</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyo-Won</given_name>
<surname>Koo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>MR-RawNet: Speaker verification system with multiple temporal resolutions for variable duration utterances using raw waveforms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2125</first_page>
						<last_page>2129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-939</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dejan</given_name>
<surname>Porjazovski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anssi</given_name>
<surname>Moisio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Out-of-distribution generalisation in spoken language understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>807</first_page>
						<last_page>811</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-940</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/porjazovski24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan M.</given_name>
<surname>Martín-Doñas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aitor</given_name>
<surname>Álvarez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eros</given_name>
<surname>Rosello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel M.</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio M.</given_name>
<surname>Peinado</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Self-supervised Embeddings and Synthetic Data Augmentation for Robust Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2085</first_page>
						<last_page>2089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-942</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/martindonas24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Kondo</surname>
</person_name>
					</contributors>
					<titles><title>PRVAE-VC2: Non-Parallel Voice Conversion by Distillation of Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4363</first_page>
						<last_page>4367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-947</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tanaka24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markéta</given_name>
<surname>Řezáčková</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Tihelka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindřich</given_name>
<surname>Matoušek</surname>
</person_name>
					</contributors>
					<titles><title>Homograph Disambiguation with Text-to-Text Transfer Transformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2785</first_page>
						<last_page>2789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-949</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rezackova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Séverine</given_name>
<surname>Guillaume</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxime</given_name>
<surname>Fily</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Michaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Wisniewski</surname>
</person_name>
					</contributors>
					<titles><title>Gender and Language Identification in Multilingual Models of Speech: Exploring the Genericity and Robustness of Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3330</first_page>
						<last_page>3334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-953</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guillaume24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beomseok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioan</given_name>
<surname>Calapodescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Gaido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>817</first_page>
						<last_page>821</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-957</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobi</given_name>
<surname>Delbruck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Chii</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Gated Recurrent Neural Network for Compute-efficient Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>677</first_page>
						<last_page>681</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-958</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cheng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Félix</given_name>
<surname>Saget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meysam</given_name>
<surname>Shamsi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Tahon</surname>
</person_name>
					</contributors>
					<titles><title>Lifelong Learning MOS Prediction for Synthetic Speech Quality Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1220</first_page>
						<last_page>1224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-959</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/saget24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Huckvale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaston</given_name>
<surname>Hilkhuysen</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating a 3-factor listener model for prediction of speech intelligibility to hearing-impaired listeners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>872</first_page>
						<last_page>876</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-961</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huckvale24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Séverin</given_name>
<surname>Baroudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name>
					</contributors>
					<titles><title>Specializing Self-Supervised Speech Representations for Speaker Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3769</first_page>
						<last_page>3773</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-962</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baroudi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Biasing Speech Recognition in Speech-enhanced Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>257</first_page>
						<last_page>261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-965</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gong24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yangze</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>A Transcription Prompt-based Efficient Audio Large Language Model for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1905</first_page>
						<last_page>1909</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-968</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyu</given_name>
<surname>Qiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Tessier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aidan</given_name>
<surname>Pine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4963</first_page>
						<last_page>4967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-969</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gong24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuejiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4653</first_page>
						<last_page>4657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-971</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meng24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuto</given_name>
<surname>Igarashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Noise-Robust Voice Conversion by Conditional Denoising Training Using Latent Variables of Recording Quality and Environment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2750</first_page>
						<last_page>2754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-972</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/igarashi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuoliang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning and Inter-Speaker Distribution Alignment Based Unsupervised Domain Adaptation for Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3794</first_page>
						<last_page>3798</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-973</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kubilay Can</given_name>
<surname>Demir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Belén</given_name>
<surname>Lojo Rodríguez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Weise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Towards Intelligent Speech Assistants in Operating Rooms: A Multimodal Model for Surgical Workflow Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1465</first_page>
						<last_page>1469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-975</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/demir24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dong-Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Overfitting in Structured Pruning of ASR Models with Gradient-Guided Parameter Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4493</first_page>
						<last_page>4497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-976</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Schrüfer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Milling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Burkhardt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Eyben</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Are you sure? Analysing Uncertainty Quantification Approaches for Real-world Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3210</first_page>
						<last_page>3214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-977</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/schrufer24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Anderer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Reich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Wölfel</surname>
</person_name>
					</contributors>
					<titles><title>MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech, OCR, and Visual Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1375</first_page>
						<last_page>1379</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-978</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/anderer24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinlong</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yayue</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1800</first_page>
						<last_page>1804</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-980</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xue24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name>
					</contributors>
					<titles><title>Combining Acoustic Feature Sets for Detecting Mild Cognitive Impairment in the Interspeech'24 TAUKADIAL Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>957</first_page>
						<last_page>961</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-984</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gosztolya24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shidong</given_name>
<surname>Shang</surname>
</person_name>
					</contributors>
					<titles><title>Pinyin Regularization in Error Correction for Chinese Speech Recognition with Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1910</first_page>
						<last_page>1914</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-987</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maryam</given_name>
<surname>Naderi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Nanchen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sevada</given_name>
<surname>Hovsepyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Towards interfacing large language models with ASR systems using confidence measures and prompting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2980</first_page>
						<last_page>2984</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-989</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/naderi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Delphine</given_name>
<surname>Charuau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Briglia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erika</given_name>
<surname>Godde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gérard</given_name>
<surname>Bailly</surname>
</person_name>
					</contributors>
					<titles><title>Training speech-breathing coordination in computer-assisted reading</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5128</first_page>
						<last_page>5132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-992</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/charuau24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Murata</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Capability of Mamba in Speech Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>237</first_page>
						<last_page>241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-994</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/miyazaki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mercedes</given_name>
<surname>Vetráb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronika</given_name>
<surname>Svindt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judit</given_name>
<surname>Bóna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ildikó</given_name>
<surname>Hoffmann</surname>
</person_name>
					</contributors>
					<titles><title>Wav2vec 2.0 Embeddings Are No Swiss Army Knife -- A Case Study for Multiple Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2499</first_page>
						<last_page>2503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-995</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gosztolya24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Kalabakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Gonzalez-Machorro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Eyben</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bert</given_name>
<surname>Arnrich</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Analysis of Federated Learning for Speech-Based Cognitive Decline Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2455</first_page>
						<last_page>2459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-996</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kalabakov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Herzig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Friedrich</given_name>
<surname>Wicke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathleen</given_name>
<surname>Wermke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Razieh</given_name>
<surname>Khamsehashari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiko</given_name>
<surname>Dahlem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Towards Classifying Mother Tongue from Infant Cries - Findings Substantiating Prenatal Learning Theory</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4199</first_page>
						<last_page>4203</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1000</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/polzehl24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jérémy</given_name>
<surname>Giroud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirsty</given_name>
<surname>Phillips</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew H.</given_name>
<surname>Davis</surname>
</person_name>
					</contributors>
					<titles><title>Behavioral evidence for higher speech rate convergence following natural than artificial time altered speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2610</first_page>
						<last_page>2614</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1001</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/giroud24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Laptev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Fast Context-Biasing for CTC and Transducer ASR models with CTC-based Word Spotter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>757</first_page>
						<last_page>761</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1002</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/andrusenko24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huihang</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanlu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ZiJin</given_name>
<surname>Yao</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Large Language Models to Refine Automatic Feedback Generation at Articulatory Level in Computer Aided Pronunciation Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2600</first_page>
						<last_page>2604</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1005</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhong24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiuming</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas Fang</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>SAML: Speaker Adaptive Mixture of LoRA Experts for End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>777</first_page>
						<last_page>781</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1006</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Interactive Evaluation of TTS Models in Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2965</first_page>
						<last_page>2969</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1008</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronika</given_name>
<surname>Svindt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judit</given_name>
<surname>Bóna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ildikó</given_name>
<surname>Hoffmann</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Longitudinal Investigation of Multiple Sclerosis Subjects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>942</first_page>
						<last_page>946</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1009</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gosztolya24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Imen</given_name>
<surname>Ben-Amor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Francois</given_name>
<surname>Bonastre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name>
					</contributors>
					<titles><title>Extraction of interpretable and shared speaker-specific speech attributes through binary auto-encoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3230</first_page>
						<last_page>3234</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1011</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/benamor24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anisia</given_name>
<surname>Popescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurence</given_name>
<surname>Devillers</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition with parallel L1 and L2 acoustic phone models to evaluate /l/ allophony in L2 English speech production</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1015</first_page>
						<last_page>1019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1014</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/popescu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zizhen</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>MUSE: Flexible Voiceprint Receptive Fields and Multi-Path Fusion Enhanced Taylor Transformer for U-Net-based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>672</first_page>
						<last_page>676</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1017</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yin-Long</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Hong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Clever Hans Effect Found in Automatic Detection of Alzheimer's Disease through Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2435</first_page>
						<last_page>2439</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1018</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Biswajit</given_name>
<surname>Karan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Jansen van Vüren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Febe</given_name>
<surname>de Wet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>A Transformer-Based Voice Activity Detector</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3819</first_page>
						<last_page>3823</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1019</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/karan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tasnima</given_name>
<surname>Sadekova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikhail</given_name>
<surname>Kudinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vadim</given_name>
<surname>Popov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Assel</given_name>
<surname>Yermekova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Khrapov</surname>
</person_name>
					</contributors>
					<titles><title>PitchFlow: adding pitch control to a Flow-matching based TTS model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4418</first_page>
						<last_page>4422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1023</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sadekova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tonmoy</given_name>
<surname>Rajkhowa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amartya Roy</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sankalp</given_name>
<surname>Nagaonkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Achyut Mani</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>TM-PATHVQA: 90000+ Textless Multilingual Questions for Medical Visual Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4034</first_page>
						<last_page>4038</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1036</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rajkhowa24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erfan A.</given_name>
<surname>Shams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iona</given_name>
<surname>Gessinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick Cormac</given_name>
<surname>English</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name>
					</contributors>
					<titles><title>Are Articulatory Feature Overlaps Shrouded in Speech Embeddings?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4608</first_page>
						<last_page>4612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1039</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shams24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sevada</given_name>
<surname>Hovsepyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Neurocomputational model of speech recognition for pathological speech detection: a case study on Parkinson's disease speech detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3590</first_page>
						<last_page>3594</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1041</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hovsepyan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Misra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark C.</given_name>
<surname>Fuhs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monika</given_name>
<surname>Woszczyna</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Confidence Estimation Measures for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>22</first_page>
						<last_page>26</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1044</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chowdhury24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Learn and Don't Forget: Adding a New Language to ASR Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2544</first_page>
						<last_page>2548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1045</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/qian24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Zaïdi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc-André</given_name>
<surname>Carbonneau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Spoken-Term Discovery using Discrete Speech Units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3620</first_page>
						<last_page>3624</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1051</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vanniekerk24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mukhtar</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oli Danyi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name>
					</contributors>
					<titles><title>Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3625</first_page>
						<last_page>3629</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1054</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mohamed24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinlong</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yayue</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yicheng</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>682</first_page>
						<last_page>686</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1056</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xue24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Plaquet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name>
					</contributors>
					<titles><title>On the calibration of powerset speaker diarization models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3764</first_page>
						<last_page>3768</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1060</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/plaquet24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kishan</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicola</given_name>
<surname>Pia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Korse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Brendel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Fuchs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Multrus</surname>
</person_name>
					</contributors>
					<titles><title>On Improving Error Resilience of Neural End-to-End Speech Coders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1755</first_page>
						<last_page>1759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1061</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gupta24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naotaka</given_name>
<surname>Kawata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazutoshi</given_name>
<surname>Shinoda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiga</given_name>
<surname>Yamane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keita</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name>
					</contributors>
					<titles><title>Unified Multi-Talker ASR with and without Target-speaker Enrollment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>727</first_page>
						<last_page>731</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1062</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/masumura24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Joint Speaker Features Learning for Audio-visual Multichannel Speech Separation and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1925</first_page>
						<last_page>1929</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1063</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>PARIS: Pseudo-AutoRegressIve Siamese Training for Online Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>582</first_page>
						<last_page>586</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1066</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jizhen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Enhancement by Integrating Inter-Channel and Band Features with Dual-branch Conformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1720</first_page>
						<last_page>1724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1069</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vasileios</given_name>
<surname>Moschopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanasis</given_name>
<surname>Kotsiopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Peso Parada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Nikiforidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Stergiadis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Papakostas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Asif</given_name>
<surname>Jalal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jisi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasios</given_name>
<surname>Drosou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthikeyan</given_name>
<surname>Saravanan</surname>
</person_name>
					</contributors>
					<titles><title>Exploring compressibility of transformer based text-to-music (TTM) models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3300</first_page>
						<last_page>3304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1071</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/moschopoulos24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Muller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephane</given_name>
<surname>Ragot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laetitia</given_name>
<surname>Gros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierrick</given_name>
<surname>Philippe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Scalart</surname>
</person_name>
					</contributors>
					<titles><title>Speech quality evaluation of neural audio codecs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1760</first_page>
						<last_page>1764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1072</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/muller24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huang-Cheng</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4633</first_page>
						<last_page>4637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1073</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Judith</given_name>
<surname>Dineley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Carr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren L.</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catriona</given_name>
<surname>Lucas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zahia</given_name>
<surname>Rahman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faith</given_name>
<surname>Matcham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johnny</given_name>
<surname>Downs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard J.</given_name>
<surname>Dobson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F.</given_name>
<surname>Quatieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Variability of speech timing features across repeated recordings: a comparison of open-source extraction techniques</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2015</first_page>
						<last_page>2019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1074</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dineley24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahdi</given_name>
<surname>Amiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ina</given_name>
<surname>Kodrasi</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Robustness Analysis in Automatic Pathological Speech Detection Approaches</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1415</first_page>
						<last_page>1419</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1075</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/amiri24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chun-Yi</given_name>
<surname>Kuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ping</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4144</first_page>
						<last_page>4148</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1076</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kuan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiyuan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niki</given_name>
<surname>Trigoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Markham</surname>
</person_name>
					</contributors>
					<titles><title>Pre-training Feature Guided Diffusion Model for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1185</first_page>
						<last_page>1189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1077</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chung-Wen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berlin</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing Automatic Speech Assessment: W-RankSim Regularization and Hybrid Feature Fusion Strategies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4004</first_page>
						<last_page>4008</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1078</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Livia</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning of Context and Feedback Embeddings in Spoken Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2955</first_page>
						<last_page>2959</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1082</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/qian24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolò</given_name>
<surname>Loddo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisca</given_name>
<surname>Pessanha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Almila</given_name>
<surname>Akdag</surname>
</person_name>
					</contributors>
					<titles><title>What if HAL breathed? Enhancing Empathy in Human-AI Interactions with Breathing Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2625</first_page>
						<last_page>2629</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1083</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/loddo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Khurana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>ZeroST: Zero-Shot Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>392</first_page>
						<last_page>396</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1088</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/khurana24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Goria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roseline</given_name>
<surname>Polle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Fara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Revealing Confounding Biases: A Novel Benchmarking Approach for Aggregate-Level Performance Metrics in Health Assessments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1440</first_page>
						<last_page>1444</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1092</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/goria24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Block Medin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucile</given_name>
<surname>Gelin</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Models for Phoneme Recognition: Applications in Children's Speech for Reading Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5168</first_page>
						<last_page>5172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1095</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/blockmedin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jules</given_name>
<surname>Cauzinille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoît</given_name>
<surname>Favre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dena</given_name>
<surname>Clink</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdul Hamid</given_name>
<surname>Ahmad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Rey</surname>
</person_name>
					</contributors>
					<titles><title>Investigating self-supervised speech models' ability to classify animal vocalizations: The case of gibbon's vocal signatures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>132</first_page>
						<last_page>136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1096</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cauzinille24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justina</given_name>
<surname>Lutz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Strube</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Papazova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alkomiet</given_name>
<surname>Hasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Gender-Specific Speech Patterns in Automatic Suicide Risk Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1095</first_page>
						<last_page>1099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1097</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gerczuk24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Rolland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name>
					</contributors>
					<titles><title>Introduction To Partial Fine-tuning: A Comprehensive Evaluation Of End-to-end Children’s Automatic Speech Recognition Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5178</first_page>
						<last_page>5182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1102</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rolland24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Rolland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name>
					</contributors>
					<titles><title>Shared-Adapters: A Novel Transformer-based Parameter Efficient Transfer Learning Approach For Children’s Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2370</first_page>
						<last_page>2374</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1105</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rolland24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihiro</given_name>
<surname>Takamune</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanami</given_name>
<surname>Imamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>177</first_page>
						<last_page>181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1107</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/seki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina R.</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan L.</given_name>
<surname>Preston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Examining Vocal Tract Coordination in Childhood Apraxia of Speech with Acoustic-to-Articulatory Speech Inversion Feature Sets</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5138</first_page>
						<last_page>5142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1114</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/benway24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Stein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Modeling probabilistic reduction across domains with Naive Discriminative Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4253</first_page>
						<last_page>4257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1118</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/stein24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriele</given_name>
<surname>Ciravegna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Fantini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erika</given_name>
<surname>Crosetti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Succo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tania</given_name>
<surname>Cerquitelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Baralis</surname>
</person_name>
					</contributors>
					<titles><title>Voice Disorder Analysis: a Transformer-based Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3040</first_page>
						<last_page>3044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1122</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/koudounas24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>KiHyun</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joonson</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Disentangled Representation Learning for Environment-agnostic Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2130</first_page>
						<last_page>2134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1124</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nam24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name>
					</contributors>
					<titles><title>VAE-based Phoneme Alignment Using Gradient Annealing and SSL Acoustic Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3814</first_page>
						<last_page>3818</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1127</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/koriyama24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iva</given_name>
<surname>Ewert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Borsdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Does the Lombard Effect Matter in Speech Separation? Introducing the Lombard-GRID-2mix Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>577</first_page>
						<last_page>581</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1131</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ewert24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cliodhna</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guy</given_name>
<surname>Brown</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicola</given_name>
<surname>Dibben</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Effects of Facial Feminisation Surgery on Speech and Singing: A Case Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3065</first_page>
						<last_page>3069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1132</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hughes24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaëlle</given_name>
<surname>Laperrière</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Ghannay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bassam</given_name>
<surname>Jabaian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>A dual task learning approach to fine-tune a multilingual semantic speech encoder for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>812</first_page>
						<last_page>816</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1133</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/laperriere24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Kealey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Grondin</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Improved MVDR Beamforming for Sound Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2175</first_page>
						<last_page>2179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1136</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kealey24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiko</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Neural Blind Source Separation and Diarization for Distant Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>722</first_page>
						<last_page>726</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1137</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bando24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Cheng</surname>
</person_name>
					</contributors>
					<titles><title>Context-Aware Speech Recognition Using Prompts for Language Learners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4009</first_page>
						<last_page>4013</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1142</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cheng24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khalid</given_name>
<surname>Daoudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solange</given_name>
<surname>Milhé de Saint Victor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Foubert-Samier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margherita</given_name>
<surname>Fabbri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Pavy-Le Traon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Rascol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wassilios G.</given_name>
<surname>Meissner</surname>
</person_name>
					</contributors>
					<titles><title>Electroglottography for the assessment of dysphonia in Parkinson's disease and multiple system atrophy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4124</first_page>
						<last_page>4128</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1144</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/daoudi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arunav</given_name>
<surname>Arya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murtiza</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Nathwani</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Wavelet Scattering Transform for an Unsupervised Speaker Diarization in Deep Neural Network Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>47</first_page>
						<last_page>51</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1146</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/arya24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Barnhill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name>
					</contributors>
					<titles><title>ANIMAL-CLEAN – A Deep Denoising Toolkit for Animal-Independent Signal Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>632</first_page>
						<last_page>636</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1151</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/barnhill24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Fernandez-Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honglie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2820</first_page>
						<last_page>2824</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1153</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fernandezlopez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Tanner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgan</given_name>
<surname>Sonderegger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jane</given_name>
<surname>Stuart-Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Kendall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeff</given_name>
<surname>Mielke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Dodsworth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Thomas</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the anatomy of articulation rate in spontaneous English speech: relationships between utterance length effects and social factors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>467</first_page>
						<last_page>471</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1154</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tanner24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lila</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cédric</given_name>
<surname>Gendrot</surname>
</person_name>
					</contributors>
					<titles><title>Using wav2vec 2.0 for phonetic classification tasks: methodological aspects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1530</first_page>
						<last_page>1534</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1155</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harry</given_name>
<surname>Coppock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>Neural Compression Augmentation for Contrastive Audio Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3335</first_page>
						<last_page>3339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1156</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankita</given_name>
<surname>Pasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiko</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoru</given_name>
<surname>Fukayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Speech Representations are More Phonetic than Semantic</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4578</first_page>
						<last_page>4582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1157</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/choi24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wiebke</given_name>
<surname>Hutiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron Yi</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>As Biased as You Measure: Methodological Pitfalls of Bias Evaluations in Speaker Verification Research</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4268</first_page>
						<last_page>4272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1158</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hutiri24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Charles</given_name>
<surname>McGhee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Highly Intelligible Speaker-Independent Articulatory Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3375</first_page>
						<last_page>3379</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1160</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mcghee24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xizi</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>McGregor</surname>
</person_name>
					</contributors>
					<titles><title>Prompt Tuning for Speech Recognition on Unknown Spoken Name Entities</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>762</first_page>
						<last_page>766</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1162</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wei24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rémi</given_name>
<surname>Uro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Tahon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Doukhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name>
					</contributors>
					<titles><title>Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3560</first_page>
						<last_page>3564</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1163</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/uro24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liwei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huihui</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongya</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhonghua</given_name>
<surname>Fu</surname>
</person_name>
					</contributors>
					<titles><title>HarmoNet: Partial DeepFake Detection Network based on Multi-scale HarmoF0 Feature Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2255</first_page>
						<last_page>2259</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1164</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nhan</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>von Zansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Kautonen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Voskoboinik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamas</given_name>
<surname>Grosz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raili</given_name>
<surname>Hilden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Automated content assessment and feedback for Finnish L2 learners in a picture description speaking task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>317</first_page>
						<last_page>321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1166</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/phan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mathilde</given_name>
<surname>Hutin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liesbeth</given_name>
<surname>Degand</surname>
</person_name>
					</contributors>
					<titles><title>Uh, um and mh: Are filled pauses prone to conversational converge?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3575</first_page>
						<last_page>3579</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1168</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hutin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iona</given_name>
<surname>Gessinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bistra</given_name>
<surname>Andreeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin R.</given_name>
<surname>Cowan</surname>
</person_name>
					</contributors>
					<titles><title>The Use of Modifiers and f0 in Remote Referential Communication with Human and Computer Partners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1575</first_page>
						<last_page>1579</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1169</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gessinger24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Borsdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>wTIMIT2mix: A Cocktail Party Mixtures Database to Study Target Speaker Extraction for Normal and Whispered Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5038</first_page>
						<last_page>5042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1172</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/borsdorf24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis</given_name>
<surname>Bahrman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Fontaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaël</given_name>
<surname>Richard</surname>
</person_name>
					</contributors>
					<titles><title>Speech dereverberation constrained on room impulse response characteristics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>622</first_page>
						<last_page>626</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1173</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bahrman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivek</given_name>
<surname>Govindan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Paturi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>Speakers Unembedded: Embedding-free Approach to Long-form Neural Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>27</first_page>
						<last_page>31</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1174</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neelesh</given_name>
<surname>Samptur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanuka</given_name>
<surname>Bhattacharjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Chakravarty K</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seena</given_name>
<surname>Vengalil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Belur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atchayaram</given_name>
<surname>Nalini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Syllable Discriminability during Diadochokinetic Task with Increasing Dysarthria Severity for Patients with Amyotrophic Lateral Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4114</first_page>
						<last_page>4118</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1175</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/samptur24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Doukhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rémi</given_name>
<surname>Uro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ondel-Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Devauchelle</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Configurations across Genders and Periods in French Radio and TV archives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3085</first_page>
						<last_page>3089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1177</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/elie24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingyun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name>
					</contributors>
					<titles><title>Reading Miscue Detection in Primary School through Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5153</first_page>
						<last_page>5157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1180</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gao24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michaela</given_name>
<surname>Watkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Boersma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting Pitch Jumps: F0 Ratio in Seoul Korean</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3135</first_page>
						<last_page>3139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1184</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/watkins24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuanjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roger</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Singing Voice Graph Modeling for SingFake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4843</first_page>
						<last_page>4847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1185</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin-Yun</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>György</given_name>
<surname>Fazekas</surname>
</person_name>
					</contributors>
					<titles><title>Differentiable Time-Varying Linear Prediction in the Context of End-to-End Analysis-by-Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1820</first_page>
						<last_page>1824</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1187</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuanjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawei</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jyh-Shing Roger</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Neural Codec-based Adversarial Sample Detection for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>522</first_page>
						<last_page>526</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1191</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Trung</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Aponte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhito</given_name>
<surname>Koishida</surname>
</person_name>
					</contributors>
					<titles><title>LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3395</first_page>
						<last_page>3399</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1192</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adaeze</given_name>
<surname>Adigwe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarenne</given_name>
<surname>Wallbridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>What do people hear? Listeners’ Perception of Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1210</first_page>
						<last_page>1214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1193</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/adigwe24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Shakeel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>352</first_page>
						<last_page>356</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1194</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/peng24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazutoshi</given_name>
<surname>Shinoda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keita</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Kobashikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Learning from Multiple Annotator Biased Labels in Multimodal Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4089</first_page>
						<last_page>4093</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1197</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shinoda24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shabnam</given_name>
<surname>Ghaffarzadegan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Bondi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Chang</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abinaya</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ho-Hsiang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hans-Georg</given_name>
<surname>Horst</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samarjit</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Sound of Traffic: A Dataset for Acoustic Traffic Identification and Counting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>117</first_page>
						<last_page>121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1205</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ghaffarzadegan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Weise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kubilay Can</given_name>
<surname>Demir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bjoern</given_name>
<surname>Heismann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker- and Text-Independent Estimation of Articulatory Movements and Phoneme Alignments from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1545</first_page>
						<last_page>1549</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1208</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/weise24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zakaria</given_name>
<surname>Aldeneh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Skyler</given_name>
<surname>Seto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Likhomanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Shum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Hussen Abdelaziz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barry-John</given_name>
<surname>Theobald</surname>
</person_name>
					</contributors>
					<titles><title>Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4648</first_page>
						<last_page>4652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1212</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/aldeneh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Donna</given_name>
<surname>Erickson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malin</given_name>
<surname>Svensson Lundmark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adelaide</given_name>
<surname>Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leticia</given_name>
<surname>Rebollo Couto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João Antonio de</given_name>
<surname>Moraes</surname>
</person_name>
					</contributors>
					<titles><title>Collecting Mandible Movement in Brazilian Portuguese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3145</first_page>
						<last_page>3149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1216</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/erickson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurav</given_name>
<surname>Pahuja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Ivucic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Himmelmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Graphic and Convolutional Neural Networks for Auditory Attention Detection with EEG</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>442</first_page>
						<last_page>446</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1217</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pahuja24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seong-Gyun</given_name>
<surname>Leem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fulford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jukka-Pekka</given_name>
<surname>Onnela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Gard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Keep, Delete, or Substitute: Frame Selection Strategy for Noise-Robust Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3734</first_page>
						<last_page>3738</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1218</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/leem24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Flavio</given_name>
<surname>Giobergia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliana</given_name>
<surname>Pastor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Baralis</surname>
</person_name>
					</contributors>
					<titles><title>A Contrastive Learning Approach to Mitigate Bias in Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>827</first_page>
						<last_page>831</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1219</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/koudounas24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debasmita</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Run</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Switching Tongues, Sharing Hearts: Identifying the Relationship between Empathy and Code-switching in Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>492</first_page>
						<last_page>496</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1224</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bhattacharya24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Goncalves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donita</given_name>
<surname>Robinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Richerson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Bridging Emotions Across Languages: Low Rank Adaptation for Multilingual Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4688</first_page>
						<last_page>4692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1226</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/goncalves24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abinay Reddy</given_name>
<surname>Naini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Goncalves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mary A.</given_name>
<surname>Kohler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donita</given_name>
<surname>Robinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Richerson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>WHiSER: White House Tapes Speech Emotion Recognition Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1595</first_page>
						<last_page>1599</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1227</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/naini24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janne</given_name>
<surname>Pylkkönen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannes</given_name>
<surname>Heikinheimo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgan</given_name>
<surname>McGuire</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Multilingual Voice Toxicity Detection with Speech-Text Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4298</first_page>
						<last_page>4302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1228</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dushyant</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandramouli</given_name>
<surname>Shama Sastry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanislav</given_name>
<surname>Kruchinin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Fosburgh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick A.</given_name>
<surname>Naylor</surname>
</person_name>
					</contributors>
					<titles><title>XANE: eXplainable Acoustic Neural Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3824</first_page>
						<last_page>3828</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1229</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dumpala24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Spyretta</given_name>
<surname>Leivaditi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsunari</given_name>
<surname>Matsushima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shekhar</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vass</given_name>
<surname>Verkhodanova</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Tuning Strategies for Dutch Dysarthric Speech Recognition: Evaluating the Impact of Healthy, Disease-Specific, and Speaker-Specific Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1295</first_page>
						<last_page>1299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1231</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/leivaditi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Pärnamaa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ando</given_name>
<surname>Saabas</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Speech Enhancement Without a Separate Speaker Embedding Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4863</first_page>
						<last_page>4867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1234</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/parnamaa24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deepanshu</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Latorre</surname>
</person_name>
					</contributors>
					<titles><title>Positional Description for Numerical Normalization </title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2810</first_page>
						<last_page>2814</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1237</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gupta24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuele</given_name>
<surname>Cornell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoheng</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Pirklbauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Sach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>URGENT Challenge: Universality, Robustness, and Generalizability For Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4868</first_page>
						<last_page>4872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1239</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced Reverberation as Supervision for Unsupervised Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>607</first_page>
						<last_page>611</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1241</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/saijo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>ChengHung</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Embedding Learning for Preference-based Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2685</first_page>
						<last_page>2689</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1243</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hu24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pravin</given_name>
<surname>Mote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Domain Adaptation for Speech Emotion Recognition using K-Nearest Neighbors Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1045</first_page>
						<last_page>1049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1248</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mote24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Mendonça</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Pompili</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Macro-descriptors for Alzheimer's disease detection using large language models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1975</first_page>
						<last_page>1979</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1255</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/botelho24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ali N.</given_name>
<surname>Salman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongyang</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreeram Suresh</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>İsmail Rasim</given_name>
<surname>Ülgen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name>
					</contributors>
					<titles><title>Towards Naturalistic Voice Conversion: NaturalVoices Dataset with an Automatic Processing Pipeline</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4358</first_page>
						<last_page>4362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1256</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/salman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Shakeel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Contextualized End-to-end Automatic Speech Recognition with Intermediate Biasing Loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3909</first_page>
						<last_page>3913</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1257</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shakeel24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ailin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pepijn</given_name>
<surname>Vunderink</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Vargas Quiros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chirag</given_name>
<surname>Raman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayley</given_name>
<surname>Hung</surname>
</person_name>
					</contributors>
					<titles><title>How Private is Low-Frequency Speech Audio in the Wild? An Analysis of Verbal Intelligibility by Humans and Machines</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2725</first_page>
						<last_page>2729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1258</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Romain</given_name>
<surname>Serizel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Extension of Pre-trained Models for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2135</first_page>
						<last_page>2139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1260</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mosner24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William</given_name>
<surname>Ravenscroft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Close</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Goetze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Soleymanpour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark C.</given_name>
<surname>Fuhs</surname>
</person_name>
					</contributors>
					<titles><title>Transcription-Free Fine-Tuning of Speech Separation Models for Noisy and Reverberant Multi-Speaker Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4998</first_page>
						<last_page>5002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1264</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ravenscroft24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsun-An</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heeyoul</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minje</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Representation Loss Between Timed Text and Audio for Regularized Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>592</first_page>
						<last_page>596</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1265</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hsieh24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1740</first_page>
						<last_page>1744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1266</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianyuan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>PFCA-Net: Pyramid Feature Fusion and Cross Content Attention Network for Automated Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1130</first_page>
						<last_page>1134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1268</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sun24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woo-Jin</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Independent Acoustic-to-Articulatory Inversion through Multi-Channel Attention Discriminator</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1540</first_page>
						<last_page>1544</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1269</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chung24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maria</given_name>
<surname>Teleki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangjue</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soohwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Caverlee</surname>
</person_name>
					</contributors>
					<titles><title>Comparing ASR Systems in the Context of Speech Disfluencies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4548</first_page>
						<last_page>4552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1270</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/teleki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>García</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Mahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolás</given_name>
<surname>Grágeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Luzanto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Bohmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Néstor</given_name>
<surname>Becerra Yoma</surname>
</person_name>
					</contributors>
					<titles><title>Speech emotion recognition with deep learning beamforming  on a distant human-robot interaction scenario</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3215</first_page>
						<last_page>3219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1273</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/garcia24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarthak</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name>
					</contributors>
					<titles><title>Audio Mamba: Selective State Spaces for Self-Supervised Audio Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>552</first_page>
						<last_page>556</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1274</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yadav24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brady</given_name>
<surname>Houston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Omid</given_name>
<surname>Sadjadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejiang</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Vishnubhotla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Improving Multilingual ASR Robustness to Errors in Language Input</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1250</first_page>
						<last_page>1254</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1278</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/houston24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dernoncourt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghyun</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanieh</given_name>
<surname>Deilamsalehy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Rossi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan Hung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung</given_name>
<surname>Bui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thien Huu</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3799</first_page>
						<last_page>3803</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1280</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aneesha</given_name>
<surname>Sampath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minxue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4119</first_page>
						<last_page>4123</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1281</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/perez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Klein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianxiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Casal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elie</given_name>
<surname>Khoury</surname>
</person_name>
					</contributors>
					<titles><title>Source Tracing of Audio Deepfake Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1100</first_page>
						<last_page>1104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1283</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/klein24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Cord-Landwehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Once more Diarization: Improving meeting transcription systems through segment-level speaker reassignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1615</first_page>
						<last_page>1619</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1286</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/boeddeker24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Pîrlogeanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Octavian</given_name>
<surname>Pascu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandru-Lucian</given_name>
<surname>Georgescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid-Diarization System with Overlap Post-Processing for the DISPLACE 2024 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1625</first_page>
						<last_page>1629</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1287</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pirlogeanu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chris</given_name>
<surname>Bras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Using articulated speech EEG signals for imagined speech decoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>407</first_page>
						<last_page>411</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1289</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bras24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emmy</given_name>
<surname>Phung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harsh</given_name>
<surname>Deshpande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Emami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanishk</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>AR-NLU: A Framework for Enhancing Natural Language Understanding Model Robustness against ASR Errors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1325</first_page>
						<last_page>1329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1292</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/phung24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alireza</given_name>
<surname>Bayestehtashk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amit</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mike</given_name>
<surname>Wurtz</surname>
</person_name>
					</contributors>
					<titles><title>Design of Feedback Active Noise Cancellation Filter Using Nested Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3270</first_page>
						<last_page>3274</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1295</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bayestehtashk24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paige</given_name>
<surname>Tuttösí</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>H. Henny</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fenqi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Denis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Julien</given_name>
<surname>Aucouturier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelica</given_name>
<surname>Lim</surname>
</person_name>
					</contributors>
					<titles><title>Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1010</first_page>
						<last_page>1014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1296</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tuttosi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alan</given_name>
<surname>Baade</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Neural Codec Language Models for Disentangled and Textless Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>182</first_page>
						<last_page>186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1298</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baade24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Friedrichs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Lancheros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Kirkham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Clark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clemens</given_name>
<surname>Lutz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steven</given_name>
<surname>Moran</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Co-Registration of Simultaneous Electromagnetic Articulography and Electroencephalography for Precise Articulatory and Neural Data Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3120</first_page>
						<last_page>3124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1299</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/friedrichs24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Octavian</given_name>
<surname>Pascu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Stan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeta</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title>Towards generalisable and calibrated audio deepfake detection with self-supervised representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4828</first_page>
						<last_page>4832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1302</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pascu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwangyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Te</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name>
					</contributors>
					<titles><title>DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4154</first_page>
						<last_page>4158</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1306</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shon24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wazeer</given_name>
<surname>Zulfikar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nishat</given_name>
<surname>Protyasha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Camila</given_name>
<surname>Canales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heli</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Sarnie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Nowinski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nataliya</given_name>
<surname>Kosmyna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paige</given_name>
<surname>Townsend</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophia</given_name>
<surname>Yuditskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanya</given_name>
<surname>Talkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Utkarsh Oggy</given_name>
<surname>Sarawgi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>McDougle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Quatieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pattie</given_name>
<surname>Maes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Mody</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Speech Motor Movement using Surface Electromyography in Minimally Verbal Adults with Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5103</first_page>
						<last_page>5107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1309</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zulfikar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Janiczek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dading</given_name>
<surname>Chong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongyang</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arlo</given_name>
<surname>Faria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuzong</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-modal Adversarial Training for Zero-Shot Voice Cloning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3405</first_page>
						<last_page>3409</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1313</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/janiczek24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>ParaCLAP – Towards a general language-audio model for computational paralinguistic tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1155</first_page>
						<last_page>1159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1315</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jing24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emma C.L.</given_name>
<surname>Leschly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Digital Biomarkers for Longitudinal Tracking of Speech Impairment Severity in ALS: An Investigation of Clinically Important Differences</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2460</first_page>
						<last_page>2464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1318</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/neumann24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sefik Emre</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manthan</given_name>
<surname>Thakker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Canrun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zirun</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name>
					</contributors>
					<titles><title>Total-Duration-Aware Duration Modeling for Text-to-Speech Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2290</first_page>
						<last_page>2294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1327</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/eskimez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keita</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazutoshi</given_name>
<surname>Shinoda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Participant-Pair-Wise Bottleneck Transformer for Engagement Estimation from Video Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4079</first_page>
						<last_page>4083</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1329</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/suzuki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Fernandez-Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boqian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mykola</given_name>
<surname>Pechenizkiy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Decebal Constantin</given_name>
<surname>Mocanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwei</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Data Pruning for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4488</first_page>
						<last_page>4492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1330</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xiao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aryan</given_name>
<surname>Chaudhary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arshdeep</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>Efficient CNNs with Quaternion Transformations and Pruning for Audio Tagging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1150</first_page>
						<last_page>1154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1331</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chaudhary24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yatong</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhito</given_name>
<surname>Koishida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somayeh</given_name>
<surname>Sojoudi</surname>
</person_name>
					</contributors>
					<titles><title>ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3285</first_page>
						<last_page>3289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1333</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bai24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarina</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lyonel</given_name>
<surname>Behringer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Zalkow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phat</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuël A. P.</given_name>
<surname>Habets</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Meta Learning Text-to-Speech Synthesis in over 7000 Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4958</first_page>
						<last_page>4962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1335</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lux24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sefik Emre</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manthan</given_name>
<surname>Thakker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zirun</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufei</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinzhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>687</first_page>
						<last_page>691</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1336</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Bott</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Controlling Emotion in Text-to-Speech with Natural Language Prompts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1795</first_page>
						<last_page>1799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1337</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bott24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pin-Jui</given_name>
<surname>Ku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Language-Universal Speech Attributes Modeling for Zero-Shot Multilingual Spoken Keyword Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>342</first_page>
						<last_page>346</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1342</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanis</given_name>
<surname>Labrak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adel</given_name>
<surname>Moumen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Dufour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mickael</given_name>
<surname>Rouvier</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot End-To-End Spoken Question Answering In Medical Domain</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2020</first_page>
						<last_page>2024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1344</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/labrak24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zakaria</given_name>
<surname>Aldeneh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Gichamba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barry-John</given_name>
<surname>Theobald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Hussen Abdelaziz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4278</first_page>
						<last_page>4282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1345</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jung24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunit</given_name>
<surname>Sivasankaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4164</first_page>
						<last_page>4168</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1346</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diamantino</given_name>
<surname>Caseiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsendsuren</given_name>
<surname>Munkhdalai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pat</given_name>
<surname>Rondon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Golan</given_name>
<surname>Pundak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ding</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Moreno Mengibar</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>282</first_page>
						<last_page>286</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1349</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Dimitriadis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth S.</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Can Synthetic Audio From Generative Foundation Models Assist Audio Recognition and Speech Modeling?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>542</first_page>
						<last_page>546</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1350</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/feng24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Grant</given_name>
<surname>Anderson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emma</given_name>
<surname>Hart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitra</given_name>
<surname>Gkatzia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Beaver</surname>
</person_name>
					</contributors>
					<titles><title>Automated Human-Readable Label Generation in Open Intent Discovery</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3540</first_page>
						<last_page>3544</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1351</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/anderson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Barrera-Altuna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daeun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zaima</given_name>
<surname>Zarnaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyoung</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungbae</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>The Interspeech 2024 TAUKADIAL Challenge: Multilingual Mild Cognitive Impairment Detection with Multimodal Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>967</first_page>
						<last_page>971</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1352</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/barreraaltuna24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruchao</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natarajan</given_name>
<surname>Balaji Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5173</first_page>
						<last_page>5177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1353</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>To what extent can ASV systems naturally defend against spoofing attacks?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3240</first_page>
						<last_page>3244</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1354</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jung24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengwei</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cairong</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>MSA-DPCRN: A Multi-Scale Asymmetric Dual-Path Convolution Recurrent Network with Attentional Feature Fusion for Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>162</first_page>
						<last_page>166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1355</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ni24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Koizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Riesa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haruko</given_name>
<surname>Ishikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Bacchiani</surname>
</person_name>
					</contributors>
					<titles><title>FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1835</first_page>
						<last_page>1839</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1356</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ma24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carly</given_name>
<surname>Demopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linnea</given_name>
<surname>Lampinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Preciado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Preliminary Investigation of Psychometric Properties of a Novel Multimodal Dialog Based Affect Production Task in Children and Adolescents with Autism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5123</first_page>
						<last_page>5127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1359</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/demopoulos24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiyao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aobo</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Dysarthric Speech Recognition for Unseen Speakers via Prototype-Based Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1305</first_page>
						<last_page>1309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1360</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shruti</given_name>
<surname>Palaskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ognjen</given_name>
<surname>Rudovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Dharur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Pesce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Krishna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aswin</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jack</given_name>
<surname>Berkowitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Hussen Abdelaziz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Adya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Tewfik</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4778</first_page>
						<last_page>4782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1361</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/palaskar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Landini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Spoof Diarization: &quot;What Spoofed When&quot; in Partially Spoofed Audio</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>502</first_page>
						<last_page>506</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1365</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehua</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuyi</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingjiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Dynamic Sparse Transformer for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>667</first_page>
						<last_page>671</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1368</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tomita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingxiang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noriko</given_name>
<surname>Nakanishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and Visualization of Directional Diversity in Listening Fluency of World Englishes Speakers in the Framework of Mutual Shadowing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4024</first_page>
						<last_page>4028</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1373</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tomita24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>An Effective Local Prototypical Mapping Network for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1055</first_page>
						<last_page>1059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1374</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Target Speaker Extraction with Curriculum Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4348</first_page>
						<last_page>4352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1375</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stijn</given_name>
<surname>Kindt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nilesh</given_name>
<surname>Madhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced Deep Speech Separation in Clustered Ad Hoc Distributed Microphone Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2185</first_page>
						<last_page>2189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1378</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Behnam</given_name>
<surname>Gholami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>El-Khamy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>KeeBong</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for Tiny Speech Enhancement with Latent Feature Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>652</first_page>
						<last_page>656</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1383</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gholami24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cathy</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordi W J</given_name>
<surname>van Unnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianne C M</given_name>
<surname>Botman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonard H</given_name>
<surname>van den Berg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruben P A</given_name>
<surname>van Eijk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>How Consistent are Speech-Based Biomarkers in Remote Tracking of ALS Disease Progression Across Languages? A Case Study of English and Dutch</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2005</first_page>
						<last_page>2009</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1390</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kothare24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dingdong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4398</first_page>
						<last_page>4402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1392</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liu</given_name>
<surname>Xiaowang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Study on the Information Mechanism of the 3rd Tone Sandhi Rule in Mandarin Disyllabic Words</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1560</first_page>
						<last_page>1564</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1393</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xiaowang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Byeongjoo</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karren</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Hamilton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Sheaffer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miguel</given_name>
<surname>Sarabia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oncel</given_name>
<surname>Tuzel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Hao Rick</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Novel-view Acoustic Synthesis From 3D Reconstructed Rooms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3260</first_page>
						<last_page>3264</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1396</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ahn24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bence Mark</given_name>
<surname>Halpern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Tienkamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lester Phillip</given_name>
<surname>Violeta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teja</given_name>
<surname>Rebernik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastiaan</given_name>
<surname>de Visscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Max</given_name>
<surname>Witjes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Wieling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Defne</given_name>
<surname>Abur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying the effect of speech pathology on automatic and human speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3015</first_page>
						<last_page>3019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1400</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/halpern24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinghao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiwei</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenqing</given_name>
<surname>Cheng</surname>
</person_name>
					</contributors>
					<titles><title>Active Speaker Detection in Fisheye Meeting Scenes with Scene Spatial Spectrums</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4283</first_page>
						<last_page>4287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1402</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwanath Pratap</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Malato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ville</given_name>
<surname>Hautamäki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>ROAR: Reinforcing Original to Augmented Data Ratio Dynamics for Wav2vec2.0 Based ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2885</first_page>
						<last_page>2889</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1403</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/singh24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxuan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenlu</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyi</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aijun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunyu</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Decoding Human Language Acquisition: EEG Evidence for Predictive Probabilistic Statistics in Word Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2605</first_page>
						<last_page>2609</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1404</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Yeza</given_name>
<surname>Baihaqi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel</given_name>
<surname>Garcia Contreras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seiya</given_name>
<surname>Kawano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Yoshino</surname>
</person_name>
					</contributors>
					<titles><title>Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4059</first_page>
						<last_page>4063</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1406</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baihaqi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sahil</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youshan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Vision Transformer Segmentation for Visual Bird Sound Denoising</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>122</first_page>
						<last_page>126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1412</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kumar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vahid</given_name>
<surname>Khanagha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitris</given_name>
<surname>Koutsaidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaustubh</given_name>
<surname>Kalgaonkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>Interference Aware Training Target for DNN based joint Acoustic Echo Cancellation and Noise Suppression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>152</first_page>
						<last_page>156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1414</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/khanagha24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yashish M.</given_name>
<surname>Siriwardena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Swedlow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Audrey</given_name>
<surname>Howard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evan</given_name>
<surname>Gitterman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Darcy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Fanelli</surname>
</person_name>
					</contributors>
					<titles><title>Accent Conversion with Articulatory Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4383</first_page>
						<last_page>4387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1416</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/siriwardena24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Desh</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paola</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Neural Transducer for Multilingual ASR with Synchronized Language Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3994</first_page>
						<last_page>3998</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1418</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hussein24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haolan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amin</given_name>
<surname>Edraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wai-Yip</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iván</given_name>
<surname>López-Espejo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name>
					</contributors>
					<titles><title>No-Reference Speech Intelligibility Prediction Leveraging a Noisy-Speech ASR Pre-Trained Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3849</first_page>
						<last_page>3853</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1421</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bao</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijiang</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroko</given_name>
<surname>Dodge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayu</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Translingual Language Markers for Cognitive Assessment from Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>977</first_page>
						<last_page>981</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1422</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hoang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hengchao</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongyao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanchang</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daimeng</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Speech Summarization Using Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1950</first_page>
						<last_page>1954</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1428</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuhua</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qirong</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>PL-TTS: A Generalizable Prompt-based Diffusion TTS Augmented by Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4888</first_page>
						<last_page>4892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1429</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fabian</given_name>
<surname>Ritter-Gutierrez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Po</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeremy H. M.</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy F.</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng-Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Dataset-Distillation Generative Model for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2640</first_page>
						<last_page>2644</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1430</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rittergutierrez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Hai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mounya</given_name>
<surname>Elhilali</surname>
</person_name>
					</contributors>
					<titles><title>DreamVoice: Text-Guided Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4373</first_page>
						<last_page>4377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1432</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hai24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuwu</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haitao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced Feature Learning with Normalized Knowledge Distillation for Audio Tagging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1695</first_page>
						<last_page>1699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1433</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>George</given_name>
<surname>Joseph</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Baby</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Personalization for Automatic Speech Recognition using Weight-Decomposed Low-Rank Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2875</first_page>
						<last_page>2879</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1434</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/joseph24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yinlin</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yening</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinqiao</given_name>
<surname>Dou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuehai</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>FLY-TTS: Fast, Lightweight and High-Quality End-to-End Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4908</first_page>
						<last_page>4912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1435</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guo24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanbin</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Andreev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Azat</given_name>
<surname>Saginbaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Babaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>WonJun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosang</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>647</first_page>
						<last_page>651</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1444</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bae24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanya</given_name>
<surname>Talkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sherman</given_name>
<surname>Charles</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chelsea</given_name>
<surname>Krantsevich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kan</given_name>
<surname>Kawabata</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Cognitive Impairment And Alzheimer's Disease Using a Speech- and Language-Based Protocol</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3025</first_page>
						<last_page>3029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1446</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/talkar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyunjae</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonbin</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>JenGAN: Stacked Shifted Filters in GAN-Based Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3879</first_page>
						<last_page>3883</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1447</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cho24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iwen E</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Van Gysel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Hung</given_name>
<surname>Siu</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-based Model for ASR N-Best Rescoring and Rewriting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3505</first_page>
						<last_page>3509</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1449</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaewon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Sound of Vision: Audio Generation from Visual Text Embedding through Training Domain Discriminator</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3305</first_page>
						<last_page>3309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1451</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganjun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dehui</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohui</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Lv</surname>
</person_name>
					</contributors>
					<titles><title>DysArinVox: DYSphonia &amp; DYSarthria mandARIN speech corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>932</first_page>
						<last_page>936</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1452</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noumida</given_name>
<surname>A</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name>
					</contributors>
					<titles><title>Multi-label Bird Species Classification from Field Recordings using Mel_Graph-GCN Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4793</first_page>
						<last_page>4797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1453</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/a24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sai Srujana</given_name>
<surname>Buddi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satyam</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Utkarsh</given_name>
<surname>Sarawgi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shivesh</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ognjen</given_name>
<surname>Rudovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Hussen Abdelaziz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Adya</surname>
</person_name>
					</contributors>
					<titles><title>Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4768</first_page>
						<last_page>4772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1454</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/buddi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ho-Young</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Retrieval-Augmented Classifier Guidance for Audio Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3310</first_page>
						<last_page>3314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1456</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/choi24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Payal</given_name>
<surname>Mohapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shamika</given_name>
<surname>Likhite</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subrata</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bashima</given_name>
<surname>Islam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Missingness-resilient Video-enhanced Multimodal Disfluency Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5093</first_page>
						<last_page>5097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1458</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mohapatra24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qifei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhua</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Modal Fusion by Alignment and Label Matching for Multimodal Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4663</first_page>
						<last_page>4667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1462</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Generating Speakers by Prompting Listener Impressions for Pre-trained Multi-Speaker Text-to-Speech Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4428</first_page>
						<last_page>4432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1465</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junxu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Speaker Verification with Mini-Batch Prediction Correction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4713</first_page>
						<last_page>4717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1466</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>Text Injection for Neural Contextual Biasing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2985</first_page>
						<last_page>2989</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1471</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meng24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik B.</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiongqiong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Attentive Merging of Hidden Embeddings from Pre-trained Speech Model for Anti-spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2090</first_page>
						<last_page>2094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1472</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pan24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Onda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joonyong</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name>
					</contributors>
					<titles><title>A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3600</first_page>
						<last_page>3604</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1473</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/onda24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>H4C-TTS: Leveraging Multi-Modal Historical Context for Conversational Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4933</first_page>
						<last_page>4937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1480</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/seong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daryush D.</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarrad H. Van</given_name>
<surname>Stan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamzeh</given_name>
<surname>Ghasemzadeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert E.</given_name>
<surname>Hillman</surname>
</person_name>
					</contributors>
					<titles><title>Comparing ambulatory voice measures during daily life with brief laboratory assessments in speakers with and without vocal hyperfunction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1455</first_page>
						<last_page>1459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1484</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mehta24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Seki</surname>
</person_name>
					</contributors>
					<titles><title>Improved Remixing Process for Domain Adaptation-Based Speech Enhancement by Mitigating Data Imbalance in Signal-to-Noise Ratio</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1710</first_page>
						<last_page>1714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1488</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fulin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set Speaker-Identification Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4263</first_page>
						<last_page>4267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1490</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>A F M</given_name>
<surname>Saif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisha</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songtao</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>M2ASR: Multilingual Multi-task Automatic Speech Recognition via Multi-objective Optimization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1240</first_page>
						<last_page>1244</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1492</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/saif24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liangyu</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruchit</given_name>
<surname>Agrawal</surname>
</person_name>
					</contributors>
					<titles><title>MMSD-Net: Towards Multi-modal Stuttering Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5113</first_page>
						<last_page>5117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1497</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nie24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyue</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huali</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nengheng</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>DBD-CI: Doubling the Band Density for Bilateral Cochlear Implants</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2595</first_page>
						<last_page>2599</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1505</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junkun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aswin Shanmugam</given_name>
<surname>Subramanian</surname>
</person_name>
					</contributors>
					<titles><title>Soft Language Identification for Language-Agnostic Many-to-One End-to-End Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>377</first_page>
						<last_page>381</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1507</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Saeki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumi</given_name>
<surname>Maiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4943</first_page>
						<last_page>4947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1508</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/saeki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Lv</surname>
</person_name>
					</contributors>
					<titles><title>Prompt Link Multimodal Fusion in Multimodal Sentiment Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4668</first_page>
						<last_page>4672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1512</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min-Han</given_name>
<surname>Shih</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ho-Lam</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chi</given_name>
<surname>Pai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-Hao</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guan-Ting</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>GSQA: An End-to-End Model for Generative Spoken Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2970</first_page>
						<last_page>2974</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1514</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shih24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiyang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehua</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas Fang</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Whisper-PMFA: Partial Multi-Scale Feature Aggregation for Speaker Verification using Whisper Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2680</first_page>
						<last_page>2684</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1515</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Oura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hideaki</given_name>
<surname>Kikuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Preprocessing for acoustic-to-articulatory inversion using real-time MRI movies of Japanese speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1550</first_page>
						<last_page>1554</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1517</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/oura24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuting</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guodong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Ruan</surname>
</person_name>
					</contributors>
					<titles><title>Learning from Back Chunks: Acquiring More Future Knowledge for Streaming ASR Models via Self Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4458</first_page>
						<last_page>4462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1527</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yakun</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanrou</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4433</first_page>
						<last_page>4437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1531</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/song24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaoxun</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Comparing Discrete and Continuous Space LLMs for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2509</first_page>
						<last_page>2513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1533</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chun</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tai-Shih</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1195</first_page>
						<last_page>1199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1540</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yin24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Hosamath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lakshmi</given_name>
<surname>Arbatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doug</given_name>
<surname>Habberstad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christiane</given_name>
<surname>Suendermann-Oeft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meredith</given_name>
<surname>Bartlett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cathy</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Sukhdev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kolja</given_name>
<surname>Wilms</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anusha</given_name>
<surname>Badathala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandrine</given_name>
<surname>Istas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Ruhmel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bryan</given_name>
<surname>Hansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madeline</given_name>
<surname>Hannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Henley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arthur</given_name>
<surname>Wallace</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ira</given_name>
<surname>Shoulson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Suendermann-Oeft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Towards Scalable Remote Assessment of Mild Cognitive Impairment Via Multimodal Dialog</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1985</first_page>
						<last_page>1989</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1541</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/roesler24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nahomi</given_name>
<surname>Kusunoki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Multi-Task Learning with CTC and Recursive Operation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2855</first_page>
						<last_page>2859</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1542</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kusunoki24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eva</given_name>
<surname>Szekely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxwell</given_name>
<surname>Hope</surname>
</person_name>
					</contributors>
					<titles><title>An inclusive approach to creating a palette of synthetic voices for gender diversity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3070</first_page>
						<last_page>3074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1543</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/szekely24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dail</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da-Hee</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeonghwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moa</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaemo</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han-gil</given_name>
<surname>Moon</surname>
</person_name>
					</contributors>
					<titles><title>Guided conditioning with predictive network on score-based diffusion model for speech enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1190</first_page>
						<last_page>1194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1545</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Osamu</given_name>
<surname>Take</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information Toward Environment-adaptive Dialogue Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1860</first_page>
						<last_page>1864</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1554</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/take24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanzhao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liumeng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinfa</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanjun</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunlin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3390</first_page>
						<last_page>3394</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1559</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woon-Haeng</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joongyu</given_name>
<surname>Maeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoseb</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namhyun</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Centroid Estimation with Transformer-Based Speaker Embedder for Robust Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4333</first_page>
						<last_page>4337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1560</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/heo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinyi</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changqing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Depression Enhances Internal Inconsistency between Spoken and Semantic Emotion: Evidence from the Analysis of Emotion Expression in Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4219</first_page>
						<last_page>4223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1562</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitali</given_name>
<surname>Chakrabarti</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech-Based Dysarthria Detection using Multi-task Learning with Gradient Projection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>902</first_page>
						<last_page>906</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1563</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xiong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christina</given_name>
<surname>Tånnander</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shivam</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Beskow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name>
					</contributors>
					<titles><title>Beyond graphemes and phonemes: continuous phonological features in neural text-to-speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2815</first_page>
						<last_page>2819</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1565</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tannander24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Kaveh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghav</given_name>
<surname>Nautiyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anvitha</given_name>
<surname>Kachinthaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tavish</given_name>
<surname>Mishra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rikky</given_name>
<surname>Muller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala Krishna</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Towards EMG-to-Speech with Necklace Form Factor</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>402</first_page>
						<last_page>406</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1568</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>LingWav2Vec2: Linguistic-augmented wav2vec 2.0 for Vietnamese Mispronunciation Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2355</first_page>
						<last_page>2359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1569</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nguyen24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Lancia</surname>
</person_name>
					</contributors>
					<titles><title>A multimodal approach to study the nature of coordinative patterns underlying speech rhythm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>397</first_page>
						<last_page>401</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1571</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vahid</given_name>
<surname>Noroozi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somshubra</given_name>
<surname>Majumdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Instruction Data Generation and Unsupervised Adaptation for Speech Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4049</first_page>
						<last_page>4053</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1575</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/noroozi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaoxiang</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuya</given_name>
<surname>Matsumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Tsuboi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasuhiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Nakatsubo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Maesawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuta</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masahisa</given_name>
<surname>Katsuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Kudo</surname>
</person_name>
					</contributors>
					<titles><title>Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2445</first_page>
						<last_page>2449</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1577</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuyuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengqiang</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peiyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hua</given_name>
<surname>Hua</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Expressive paragraph text-to-speech synthesis with multi-step variational autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1815</first_page>
						<last_page>1819</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1581</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shivam</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harm</given_name>
<surname>Lameris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Punmiya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Beskow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva</given_name>
<surname>Szekely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name>
					</contributors>
					<titles><title>Should you use a probabilistic duration model in TTS? Probably! Especially for spontaneous speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2285</first_page>
						<last_page>2289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1582</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mehta24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haitong</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaehyun</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name>
					</contributors>
					<titles><title>Acceleration of Posteriorgram-based DTW by Distilling the Class-to-class Distances Encoded in the Classifier Used to Calculate Posteriors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2995</first_page>
						<last_page>2999</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1583</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sun24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qirui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>MaskSR: Masked Language Model for Full-band Speech Restoration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2275</first_page>
						<last_page>2279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1584</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesuraj</given_name>
<surname>Bandekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deekshitha</given_name>
<surname>G</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandhya</given_name>
<surname>B</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhayjeet</given_name>
<surname>S</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Savitha</given_name>
<surname>Murthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyanka</given_name>
<surname>Pai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasa</given_name>
<surname>Raghavan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raoul</given_name>
<surname>Nanavati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Adapter pre-training for improved speech recognition in unseen domains using low resource adapter tuning of self-supervised models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2529</first_page>
						<last_page>2533</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1587</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/udupa24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Galvez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Kaldewey</surname>
</person_name>
					</contributors>
					<titles><title>Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on GPU</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>277</first_page>
						<last_page>281</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1591</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/galvez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Weirich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Duran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefanie</given_name>
<surname>Jannedy</surname>
</person_name>
					</contributors>
					<titles><title>Gender and age based f0-variation in the German Plapper Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1565</first_page>
						<last_page>1569</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1592</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/weirich24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>EunGi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oh</given_name>
<surname>Hyun-Bin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kim</given_name>
<surname>Sung-Bin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corentin</given_name>
<surname>Nivelet Etcheberry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suekyeong</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janghoon</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Hyun</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Speech-Driven 3D Facial Animation with Audio-Visual Guidance from Lip Reading Expert</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2940</first_page>
						<last_page>2944</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1595</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/eungi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liangwei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiren</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huanhuan</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Speech and Music Discrimination Through the Integration of Static and Dynamic Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4318</first_page>
						<last_page>4322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1596</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinchen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingting</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Su-Jing</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CDSD: Chinese Dysarthria Speech Database</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4109</first_page>
						<last_page>4113</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1597</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qingye</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Lancia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noel</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>A novel experimental design for the study of listener-to-listener convergence in phoneme categorization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2615</first_page>
						<last_page>2619</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1598</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shen24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchun</given_name>
<surname>Shu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifeng</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Error Correction by Paying Attention to Both Acoustic and Confidence References for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3500</first_page>
						<last_page>3504</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1605</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meiling</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengjie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haofeng</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Towards End-to-End Unified Recognition for Mandarin and Cantonese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2365</first_page>
						<last_page>2369</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1606</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ted</given_name>
<surname>Kye</surname>
</person_name>
					</contributors>
					<titles><title>Affricates in Lushootseed</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3699</first_page>
						<last_page>3703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1607</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kye24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyang</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>ED-sKWS: Early-Decision Spiking Neural Networks for Rapid, and Energy-Efficient Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4528</first_page>
						<last_page>4532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1609</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/song24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zimeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongxuan</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengting</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Yuen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>The Production of Contrastive Focus by  7 to 13-year-olds Learning Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4184</first_page>
						<last_page>4188</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1611</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarina</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Probing the Feasibility of Multilingual Speaker Anonymization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4448</first_page>
						<last_page>4452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1615</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meyer24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuejiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianmin</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Large Language Model-based FMRI Encoding of Language Functions for Subjects with Neurocognitive Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1485</first_page>
						<last_page>1489</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1616</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjun</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Personality-memory Gated Adaptation: An Efficient Speaker Adaptation for Personalized End-to-end Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2870</first_page>
						<last_page>2874</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1621</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Perfler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Lostanlen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Ehler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Balazs</surname>
</person_name>
					</contributors>
					<titles><title>Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5013</first_page>
						<last_page>5017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1622</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/haider24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Umer</given_name>
<surname>Sheikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Abid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuiyan Sanjid</given_name>
<surname>Shafique</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asif</given_name>
<surname>Hanif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad Haris</given_name>
<surname>Khan</surname>
</person_name>
					</contributors>
					<titles><title>Bird Whisperer: Leveraging Large Pre-trained Acoustic Model for Bird Call Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5028</first_page>
						<last_page>5032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1623</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sheikh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Coulange</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsuneo</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solange</given_name>
<surname>Rossato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Masperi</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Impact of Pausing and Lexical Stress Patterns on L2 English Comprehensibility in Real Time</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1030</first_page>
						<last_page>1034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1627</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/coulange24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rhiannon</given_name>
<surname>Mogridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name>
					</contributors>
					<titles><title>Learning from memory-based models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2360</first_page>
						<last_page>2364</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1628</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mogridge24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chan-yeong</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-seo</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyo-Won</given_name>
<surname>Koo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Noise Robustness in Self-supervised Pre-trained Model for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2665</first_page>
						<last_page>2669</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1630</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lim24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>2DP-2MRC: 2-Dimensional Pointer-based Machine Reading Comprehension Method for Multimodal Moment Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5073</first_page>
						<last_page>5077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1633</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/he24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fang</surname>
</person_name>
					</contributors>
					<titles><title>On The Performance of EMA-synchronized Speech and Stand-alone Speech in Acoustic-to-articulatory Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3110</first_page>
						<last_page>3114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1637</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raj</given_name>
<surname>Gothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mildred</given_name>
<surname>Pereira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nagesh</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>A Dataset and Two-pass System for Reading Miscue Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4014</first_page>
						<last_page>4018</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1639</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gothi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianwei</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liming</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhichang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>GPA: Global and Prototype Alignment for Audio-Text Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5078</first_page>
						<last_page>5082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1642</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xie24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaqian</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenguang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>On Calibration of Speech Classification Models: Insights from Energy-Based Model Investigations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3175</first_page>
						<last_page>3179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1643</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taisei</given_name>
<surname>Omine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenta</given_name>
<surname>Akita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reiji</given_name>
<surname>Tsuruno</surname>
</person_name>
					</contributors>
					<titles><title>Robust Laughter Segmentation with Automatic Diverse Data Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4748</first_page>
						<last_page>4752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1644</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/omine24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wing-Zin</given_name>
<surname>Leung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mattias</given_name>
<surname>Cross</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Goetze</surname>
</person_name>
					</contributors>
					<titles><title>Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2494</first_page>
						<last_page>2498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1645</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/leung24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Denise</given_name>
<surname>Moussa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Bergmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Riess</surname>
</person_name>
					</contributors>
					<titles><title>Unmasking Neural Codecs: Forensic Identification of AI-compressed Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2260</first_page>
						<last_page>2264</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1652</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/moussa24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaqian</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenguang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Energy-Based Models for Out-of-Distribution Detection in Dialect Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1640</first_page>
						<last_page>1644</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1657</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglai</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Attention-Guided WaveNet for EEG-to-MEL Spectrogram Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2620</first_page>
						<last_page>2624</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1662</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raul</given_name>
<surname>Monteiro</surname>
</person_name>
					</contributors>
					<titles><title>Adding User Feedback To Enhance CB-Whisper</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>347</first_page>
						<last_page>351</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1664</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/monteiro24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Wesolek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Gulgowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Blaszczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marzena</given_name>
<surname>Zygis</surname>
</person_name>
					</contributors>
					<titles><title>The influence of L2 accent strength and different error types on personality trait ratings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2</first_page>
						<last_page>6</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1669</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wesolek24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashishkumar</given_name>
<surname>Gudmalwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nirmesh</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai</given_name>
<surname>Akarsh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pankaj</given_name>
<surname>Wasnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name>
					</contributors>
					<titles><title>VECL-TTS: Voice identity and Emotional style controllable Cross-Lingual Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3000</first_page>
						<last_page>3004</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1672</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gudmalwar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenifer</given_name>
<surname>Vega Rodriguez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Vallée</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Savariaux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvain</given_name>
<surname>Gerber</surname>
</person_name>
					</contributors>
					<titles><title>Nasal Air Flow During Speech Production In Korebaju</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3694</first_page>
						<last_page>3698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1674</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vegarodriguez24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juhwan</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>WooSeok</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungwoong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soojoong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>UNIQUE : Unsupervised Network for Integrated Speech Quality Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4948</first_page>
						<last_page>4952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1675</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yoon24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuenan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Audio Captioning with Encoder-Level Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1160</first_page>
						<last_page>1164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1680</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuankun</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaopeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongwei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuchen</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>Generalized Fake Audio Detection via Deep Stable Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4773</first_page>
						<last_page>4777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1686</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoxiang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziqi</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction and Recognition in Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4074</first_page>
						<last_page>4078</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1688</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuenan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingyue</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Zero-shot Audio Classification using Sound Attribute Knowledge from Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4808</first_page>
						<last_page>4812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1692</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neha</given_name>
<surname>Sahipjohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashishkumar</given_name>
<surname>Gudmalwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nirmesh</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pankaj</given_name>
<surname>Wasnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name>
					</contributors>
					<titles><title>DubWise: Video-Guided Speech Duration Control in Multimodal LLM-based Text-to-Speech for Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2960</first_page>
						<last_page>2964</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1700</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sahipjohn24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baihan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuenan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>FakeSound: Deepfake General Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>112</first_page>
						<last_page>116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1703</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xie24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomi H.</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosa</given_name>
<surname>Gonzalez Hautamäki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Detection by the Individual Listener and the Crowd: Parametric Models Applicable to Bonafide and Deepfake Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3654</first_page>
						<last_page>3658</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1704</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kinnunen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Serialized Output Training by Learned Dominance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>712</first_page>
						<last_page>716</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1710</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan Honza</given_name>
<surname>Cernocky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murat</given_name>
<surname>Saraçlar</surname>
</person_name>
					</contributors>
					<titles><title>Pretraining End-to-End Keyword Search with Automatically Discovered Acoustic Units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5068</first_page>
						<last_page>5072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1713</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yusuf24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyong</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muyong</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>LUPET: Incorporating Hierarchical Information Path into Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3979</first_page>
						<last_page>3983</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1714</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minglin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Seamless Language Expansion: Enhancing Multilingual Mastery in Self-Supervised Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4973</first_page>
						<last_page>4977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1716</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martha</given_name>
<surname>Schubert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Duran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Siegert</surname>
</person_name>
					</contributors>
					<titles><title>Challenges of German Speech Recognition: A Study on Multi-ethnolectal Speech Among Adolescents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3045</first_page>
						<last_page>3049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1717</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/schubert24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taewoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Choonsang</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young Han</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Period Singer: Integrating Periodic and Aperiodic Variational Autoencoders for Natural-Sounding End-to-End Singing Voice Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1875</first_page>
						<last_page>1879</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1720</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nao</given_name>
<surname>Hodoshima</surname>
</person_name>
					</contributors>
					<titles><title>Effects of talker and playback rate of reverberation-induced speech on speech intelligibility of older adults</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4229</first_page>
						<last_page>4232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1721</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hodoshima24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Dhiman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Gowda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aalekhya Satya</given_name>
<surname>Narayani</surname>
</person_name>
					</contributors>
					<titles><title>FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4733</first_page>
						<last_page>4737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1723</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/behera24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zirui</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinzhou</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haiyan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingting</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>DGPN: A Dual Graph Prototypical Network for Few-Shot Speech Spoofing Algorithm Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1125</first_page>
						<last_page>1129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1724</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ge24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Baihan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuenan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>DiveSound: LLM-Assisted Automatic Taxonomy Construction for Diverse Audio Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4798</first_page>
						<last_page>4802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1726</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuzanna</given_name>
<surname>Miodonska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Kręcichwost</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewa</given_name>
<surname>Kwaśniok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agata</given_name>
<surname>Sage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pawel</given_name>
<surname>Badura</surname>
</person_name>
					</contributors>
					<titles><title>Frication noise features of Polish voiceless dental fricative and affricate produced by children with and without speech disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3125</first_page>
						<last_page>3129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1731</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/miodonska24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huamin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Cross-modal Features Interaction-and-Aggregation Network with Self-consistency Training for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2335</first_page>
						<last_page>2339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1733</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hu24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>TSP-TTS: Text-based Style Predictor with Residual Vector Quantization for Expressive Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1780</first_page>
						<last_page>1784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1734</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/seong24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziping</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haishuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>MFDR: Multiple-stage Fusion and Dynamically Refined Network for Multimodal Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3719</first_page>
						<last_page>3723</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1735</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bubai</given_name>
<surname>Maji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajlakshmi</given_name>
<surname>Guha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurobinda</given_name>
<surname>Routray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shazia</given_name>
<surname>Nasreen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debabrata</given_name>
<surname>Majumdar</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Layer-Wise Speech Representations in Self-Supervised Learning Models: A Cross-Lingual Study in Detecting Depression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3020</first_page>
						<last_page>3024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1737</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/maji24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reihaneh</given_name>
<surname>Amooie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wietse</given_name>
<surname>de Vries</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Tienkamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rik</given_name>
<surname>van Noord</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Wieling</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Self-Supervised Speech Representations for Cross-lingual Acoustic-to-Articulatory Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4603</first_page>
						<last_page>4607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1740</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hao24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyong</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muyong</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Parameter-efficient Language Extension Framework for Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3929</first_page>
						<last_page>3933</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1745</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debasish Ray</given_name>
<surname>Mohapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Zappi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>2.5D Vocal Tract Modeling: Bridging Low-Dimensional Efficiency with 3D Accuracy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>17</first_page>
						<last_page>21</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1749</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mohapatra24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesuraj</given_name>
<surname>Bandekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory synthesis using representations learnt through phonetic label-aware contrastive loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>427</first_page>
						<last_page>431</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1756</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bandekar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yongjie</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Fully Few-shot Class-incremental Audio Classification Using Expandable Dual-embedding Extractor</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4788</first_page>
						<last_page>4792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1758</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/si24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rastislav</given_name>
<surname>Rabatin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Seide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ernie</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Navigating the Minefield of MT Beam Search in Cascaded Streaming Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>372</first_page>
						<last_page>376</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1759</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rabatin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ling</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengtao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengxiang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guojiang</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Speech Self-Supervised Learning Models and Large Language Models for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3954</first_page>
						<last_page>3958</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1760</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dong24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anbai</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufeng</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingyi</given_name>
<surname>Fan</surname>
</person_name>
					</contributors>
					<titles><title>AnoPatch: Towards Better Consistency in Machine Anomalous Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>107</first_page>
						<last_page>111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1761</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jiang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nguyen</given_name>
<surname>Manh Tien Anh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thach</given_name>
<surname>Ho Sy</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Recognition with Prompt-based Contextualized ASR and LLM-based Re-predictor</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>737</first_page>
						<last_page>741</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1762</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/manhtienanh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katelyn</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia</given_name>
<surname>Gully</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Daffern</surname>
</person_name>
					</contributors>
					<titles><title>Familiar and Unfamiliar Speaker Identification in Speech and Singing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>472</first_page>
						<last_page>476</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1763</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/taylor24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijian</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>BS-PLCNet 2: Two-stage Band-split Packet Loss Concealment Network with Intra-model Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1750</first_page>
						<last_page>1754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1764</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuchen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyu</given_name>
<surname>Qiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongwei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaopeng</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>PPPR: Portable Plug-in Prompt Refiner for Text to Audio Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4898</first_page>
						<last_page>4902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1771</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minglin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Prompting Large Language Models with Mispronunciation Detection and Diagnosis Abilities</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2990</first_page>
						<last_page>2994</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1772</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keigo</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukoh</given_name>
<surname>Wakabayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kengo</given_name>
<surname>Ohta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>Boosting CTC-based ASR using inter-layer attention-based CTC loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2860</first_page>
						<last_page>2864</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1776</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hojo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea Lorena</given_name>
<surname>Aldana Blanco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cassia</given_name>
<surname>Valentini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aidan</given_name>
<surname>Pine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Experimental evaluation of MOS, AB and BWS listening test designs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2695</first_page>
						<last_page>2699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1778</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wells24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beida</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mijit</given_name>
<surname>Ablimit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hankiz</given_name>
<surname>Yilahun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Askar</given_name>
<surname>Hamdulla</surname>
</person_name>
					</contributors>
					<titles><title>Convolutional gated MLP and attention improve end-to-end spoken language understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3520</first_page>
						<last_page>3524</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1780</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zheng24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alon</given_name>
<surname>Vinnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Ivry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aviv</given_name>
<surname>Hurvitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Abramovski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Koubi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Gurvich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shai</given_name>
<surname>Peer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin Martinez</given_name>
<surname>Elizalde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shalev</given_name>
<surname>Shaer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stav</given_name>
<surname>Yagev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Asher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunit</given_name>
<surname>Sivasankaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eyal</given_name>
<surname>Krupka</surname>
</person_name>
					</contributors>
					<titles><title>NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5003</first_page>
						<last_page>5007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1788</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vinnikov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinpeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Pu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2514</first_page>
						<last_page>2518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1790</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ia_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunrui</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>LoRA-MER: Low-Rank Adaptation of Pre-Trained Speech Models for Multimodal Emotion Recognition Using Mutual Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4658</first_page>
						<last_page>4662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1793</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cai24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kim</given_name>
<surname>Sung-Bin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lee</given_name>
<surname>Chae-Yeon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gihun</given_name>
<surname>Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oh</given_name>
<surname>Hyun-Bin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janghoon</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suekyeong</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Hyun</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1380</first_page>
						<last_page>1384</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1794</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sungbin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shangbin</given_name>
<surname>Mo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ling</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengtao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjun</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxin</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>DGSRN: Noise-Robust Speech Recognition Method with Dual-Path Gated Spectral Refinement Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5018</first_page>
						<last_page>5022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1796</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gahye</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunjung</given_name>
<surname>Eom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Selina S.</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghee</given_name>
<surname>Ha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Jin</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungmin</given_name>
<surname>So</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Children Speech Sound Disorder Detection with Age and Speaker Bias Mitigation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1420</first_page>
						<last_page>1424</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1799</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I-Ping</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Collaborative Contrastive Learning for Hypothesis Domain Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3225</first_page>
						<last_page>3229</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1800</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chien24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiabao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuyi</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>YOLOPitch: A Time-Frequency Dual-Branch YOLO Model for Pitch Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>72</first_page>
						<last_page>76</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1805</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ja_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qi</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Mandarin T3 Production by Chinese and Japanese Native Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1035</first_page>
						<last_page>1039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1806</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>De La Fuente Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fasih</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Davida</given_name>
<surname>Fromm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>MacWhinney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alyssa</given_name>
<surname>Lanzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Ning</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Ju</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chien</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Connected Speech-Based Cognitive Assessment in Chinese and English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>947</first_page>
						<last_page>951</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1807</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/luz24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jae-Hong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Eon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong-Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DoHee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Online Subloop Search via Uncertainty Quantization for Efficient Test-Time Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2880</first_page>
						<last_page>2884</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1813</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoqiang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guodong</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4483</first_page>
						<last_page>4487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1814</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arnav</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Medha</given_name>
<surname>Hira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anubha</given_name>
<surname>Gupta</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2340</first_page>
						<last_page>2344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1820</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/goel24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilseok</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-Seok</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Keyword-Incremental Learning with Total Calibration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5083</first_page>
						<last_page>5087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1823</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amit</given_name>
<surname>Meghanani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2835</first_page>
						<last_page>2839</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1824</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meghanani24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddique</given_name>
<surname>Latif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raja</given_name>
<surname>Jurdak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Transformer-Enhanced Deep Reinforcement Learning for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1600</first_page>
						<last_page>1604</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1827</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/latif24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunseop</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee Suk</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Harvill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang D.</given_name>
<surname>Yoo</surname>
</person_name>
					</contributors>
					<titles><title>LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3490</first_page>
						<last_page>3494</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1829</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yoon24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chara</given_name>
<surname>Tsoukala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Katsouros</surname>
</person_name>
					</contributors>
					<titles><title>The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3969</first_page>
						<last_page>3973</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1830</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/paraskevopoulos24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiquan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>An Exploration of Length Generalization in Transformer-Based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1725</first_page>
						<last_page>1729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1831</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shareef Babu</given_name>
<surname>Kalluri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prachi</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pratik</given_name>
<surname>Roy Chowdhuri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apoorva</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shikha</given_name>
<surname>Baghel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradyoth</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swapnil</given_name>
<surname>Sontakke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>K T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.R. Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepu</given_name>
<surname>Vijayasenan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>The Second DISPLACE Challenge: DIarization of SPeaker and LAnguage in Conversational Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1630</first_page>
						<last_page>1634</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1833</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kalluri24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Freixes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Arnela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan Claudi</given_name>
<surname>Socoró</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis</given_name>
<surname>Joglar-Ongay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oriol</given_name>
<surname>Guasch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesc</given_name>
<surname>Alías-Pujol</surname>
</person_name>
					</contributors>
					<titles><title>Glottal inverse filtering and vocal tract tuning for the numerical simulation of vowel /a/ with different levels of vocal effort</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3115</first_page>
						<last_page>3119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1835</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/freixes24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuochen</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Zhuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangyu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boshi</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiaochu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Approach for Chord-Conditioned Song Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1890</first_page>
						<last_page>1894</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1837</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gao24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoxiong</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4273</first_page>
						<last_page>4277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1840</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minmin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name>
					</contributors>
					<titles><title>Intrusive schwa within French stop-liquid clusters: An acoustic analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3709</first_page>
						<last_page>3713</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1841</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaidi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangjin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>LAFMA: A Latent Flow Matching Model for Text-to-Audio Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4813</first_page>
						<last_page>4817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1848</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaopeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuankun</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongwei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuchen</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>Genuine-Focused Learning using Mask AutoEncoder for Generalized Fake Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4848</first_page>
						<last_page>4852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1851</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xueyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dingdong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4129</first_page>
						<last_page>4133</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1852</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peikun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhao</given_name>
<surname>Shan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4468</first_page>
						<last_page>4472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1853</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valentin</given_name>
<surname>Pelloin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Léna</given_name>
<surname>Dodson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Émile</given_name>
<surname>Chapuis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Hervé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Doukhan</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Classification of News Subjects in Broadcast News: Application to a Gender Bias Representation Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3055</first_page>
						<last_page>3059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1854</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pelloin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuanru</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshul</given_name>
<surname>Kashyap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayati</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brittany</given_name>
<surname>Morin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Baquirin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jet</given_name>
<surname>Vonk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Ezzes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zachary</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Tempini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>937</first_page>
						<last_page>941</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1855</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhou24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Ning</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhichao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jixun</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengxiao</given_name>
<surname>Bi</surname>
</person_name>
					</contributors>
					<titles><title>DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>197</first_page>
						<last_page>201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1857</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ning24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao-Wei</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirofumi</given_name>
<surname>Inaguma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilia</given_name>
<surname>Kulikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruslan</given_name>
<surname>Mavlyutov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravya</given_name>
<surname>Popuri</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Decoder-only Large Language Models for Speech-to-text Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>832</first_page>
						<last_page>836</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1858</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dake</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinfa</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liumeng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongmao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Text-aware and Context-aware Expressive Audiobook Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1790</first_page>
						<last_page>1794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1862</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/guo24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhesong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bilei</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>52</first_page>
						<last_page>56</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1863</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yudong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rukiye</given_name>
<surname>Ruzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manwa</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaofeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Optical Flow Guided Tongue Trajectory Generation for Diffusion-based Acoustic to Articulatory Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>417</first_page>
						<last_page>421</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1864</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiang-Li</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Fen</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Deep Prosodic Features in Tandem with Perceptual Judgments of Word Reduction for Tone Recognition in Conversed Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4553</first_page>
						<last_page>4557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1869</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lu24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>Kunmei</surname>
</person_name>
					</contributors>
					<titles><title>Modelling Lexical Characteristics of the Healthy Aging Population: A Corpus-Based Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1090</first_page>
						<last_page>1094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1871</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kunmei24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daimeng</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hengchao</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ZongYao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhanglin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanchang</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianghui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Smoothed kNN Speaker Adaptation for End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2390</first_page>
						<last_page>2394</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1873</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ka_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Rin</given_name>
<surname>Jeoung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilseok</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Speaker Embedding Extraction Using a Twofold Sliding Window Algorithm for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3749</first_page>
						<last_page>3753</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1874</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/choi24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mun-Hak</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Hong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DoHee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Eun</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Balanced-Wav2Vec: Enhancing Stability and Robustness of Representation Learning Through Sample Reweighting Techniques</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5058</first_page>
						<last_page>5062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1875</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuning</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>The Interspeech 2024 Challenge on Speech Processing Using Discrete Units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2559</first_page>
						<last_page>2563</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1878</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lichun</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Change Detection with Weighted-sum Knowledge Distillation based on Self-supervised Pre-trained Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1655</first_page>
						<last_page>1659</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1885</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/su24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeh-Sheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shu-Chuan</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jyh-Shing Roger</given_name>
<surname>Jang</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Phonemic Transcription and Whisper toward Clinically Significant Indices for Automatic Child Speech Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2440</first_page>
						<last_page>2444</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1887</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Asynchronous Voice Anonymization Using Adversarial Perturbation On Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4443</first_page>
						<last_page>4447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1888</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yishuang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Integrated Features Based on Pre-trained Models for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2140</first_page>
						<last_page>2144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1889</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24la_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiu</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehmet Hamza</given_name>
<surname>Erol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arda</given_name>
<surname>Senocak</surname>
</person_name>
					</contributors>
					<titles><title>ElasticAST: An Audio Spectrogram Transformer for All Length and Resolutions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4743</first_page>
						<last_page>4747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1890</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/feng24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengyun</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenfeng</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaodan</given_name>
<surname>Zhai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>E-Paraformer: A Faster and Better  Parallel Transformer for Non-autoregressive End-to-End Mandarin Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>267</first_page>
						<last_page>271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1891</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Matsubara</surname>
</person_name>
					</contributors>
					<titles><title>Utilization of Text Data for Response Timing Detection in Attentive Listening</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3565</first_page>
						<last_page>3569</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1892</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/watanabe24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyun</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinan</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runsen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2915</first_page>
						<last_page>2919</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1895</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cui24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chia-Kai</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chih-Chun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Hsien</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modality Diffusion Modeling and Sampling for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3924</first_page>
						<last_page>3928</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1898</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yeh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ainikaerjiang</given_name>
<surname>Aimaiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liting</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gulinigeer</given_name>
<surname>Abudouwaili</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wushour</given_name>
<surname>Silamu</surname>
</person_name>
					</contributors>
					<titles><title>An Uyghur Extension to the MASSIVE Multi-lingual Spoken Language Understanding Corpus with Comprehensive Evaluations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3525</first_page>
						<last_page>3529</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1900</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/aimaiti24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1895</first_page>
						<last_page>1899</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1903</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baskar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mojtaba</given_name>
<surname>Kadkhodaie Elyaderani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Glover</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schaaf</surname>
</person_name>
					</contributors>
					<titles><title>Reference-Free Estimation of the Quality of Clinical Notes Generated from Doctor-Patient Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1425</first_page>
						<last_page>1429</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1907</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kadkhodaieelyaderani24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianqi</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Form and Function in Prosodic Representation:  In the Case of 'ma' in Tianjin Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2055</first_page>
						<last_page>2059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1909</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/geng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muqiao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Cappellazzo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>Towards Unified Evaluation of Continual Learning in Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3515</first_page>
						<last_page>3519</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1911</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shijie</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minglu</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jichen</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Synthesizing Long-Form Speech merely from Sentence-Level Corpus with Content Extrapolation and LLM Contextual Enrichment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3430</first_page>
						<last_page>3434</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1913</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lai24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bingliang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangping</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiyu</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Age-related Differences in Acoustic Cues for the Perception of Checked Syllables in Shengzhou Wu</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4233</first_page>
						<last_page>4237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1915</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhao24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreyan</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Seth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Purva</given_name>
<surname>Chiniya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Utkarsh</given_name>
<surname>Tyagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramani</given_name>
<surname>Duraiswami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh</given_name>
<surname>Manocha</surname>
</person_name>
					</contributors>
					<titles><title>LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1920</first_page>
						<last_page>1924</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1918</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ghosh24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Doukhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena</given_name>
<surname>Dodson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manon</given_name>
<surname>Conan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valentin</given_name>
<surname>Pelloin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurélien</given_name>
<surname>Clamouse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mélina</given_name>
<surname>Lepape</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Géraldine</given_name>
<surname>Van Hille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Méadel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marlène</given_name>
<surname>Coulomb-Gully</surname>
</person_name>
					</contributors>
					<titles><title>Gender Representation in TV and Radio: Automatic Information Extraction methods versus Manual Analyses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3060</first_page>
						<last_page>3064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1921</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/doukhan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>ASTRA: Aligning Speech and Text Representations for Asr without Sampling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3904</first_page>
						<last_page>3908</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1924</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gaur24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anaïs</given_name>
<surname>Rameau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satrajit</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Sigaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Elemento</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Christophe</given_name>
<surname>Belisle-Pipon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vardit</given_name>
<surname>Ravitsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Powell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alistair</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Dorr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Payne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Micah</given_name>
<surname>Boyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Watts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruth</given_name>
<surname>Bahr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Rudzicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Lerner-Ellis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaheen</given_name>
<surname>Awan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Don</given_name>
<surname>Bolser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yael</given_name>
<surname>Bensoussan</surname>
</person_name>
					</contributors>
					<titles><title>Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1445</first_page>
						<last_page>1449</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1926</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/rameau24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuewei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huanbin</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Sub-PNWR: Speech Enhancement Based on Signal Sub-Band Splitting and Pseudo Noisy Waveform Reconstruction Loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>657</first_page>
						<last_page>661</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1927</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai-Wei</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-Hao</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Exploring In-Context Learning of Textless Speech Language Model for Speech Classification Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4139</first_page>
						<last_page>4143</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1932</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akihiro</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroyuki</given_name>
<surname>Nagano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Chike</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masaki</given_name>
<surname>Nose</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Learning for ASR Pre-Training with Uniquely Determined Target Labels and Controlling Cepstrum Truncation for Speech Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5048</first_page>
						<last_page>5052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1933</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kato24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fangjing</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaozhe</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinya</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Speech Topic Classification Based on Multi-Scale and Graph Attention Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4313</first_page>
						<last_page>4317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1934</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/niu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3959</first_page>
						<last_page>3963</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1938</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tian24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fathima</given_name>
<surname>Zaheera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Supritha</given_name>
<surname>Shetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gayadhar</given_name>
<surname>Pradhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>K T</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Dysarthria using Speech and synthetically generated Electroglottograph signal</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4104</first_page>
						<last_page>4108</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1939</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zaheera24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixiang</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongqing</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiying</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangqiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yibo</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Auditory Spatial Attention Detection Based on Feature Disentanglement and Brain Connectivity-Informed Graph Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>887</first_page>
						<last_page>891</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1940</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/niu24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianhua</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Realistic Emotional Voice Conversion using Controllable Emotional Intensity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>202</first_page>
						<last_page>206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1941</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/qi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jincen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuangao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Boosting Cross-Corpus Speech Emotion Recognition using CycleGAN with Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1605</first_page>
						<last_page>1609</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1947</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ia_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hailun</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianhua</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Distribution Adaptation for Unsupervised Cross-corpus Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3739</first_page>
						<last_page>3743</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1948</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lu24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Bitterman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Levi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hilel Hagai</given_name>
<surname>Diamandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Gannot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tal</given_name>
<surname>Rosenwein</surname>
</person_name>
					</contributors>
					<titles><title>RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding using Contrastive Learning with Application to Room Shape Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3280</first_page>
						<last_page>3284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1951</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bitterman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaixun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longtao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper </title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2534</first_page>
						<last_page>2538</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1953</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jincen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hailun</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongli</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Confidence-aware Hypothesis Transfer Networks for Source-Free Cross-Corpus Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1050</first_page>
						<last_page>1054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1956</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ja_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ofer</given_name>
<surname>Schwartz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Gannot</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Joint Bemforming and Acoustic Echo Cancellation Structure for Conference Call Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>167</first_page>
						<last_page>171</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1957</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/schwartz24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiyuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shekhar</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name>
					</contributors>
					<titles><title>A Functional Trade-off between Prosodic and Semantic Cues in Conveying Sarcasm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1070</first_page>
						<last_page>1074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1962</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ma_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Papadimitriou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Continuous Fingerspelling Recognition via Visual Alignment Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>922</first_page>
						<last_page>926</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1966</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/papadimitriou24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumi</given_name>
<surname>Maiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>IndicMOS: Multilingual MOS Prediction for 7 Indian languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2690</first_page>
						<last_page>2694</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1967</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/udupa24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiuwen</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bornali</given_name>
<surname>Phukon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Tuning Automatic Speech Recognition for People with Parkinson's: An Effective Strategy for Enhancing Speech Technology Accessibility</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2485</first_page>
						<last_page>2489</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1969</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zheng24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Häsner</surname>
</person_name>
					</contributors>
					<titles><title>Measurement and simulation of pressure losses due to airflow in vocal tract models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3105</first_page>
						<last_page>3109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1970</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/birkholz24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khanh</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Chau</surname>
</person_name>
					</contributors>
					<titles><title>Improving Streaming Speech Recognition With Time-Shifted Contextual Attention And Dynamic Right Context Masking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4478</first_page>
						<last_page>4482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1971</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/le24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vu</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet Thanh</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoa Nguyen</given_name>
<surname>Xuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pham</given_name>
<surname>Nhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phuong</given_name>
<surname>Dat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thi Thu Trang</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>VSASV: a Vietnamese Dataset for Spoofing-Aware Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4288</first_page>
						<last_page>4292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1972</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hoang24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuyen</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khanh</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Dang</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh</given_name>
<surname>Vu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huyen</given_name>
<surname>Ngo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woomyoung</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thi Thu Trang</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>VN-SLU: A Vietnamese Spoken Language Understanding Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1335</first_page>
						<last_page>1339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1976</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tran24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunayana</given_name>
<surname>Sitaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name>
					</contributors>
					<titles><title>MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5053</first_page>
						<last_page>5057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1978</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yadav24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mattias</given_name>
<surname>Nilsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Riccardo</given_name>
<surname>Miccini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clement</given_name>
<surname>Laroche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Piechowiak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Friedemann</given_name>
<surname>Zenke</surname>
</person_name>
					</contributors>
					<titles><title>Resource-Efficient Speech Quality Prediction through Quantization Aware Training and Binary Activation Maps</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2975</first_page>
						<last_page>2979</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1979</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nilsson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atli</given_name>
<surname>Sigurgeirsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eddie L.</given_name>
<surname>Ungless</surname>
</person_name>
					</contributors>
					<titles><title>Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling Queer Voices.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3050</first_page>
						<last_page>3054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1982</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sigurgeirsson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adham</given_name>
<surname>Ibrahim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shady</given_name>
<surname>Shehata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mukhtar</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Abdul-Mageed</surname>
</person_name>
					</contributors>
					<titles><title>What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1590</first_page>
						<last_page>1594</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1983</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ibrahim24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Blatt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Krishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Joint vs Sequential Speaker-Role Detection and Automatic Speech Recognition for Air-traffic Control</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3759</first_page>
						<last_page>3763</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1987</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/blatt24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiqin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peiji</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yicheng</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhisheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors Based on Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1785</first_page>
						<last_page>1789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1989</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24na_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Ray Barman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakuntala</given_name>
<surname>Mahanta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj Kumar</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Deciphering Assamese Vowel Harmony with Featural InfoWaveGAN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1510</first_page>
						<last_page>1514</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1990</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/raybarman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyabrata</given_name>
<surname>Mallick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aalekhya Satya</given_name>
<surname>Narayani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Towards Multilingual Audio-Visual Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4039</first_page>
						<last_page>4043</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-1993</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/phukan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongchen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiwon</given_name>
<surname>Yun</surname>
</person_name>
					</contributors>
					<titles><title>Influences of Morphosyntax and Semantics on the Intonation of Mandarin Chinese Wh-indeterminates</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2070</first_page>
						<last_page>2074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2004</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhyeok</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeong-Seok</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghoon</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeongju</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juheon</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>DualSpeech: Enhancing Speaker-Fidelity and Text-Intelligibility Through Dual Classifier-Free Guidance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4423</first_page>
						<last_page>4427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2005</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yang24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruijie</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>How Do Neural Spoofing Countermeasures Detect Partially Spoofed Audio?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1105</first_page>
						<last_page>1109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2009</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tillmann</given_name>
<surname>Pistor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Leemann</surname>
</person_name>
					</contributors>
					<titles><title>Echoes of Implicit Bias Exploring Aesthetics and Social Meanings of Swiss German Dialect Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>447</first_page>
						<last_page>451</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2013</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pistor24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aryan</given_name>
<surname>Chaudhary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name>
					</contributors>
					<titles><title>QGAN: Low Footprint Quaternion Neural Vocoder for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3874</first_page>
						<last_page>3878</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2014</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chaudhary24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kelly</given_name>
<surname>Davis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eren</given_name>
<surname>Gölge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Görkem</given_name>
<surname>Göknar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iulian</given_name>
<surname>Gulea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Logan</given_name>
<surname>Hart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aya</given_name>
<surname>Aljafari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reuben</given_name>
<surname>Morais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Olayemi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Weber</surname>
</person_name>
					</contributors>
					<titles><title>XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4978</first_page>
						<last_page>4982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2016</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/casanova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghe</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feilong</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Sign Value Constraint Decomposition for Efficient 1-Bit Quantization of Speech Translation Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>842</first_page>
						<last_page>846</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2022</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Devang</given_name>
<surname>Kulshreshtha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Pappas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brady</given_name>
<surname>Houston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saket</given_name>
<surname>Dingliwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name>
					</contributors>
					<titles><title>Sequential Editing for Lifelong Training of Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3919</first_page>
						<last_page>3923</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2027</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kulshreshtha24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincenzo Norman</given_name>
<surname>Vitale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Loredana</given_name>
<surname>Schettino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Cutugno</surname>
</person_name>
					</contributors>
					<titles><title>Rich speech signal: exploring and exploiting  end-to-end automatic speech recognizers’ ability to model hesitation phenomena</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>222</first_page>
						<last_page>226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2029</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vitale24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Favaro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyu</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Universal Speech Representations for Detecting and Assessing the Severity of Mild Cognitive Impairment Across Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>972</first_page>
						<last_page>976</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2030</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/favaro24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zitha</given_name>
<surname>Sasindran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harsha</given_name>
<surname>Yelchuri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>T. V.</given_name>
<surname>Prabhakar</surname>
</person_name>
					</contributors>
					<titles><title>SeMaScore: A new evaluation metric for automatic speech recognition tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4558</first_page>
						<last_page>4562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2033</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sasindran24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhasi</given_name>
<surname>K. C.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noumida</given_name>
<surname>A</surname>
</person_name>
					</contributors>
					<titles><title>Attention-augmented X-vectors for the Evaluation of Mimicked Speech Using Sparse Autoencoder-LSTM framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3804</first_page>
						<last_page>3808</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2036</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kc24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vidar Freyr</given_name>
<surname>Gudmundsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keve Márton</given_name>
<surname>Gönczi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malin</given_name>
<surname>Svensson Lundmark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donna</given_name>
<surname>Erickson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name>
					</contributors>
					<titles><title>The MARRYS helmet: A new device for researching and training “jaw dancing”</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1400</first_page>
						<last_page>1404</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2039</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gudmundsson24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Jakhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudhanshu</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Baby</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Approach to Multilingual Automatic Speech Recognition with Improved Language Identification for Indic Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3949</first_page>
						<last_page>3953</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2043</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jakhar24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrick Cormac</given_name>
<surname>English</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John D.</given_name>
<surname>Kelleher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name>
					</contributors>
					<titles><title>Searching for Structure: Appraising the Organisation of Speech Features in wav2vec 2.0 Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4613</first_page>
						<last_page>4617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2047</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/english24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danilo</given_name>
<surname>de Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Welker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julius</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3854</first_page>
						<last_page>3858</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2051</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/deoliveira24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liam</given_name>
<surname>Kelley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Di Carlo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya Arie</given_name>
<surname>Nugraha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Fontaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuyoshi</given_name>
<surname>Yoshii</surname>
</person_name>
					</contributors>
					<titles><title>RIR-in-a-Box: Estimating Room Acoustics from 3D Mesh Data through Shoebox Approximation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3255</first_page>
						<last_page>3259</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2053</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kelley24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Himanshu</given_name>
<surname>Maurya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atli</given_name>
<surname>Sigurgeirsson</surname>
</person_name>
					</contributors>
					<titles><title>A Human-in-the-Loop Approach to Improving Cross-Text Prosody Transfer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2295</first_page>
						<last_page>2299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2055</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/maurya24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Challenging margin-based speaker embedding extractors by using the variational information bottleneck</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3220</first_page>
						<last_page>3224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2058</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/stafylakis24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Federico</given_name>
<surname>Lo Iacono</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valentina</given_name>
<surname>Colonna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Romano</surname>
</person_name>
					</contributors>
					<titles><title>Preservation, conservation and phonetic study of the voices of Italian poets: A study on the seven years of the VIP archive</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3664</first_page>
						<last_page>3668</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2060</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/loiacono24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Si-Ioi</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kimberly D.</given_name>
<surname>Mueller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>Segmental and Suprasegmental Speech Foundation Models for Classifying Cognitive Risk Factors: Evaluating Out-of-the-Box Performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>917</first_page>
						<last_page>921</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2063</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ng24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miku</given_name>
<surname>Nishihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aidan</given_name>
<surname>Pine</surname>
</person_name>
					</contributors>
					<titles><title>Low-dimensional Style Token Control for Hyperarticulated Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3385</first_page>
						<last_page>3389</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2074</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nishihara24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Janek</given_name>
<surname>Ebbers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Sound Event Bounding Boxes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>562</first_page>
						<last_page>566</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2075</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ebbers24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youssef</given_name>
<surname>Nafea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shady</given_name>
<surname>Shehata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeerak</given_name>
<surname>Talat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Aboeitta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Sharshar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preslav</given_name>
<surname>Nakov</surname>
</person_name>
					</contributors>
					<titles><title>AraOffence: Detecting Offensive Speech Across Dialects in Arabic Media</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4303</first_page>
						<last_page>4307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2077</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nafea24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Khokhlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Prisyach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Mitrofanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Dutov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Agafonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Timofeeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Romanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Korenevsky</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Room Impulse Responses and its application for channel verification and diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3250</first_page>
						<last_page>3254</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2081</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/khokhlov24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>De Luca</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Clark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name>
					</contributors>
					<titles><title>NumberLie: a game-based experiment to understand the acoustics of deception and truthfulness</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3659</first_page>
						<last_page>3663</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2082</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/deluca24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongshuai</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debasish Ray</given_name>
<surname>Mohapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Vocal Tract Like Acoustic Tubes Using the Immersed Boundary Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3415</first_page>
						<last_page>3419</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2087</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johanna</given_name>
<surname>Cronenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Chitoran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name>
					</contributors>
					<titles><title>Crosslinguistic Comparison of Acoustic Variation in the Vowel Sequences /ia/ and /io/ in Four Romance Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3689</first_page>
						<last_page>3693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2090</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cronenberg24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Gusev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Avdeeva</surname>
</person_name>
					</contributors>
					<titles><title>Improvement Speaker Similarity for Zero-Shot Any-to-Any Voice Conversion of Whispered and Regular Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2735</first_page>
						<last_page>2739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2091</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gusev24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1770</first_page>
						<last_page>1774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2093</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Coffey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Camila</given_name>
<surname>Scaff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>The Difficulty and Importance of Estimating the Lower and Upper Bounds of Infant Speech Exposure</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3615</first_page>
						<last_page>3619</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2102</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/coffey24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Murzaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adil</given_name>
<surname>Soubki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Owen</given_name>
<surname>Rambow</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Belief Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1075</first_page>
						<last_page>1079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2103</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/murzaku24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4623</first_page>
						<last_page>4627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2105</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wagner24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Allahsera</given_name>
<surname>Tapo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éric</given_name>
<surname>Le Ferrand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoey</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Homan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Prud'hommeaux</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Speech Data Diversity to Document Indigenous Heritage and Culture</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5088</first_page>
						<last_page>5092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2107</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tapo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Badr M.</given_name>
<surname>Abdullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammed Maqsood</given_name>
<surname>Shaik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Wave to Interlingua: Analyzing Representations of Multilingual Speech Transformers for Spoken Language Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>362</first_page>
						<last_page>366</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2109</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/abdullah24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingshuai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuangqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaopeng</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanjun</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijian</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>RaD-Net 2: A causal two-stage repairing and denoising speech enhancement network with knowledge distillation and complex axial self-attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1700</first_page>
						<last_page>1704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2114</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomas</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Weise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Speech and Language Analysis for the Assessment of Mild Cognitive Impairment: Outcomes from the Taukadial Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>982</first_page>
						<last_page>986</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2115</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pereztoro24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Yakovlev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rostislav</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Balykin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Malov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Okhotnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Torgashov</surname>
</person_name>
					</contributors>
					<titles><title>Reshape Dimensions Network for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3235</first_page>
						<last_page>3239</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2116</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yakovlev24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Maciejewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Klement</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruizhe</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the Santa Barbara Corpus: Challenges of the Breadth of Conversational Spoken Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2155</first_page>
						<last_page>2159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2119</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/maciejewski24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Large Language Models for Dysfluency Detection in Stuttered Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5118</first_page>
						<last_page>5122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2120</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wagner24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johannah</given_name>
<surname>O'Mahony</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>Well, what can you do with messy data? Exploring the prosody and pragmatic function of the discourse marker &quot;well&quot; with found data and speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4084</first_page>
						<last_page>4088</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2122</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/omahony24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhongweiyang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali</given_name>
<surname>Aroudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung-Suk</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Nesta</surname>
</person_name>
					</contributors>
					<titles><title>FoVNet: Configurable Field-of-View Speech Enhancement with Low Computation and Distortion for Smart Glasses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3350</first_page>
						<last_page>3354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2124</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Unger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Evaluation of a Sentence Memory Test for Preschool Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5158</first_page>
						<last_page>5162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2125</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baumann24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinzuomu</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiba</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benlai</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengjie</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of Speech-Silence and Word-Punctuation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2305</first_page>
						<last_page>2309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2133</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhong24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Towards Self-Attention Understanding for Automatic Articulatory Processes Analysis in Cleft Lip and Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2430</first_page>
						<last_page>2434</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2134</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baumann24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pooneh</given_name>
<surname>Mousavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarod</given_name>
<surname>Duret</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salah</given_name>
<surname>Zaiem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Della Libera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Ploujnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cem</given_name>
<surname>Subakan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>How Should We Extract Discrete Audio Tokens from Self-Supervised Models?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2554</first_page>
						<last_page>2558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2135</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mousavi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominika</given_name>
<surname>Woszczyk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ranya</given_name>
<surname>Aloufi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soteris</given_name>
<surname>Demetriou</surname>
</person_name>
					</contributors>
					<titles><title>Prosody-Driven Privacy-Preserving Dementia Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3035</first_page>
						<last_page>3039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2137</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/woszczyk24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toan Quoc</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kishan</given_name>
<surname>KC</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thuy</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Reinforcement Learning from Answer Reranking Feedback for Retrieval-Augmented Answer Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4044</first_page>
						<last_page>4048</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2147</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/nguyen24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gasser</given_name>
<surname>Elbanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zohreh</given_name>
<surname>Mostaani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Heart Activity from Speech using Data-driven and Knowledge-based features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4758</first_page>
						<last_page>4762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2150</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/elbanna24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinuk</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debadatta</given_name>
<surname>Dash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Ferrari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Direct Speech Synthesis from Non-Invasive, Neuromagnetic Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>412</first_page>
						<last_page>416</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2153</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kwon24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Menglu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Ping</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Interpretable Temporal Class Activation Representation for Audio Spoofing  Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1120</first_page>
						<last_page>1124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2156</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24oa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Mihajlik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mate S</given_name>
<surname>Kadar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Linke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katalin</given_name>
<surname>Mády</surname>
</person_name>
					</contributors>
					<titles><title>On Disfluency and Non-lexical Sound Labeling for End-to-end Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1270</first_page>
						<last_page>1274</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2157</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mihajlik24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Looney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolay D.</given_name>
<surname>Gaubitch</surname>
</person_name>
					</contributors>
					<titles><title>Robust spread spectrum speech watermarking using linear prediction and deep spectral shaping</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2715</first_page>
						<last_page>2719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2165</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/looney24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanha</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Azcarreta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>All Neural Low-latency Directional Speech Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4328</first_page>
						<last_page>4332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2168</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/pandey24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benazir</given_name>
<surname>Mumtaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miriam</given_name>
<surname>Butt</surname>
</person_name>
					</contributors>
					<titles><title>Urdu Alternative Questions: A Hat Pattern</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2075</first_page>
						<last_page>2079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2172</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mumtaz24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jack</given_name>
<surname>Goldberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of articulatory setting for L1 and L2 English speakers using MRI data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1020</first_page>
						<last_page>1024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2175</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/huang24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuang</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarun</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>DeWinder: Single-Channel Wind Noise Reduction using Ultrasound Sensing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>627</first_page>
						<last_page>631</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2180</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yuan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Amaan</given_name>
<surname>Sayeed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanan</given_name>
<surname>Aldarmaki</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Word2Vec: Learning Skipgram Embeddings from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2920</first_page>
						<last_page>2924</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2181</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sayeed24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giulia</given_name>
<surname>Sanguedolce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Brook</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dragos C.</given_name>
<surname>Gruia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick A.</given_name>
<surname>Naylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fatemeh</given_name>
<surname>Geranmayeh</surname>
</person_name>
					</contributors>
					<titles><title>When Whisper Listens to Aphasia: Advancing Robust Post-Stroke Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1995</first_page>
						<last_page>1999</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2183</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sanguedolce24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sidi Yaya Arnaud</given_name>
<surname>Yarga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sean U N</given_name>
<surname>Wood</surname>
</person_name>
					</contributors>
					<titles><title>Neuromorphic Keyword Spotting with Pulse Density Modulation MEMS Microphones</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3275</first_page>
						<last_page>3279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2185</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yarga24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuzhe</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Favaro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Complementary Nature of Speech and Eye Movements for Profiling Neurological Disorders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1475</first_page>
						<last_page>1479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2186</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ka_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amrutha</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Driss</given_name>
<surname>Khalil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christof</given_name>
<surname>Schuepbach</surname>
</person_name>
					</contributors>
					<titles><title>Speech and Language Recognition with Low-rank Adaptation of Pretrained Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2825</first_page>
						<last_page>2829</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2187</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/prasad24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwangyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Te</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Convolution-Augmented Parameter-Efficient Fine-Tuning for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2830</first_page>
						<last_page>2834</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2188</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan Him Nok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Salzmann</surname>
</person_name>
					</contributors>
					<titles><title>Do we EXPECT TO find phonetic traces for syntactic traces?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4258</first_page>
						<last_page>4262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2190</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chaofei</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaimie M.</given_name>
<surname>Henderson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Manning</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francis R.</given_name>
<surname>Willett</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Quantitative Analysis of Coarticulation with a Phoneme-to-Articulatory Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3095</first_page>
						<last_page>3099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2191</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fan24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Angelo</given_name>
<surname>Ortiz Tandazo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schatz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>Simulating articulatory trajectories with phonological feature interpolation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3595</first_page>
						<last_page>3599</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2192</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ortiztandazo24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tejes</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Low Resource and Multilingual Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3989</first_page>
						<last_page>3993</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2199</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/srivastava24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eitan</given_name>
<surname>Abecassis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clara</given_name>
<surname>Fernandez-Labrador</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Schroers</surname>
</person_name>
					</contributors>
					<titles><title>RAST: A Reference-Audio Synchronization Tool for Dubbed Content</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>67</first_page>
						<last_page>71</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2203</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/meyer24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Morozova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanghao</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabine</given_name>
<surname>Stoll</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Bangerter</surname>
</person_name>
					</contributors>
					<titles><title>Measuring acoustic dissimilarity of hierarchical markers in task-oriented dialogue with MFCC-based dynamic time warping</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4763</first_page>
						<last_page>4767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2204</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/morozova24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Escobar-Grisales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian David</given_name>
<surname>Ríos-Urrego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adolfo M.</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>It’s Time to Take Action: Acoustic Modeling of Motor Verbs to Detect Parkinson’s Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1965</first_page>
						<last_page>1969</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2205</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/escobargrisales24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abderrahim</given_name>
<surname>Fathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolin</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name>
					</contributors>
					<titles><title>On the impact of several regularization techniques on label noise robustness of self-supervised speaker verification systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2670</first_page>
						<last_page>2674</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2206</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/fathan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Krishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Badr M.</given_name>
<surname>Abdullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>On the Encoding of Gender in Transformer-based ASR Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3090</first_page>
						<last_page>3094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2209</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/krishnan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianrui</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>SOT Triggered Neural Clustering for Speaker Attributed ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>717</first_page>
						<last_page>721</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2211</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zheng24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ariëlle</given_name>
<surname>Reitsema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leanne</given_name>
<surname>van Lambalgen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Preining</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saskia</given_name>
<surname>Galindo Jong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiya</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Learning in Lexical Tone: Phonetic Similarity vs. Phonological Categories</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4248</first_page>
						<last_page>4252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2212</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/reitsema24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wonjune</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deb</given_name>
<surname>Roy</surname>
</person_name>
					</contributors>
					<titles><title>Prompting Large Language Models with Audio for General-Purpose Speech Summarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1955</first_page>
						<last_page>1959</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2213</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehua Kcriss</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meiying Melissa</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pinxin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>GTR-Voice: Articulatory Phonetics Informed Controllable Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1775</first_page>
						<last_page>1779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2216</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24pa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name>
					</contributors>
					<titles><title>Phonological-Level Mispronunciation Detection and Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>307</first_page>
						<last_page>311</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2217</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shahin24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatsunari</given_name>
<surname>Takagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukoh</given_name>
<surname>Wakabayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>Text-only Domain Adaptation for CTC-based Speech Recognition through Substitution of Implicit Linguistic Information in the Search Space</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>287</first_page>
						<last_page>291</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2222</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/takagi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishi</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejas</given_name>
<surname>Prabhune</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Segmentation for Vocal Tract Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>422</first_page>
						<last_page>426</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2223</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jain24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gowtham</given_name>
<surname>Premananth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashish M.</given_name>
<surname>Siriwardena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Resnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonia</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deanna</given_name>
<surname>L.Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>A Multimodal Framework for the Assessment of the Schizophrenia Spectrum</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1470</first_page>
						<last_page>1474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2224</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/premananth24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gautam Siddharth</given_name>
<surname>Kashyap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Are Paralinguistic Representations all that is needed for Speech Emotion Recognition?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4698</first_page>
						<last_page>4702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2233</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/phukan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Justin</given_name>
<surname>Lovelace</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soham</given_name>
<surname>Ray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwangyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kilian Q.</given_name>
<surname>Weinberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Sample-Efficient Diffusion for Text-To-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4403</first_page>
						<last_page>4407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2235</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lovelace24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomas</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangxu</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maureen</given_name>
<surname>Stone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Zhuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jerry L.</given_name>
<surname>Prince</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonghye</given_name>
<surname>Woo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning Approach for Assessment of Phonological Precision in Patients with Tongue Cancer Using MRI Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>927</first_page>
						<last_page>931</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2236</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ariasvergara24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manila</given_name>
<surname>Kodali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tuning of Pre-trained Models for Classification of Vocal Intensity Category from Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>482</first_page>
						<last_page>486</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2237</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kodali24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prad</given_name>
<surname>Kadambi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tristan</given_name>
<surname>Mahr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Annear</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henry</given_name>
<surname>Nomeland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Hustad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>How Does Alignment Error Affect Automated Pronunciation Scoring in Children's Speech?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5133</first_page>
						<last_page>5137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2239</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kadambi24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenzi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Wormald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Foulkes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Poppy</given_name>
<surname>Welch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Finnian</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>van der Vloed</surname>
</person_name>
					</contributors>
					<titles><title>Voice quality in telephone speech: Comparing acoustic measures between VoIP telephone and high-quality recordings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1570</first_page>
						<last_page>1574</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2240</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/xu24j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yongyi</given_name>
<surname>Zang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jionghao</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengyuan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxiao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>CtrSVDD: A Benchmark Dataset and Baseline Analysis for Controlled Singing Voice Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4783</first_page>
						<last_page>4787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2242</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>DNN-based monaural speech enhancement using alternate analysis windows for phase and magnitude modification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1705</first_page>
						<last_page>1709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2244</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dena</given_name>
<surname>Mujtaba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nihar R.</given_name>
<surname>Mahapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megan</given_name>
<surname>Arney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J. Scott</given_name>
<surname>Yaruss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caryn</given_name>
<surname>Herring</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Bin</surname>
</person_name>
					</contributors>
					<titles><title>Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1275</first_page>
						<last_page>1279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2246</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mujtaba24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>May Pik Yu</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianjing</given_name>
<surname>Kuang</surname>
</person_name>
					</contributors>
					<titles><title>Pitch-driven adjustments in tongue positions: Insights from ultrasound imaging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3150</first_page>
						<last_page>3154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2247</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Heng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Bartelds</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vanya</given_name>
<surname>Bannihatti Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Jurafsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1230</first_page>
						<last_page>1234</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2248</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khai</given_name>
<surname>Le-Duc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khai-Nguyen</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Vo-Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Truong-Son</given_name>
<surname>Hy</surname>
</person_name>
					</contributors>
					<titles><title>Real-time Speech Summarization for Medical Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1960</first_page>
						<last_page>1964</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2250</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/leduc24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xutai</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirofumi</given_name>
<surname>Inaguma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2569</first_page>
						<last_page>2573</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2251</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hazim</given_name>
<surname>Bukhari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soham</given_name>
<surname>Deshmukh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hira</given_name>
<surname>Dhamyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>SELM: Enhancing Speech Emotion Recognition for Out-of-Domain Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2345</first_page>
						<last_page>2349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2257</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bukhari24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul</given_name>
<surname>Best</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santiago</given_name>
<surname>Cuervo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning from Whisper for Microscopic Intelligibility Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3839</first_page>
						<last_page>3843</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2258</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/best24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Irene</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgan</given_name>
<surname>Sonderegger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>The</given_name>
<surname>Spade Consortium</surname>
</person_name>
					</contributors>
					<titles><title>Modelled Multivariate Overlap: A method for measuring vowel merger</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>457</first_page>
						<last_page>461</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2260</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/smith24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tejumade</given_name>
<surname>Afonja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobi</given_name>
<surname>Olatunji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sewade</given_name>
<surname>Ogun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naome A.</given_name>
<surname>Etori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abraham</given_name>
<surname>Owodunni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moshood</given_name>
<surname>Yekini</surname>
</person_name>
					</contributors>
					<titles><title>Performant ASR Models for Medical Entities in Accented Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2315</first_page>
						<last_page>2319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2261</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/afonja24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prakash</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongwan</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophia X.</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christina</given_name>
<surname>Hagedorn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Byrd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uttam K.</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna S.</given_name>
<surname>Nayak</surname>
</person_name>
					</contributors>
					<titles><title>State-of-the-art speech production MRI protocol for new 0.55 Tesla scanners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2590</first_page>
						<last_page>2594</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2263</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kumar24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>RIR-SF: Room Impulse Response Based Spatial Feature for Target Speech Recognition in Multi-Channel Multi-Speaker Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4988</first_page>
						<last_page>4992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2264</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shao24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Kroll</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kelsey</given_name>
<surname>Kraus</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing the role of human evaluation in LLM-based spoken document summarization systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1935</first_page>
						<last_page>1939</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2268</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kroll24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robin</given_name>
<surname>Netzorg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alyssa</given_name>
<surname>Cote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sumi</given_name>
<surname>Koshin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Klo Vivienne</given_name>
<surname>Garoute</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala Krishna</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Speech After Gender: A Trans-Feminine Perspective on Next Steps for Speech Science and Technology</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3075</first_page>
						<last_page>3079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2269</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/netzorg24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Irina-Elena</given_name>
<surname>Veliche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuangqun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineeth</given_name>
<surname>Ayyat Kochaniyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuchun</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Towards measuring fairness in speech recognition: Fair-Speech dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1385</first_page>
						<last_page>1389</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2273</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/veliche24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Multi-Speaker ASR Using Target Speaker’s Solo Segment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4993</first_page>
						<last_page>4997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2274</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunice</given_name>
<surname>Akani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederic</given_name>
<surname>Bechet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoît</given_name>
<surname>Favre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Romain</given_name>
<surname>Gemignani</surname>
</person_name>
					</contributors>
					<titles><title>Unified Framework for Spoken Language Understanding and Summarization in Task-Based Human Dialog processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3535</first_page>
						<last_page>3539</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2276</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/akani24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sewade</given_name>
<surname>Ogun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abraham T.</given_name>
<surname>Owodunni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobi</given_name>
<surname>Olatunji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eniola</given_name>
<surname>Alese</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Babatunde</given_name>
<surname>Oladimeji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejumade</given_name>
<surname>Afonja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kayode</given_name>
<surname>Olaleye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naome A.</given_name>
<surname>Etori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tosin</given_name>
<surname>Adewumi</surname>
</person_name>
					</contributors>
					<titles><title>1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1855</first_page>
						<last_page>1859</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2281</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ogun24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minxue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mimansa</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>From Text to Emotion: Unveiling the Emotion Annotation Capabilities of LLMs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2650</first_page>
						<last_page>2654</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2282</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/niu24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dawei</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edison</given_name>
<surname>Thomaz</surname>
</person_name>
					</contributors>
					<titles><title>Improving Audio Classification with Low-Sampled Microphone Input: An Empirical Study Using Model Self-Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>87</first_page>
						<last_page>91</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2285</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liang24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emily P.</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myriam</given_name>
<surname>Lapierre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gina-Anne</given_name>
<surname>Levow</surname>
</person_name>
					</contributors>
					<titles><title>The Use of Phone Categories and Cross-Language Modeling for Phone Alignment of Panãra</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1505</first_page>
						<last_page>1509</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2286</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ahn24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Colgiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Spinu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasaman</given_name>
<surname>Rafat</surname>
</person_name>
					</contributors>
					<titles><title>Bilingual Rhotic Production Patterns: A Generational Comparison of Spanish-English Bilingual Speakers in Canada</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1025</first_page>
						<last_page>1029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2287</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/colgiu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nana</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youxiang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohui</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John A.</given_name>
<surname>Batsis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caroline</given_name>
<surname>Summerour</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Multimodal Features of Spontaneous Voice Assistant Commands for Mild Cognitive Impairment Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3030</first_page>
						<last_page>3034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2288</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>K R</given_name>
<surname>Prajwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Triantafyllos</given_name>
<surname>Afouras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Zisserman</surname>
</person_name>
					</contributors>
					<titles><title>Speech Recognition Models are Strong Lip-readers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2425</first_page>
						<last_page>2429</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2290</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/prajwal24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuning</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2564</first_page>
						<last_page>2568</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2291</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tang24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vrushank</given_name>
<surname>Changawala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Rudzicz</surname>
</person_name>
					</contributors>
					<titles><title>Whister: Using Whisper’s representations for Stuttering detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>897</first_page>
						<last_page>901</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2293</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/changawala24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Krishna C.</given_name>
<surname>Puvvada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleksii</given_name>
<surname>Hrinchuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin Rao</given_name>
<surname>Koluguri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunal</given_name>
<surname>Dhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somshubra</given_name>
<surname>Majumdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Rastorgueva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3964</first_page>
						<last_page>3968</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2294</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/puvvada24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junming</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>LanTian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Askar</given_name>
<surname>Hamdulla</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Keyword Spotting from Mixed Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5063</first_page>
						<last_page>5067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2296</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/yuan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xintong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqian</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>292</first_page>
						<last_page>296</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2297</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24la_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Annika</given_name>
<surname>Heuser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Kendall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miguel</given_name>
<surname>del Rio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quinn</given_name>
<surname>McNamara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nishchal</given_name>
<surname>Bhandari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corey</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Migüel</given_name>
<surname>Jetté</surname>
</person_name>
					</contributors>
					<titles><title>Quantification of stylistic differences in human- and ASR-produced transcripts of African American English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4538</first_page>
						<last_page>4542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2300</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/heuser24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Tavernor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yara</given_name>
<surname>El-Tawil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual Annotators to Capture Emotional Variability</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3195</first_page>
						<last_page>3199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2307</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tavernor24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavan</given_name>
<surname>Kalyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pushpak</given_name>
<surname>Bhattacharyya</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Arithmetic: Emotional Speech Synthesis via Weight Space Interpolation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1805</first_page>
						<last_page>1809</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2311</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kalyan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatian</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyue</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3435</first_page>
						<last_page>3439</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2320</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiru</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linyu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qun</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>OR-TSE: An Overlap-Robust Speaker Encoder for Target Speech Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>587</first_page>
						<last_page>591</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2322</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyung Yong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeong-Yeol</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunkyu</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihwan</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shukjae</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yooncheol</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinseok</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youshin</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Woo</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanbin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Self-training ASR Guided by Unsupervised ASR Teacher</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2865</first_page>
						<last_page>2869</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2323</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suyuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Molly</given_name>
<surname>Babel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>A comparison of voice similarity through acoustics, human perception and deep neural network (DNN) speaker verification systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3674</first_page>
						<last_page>3678</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2331</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuheyra</given_name>
<surname>Tokac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Cole</surname>
</person_name>
					</contributors>
					<titles><title>Phonological Symmetry Does Not Predict Generalization of Perceptual Adaptation to Vowels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4243</first_page>
						<last_page>4247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2334</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tokac24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Robustness of Text-to-Speech Synthesis Based on Diffusion Probabilistic Models to Heavily Noisy Transcriptions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4408</first_page>
						<last_page>4412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2337</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/feng24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Annika</given_name>
<surname>Heuser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianjing</given_name>
<surname>Kuang</surname>
</person_name>
					</contributors>
					<titles><title>Information-theoretic hypothesis generation of relative cue weighting for the voicing contrast</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3585</first_page>
						<last_page>3589</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2340</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/heuser24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>de la Fuente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Jurafsky</surname>
</person_name>
					</contributors>
					<titles><title>A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1290</first_page>
						<last_page>1294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2341</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/delafuente24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linhan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dake</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuepeng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liumeng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiming</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>WenetSpeech4TTS: A 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1840</first_page>
						<last_page>1844</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2343</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ma24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Dikaios</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abraham</given_name>
<surname>Nunes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Rudzicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudolf</given_name>
<surname>Uher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sageev</given_name>
<surname>Oore</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Embeddings for Detecting Individual Symptoms of Depression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1450</first_page>
						<last_page>1454</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2344</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/dumpala24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghe</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feilong</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge-Preserving Pluggable Modules for Multilingual Speech Translation Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>367</first_page>
						<last_page>371</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2346</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oguzhan</given_name>
<surname>Baser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaan</given_name>
<surname>Kale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep P.</given_name>
<surname>Chinchali</surname>
</person_name>
					</contributors>
					<titles><title>SecureSpectra: Safeguarding Digital Identity from Deep Fake Threats via Intelligent Signatures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1115</first_page>
						<last_page>1119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2349</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/baser24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Fusion of Music Theory-Inspired and Self-Supervised Representations for Improved Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3724</first_page>
						<last_page>3728</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2350</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shi24i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Max</given_name>
<surname>Morrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cameron</given_name>
<surname>Churchwell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Pruyne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bryan</given_name>
<surname>Pardo</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Grained and Interpretable Neural Speech Editing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>187</first_page>
						<last_page>191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2351</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/morrison24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>McNeill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rivka</given_name>
<surname>Levitan</surname>
</person_name>
					</contributors>
					<titles><title>Autoregressive cross-interlocutor attention scores meaningfully capture conversational dynamics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2945</first_page>
						<last_page>2949</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2352</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mcneill24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuning</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunlei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>TokSing: Singing Voice Synthesis based on Discrete Tokens</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2549</first_page>
						<last_page>2553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2360</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wu24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viyazonuo</given_name>
<surname>Terhiija</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name>
					</contributors>
					<titles><title>Voiced and voiceless laterals in Angami</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3704</first_page>
						<last_page>3708</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2361</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/terhiija24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linhan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinfa</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanjun</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhichao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wendi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Vec-Tok-VC+: Residual-enhanced Robust Zero-shot Voice Conversion with Progressive Constraints in a Dual-mode Training Strategy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2745</first_page>
						<last_page>2749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2362</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ma24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jehyun</given_name>
<surname>Kyung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Serin</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Multimodal Emotion Recognition through ASR Error Compensation and LLM Fine-Tuning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4683</first_page>
						<last_page>4687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2364</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kyung24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Slava</given_name>
<surname>Shechtman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avihu</given_name>
<surname>Dekel</surname>
</person_name>
					</contributors>
					<titles><title>Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4174</first_page>
						<last_page>4178</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2366</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/shechtman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junghun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ka Hyun</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoyoung</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>U</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Domain-Aware Data Selection for Speech Classification via Meta-Reweighting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>797</first_page>
						<last_page>801</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2368</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiali</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Elgaar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nidhi</given_name>
<surname>Vakil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadi</given_name>
<surname>Amiri</surname>
</person_name>
					</contributors>
					<titles><title>CogniVoice: Multimodal and Multilingual Fusion Networks for Mild Cognitive Impairment Assessment from Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4308</first_page>
						<last_page>4312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2370</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/cheng24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaowen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>QHM-GAN: Neural Vocoder based on Quasi-Harmonic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3889</first_page>
						<last_page>3893</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2371</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tian-Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu-Cheng</given_name>
<surname>Yin</surname>
</person_name>
					</contributors>
					<titles><title>Transmitted and Aggregated Self-Attention for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>227</first_page>
						<last_page>231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2374</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhang24q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiyu</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>On the Effectiveness of Acoustic BPE in Decoder-Only TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4134</first_page>
						<last_page>4138</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2375</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24qa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tahir</given_name>
<surname>Javed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janki</given_name>
<surname>Nawale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakshi</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eldho</given_name>
<surname>George</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaushal</given_name>
<surname>Bhogale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deovrat</given_name>
<surname>Mehendale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>LAHAJA: A Robust Multi-accent Benchmark for Evaluating Hindi ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2320</first_page>
						<last_page>2324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2376</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/javed24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexis</given_name>
<surname>DeMaere</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>van Rootselaar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangfang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robbin</given_name>
<surname>Gibb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claudia L. R.</given_name>
<surname>Gonzalez</surname>
</person_name>
					</contributors>
					<titles><title>On the relationship between speech production and vocabulary size in 3-5 year olds</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4194</first_page>
						<last_page>4198</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2377</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/demaere24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jens</given_name>
<surname>Heitkaemper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joe</given_name>
<surname>Caroselli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Howard</surname>
</person_name>
					</contributors>
					<titles><title>TfCleanformer: A streaming, array-agnostic, full- and sub-band modeling front-end for robust ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4473</first_page>
						<last_page>4477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2378</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/heitkaemper24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaeuk</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sohee</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Neural ATSM: Fully Neural Network-based Adaptive Time-Scale Modification Using Sentence-Specific Dynamic Control</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4903</first_page>
						<last_page>4907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2380</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lee24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sara</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gina-Anne</given_name>
<surname>Levow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Wright</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Influence of Stance-Taking on Conversational Timing of Task-Oriented Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3555</first_page>
						<last_page>3559</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2381</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ng24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Darshan</given_name>
<surname>Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>MULTI-CONVFORMER: Extending Conformer with Multiple Convolution Kernels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>232</first_page>
						<last_page>236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2384</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/prabhu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenhui</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition with Multi-level Acoustic and Semantic Information Extraction and Interaction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1060</first_page>
						<last_page>1064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2385</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/gao24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junwen</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangyuan</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Dong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Pre-trained Feature Fusion and Matching for Mild Cognitive Impairment Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>962</first_page>
						<last_page>966</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2386</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/duan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Kondo</surname>
</person_name>
					</contributors>
					<titles><title>FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>192</first_page>
						<last_page>196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2387</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kaneko24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sangwon</given_name>
<surname>Ryu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heejin</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary Geunbae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungseul</given_name>
<surname>Ok</surname>
</person_name>
					</contributors>
					<titles><title>Key-Element-Informed sLLM Tuning for Document Summarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1940</first_page>
						<last_page>1944</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2389</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ryu24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haechan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junho</given_name>
<surname>Myung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seoyoung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungpah</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongyeop</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juho</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>LearnerVoice: A Dataset of Non-Native English Learners’ Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2325</first_page>
						<last_page>2329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2392</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kim24v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kalvin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Hui</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsuan-Ming</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Holliday</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Speech Representations Still Struggle with African American Vernacular English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4643</first_page>
						<last_page>4647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2394</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaushal Santosh</given_name>
<surname>Bhogale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deovrat</given_name>
<surname>Mehendale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niharika</given_name>
<surname>Parasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sathish Kumar Reddy</given_name>
<surname>G</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tahir</given_name>
<surname>Javed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pratyush</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2519</first_page>
						<last_page>2523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2396</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/bhogale24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sai Harshitha</given_name>
<surname>Aluru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jhansi</given_name>
<surname>Mallela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>Post-Net: A linguistically inspired sequence-dependent transformed neural architecture for automatic syllable stress detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3340</first_page>
						<last_page>3344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2400</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/aluru24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Conor</given_name>
<surname>Atkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Wood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed Ali</given_name>
<surname>Kaafar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Asghar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nardine</given_name>
<surname>Basta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Kepkowski</surname>
</person_name>
					</contributors>
					<titles><title>ConvoCache: Smart Re-Use of Chatbot Responses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2950</first_page>
						<last_page>2954</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2402</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/atkins24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liisa</given_name>
<surname>Rätsep</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rasmus</given_name>
<surname>Lellep</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Fishel</surname>
</person_name>
					</contributors>
					<titles><title>Enabling Conversational Speech Synthesis using Noisy Spontaneous Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4923</first_page>
						<last_page>4927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2403</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ratsep24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jhansi</given_name>
<surname>Mallela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai Harshitha</given_name>
<surname>Aluru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>A comparative analysis of sequential models that integrate syllable dependency for automatic syllable stress detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3829</first_page>
						<last_page>3833</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2404</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mallela24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanjun</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danming</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>FreeV: Free Lunch For Vocoders Through Pseudo Inversed Mel Filter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3869</first_page>
						<last_page>3873</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2407</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lv24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiayan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shenghui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>MinSpeech: A Corpus of Southern Min Dialect for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2330</first_page>
						<last_page>2334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2414</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lin24m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chetan</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaishnavi</given_name>
<surname>Chandwanshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>A comparative study of the impact of voiceless alveolar and palato-alveolar sibilants in English on lip aperture and protrusion during VCV production</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3100</first_page>
						<last_page>3104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2415</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sharma24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srija</given_name>
<surname>Anand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Praveen</given_name>
<surname>Srinivasa Varadhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashwin</given_name>
<surname>Sankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giri</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for Practical Applications through Low-Effort Data Strategies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1200</first_page>
						<last_page>1204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2418</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/anand24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changli</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenyi</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianzhao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Can Large Language Models Understand Spatial Audio?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4149</first_page>
						<last_page>4153</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2419</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Praveen</given_name>
<surname>Srinivasa Varadhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashwin</given_name>
<surname>Sankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giri</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>Rasa: Building Expressive Speech Synthesis Systems for Indian Languages in Low-resource Settings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1830</first_page>
						<last_page>1834</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2421</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/srinivasavaradhan24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haotian</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Feedback Mechanism for Simultaneous Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>852</first_page>
						<last_page>856</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2426</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tan24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Tammen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Doclo</surname>
</person_name>
					</contributors>
					<titles><title>Array Geometry-Robust Attention-Based Neural Beamformer for Moving Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3345</first_page>
						<last_page>3349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2427</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tammen24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuankun</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongwei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaopeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuchen</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1390</first_page>
						<last_page>1394</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2428</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lu24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Darshan</given_name>
<surname>Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Omkar</given_name>
<surname>Nitsure</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Improving Self-supervised Pre-training using Accent-Specific Codebooks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2310</first_page>
						<last_page>2314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2438</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/prabhu24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Osman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel Z.</given_name>
<surname>Kaplan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamer</given_name>
<surname>Nadeem</surname>
</person_name>
					</contributors>
					<titles><title>SER Evals: In-domain and Out-of-domain benchmarking for speech emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1395</first_page>
						<last_page>1399</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2440</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/osman24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Socrates</given_name>
<surname>Vakirtzian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chara</given_name>
<surname>Tsoukala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Bompolas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Mouzou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivian</given_name>
<surname>Stamou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Dimakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stella</given_name>
<surname>Markantonatou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Ralli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Anastasopoulos</surname>
</person_name>
					</contributors>
					<titles><title>Speech Recognition for Greek Dialects: A Challenging Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3974</first_page>
						<last_page>3978</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2443</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/vakirtzian24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vivian G.</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>In search of structure and correspondence in intra-speaker trial-to-trial variability</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>452</first_page>
						<last_page>456</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2456</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ra_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martino</given_name>
<surname>Ciaperoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aristides</given_name>
<surname>Gionis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panagiotis</given_name>
<surname>Karras</surname>
</person_name>
					</contributors>
					<titles><title>Beam-search SIEVE for low-memory speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>272</first_page>
						<last_page>276</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2457</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ciaperoni24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joonas</given_name>
<surname>Kalda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Lebourdais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Séverin</given_name>
<surname>Baroudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name>
					</contributors>
					<titles><title>TalTech-IRIT-LIS Speaker and Language Diarization Systems for DISPLACE 2024</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1635</first_page>
						<last_page>1639</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2462</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kalda24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenxiong</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gongfan</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinchao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4878</first_page>
						<last_page>4882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2467</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tan24c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianhao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2145</first_page>
						<last_page>2149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2476</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/wang24ma_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenyu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shibiao</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Comprehensive Investigation on Speaker Augmentation for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2160</first_page>
						<last_page>2164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2478</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/zhou24f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vrunda N.</given_name>
<surname>Sukhadia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name>
					</contributors>
					<titles><title>Children’s Speech Recognition through Discrete Token Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>5143</first_page>
						<last_page>5147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2481</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sukhadia24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dehua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harold</given_name>
<surname>Chui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Luk</surname>
</person_name>
					</contributors>
					<titles><title>Learning Representation of Therapist Empathy in Counseling Conversation Using Siamese Hierarchical Attention Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1085</first_page>
						<last_page>1089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2483</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/tao24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Robust Voice Activity Detection using Locality-Sensitive Hashing and Residual Frequency-Temporal Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>242</first_page>
						<last_page>246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2489</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24sa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marianne</given_name>
<surname>de Heer Kloots</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Willem</given_name>
<surname>Zuidema</surname>
</person_name>
					</contributors>
					<titles><title>Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4593</first_page>
						<last_page>4597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2490</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/deheerkloots24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atharva</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miguel</given_name>
<surname>Couceiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>4628</first_page>
						<last_page>4632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2494</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/kulkarni24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Hönig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hartmut</given_name>
<surname>Lehfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hillemacher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Infusing Acoustic Pause Context into Text-Based Dementia Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1980</first_page>
						<last_page>1984</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2496</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/braun24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaolou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehua</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Fake Video Detection by Audio-Visual Consistency</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2935</first_page>
						<last_page>2939</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2497</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/li24ta_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heejin</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonjun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary Geunbae</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Feature Mixup for Balanced Multi-aspect Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>312</first_page>
						<last_page>316</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2498</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/do24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Darshan</given_name>
<surname>Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunita</given_name>
<surname>Sarawagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>SALSA: Speedy ASR-LLM Synchronous Aggregation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3485</first_page>
						<last_page>3489</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2499</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/mittal24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuma</given_name>
<surname>Okamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamato</given_name>
<surname>Ohtani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sota</given_name>
<surname>Shimizu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Challenge of Singing Voice Synthesis Using Only Text-To-Speech Corpus With FIRNet Source-Filter Neural Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1870</first_page>
						<last_page>1874</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2504</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/okamoto24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuepeng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Towards Expressive Zero-Shot Speech Synthesis with Hierarchical Prosody Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2300</first_page>
						<last_page>2304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2506</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jiang24d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehua</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1930</first_page>
						<last_page>1934</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2509</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chen24y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joyshree</given_name>
<surname>Chakraborty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leena</given_name>
<surname>Dihingia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Sinha</surname>
</person_name>
					</contributors>
					<titles><title>On Comparing Time- and Frequency-Domain Rhythm Measures in Classifying Assamese Dialects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2060</first_page>
						<last_page>2064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2513</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/chakraborty24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zening</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Emotion-Aware Speech Self-Supervised Representation Learning with Intensity Knowledge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3180</first_page>
						<last_page>3184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2518</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/liu24r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vasista Sai</given_name>
<surname>Lodagala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoutrik</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>F</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>All Ears: Building Self-Supervised Learning based ASR models for Indian Languages at scale</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3944</first_page>
						<last_page>3948</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2520</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/lodagala24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yo-Han</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wencke</given_name>
<surname>Liermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong-Seok</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hi</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong-Uk</given_name>
<surname>Bang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Joo</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Backchannel prediction, based on who, when and what</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3570</first_page>
						<last_page>3574</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2523</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/park24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoqin</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3200</first_page>
						<last_page>3204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2525</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/sun24e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bao Thang</given_name>
<surname>Ta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh Tu</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Hai</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huynh Thi</given_name>
<surname>Thanh Binh</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing No-Reference Speech Quality Assessment with Pairwise, Triplet Ranking Losses, and ASR Pretraining</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2700</first_page>
						<last_page>2704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2527</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ta24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kumar</given_name>
<surname>Neelabh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishnu</given_name>
<surname>Sreekumar</surname>
</person_name>
					</contributors>
					<titles><title>From Sound to Meaning in the Auditory Cortex: A Neuronal Representation and Classification Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>1490</first_page>
						<last_page>1494</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2531</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/neelabh24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roland</given_name>
<surname>Hartanto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>MSDET: Multitask Speaker Separation and Direction-of-Arrival Estimation Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2170</first_page>
						<last_page>2174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2537</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/hartanto24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bao Thang</given_name>
<surname>Ta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Hai</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huynh Thi</given_name>
<surname>Thanh Binh</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Non-Matching Reference Speech Quality Assessment through Dynamic Weight Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>3859</first_page>
						<last_page>3863</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2538</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/ta24b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Min</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>A Unit-based System and Dataset for Expressive Direct Speech-to-Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>382</first_page>
						<last_page>386</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2548</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/min24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bonian</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiyao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueheng</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meishan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>LLM-Driven Multimodal Opinion Expression Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>1</day>
						<year>2024</year>
					</publication_date>
					<pages>
						<first_page>2930</first_page>
						<last_page>2934</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2024-2550</doi>
						<resource>https://www.isca-archive.org/interspeech_2024/jia24_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>
