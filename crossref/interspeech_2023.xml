<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2023</doi_batch_id>
		<timestamp>1705398680284827</timestamp>
		<depositor>
			<depositor_name>Martin Cooke</depositor_name> 
			<email_address>m.cooke@ikerbasque.org</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant> 
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>INTERSPEECH 2023</conference_name>
				<conference_acronym>interspeech_2023</conference_acronym>
				<conference_date>20-24 August 2023</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>INTERSPEECH 2023</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2023</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2023</doi>
					<timestamp>1705398680284827</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2023/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianfang</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kikuo</given_name>
<surname>Maekawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukiko</given_name>
<surname>Nota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Hirata</surname>
</person_name>
					</contributors>
					<titles><title>Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1464</first_page>
						<last_page>1467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-36</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingxiao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian-Huang</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohua</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2068</first_page>
						<last_page>2072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-39</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frigyes Viktor</given_name>
<surname>Arthur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Péter</given_name>
<surname>Nagy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ádám</given_name>
<surname>Boncz</surname>
</person_name>
					</contributors>
					<titles><title>Towards Ultrasound Tongue Image prediction from EEG during speech production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1164</first_page>
						<last_page>1168</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-40</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/csapo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuxin</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaowei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1134</first_page>
						<last_page>1138</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-41</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cheng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Desh</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>GPU-accelerated Guided Source Separation for Meeting Transcription</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3507</first_page>
						<last_page>3511</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-42</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/raj23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jared</given_name>
<surname>Sharp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Faytak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hasutai Fei Xiong</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4718</first_page>
						<last_page>4722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-44</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sharp23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuxin</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wanshi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaowei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>690</first_page>
						<last_page>694</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-46</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cheng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seongyeon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bohyung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Hyun</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4319</first_page>
						<last_page>4323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-58</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/park23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edward</given_name>
<surname>Fish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Michieli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mete</given_name>
<surname>Ozay</surname>
</person_name>
					</contributors>
					<titles><title>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3232</first_page>
						<last_page>3236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-61</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fish23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>María Luisa</given_name>
<surname>García Lecumberri</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1798</first_page>
						<last_page>1802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-63</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cooke23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashwin</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>Mapping Phonemes to Acoustic Symbols and Codes Using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4758</first_page>
						<last_page>4762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-68</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuxin</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongsheng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3969</first_page>
						<last_page>3973</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-69</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Christ</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Regina</given_name>
<surname>Kushtanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Teynor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Speech-Based Classification of Defensive Communication: A Novel Dataset and Results</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2703</first_page>
						<last_page>2707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-76</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/amiriparian23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Max</given_name>
<surname>Bain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaesung</given_name>
<surname>Huh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tengda</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Zisserman</surname>
</person_name>
					</contributors>
					<titles><title>WhisperX: Time-Accurate Speech Transcription of Long-Form Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4489</first_page>
						<last_page>4493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-78</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bain23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chaoyue</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiakui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daoming</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baoxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian-Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qunyan</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Stable Speech Emotion Recognition with Head-k-Pooling Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>661</first_page>
						<last_page>665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-80</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ding23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaoxiang</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuya</given_name>
<surname>Matsumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Kudo</surname>
</person_name>
					</contributors>
					<titles><title>Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3759</first_page>
						<last_page>3763</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-85</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/dang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan Carlos</given_name>
<surname>Martínez-Sevilla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>María</given_name>
<surname>Alfaro-Contreras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose J.</given_name>
<surname>Valero-Mas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Calvo-Zaragoza</surname>
</person_name>
					</contributors>
					<titles><title>Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2793</first_page>
						<last_page>2797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-88</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/martinezsevilla23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Michieli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Peso Parada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mete</given_name>
<surname>Ozay</surname>
</person_name>
					</contributors>
					<titles><title>Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1628</first_page>
						<last_page>1632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-90</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/michieli23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuxin</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaowei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>C²A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>695</first_page>
						<last_page>699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-93</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cheng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2263</first_page>
						<last_page>2267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-97</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chu-Xiao</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Yi</given_name>
<surname>Leng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu-Jun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Fooling Speaker Identification Systems with Adversarial Background Music</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3142</first_page>
						<last_page>3146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-100</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zuo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nana</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2918</first_page>
						<last_page>2922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-101</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name>
					</contributors>
					<titles><title>pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1983</first_page>
						<last_page>1987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-105</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bredin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyungmin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeontaek</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mun-Hwan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Gee</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Attention Gate Between Capsules in Fully Capsule-Network Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>874</first_page>
						<last_page>878</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-106</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karolina</given_name>
<surname>Broś</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic cues to stress perception in Spanish – a mismatch negativity study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2598</first_page>
						<last_page>2602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-109</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bros23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenhao</given_name>
<surname>Shuai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaohua</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Gan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongqing</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5112</first_page>
						<last_page>5116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-113</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shuai23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Myunghun</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3924</first_page>
						<last_page>3928</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-116</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jung23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wang</given_name>
<surname>Chunhui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xing</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5401</first_page>
						<last_page>5405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-119</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chunhui23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Subba Reddy</given_name>
<surname>Oota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veeral</given_name>
<surname>Agarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mounika</given_name>
<surname>Marreddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manish</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raju</given_name>
<surname>Bapi</surname>
</person_name>
					</contributors>
					<titles><title>Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5167</first_page>
						<last_page>5171</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-121</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/oota23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujia</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaofei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank K.</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4883</first_page>
						<last_page>4887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-122</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xiao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1998</first_page>
						<last_page>2002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-125</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zeng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabhchand</given_name>
<surname>Bhati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Segmental SpeechCLIP: Utilizing Pretrained Image-text Models for Audio-Visual Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>431</first_page>
						<last_page>435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-135</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bhati23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maximillian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Pre-Finetuning for Few-Shot Emotional Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3602</first_page>
						<last_page>3606</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-136</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuchun</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Dependent Structure for Utterances in ASR Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3237</first_page>
						<last_page>3241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-137</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Multi-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2923</first_page>
						<last_page>2927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-142</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianzhao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yist Y.</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2908</first_page>
						<last_page>2912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-144</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanping</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael D.</given_name>
<surname>Tyler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Burnham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine T.</given_name>
<surname>Best</surname>
</person_name>
					</contributors>
					<titles><title>L2-Mandarin regional accent variability during Mandarin tone-word training facilitates English listeners’ subsequent tone categorizations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4259</first_page>
						<last_page>4263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-145</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kamil</given_name>
<surname>Deja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgi</given_name>
<surname>Tinchev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marta</given_name>
<surname>Czarnowska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marius</given_name>
<surname>Cotescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name>
					</contributors>
					<titles><title>Diffusion-based accent modelling in speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5516</first_page>
						<last_page>5520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-154</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deja23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Sarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Bottino</surname>
</person_name>
					</contributors>
					<titles><title>Description and analysis of the KPT system for NIST Language Recognition Evaluation 2022</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1933</first_page>
						<last_page>1937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-155</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sarni23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li-Fang</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Holliday</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1284</first_page>
						<last_page>1288</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-159</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lai23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xia</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phoenix</given_name>
<surname>Meadowlark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shivani</given_name>
<surname>Agrawal</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Parrotron for on-device speech-to-speech conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2033</first_page>
						<last_page>2037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-160</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rybakov23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shu-Chuan</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Fen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang-Li</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Model-assisted Lexical Tone Evaluation of three-year-old Chinese-speaking Children by also Considering Segment Production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3909</first_page>
						<last_page>3913</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-161</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tseng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Fujimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name>
					</contributors>
					<titles><title>Multi-channel separation of dynamic speech and sound events</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3749</first_page>
						<last_page>3753</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-164</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fujimura23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhewen</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongqing</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Time-frequency Domain Filter-and-sum Network for Multi-channel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3689</first_page>
						<last_page>3693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-165</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yufeng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Time-Domain Speech Enhancement for Robust Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4913</first_page>
						<last_page>4917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-167</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sara</given_name>
<surname>Papi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Turchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name>
					</contributors>
					<titles><title>AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3974</first_page>
						<last_page>3978</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-170</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/papi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Turk</surname>
</person_name>
					</contributors>
					<titles><title>Optimal control of speech with context-dependent articulatory targets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4209</first_page>
						<last_page>4213</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-172</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/elie23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eun Jung</given_name>
<surname>Yeo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>166</first_page>
						<last_page>170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-173</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yeo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Scheck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1174</first_page>
						<last_page>1178</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-174</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/scheck23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Bondi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shabnam</given_name>
<surname>Ghaffarzadegan</surname>
</person_name>
					</contributors>
					<titles><title>Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>326</first_page>
						<last_page>330</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-176</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mayank Kumar</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Onoe</surname>
</person_name>
					</contributors>
					<titles><title>Iteratively Improving Speech Recognition and Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>206</first_page>
						<last_page>210</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-177</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/singh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jerome R.</given_name>
<surname>Bellegarda</surname>
</person_name>
					</contributors>
					<titles><title>Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1444</first_page>
						<last_page>1448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-180</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bellegarda23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenyu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyan</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3417</first_page>
						<last_page>3421</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-183</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deokjun</given_name>
<surname>Eom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woo Hyun</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyung-Rae</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Background-Sound Controllable Voice Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1698</first_page>
						<last_page>1702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-185</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/eom23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohuai</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tong</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiqing</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hua</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijian</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piao</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shenyi</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3894</first_page>
						<last_page>3898</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-186</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/le23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeremy H. M.</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huayun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy F.</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Distilling knowledge from Gaussian process teacher to neural network student</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>426</first_page>
						<last_page>430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-190</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhita</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnab</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Siegert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stober</surname>
</person_name>
					</contributors>
					<titles><title>Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2093</first_page>
						<last_page>2097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-191</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ghosh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeongsoo</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong Man</given_name>
<surname>Ro</surname>
</person_name>
					</contributors>
					<titles><title>Intelligible Lip-to-Speech Synthesis with Speech Units</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4349</first_page>
						<last_page>4353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-194</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/choi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Cappellazzo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Falavigna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>735</first_page>
						<last_page>739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-198</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cappellazzo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ramin</given_name>
<surname>Hedeshy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Menges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steffen</given_name>
<surname>Staab</surname>
</person_name>
					</contributors>
					<titles><title>CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1553</first_page>
						<last_page>1557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-201</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hedeshy23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Toki</given_name>
<surname>Sugiura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiromitsu</given_name>
<surname>Nishizaki</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation Using Improved Differentiable Automatic Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5411</first_page>
						<last_page>5415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-202</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sugiura23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Fagniart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Véronique</given_name>
<surname>Delvaux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brigitte</given_name>
<surname>Charlier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernard</given_name>
<surname>Harmegnies</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Huberlant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myriam</given_name>
<surname>Piccaluga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathy</given_name>
<surname>Huet</surname>
</person_name>
					</contributors>
					<titles><title>Nasal vowel production and grammatical processing in French-speaking children with cochlear implants and normal-hearing peers.</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4249</first_page>
						<last_page>4253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-203</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fagniart23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Plaquet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name>
					</contributors>
					<titles><title>Powerset multi-class cross entropy loss for neural speaker diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3222</first_page>
						<last_page>3226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-205</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/plaquet23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Míša Michaela</given_name>
<surname>Hejná</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adèle</given_name>
<surname>Jatteau</surname>
</person_name>
					</contributors>
					<titles><title>Aberystwyth English Pre-aspiration in Apparent Time</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3532</first_page>
						<last_page>3536</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-206</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hejna23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hector E.</given_name>
<surname>Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guy J.</given_name>
<surname>Brown</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5451</first_page>
						<last_page>5455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-209</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/romero23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre-Michel</given_name>
<surname>Bousquet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mickael</given_name>
<surname>Rouvier</surname>
</person_name>
					</contributors>
					<titles><title>Improving training datasets for resource-constrained speaker recognition neural networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3167</first_page>
						<last_page>3171</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-210</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bousquet23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Schwarz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maarten</given_name>
<surname>Van Segbroeck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammed</given_name>
<surname>Hethnawi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Predictive ASR for Latency Reduction in Voice Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>745</first_page>
						<last_page>749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-211</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/schwarz23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyungmin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haeri</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sichen</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhwan</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngho</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>869</first_page>
						<last_page>873</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-213</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunfeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhonghao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinghua</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>S2CD: Self-heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2288</first_page>
						<last_page>2292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-215</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wei23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiuxin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2488</first_page>
						<last_page>2492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-218</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Niizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasunori</given_name>
<surname>Ohishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noboru</given_name>
<surname>Harada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunio</given_name>
<surname>Kashino</surname>
</person_name>
					</contributors>
					<titles><title>Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1294</first_page>
						<last_page>1298</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-221</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/niizumi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoran</given_name>
<surname>Cvetkovic</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1533</first_page>
						<last_page>1537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-222</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yue23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tian-Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hai-Bo</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhi-Hao</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song-Lu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu-Cheng</given_name>
<surname>Yin</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>914</first_page>
						<last_page>918</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-223</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Leglaive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Girin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Alameda-Pineda</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised speech enhancement with deep dynamical generative speech and noise models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5102</first_page>
						<last_page>5106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-232</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>José</given_name>
<surname>Egas-López</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronika</given_name>
<surname>Svindt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judit</given_name>
<surname>Bóna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ildikó</given_name>
<surname>Hoffmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name>
					</contributors>
					<titles><title>Automated Multiple Sclerosis Screening Based on Encoded Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3003</first_page>
						<last_page>3007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-234</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/egaslopez23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roger K.</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name>
					</contributors>
					<titles><title>Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>401</first_page>
						<last_page>405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-235</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/moore23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Ludusan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marin</given_name>
<surname>Schröer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martina</given_name>
<surname>Rossi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name>
					</contributors>
					<titles><title>The co-use of laughter and head gestures across speech styles</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3592</first_page>
						<last_page>3596</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-240</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ludusan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yooyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Greenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliot</given_name>
<surname>Godard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asad A.</given_name>
<surname>Butt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elliot</given_name>
<surname>Singer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trang</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Mason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>Reynolds</surname>
</person_name>
					</contributors>
					<titles><title>The 2022 NIST Language Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1928</first_page>
						<last_page>1932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-241</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Cappellazzo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muqiao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Falavigna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2953</first_page>
						<last_page>2957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-242</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cappellazzo23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chirag</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Surya</given_name>
<surname>Koppisetti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ben</given_name>
<surname>Colman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali</given_name>
<surname>Shahriyari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaurav</given_name>
<surname>Bharaj</surname>
</person_name>
					</contributors>
					<titles><title>Towards Attention-based Contrastive Learning for Audio Spoof Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2758</first_page>
						<last_page>2762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-245</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/goel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rustem</given_name>
<surname>Yeshpanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saida</given_name>
<surname>Mussakhojayeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5521</first_page>
						<last_page>5525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-249</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yeshpanov23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hai</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huayi</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Emotional Voice Conversion with Semi-Supervised Generative Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2278</first_page>
						<last_page>2282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-251</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guo</given_name>
<surname>Yifan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suo</given_name>
<surname>Hongbin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wan</given_name>
<surname>Yulong</surname>
</person_name>
					</contributors>
					<titles><title>Multi-channel multi-speaker transformer for speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4918</first_page>
						<last_page>4922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-257</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yifan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dingyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoqin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shushan</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yumei</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>A Binary Keyword Spotting System with Error-Diffusion Based Feature Binarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1424</first_page>
						<last_page>1428</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-258</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyun</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linhao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenlin</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3322</first_page>
						<last_page>3326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-262</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zitong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5386</first_page>
						<last_page>5390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-263</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Sarni</surname>
</person_name>
					</contributors>
					<titles><title>From adaptive score normalization to adaptive data normalization for speaker verification systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5296</first_page>
						<last_page>5300</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-266</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cumani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ramon</given_name>
<surname>Sanabria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>406</first_page>
						<last_page>410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-268</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sanabria23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aoi</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name>
					</contributors>
					<titles><title>Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5346</first_page>
						<last_page>5350</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-270</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ito23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hendrik</given_name>
<surname>Schröter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Rosenkranz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto N.</given_name>
<surname>Escalante-B.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name>
					</contributors>
					<titles><title>Deep Multi-Frame Filtering for Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3869</first_page>
						<last_page>3873</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-272</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/schroter23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shenjie</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaokai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keke</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4538</first_page>
						<last_page>4542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-274</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keke</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaokai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>894</first_page>
						<last_page>898</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-275</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiban</given_name>
<surname>Adhikary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keith</given_name>
<surname>Vertanen</surname>
</person_name>
					</contributors>
					<titles><title>Language Model Personalization for Improved Touchscreen Typing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1344</first_page>
						<last_page>1348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-276</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/adhikary23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heerin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-won</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungmin</given_name>
<surname>So</surname>
</person_name>
					</contributors>
					<titles><title>Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2938</first_page>
						<last_page>2942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-277</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaibo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lili</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanru</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Demin</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Obstructive Sleep Apnea Detection using Pre-trained Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1139</first_page>
						<last_page>1143</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-278</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nursadul</given_name>
<surname>Mamun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H. L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>CFTNet: Complex-valued Frequency Transformation Network for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>809</first_page>
						<last_page>813</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-280</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mamun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Subba Reddy</given_name>
<surname>Oota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Trouvain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederic</given_name>
<surname>Alexandre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Hinaut</surname>
</person_name>
					</contributors>
					<titles><title>MEG Encoding using Word Context Semantics in Listening Stories</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5152</first_page>
						<last_page>5156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-282</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/oota23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingyu</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenneth</given_name>
<surname>Church</surname>
</person_name>
					</contributors>
					<titles><title>Improved Contextualized Speech Representations for Tonal Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4513</first_page>
						<last_page>4517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-283</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yuan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Otani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun</given_name>
<surname>Sawada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hidefumi</given_name>
<surname>Ohmura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouichi</given_name>
<surname>Katsurada</surname>
</person_name>
					</contributors>
					<titles><title>Speech Synthesis from Articulatory Movements Recorded by Real-time MRI</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>127</first_page>
						<last_page>131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-286</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/otani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haozhe</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name>
					</contributors>
					<titles><title>Prosody Modeling with 3D Visual Information for Expressive Video Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4863</first_page>
						<last_page>4867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-289</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3607</first_page>
						<last_page>3611</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-293</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Franka</given_name>
<surname>Zebe</surname>
</person_name>
					</contributors>
					<titles><title>Increasing aspiration of word-medial fortis plosives in Swiss Standard German</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1828</first_page>
						<last_page>1832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-295</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zebe23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qifei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>FTA-net: A Frequency and Time Attention Network for Speech Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1723</first_page>
						<last_page>1727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-296</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Cross-Domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2313</first_page>
						<last_page>2317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-297</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqin</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Geng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyue</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1893</first_page>
						<last_page>1897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-300</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zi</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1753</first_page>
						<last_page>1757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-301</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/geng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Winstead</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Iftekhar</given_name>
<surname>Tanveer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang Janet</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seye</given_name>
<surname>Ojumu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosie</given_name>
<surname>Jones</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight and Efficient Spoken Language Identification of Long-form Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>496</first_page>
						<last_page>500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-304</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Angus</given_name>
<surname>Addlesee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Damonte</surname>
</person_name>
					</contributors>
					<titles><title>Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1224</first_page>
						<last_page>1228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-307</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/addlesee23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Kondratenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolay</given_name>
<surname>Karpov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Sokolov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Savushkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Kutuzov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fyodor</given_name>
<surname>Minkin</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid Dataset for Speech Emotion Recognition in Russian Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4548</first_page>
						<last_page>4552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-311</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kondratenko23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina R</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan L</given_name>
<surname>Preston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asif</given_name>
<surname>Salekin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harshit</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>McAllister</surname>
</person_name>
					</contributors>
					<titles><title>Classifying Rhoticity of /ɹ/ in Speech Sound Disorder using Age-and-Sex Normalized Formants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4563</first_page>
						<last_page>4567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-312</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/benway23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Sostarics</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Cole</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>97</first_page>
						<last_page>101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-315</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sostarics23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsukasa</given_name>
<surname>Yoshinaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Arai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akiyoshi</given_name>
<surname>Iida</surname>
</person_name>
					</contributors>
					<titles><title>A Relationship Between Vocal Fold Vibration and Droplet Production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4199</first_page>
						<last_page>4203</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-317</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yoshinaga23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Taherian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-input Multi-output Complex Spectral Mapping for Speaker Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1070</first_page>
						<last_page>1074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-318</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/taherian23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Toshiki</given_name>
<surname>Muromachi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinobu</given_name>
<surname>Kano</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness Using Dynamic-Prompt-Tune</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2638</first_page>
						<last_page>2642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-320</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/muromachi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Use of Speech Impairment Severity for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2328</first_page>
						<last_page>2332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-322</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/geng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Cahyawijaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Holy</given_name>
<surname>Lovenia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Willy</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Frieske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascale</given_name>
<surname>Fung</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3352</first_page>
						<last_page>3356</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-327</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Background-aware Modeling for Weakly Supervised Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1199</first_page>
						<last_page>1203</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-330</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juqiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailing</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hua</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Second language identification of Vietnamese tones by native Mandarin learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4244</first_page>
						<last_page>4248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-334</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyungshin</given_name>
<surname>Ryu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>959</first_page>
						<last_page>963</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-337</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ryu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haixin</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinlong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaowei</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>A Mask Free Neural Network for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2468</first_page>
						<last_page>2472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-339</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liao</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianwei</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yandong</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2578</first_page>
						<last_page>2582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-340</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/qu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zihao</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>246</first_page>
						<last_page>250</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-341</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gholamreza</given_name>
<surname>Haffari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Shareghi</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Pre-trained Audio Encoders in the Low-Resource Condition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1498</first_page>
						<last_page>1502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-343</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lijian</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qirong</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Dong</surname>
</person_name>
					</contributors>
					<titles><title>Joint-Former: Jointly Regularized and Locally Down-sampled Conformer for Semi-supervised Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2753</first_page>
						<last_page>2757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-344</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>286</first_page>
						<last_page>290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-350</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/choi23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nik</given_name>
<surname>Vaessen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David A.</given_name>
<surname>van Leeuwen</surname>
</person_name>
					</contributors>
					<titles><title>Towards Multi-task Learning of Speech and Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4898</first_page>
						<last_page>4902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-353</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vaessen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xionghu</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minjie</given_name>
<surname>Cai</surname>
</person_name>
					</contributors>
					<titles><title>Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2728</first_page>
						<last_page>2732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-358</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ren23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingru</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianghu</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Ao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2988</first_page>
						<last_page>2992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-359</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanbyul</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghyun</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seolki</given_name>
<surname>Baek</surname>
</person_name>
					</contributors>
					<titles><title>Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1653</first_page>
						<last_page>1657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-361</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenpeng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>616</first_page>
						<last_page>620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-363</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuong Tu</given_name>
<surname>Huu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet Thanh</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thi Thu Trang</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thai Lai</given_name>
<surname>Dao</surname>
</person_name>
					</contributors>
					<titles><title>Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1014</first_page>
						<last_page>1018</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-364</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Andreev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Babaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Azat</given_name>
<surname>Saginbaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Shchekotov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aibek</given_name>
<surname>Alanov</surname>
</person_name>
					</contributors>
					<titles><title>Iterative autoregression: a novel trick to improve your low-latency speech enhancement model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2448</first_page>
						<last_page>2452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-365</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/andreev23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anastasiia</given_name>
<surname>Iashchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Andreev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Shchekotov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Babaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Vetrov</surname>
</person_name>
					</contributors>
					<titles><title>UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4294</first_page>
						<last_page>4298</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-367</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/iashchenko23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenbin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjay</given_name>
<surname>Jha</surname>
</person_name>
					</contributors>
					<titles><title>Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4454</first_page>
						<last_page>4458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-368</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sara</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keitaro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeo</given_name>
<surname>Morishima</surname>
</person_name>
					</contributors>
					<titles><title>Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3397</first_page>
						<last_page>3401</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-370</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kashiwagi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengfang</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4014</first_page>
						<last_page>4018</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-371</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinicius</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaojun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name>
					</contributors>
					<titles><title>Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5366</first_page>
						<last_page>5370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-372</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ribeiro23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenbin</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>UnSE: Unsupervised Speech Enhancement Using Optimal Transport</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4029</first_page>
						<last_page>4033</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-378</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuju</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohai</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling the Contribution of Non-native Speech in Automated Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>954</first_page>
						<last_page>958</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-380</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Le Blouch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Gengembre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Lolive</surname>
</person_name>
					</contributors>
					<titles><title>An extension of disentanglement metrics and its application to voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2878</first_page>
						<last_page>2882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-383</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruan</given_name>
<surname>van der Merwe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>441</first_page>
						<last_page>445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-385</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vandermerwe23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jizhong</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>72</first_page>
						<last_page>76</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-387</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wannaphong</given_name>
<surname>Phatthiyaphaibun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chompakorn</given_name>
<surname>Chaksangchaichot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanawin</given_name>
<surname>Rakthammanon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarana</given_name>
<surname>Nutanong</surname>
</person_name>
					</contributors>
					<titles><title>Crowdsourced Data Validation for ASR Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>551</first_page>
						<last_page>555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-389</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/phatthiyaphaibun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xian</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoneng</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3247</first_page>
						<last_page>3251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-390</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yongmao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heyang</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanzhao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruixiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caixia</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4444</first_page>
						<last_page>4448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-391</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peiwen</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shanshan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zishan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yougen</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taotao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honggang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>A Method of Audio-Visual Person Verification by Mining Connections between Time Series</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3227</first_page>
						<last_page>3231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-394</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia-Hao</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Hung</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition using Decomposed Speech via Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4553</first_page>
						<last_page>4557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-396</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hsu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiadi</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengdong</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhendong</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1943</first_page>
						<last_page>1947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-402</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyunghyun</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Joint Speech-Text Representations Without Alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1354</first_page>
						<last_page>1358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-403</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/peyser23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guolin</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mike</given_name>
<surname>Brookes</surname>
</person_name>
					</contributors>
					<titles><title>Epoch-Based Spectrum Estimation for Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4274</first_page>
						<last_page>4278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-407</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gunason23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Kawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Plata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Syga</surname>
</person_name>
					</contributors>
					<titles><title>Defense Against Adversarial Attacks on Audio DeepFake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5276</first_page>
						<last_page>5280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-409</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kawa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zechen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4254</first_page>
						<last_page>4258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-412</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziping</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haishuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2433</first_page>
						<last_page>2437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-413</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhao23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianjun</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dingding</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junfeng</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Auditory Attention Decoding using Speaker Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5172</first_page>
						<last_page>5176</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-414</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/qiu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng-Han</given_name>
<surname>Chiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ping</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5551</first_page>
						<last_page>5555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-416</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chiang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Baas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Voice Conversion With Just Nearest Neighbors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2053</first_page>
						<last_page>2057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-419</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/baas23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minseung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sein</given_name>
<surname>Cheong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>DNN-based Parameter Estimation for MVDR Beamforming and Post-filtering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3879</first_page>
						<last_page>3883</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-420</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christiaan</given_name>
<surname>Jacobs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathanaël Carraz</given_name>
<surname>Rakotonirina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Everlyn Asiko</given_name>
<surname>Chimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bruce A.</given_name>
<surname>Bassett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>436</first_page>
						<last_page>440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-421</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jacobs23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minglun</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feilong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1364</first_page>
						<last_page>1368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-423</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/han23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arka</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Udit</given_name>
<surname>Satija</surname>
</person_name>
					</contributors>
					<titles><title>AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5431</first_page>
						<last_page>5435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-428</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/roy23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orian</given_name>
<surname>Sharoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roee</given_name>
<surname>Shenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name>
					</contributors>
					<titles><title>SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5566</first_page>
						<last_page>5570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-430</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sharoni23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William N.</given_name>
<surname>Havard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaya</given_name>
<surname>Sy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Camila</given_name>
<surname>Scaff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Loann</given_name>
<surname>Peurey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>〈'〉 in Tsimane': a Preliminary Investigation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1813</first_page>
						<last_page>1817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-431</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/havard23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Diana</given_name>
<surname>Geneva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgi</given_name>
<surname>Shopov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kostadin</given_name>
<surname>Garov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Todorova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Gerdjikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stoyan</given_name>
<surname>Mihov</surname>
</person_name>
					</contributors>
					<titles><title>Accentor: An Explicit Lexical Stress Model for TTS Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4848</first_page>
						<last_page>4852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-433</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/geneva23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Swiatkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikolaj</given_name>
<surname>Babianski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Lumban Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravichander</given_name>
<surname>Vipperla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Pollet</surname>
</person_name>
					</contributors>
					<titles><title>Cross-lingual Prosody Transfer for Expressive Machine Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4838</first_page>
						<last_page>4842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-437</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/swiatkowski23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maureen</given_name>
<surname>de Seyssel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Lavechin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadrien</given_name>
<surname>Titeux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arthur</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gwendal</given_name>
<surname>Virlet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea Santos</given_name>
<surname>Revilla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Wisniewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Ludusan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>ProsAudit, a prosodic benchmark for self-supervised speech models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2963</first_page>
						<last_page>2967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-438</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deseyssel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Swiatkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikolaj</given_name>
<surname>Babianski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Coccia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Lumban Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravichander</given_name>
<surname>Vipperla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viacheslav</given_name>
<surname>Klimkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Pollet</surname>
</person_name>
					</contributors>
					<titles><title>Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5546</first_page>
						<last_page>5550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-441</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/swiatkowski23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Wormald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Foulkes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Finnian</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David van der</given_name>
<surname>Vloed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Poppy</given_name>
<surname>Welch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenzi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>591</first_page>
						<last_page>595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-443</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hughes23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linh</given_name>
<surname>The Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thinh</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dat Quoc</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5506</first_page>
						<last_page>5510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-444</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/thenguyen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michele</given_name>
<surname>Panariello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Vocoder drift in x-vector–based speaker anonymization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2863</first_page>
						<last_page>2867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-448</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/panariello23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanhan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaochen</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Robust Training for Speaker Verification against Noisy Labels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3192</first_page>
						<last_page>3196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-452</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mosner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan "Honza"</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speaker Verification with Self-Pretrained Transformer Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5361</first_page>
						<last_page>5365</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-453</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/peng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Basmah</given_name>
<surname>Alsenani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanaya</given_name>
<surname>Guha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>651</first_page>
						<last_page>655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-454</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/alsenani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vasily</given_name>
<surname>Zadorozhnyy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhito</given_name>
<surname>Koishida</surname>
</person_name>
					</contributors>
					<titles><title>SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2463</first_page>
						<last_page>2467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-456</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zadorozhnyy23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy L.</given_name>
<surname>McElwain</surname>
</person_name>
					</contributors>
					<titles><title>Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1035</first_page>
						<last_page>1039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-460</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2933</first_page>
						<last_page>2937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-461</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Fernandez-Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honglie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Haliassos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>SparseVSR: Lightweight and Noise Robust Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1603</first_page>
						<last_page>1607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-462</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fernandezlopez23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Reiss</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2413</first_page>
						<last_page>2417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-463</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/luo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Multi-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2318</first_page>
						<last_page>2322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-464</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wagner23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pin-Jie</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammed</given_name>
<surname>Saeed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ernie</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Merel</given_name>
<surname>Scholman</surname>
</person_name>
					</contributors>
					<titles><title>Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3954</first_page>
						<last_page>3958</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-466</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gebhard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and automatic prediction of exertion from speech: Contrasting objective and subjective measures collected while running</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4144</first_page>
						<last_page>4148</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-470</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/triantafyllopoulos23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wanyue</given_name>
<surname>Zhai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Wav2ToBI: a new approach to automatic ToBI transcription</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2748</first_page>
						<last_page>2752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-477</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhai23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kumari</given_name>
<surname>Nishu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsik</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devang</given_name>
<surname>Naik</surname>
</person_name>
					</contributors>
					<titles><title>Matching Latent Encoding for Audio-Text based Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1613</first_page>
						<last_page>1617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-478</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nishu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Françoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>Mixture-of-Expert Conformer for Streaming Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3327</first_page>
						<last_page>3331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-480</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdul Hameed</given_name>
<surname>Azeemi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ihsan Ayyub</given_name>
<surname>Qazi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agha Ali</given_name>
<surname>Raza</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2773</first_page>
						<last_page>2777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-481</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/azeemi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adhiraj</given_name>
<surname>Banerjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vipul</given_name>
<surname>Arora</surname>
</person_name>
					</contributors>
					<titles><title>Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1478</first_page>
						<last_page>1482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-483</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/banerjee23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Factual Consistency Oriented Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>236</first_page>
						<last_page>240</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-485</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kanda23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Denis</given_name>
<surname>Filimonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prabhat</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Speech-to-Confusion Network Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4099</first_page>
						<last_page>4103</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-486</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/filimonov23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Audio-Visual Speech Recognition with Alignment Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1598</first_page>
						<last_page>1602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-487</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hokuto</given_name>
<surname>Munakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryu</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Komatani</surname>
</person_name>
					</contributors>
					<titles><title>Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1688</first_page>
						<last_page>1692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-488</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/munakata23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2778</first_page>
						<last_page>2782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-491</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Midia</given_name>
<surname>Yousefi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongmei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3502</first_page>
						<last_page>3506</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-494</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yousefi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Shulby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Korolev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaldo Candido</given_name>
<surname>Junior</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anderson da Silva</given_name>
<surname>Soares</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Aluísio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moacir Antonelli</given_name>
<surname>Ponti</surname>
</person_name>
					</contributors>
					<titles><title>ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1244</first_page>
						<last_page>1248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-496</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/casanova23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Ioannides</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Owen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Fletcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viktor</given_name>
<surname>Rozgic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1853</first_page>
						<last_page>1857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-497</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ioannides23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hye-Sook</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels Based on Difference Thresholds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2613</first_page>
						<last_page>2617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-498</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/park23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruili</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1279</first_page>
						<last_page>1283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-503</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>So-Jeong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>TaeHo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4289</first_page>
						<last_page>4293</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-506</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/choi23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhi-Hao</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian-Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Fang</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song-Lu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu-Cheng</given_name>
<surname>Yin</surname>
</person_name>
					</contributors>
					<titles><title>InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>566</first_page>
						<last_page>570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-509</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lai23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryu</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Hara</surname>
</person_name>
					</contributors>
					<titles><title>Meta-domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-sentiment Predictions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2428</first_page>
						<last_page>2432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-510</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weixin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guochen</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenzhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1055</first_page>
						<last_page>1059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-514</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xue</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changchun</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianhong</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Monaural Speech Separation Method Based on Recurrent Attention with Parallel Branches</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3794</first_page>
						<last_page>3798</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-518</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuangqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianke</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhong</given_name>
<surname>Leng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Togneri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijian</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piao</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shenyi</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingjian</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Two-stage Progressive Neural Network for Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>795</first_page>
						<last_page>799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-521</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1468</first_page>
						<last_page>1472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-523</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueke</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tangpeng</given_name>
<surname>Dan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueguo</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5421</first_page>
						<last_page>5425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-528</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wei23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jungbae</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungtaek</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>Addressing Cold Start Problem for End-to-end Automatic Speech Scoring</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>994</first_page>
						<last_page>998</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-533</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/park23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jungil</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihoon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beomjeong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeongmin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dohee</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangjin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4374</first_page>
						<last_page>4378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-534</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huali</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianming</given_name>
<surname>Bei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nengheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Effects of hearing loss and amplification on Mandarin consonant perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2588</first_page>
						<last_page>2592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-536</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linping</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawei</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijian</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piao</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shenyi</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sixing</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ferdous</given_name>
<surname>Sohel</surname>
</person_name>
					</contributors>
					<titles><title>An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>800</first_page>
						<last_page>803</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-537</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feilong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minglun</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3447</first_page>
						<last_page>3451</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-539</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Satwinder</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruili</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Self-training Approach for Low-resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1588</first_page>
						<last_page>1592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-540</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/singh23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongxing</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2788</first_page>
						<last_page>2792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-542</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hailun</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunan</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Learning Local to Global Feature Aggregation for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1908</first_page>
						<last_page>1912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-543</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fu-An</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tien-Hong</given_name>
<surname>Lo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tzu-I</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao-Ting</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berlin</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>A Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>974</first_page>
						<last_page>978</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-550</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chun-Wei</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name>
					</contributors>
					<titles><title>Differentially Private Adapters for Parameter Efficient Acoustic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>839</first_page>
						<last_page>843</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-551</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ho23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutao</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2223</first_page>
						<last_page>2227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-552</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cui23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingjun</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanqiang</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianxiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizhong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5526</first_page>
						<last_page>5530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-553</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fanhui</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nengheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianren</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan W. H.</given_name>
<surname>Schnupp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Cochlear-implant Listeners Listening to Cochlear-implant Simulated Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4988</first_page>
						<last_page>4992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-554</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kong23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keulbit</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namhyun</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2673</first_page>
						<last_page>2677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-555</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuncai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haiqing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>GL-SSD: Global and Local Speech Style Disentanglement by vector quantization for robust sentence boundary detection in speech stream</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5042</first_page>
						<last_page>5046</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-558</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yashish M</given_name>
<surname>Siriwardena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shihab</given_name>
<surname>Shamma</surname>
</person_name>
					</contributors>
					<titles><title>Learning to Compute the Articulatory Representations of Speech with the MIRRORNET</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5137</first_page>
						<last_page>5141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-562</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/siriwardena23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Penghui</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxi</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wanlei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>271</first_page>
						<last_page>275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-563</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keita</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-level Timestamp Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2913</first_page>
						<last_page>2917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-564</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/makishima23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dacheng</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanxin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>TridentSE: Guiding Speech Enhancement with 32 Global Tokens</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3839</first_page>
						<last_page>3843</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-565</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suo</given_name>
<surname>Hongbin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulong</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5381</first_page>
						<last_page>5385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-572</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yidi</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruijie</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Target Active Speaker Detection with Audio-visual Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3152</first_page>
						<last_page>3156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-574</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhas</given_name>
<surname>BN</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Rajtmajer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saeed</given_name>
<surname>Abdullah</surname>
</person_name>
					</contributors>
					<titles><title>Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>346</first_page>
						<last_page>350</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-575</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bn23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Okamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanta</given_name>
<surname>Shimonishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Imoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kota</given_name>
<surname>Dohi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohei</given_name>
<surname>Kawaguchi</surname>
</person_name>
					</contributors>
					<titles><title>CAPTDURE: Captioned Sound Dataset of Single Sources</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1683</first_page>
						<last_page>1687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-577</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/okamoto23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fuma</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mao</given_name>
<surname>Saeki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Fujie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoichi</given_name>
<surname>Matsuyama</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Turn-Taking Model Using Visual Cues for End-of-Utterance Prediction in Spoken Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2658</first_page>
						<last_page>2662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-578</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kurata23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojun</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengchen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Transductive Feature Space Regularization for Few-shot Bioacoustic Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>571</first_page>
						<last_page>575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-579</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nguyen</given_name>
<surname>Binh Thien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukoh</given_name>
<surname>Wakabayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuting</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenta</given_name>
<surname>Iwai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanobu</given_name>
<surname>Nishiura</surname>
</person_name>
					</contributors>
					<titles><title>Weighted Von Mises Distribution-based Loss Function for Real-time STFT Phase Reconstruction Using DNN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3864</first_page>
						<last_page>3868</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-580</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/binhthien23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Factorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3342</first_page>
						<last_page>3346</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-583</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yukang</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaitao</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoguang</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiqiang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luna</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongsheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linli</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lili</given_name>
<surname>Qiu</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Word-Level Pronunciation Assessment with MASK Pre-training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>969</first_page>
						<last_page>973</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-585</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuju</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohai</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>949</first_page>
						<last_page>953</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-587</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng-Hung</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Preference-based training framework for automatic speech quality assessment using deep neural network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>546</first_page>
						<last_page>550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-589</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kanta</given_name>
<surname>Shimonishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kota</given_name>
<surname>Dohi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohei</given_name>
<surname>Kawaguchi</surname>
</person_name>
					</contributors>
					<titles><title>Anomalous Sound Detection Based on Sound Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2733</first_page>
						<last_page>2737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-591</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shimonishi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tzu-Han Zoe</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Calamia</surname>
</person_name>
					</contributors>
					<titles><title>Computational modeling of auditory brainstem responses derived from modified speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4214</first_page>
						<last_page>4218</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-593</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cheng23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yong-Hyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namhyun</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3964</first_page>
						<last_page>3968</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-597</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchun</given_name>
<surname>Shu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingyun</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5047</first_page>
						<last_page>5051</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-598</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Accelerating Transducers through Adjacent Token Merging</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1379</first_page>
						<last_page>1383</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-599</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangze</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>CASA-ASR: Context-Aware Speaker-Attributed ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>411</first_page>
						<last_page>415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-601</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chan-yeong</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-seo</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5271</first_page>
						<last_page>5275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-605</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/heo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongya</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kainan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanzhe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingbo</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuping</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5476</first_page>
						<last_page>5480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-612</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jia23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xizhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jizhong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3774</first_page>
						<last_page>3778</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-613</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhisheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3307</first_page>
						<last_page>3311</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-614</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zheng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jisung</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haram</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myungwoo</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating L2 Phonemes Using Articulatory Features for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>576</first_page>
						<last_page>580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-615</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1384</first_page>
						<last_page>1388</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-617</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/feng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wooseok</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Joon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin Sob</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongwon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungjin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Won</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2128</first_page>
						<last_page>2132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-619</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruicong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1144</first_page>
						<last_page>1148</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-620</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Language-universal Phonetic Encoder for Low-resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1429</first_page>
						<last_page>1433</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-621</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/feng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hejung</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>814</first_page>
						<last_page>818</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-623</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Kothapally</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shixiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Zoneformer: On-device Neural Beamformer For In-car Multi-zone Speech Separation, Enhancement and Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5117</first_page>
						<last_page>5121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-625</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Landini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3477</first_page>
						<last_page>3481</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-628</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/delcroix23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhisheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanrou</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1269</first_page>
						<last_page>1273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-630</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oskar</given_name>
<surname>Keding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emina</given_name>
<surname>Alickovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin A.</given_name>
<surname>Skoglund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Sandsten</surname>
</person_name>
					</contributors>
					<titles><title>Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5162</first_page>
						<last_page>5166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-633</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/keding23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiayi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weixin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Low-complexity Broadband Beampattern Synthesis using Array Response Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5122</first_page>
						<last_page>5126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-634</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>122</first_page>
						<last_page>126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-637</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1249</first_page>
						<last_page>1253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-642</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kamer Ali</given_name>
<surname>Yuksel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thiago Castro</given_name>
<surname>Ferreira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Golara</given_name>
<surname>Javadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Al-Badrashiny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmet</given_name>
<surname>Gunduz</surname>
</person_name>
					</contributors>
					<titles><title>NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>466</first_page>
						<last_page>470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-643</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yuksel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusheng</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunfeng</given_name>
<surname>Guan</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning Based ASR Robust Knowledge Selection For Spoken Dialogue System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>725</first_page>
						<last_page>729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-644</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4798</first_page>
						<last_page>4802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-645</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunfeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>42</first_page>
						<last_page>46</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-648</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/song23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huali</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanhui</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nengheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>F0inTFS: A lightweight periodicity enhancement strategy for cochlear implants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5008</first_page>
						<last_page>5012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-652</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weibin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3627</first_page>
						<last_page>3631</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-653</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Borsdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangjie</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Extraction with Detection of Presence and Absence of Target Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3714</first_page>
						<last_page>3718</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-655</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongxiang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Neural Speaker Diarization with Absolute Speaker Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3577</first_page>
						<last_page>3581</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-656</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marina</given_name>
<surname>Eni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilan</given_name>
<surname>Dinstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaniv</given_name>
<surname>Zigel</surname>
</person_name>
					</contributors>
					<titles><title>Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4154</first_page>
						<last_page>4158</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-658</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/eni23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengqiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yali</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Turbo your multi-modal classification with contrastive learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1848</first_page>
						<last_page>1852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-661</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lufei</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3407</first_page>
						<last_page>3411</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-663</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Poláček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Červa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindřich</given_name>
<surname>Žďánský</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lenka</given_name>
<surname>Weingartová</surname>
</person_name>
					</contributors>
					<titles><title>Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>446</first_page>
						<last_page>450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-664</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/polacek23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leanne</given_name>
<surname>Nortje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Visually grounded few-shot word acquisition with fewer shots</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3412</first_page>
						<last_page>3416</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-668</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nortje23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4983</first_page>
						<last_page>4987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-670</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>QingTian</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiu-Shi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3117</first_page>
						<last_page>3121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-673</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>FRA-RIR: Fast Random Approximation of the Image-source Method</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3884</first_page>
						<last_page>3888</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-675</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/luo23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dinh Son</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tung Lam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bao Thang</given_name>
<surname>Ta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tien Thanh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thi Ngoc Anh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dang Linh</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nhat Minh</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Hai</given_name>
<surname>Do</surname>
</person_name>
					</contributors>
					<titles><title>LightVoc: An Upsampling-Free GAN Vocoder Based On Conformer And Inverse Short-time Fourier Transform</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3043</first_page>
						<last_page>3047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-677</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/dang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaofei</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Afra</given_name>
<surname>Alishahi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arianna</given_name>
<surname>Bisazza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Chrupała</surname>
</person_name>
					</contributors>
					<titles><title>Wave to Syntax: Probing spoken language models for syntax</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1259</first_page>
						<last_page>1263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-679</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bryony</given_name>
<surname>Nuttall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speaker Recognition performance with matched and mismatched female bilingual speech data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>601</first_page>
						<last_page>605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-680</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nuttall23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Théo</given_name>
<surname>Mariotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvio</given_name>
<surname>Montrésor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Hugh</given_name>
<surname>Thomas</surname>
</person_name>
					</contributors>
					<titles><title>Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2783</first_page>
						<last_page>2787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-684</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mariotte23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Wong</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3889</first_page>
						<last_page>3893</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-686</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seunghan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeonggeun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyuhong</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyoung</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1633</first_page>
						<last_page>1637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-689</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muyang</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxing</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4344</first_page>
						<last_page>4348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-690</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/du23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Fen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang-Li</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2138</first_page>
						<last_page>2142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-695</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pin-Jui</given_name>
<surname>Ku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>A Neural State-Space Modeling Approach to Efficient Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3784</first_page>
						<last_page>3788</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-696</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3182</first_page>
						<last_page>3186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-697</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianjun</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenghong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>DiffSLU: Knowledge Distillation Based Diffusion Model for Cross-Lingual Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>715</first_page>
						<last_page>719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-699</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youneng</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meimei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyue</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojun</given_name>
<surname>Fei</surname>
</person_name>
					</contributors>
					<titles><title>EdenTTS: A Simple and Efficient Parallel Text-to-speech Architecture with Collaborative Duration-alignment Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4449</first_page>
						<last_page>4453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-700</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuyang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mittul</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abraham</given_name>
<surname>Woubie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reima</given_name>
<surname>Karhila</surname>
</person_name>
					</contributors>
					<titles><title>Data augmentation for children ASR and child-adult speaker classification using voice conversion methods</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4593</first_page>
						<last_page>4597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-702</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhao23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michele</given_name>
<surname>Panariello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wanying</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2868</first_page>
						<last_page>2872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-703</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/panariello23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lior</given_name>
<surname>Frenkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Goldberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shlomo E.</given_name>
<surname>Chazan</surname>
</person_name>
					</contributors>
					<titles><title>Domain Adaptation for Speech Enhancement in a Large Domain Gap</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2458</first_page>
						<last_page>2462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-705</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/frenkel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Bilingual TTS Using Language And Phonology Embedding With Embedding Strength Modulator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5531</first_page>
						<last_page>5535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-709</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iona</given_name>
<surname>Gessinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin R.</given_name>
<surname>Cowan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>Cross-linguistic Emotion Perception in Human and TTS Voices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5222</first_page>
						<last_page>5226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-711</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gessinger23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yabo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3779</first_page>
						<last_page>3783</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-714</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Max W. Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4858</first_page>
						<last_page>4862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-715</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Causal Spectro-Temporal Voice Activity Detection Based on Convolutional Encoding and Residual Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5062</first_page>
						<last_page>5066</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-716</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Burin</given_name>
<surname>Naowarat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pasquale</given_name>
<surname>D'Alterio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sibo</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bashar</given_name>
<surname>Awwad Shiekh Hasan</surname>
</person_name>
					</contributors>
					<titles><title>Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1264</first_page>
						<last_page>1268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-720</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/naowarat23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammed</given_name>
<surname>Mosuily</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lindsay</given_name>
<surname>Welch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagmohan</given_name>
<surname>Chauhan</surname>
</person_name>
					</contributors>
					<titles><title>MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2333</first_page>
						<last_page>2337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-721</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mosuily23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bajian</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongkun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zedong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Su</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangdong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2848</first_page>
						<last_page>2852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-727</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xiang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Terui</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diqun</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4923</first_page>
						<last_page>4927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-733</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ye23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengwei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>TFECN: Time-Frequency Enhanced ConvNet for Audio Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>281</first_page>
						<last_page>285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-734</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Mateju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Nouza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Červa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindrich</given_name>
<surname>Zdansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frantisek</given_name>
<surname>Kynych</surname>
</person_name>
					</contributors>
					<titles><title>Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3252</first_page>
						<last_page>3256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-737</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mateju23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sibo</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Wiesler</surname>
</person_name>
					</contributors>
					<titles><title>Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>256</first_page>
						<last_page>260</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-739</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/harding23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinwei</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name>
					</contributors>
					<titles><title>An Analysis of Goodness of Pronunciation for Child Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4613</first_page>
						<last_page>4617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-743</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiu</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehmet Hamza</given_name>
<surname>Erol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arda</given_name>
<surname>Senocak</surname>
</person_name>
					</contributors>
					<titles><title>FlexiAST: Flexibility is What AST Needs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2828</first_page>
						<last_page>2832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-745</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/feng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongfei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qijie</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peikun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>TranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>216</first_page>
						<last_page>220</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-746</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xue23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianrong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaxin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sen</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2</first_page>
						<last_page>6</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-749</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soo Hyun</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seok Wan</given_name>
<surname>Chae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngseok</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keunsang</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunjin</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lae-Hoon</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Dual-Path Transformer for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>824</first_page>
						<last_page>828</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-751</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bae23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kubilay Can</given_name>
<surname>Demir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Weise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>May</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Schmid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2348</first_page>
						<last_page>2352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-753</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/demir23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minki</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wooseok</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Ju</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunho</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4339</first_page>
						<last_page>4343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-754</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenhui</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3637</first_page>
						<last_page>3641</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-756</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tushar</given_name>
<surname>Verma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atul</given_name>
<surname>Shree</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Modi</surname>
</person_name>
					</contributors>
					<titles><title>ASR for Low Resource and Multilingual Noisy Code-Mixed Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3242</first_page>
						<last_page>3246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-757</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/verma23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingyan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haodong</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1124</first_page>
						<last_page>1128</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-758</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengwei</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangjun</given_name>
<surname>Kuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name>
					</contributors>
					<titles><title>Blank-regularized CTC for Frame Skipping in Neural Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4409</first_page>
						<last_page>4413</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-759</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaixun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhanheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingshen</given_name>
<surname>Mu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4933</first_page>
						<last_page>4937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-767</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Antonova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Bakhturina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1404</first_page>
						<last_page>1408</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-768</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/antonova23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keyu</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xian</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>BAT: Boundary aware transducer for memory-efficient and low-latency ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4963</first_page>
						<last_page>4967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-770</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/an23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Glocker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaricia</given_name>
<surname>Herygers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Munir</given_name>
<surname>Georges</surname>
</person_name>
					</contributors>
					<titles><title>Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2258</first_page>
						<last_page>2262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-772</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/glocker23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>ECAPA++: Fine-grained Deep Embedding Learning for TDNN Based Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3132</first_page>
						<last_page>3136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-777</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kathrin</given_name>
<surname>Feindt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martina</given_name>
<surname>Rossi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ghazaleh</given_name>
<surname>Esfandiari-Baiat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel G.</given_name>
<surname>Ekström</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name>
					</contributors>
					<titles><title>Cues to next-speaker projection in conversational Swedish: Evidence from reaction times</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1040</first_page>
						<last_page>1044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-778</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/feindt23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui-Chen</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>844</first_page>
						<last_page>848</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-780</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zheng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ya-Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanghao</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengchen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4803</first_page>
						<last_page>4807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-782</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Belen</given_name>
<surname>Alastruey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Drude</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahn</given_name>
<surname>Heymann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Wiesler</surname>
</person_name>
					</contributors>
					<titles><title>Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4973</first_page>
						<last_page>4977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-783</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/alastruey23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hsin-Hao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yung-Lun</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-Chi</given_name>
<surname>Yen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shu-Wei</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tai-shih</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Mandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5018</first_page>
						<last_page>5022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-786</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steven</given_name>
<surname>Vander Eeckt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Rehearsal-Free Online Continual Learning for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>944</first_page>
						<last_page>948</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-788</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vandereeckt23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anika A.</given_name>
<surname>Spiesberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iosif</given_name>
<surname>Tsangko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Abusive Speech Detection in Indic Languages Using Acoustic Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2683</first_page>
						<last_page>2687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-789</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/spiesberger23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingren</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4943</first_page>
						<last_page>4947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-791</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenwei</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Lohrenz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Sach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Möller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1583</first_page>
						<last_page>1587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-793</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Sach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Franzen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bruno</given_name>
<surname>Defraene</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristoff</given_name>
<surname>Fluyt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Strake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wouter</given_name>
<surname>Tirry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2508</first_page>
						<last_page>2512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-799</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sach23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1973</first_page>
						<last_page>1977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-800</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianqi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mads Græsbøll</given_name>
<surname>Christensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeliang</given_name>
<surname>An</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3694</first_page>
						<last_page>3698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-801</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maeva</given_name>
<surname>Garnier</surname>
</person_name>
					</contributors>
					<titles><title>Audio, Visual and Audiovisual intelligibility of vowels produced in noise</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4204</first_page>
						<last_page>4208</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-803</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/garnier23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Strauch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Serrurier</surname>
</person_name>
					</contributors>
					<titles><title>Generating high-resolution 3D real-time MRI of the vocal tract</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5142</first_page>
						<last_page>5146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-804</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/strauch23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huiyan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid Silent Speech Interface Through Fusion of Electroencephalography and Electromyography</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1184</first_page>
						<last_page>1188</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-805</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Detai</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ai</given_name>
<surname>Morimatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>17</first_page>
						<last_page>21</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-806</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xin23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huiqiang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li Lyna</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shijie</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lili</given_name>
<surname>Qiu</surname>
</person_name>
					</contributors>
					<titles><title>Accurate and Structured Pruning for Efficient Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4104</first_page>
						<last_page>4108</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-809</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wuxuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yitong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwen</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>A no-reference speech quality assessment method based on neural network with densely connected convolutional architecture</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>536</first_page>
						<last_page>540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-811</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2853</first_page>
						<last_page>2857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-815</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giulia</given_name>
<surname>Comini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name>
					</contributors>
					<titles><title>Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>999</first_page>
						<last_page>1003</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-816</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ribeiro23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ha-Yeong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2283</first_page>
						<last_page>2287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-817</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/choi23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3587</first_page>
						<last_page>3591</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-819</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziya</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lovisa</given_name>
<surname>Wihlborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cassia</given_name>
<surname>Valentini-Botinhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Watts</surname>
</person_name>
					</contributors>
					<titles><title>Controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4359</first_page>
						<last_page>4363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-820</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/khan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhisheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changli</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>82</first_page>
						<last_page>86</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-822</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianrong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sen</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2113</first_page>
						<last_page>2117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-823</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacek</given_name>
<surname>Kudera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Zahner-Ritter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jakob</given_name>
<surname>Engel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Elsässer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Hutmacher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carolin</given_name>
<surname>Worstbrock</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4783</first_page>
						<last_page>4787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-828</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kudera23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shao-Hao</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Speaking State Decoder with Transition Detection for Next Speaker Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1868</first_page>
						<last_page>1872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-830</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuting</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing the Unified Streaming and Non-streaming Model with Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1643</first_page>
						<last_page>1647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-831</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pranay</given_name>
<surname>Manocha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel Dejene</given_name>
<surname>Gebru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejan</given_name>
<surname>Markovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Richard</surname>
</person_name>
					</contributors>
					<titles><title>Spatialization Quality Metric for Binaural Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5426</first_page>
						<last_page>5430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-832</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/manocha23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Merritt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biel</given_name>
<surname>Tura-Vecino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kayoko</given_name>
<surname>Yanagisawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamil</given_name>
<surname>Pokora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelhamid</given_name>
<surname>Ezzerg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Cygert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ammar</given_name>
<surname>Abbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Bilinski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name>
					</contributors>
					<titles><title>Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>27</first_page>
						<last_page>31</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-834</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Ronssin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name>
					</contributors>
					<titles><title>ALO-VC: Any-to-any Low-latency One-shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2073</first_page>
						<last_page>2077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-836</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tamas</given_name>
<surname>Grosz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ragheb</given_name>
<surname>Al-Ghezi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aku</given_name>
<surname>Rouhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a Finnish model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>196</first_page>
						<last_page>200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-837</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/grosz23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carmen</given_name>
<surname>Mijnders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Janse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Naarding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khiet P.</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic characteristics of depression in older adults' speech: the role of covariates</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4159</first_page>
						<last_page>4163</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-839</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mijnders23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1913</first_page>
						<last_page>1917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-842</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhao</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiongwei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongzhou</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haiqing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4813</first_page>
						<last_page>4817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-843</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cui23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Reversible Neural Networks for Memory-Efficient Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3127</first_page>
						<last_page>3131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-844</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joop</given_name>
<surname>Arts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khiet P.</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>Effects of perceived gender on the perceived social function of laughter</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1878</first_page>
						<last_page>1882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-846</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/arts23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heng-Jui</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander H.</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2983</first_page>
						<last_page>2987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-847</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Rouas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaru</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Shochi</surname>
</person_name>
					</contributors>
					<titles><title>Affective attributes of French caregivers' professional speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1239</first_page>
						<last_page>1243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-848</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rouas23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhida</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baowei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minqiang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Fully-Connected Layer for Large-Scale Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2003</first_page>
						<last_page>2007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-849</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/song23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiguang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1095</first_page>
						<last_page>1099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-851</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guy</given_name>
<surname>Yariv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itai</given_name>
<surname>Gat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Wolf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Idan</given_name>
<surname>Schwartz</surname>
</person_name>
					</contributors>
					<titles><title>Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5446</first_page>
						<last_page>5450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-852</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yariv23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shikhar</given_name>
<surname>Vashishth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shikhar</given_name>
<surname>Bharadwaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vera</given_name>
<surname>Axelrod</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Partha</given_name>
<surname>Talukdar</surname>
</person_name>
					</contributors>
					<titles><title>Label Aware Speech Representation Learning For Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5351</first_page>
						<last_page>5355</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-854</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vashishth23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yin-Tse</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Han</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Chan</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jyh-Shing Roger</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Noise-Robust Bandwidth Expansion for 8K Speech Recordings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5107</first_page>
						<last_page>5111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-857</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Tilli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarina</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4788</first_page>
						<last_page>4792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-858</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lux23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giulia</given_name>
<surname>Comini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heereen</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual context-based pronunciation learning for Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>631</first_page>
						<last_page>635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-861</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/comini23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chong-En</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>A Lexical-aware Non-autoregressive Transformer-based ASR Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1434</first_page>
						<last_page>1438</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-863</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samantha</given_name>
<surname>Kotey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rozenn</given_name>
<surname>Dahyot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Query Based Acoustic Summarization for Podcasts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1483</first_page>
						<last_page>1487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-864</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kotey23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yulong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1060</first_page>
						<last_page>1064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-865</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yung-Lun</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Hao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-Chi</given_name>
<surname>Yen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shu-Wei</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tai-shih</given_name>
<surname>Chi</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5023</first_page>
						<last_page>5026</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-866</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chien23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oli Danyi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2968</first_page>
						<last_page>2972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-871</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Lehečka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef V.</given_name>
<surname>Psutka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Ircing</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>201</first_page>
						<last_page>205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-872</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lehecka23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fabian</given_name>
<surname>Kögel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bac</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabien</given_name>
<surname>Cardinaux</surname>
</person_name>
					</contributors>
					<titles><title>Towards Robust FastSpeech 2 by Modelling Residual Multimodality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4309</first_page>
						<last_page>4313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-879</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kogel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Galina</given_name>
<surname>Lavrentyeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Avdeeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Volokhov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Khmelev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Akulov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Polina</given_name>
<surname>Leonteva</surname>
</person_name>
					</contributors>
					<titles><title>On the robustness of wav2vec 2.0 based speaker recognition systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3177</first_page>
						<last_page>3181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-881</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/novoselov23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>Kovalyov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kashyap</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Issa</given_name>
<surname>Panahi</surname>
</person_name>
					</contributors>
					<titles><title>DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2493</first_page>
						<last_page>2497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-882</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kovalyov23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elad</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hai Victor</given_name>
<surname>Habi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnon</given_name>
<surname>Netzer</surname>
</person_name>
					</contributors>
					<titles><title>Towards Fully Quantized Neural Networks For Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>181</first_page>
						<last_page>185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-883</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cohen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhanheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaixun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changru</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1668</first_page>
						<last_page>1672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-884</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minghang</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaoyao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengtao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhi</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>MSAF: A Multiple Self-Attention Field Method for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2518</first_page>
						<last_page>2522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-886</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ambika</given_name>
<surname>Kirkland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5217</first_page>
						<last_page>5221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-887</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kirkland23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qinghua</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3719</first_page>
						<last_page>3723</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-889</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan-Ting</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shao-Hao</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing-Yi</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3582</first_page>
						<last_page>3586</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-890</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khandokar Md.</given_name>
<surname>Nayem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ran</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Yun</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshaya Vishnu Kudlu</given_name>
<surname>Shanbhogue</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation on Joint Task End-to-End Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1493</first_page>
						<last_page>1497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-891</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nayem23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Carvalho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name>
					</contributors>
					<titles><title>Memory-augmented conformer for improved end-to-end long-form ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2218</first_page>
						<last_page>2222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-893</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/carvalho23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fuxiang</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Esposito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4149</first_page>
						<last_page>4153</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-894</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas M.</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Sperl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Böttinger</surname>
</person_name>
					</contributors>
					<titles><title>Complex-valued neural networks for voice anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3814</first_page>
						<last_page>3818</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-901</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/muller23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Parnia</given_name>
<surname>Bahar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mattia</given_name>
<surname>Di Gangi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nick</given_name>
<surname>Rossenbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Zeineldeen</surname>
</person_name>
					</contributors>
					<titles><title>Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3949</first_page>
						<last_page>3953</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-903</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bahar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haixin</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>A GAN Speech Inpainting Model for Audio Editing Software</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5127</first_page>
						<last_page>5131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-904</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhao23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roman</given_name>
<surname>Korostik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Shabalin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2928</first_page>
						<last_page>2932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-906</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bataev23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lars</given_name>
<surname>Rumberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Gebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Ehlert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maren</given_name>
<surname>Wallbaum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ulrike</given_name>
<surname>Lüdtke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joern</given_name>
<surname>Ostermann</surname>
</person_name>
					</contributors>
					<titles><title>Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4583</first_page>
						<last_page>4587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-907</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rumberg23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanmay</given_name>
<surname>Khandelwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1214</first_page>
						<last_page>1218</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-909</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/khandelwal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoguang</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenshan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Tien</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4194</first_page>
						<last_page>4198</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-910</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin A.</given_name>
<surname>Grambow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mojtaba</given_name>
<surname>Kadkhodaie Elyaderani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alireza</given_name>
<surname>Sadeghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Fancellu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schaaf</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2338</first_page>
						<last_page>2342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-913</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiushi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianyuan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengchen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lilian H.</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volkan</given_name>
<surname>Kılıç</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2838</first_page>
						<last_page>2842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-914</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roshan</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenneth</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>BASS: Block-wise Adaptation for Speech Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1454</first_page>
						<last_page>1458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-916</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sharma23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheshu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenpeng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>919</first_page>
						<last_page>923</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-923</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Gebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lars</given_name>
<surname>Rumberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Ehlert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ulrike</given_name>
<surname>Lüdtke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joern</given_name>
<surname>Ostermann</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children’s Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4578</first_page>
						<last_page>4582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-926</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gebauer23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Neural Network Quantization For Lightweight Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5331</first_page>
						<last_page>5335</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-927</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timothy</given_name>
<surname>Piton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Pasqualotto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marjolaine</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daphné</given_name>
<surname>Bavelier</surname>
</person_name>
					</contributors>
					<titles><title>Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4573</first_page>
						<last_page>4577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-928</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/piton23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timm</given_name>
<surname>Koppelmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Semih</given_name>
<surname>Agcaer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Acoustic Scene Classification in Ultra-low Power Embedded Devices Using Privacy-preserving Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>321</first_page>
						<last_page>325</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-932</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/koppelmann23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yasumasa</given_name>
<surname>Kano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuhito</given_name>
<surname>Sudoh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Average Token Delay: A Latency Metric for Simultaneous Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4469</first_page>
						<last_page>4473</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-933</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kano23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hoyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-Wook</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong-Hwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>611</first_page>
						<last_page>615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-934</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Barcovschi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariam</given_name>
<surname>Yiwere</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Corcoran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title>Adaptation of Whisper models to child speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5242</first_page>
						<last_page>5246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-935</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jain23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Verhulst</surname>
</person_name>
					</contributors>
					<titles><title>Biophysically-inspired single-channel speech enhancement in the time domain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>775</first_page>
						<last_page>779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-936</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wen23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Si-Ioi</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cymie Wing-Yee</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Study on Using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4643</first_page>
						<last_page>4647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-937</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianyuan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volkan</given_name>
<surname>Kılıç</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4164</first_page>
						<last_page>4168</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-943</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sun23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Judith</given_name>
<surname>Dineley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Carr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faith</given_name>
<surname>Matcham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johnny</given_name>
<surname>Downs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard J. B.</given_name>
<surname>Dobson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F.</given_name>
<surname>Quatieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2373</first_page>
						<last_page>2377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-947</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/dineley23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julian</given_name>
<surname>Linke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mate</given_name>
<surname>Kadar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gergely</given_name>
<surname>Dosinszky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Mihajlik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gernot</given_name>
<surname>Kubin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>What do self-supervised speech representations encode? An analysis of languages, varieties, speaking styles and speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5371</first_page>
						<last_page>5375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-951</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/linke23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Nielsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Steedman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name>
					</contributors>
					<titles><title>Parsing dialog turns with prosodic features in English</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2633</first_page>
						<last_page>2637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-952</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nielsen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Turk</surname>
</person_name>
					</contributors>
					<titles><title>Estimating virtual targets for lingual stop consonants using general Tau theory</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3083</first_page>
						<last_page>3087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-953</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/elie23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Alvarado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolás</given_name>
<surname>Grágeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Luzanto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Mahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Wuth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Mendoza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Stern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Néstor</given_name>
<surname>Becerra Yoma</surname>
</person_name>
					</contributors>
					<titles><title>Respiratory distress estimation in human-robot interaction scenario</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1763</first_page>
						<last_page>1767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-963</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/alvarado23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ignacio</given_name>
<surname>Calderon De Palma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura S.</given_name>
<surname>Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Lopez Valdes</surname>
</person_name>
					</contributors>
					<titles><title>Effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5192</first_page>
						<last_page>5196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-964</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/calderondepalma23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ammar</given_name>
<surname>Abbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Karlapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bastian</given_name>
<surname>Schnell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Penny</given_name>
<surname>Karanasou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcel</given_name>
<surname>Granero Moya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amith</given_name>
<surname>Nagaraj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayman</given_name>
<surname>Boustati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Peinelt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name>
					</contributors>
					<titles><title>eCat: An End-to-End Model for Multi-Speaker TTS &#38; Many-to-Many Fine-Grained Prosody Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3387</first_page>
						<last_page>3391</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-965</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/abbas23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Baptiste</given_name>
<surname>Pouthier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Pilati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giacomo</given_name>
<surname>Valenti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Bouveyron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederic</given_name>
<surname>Precioso</surname>
</person_name>
					</contributors>
					<titles><title>Another Point of View on Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4089</first_page>
						<last_page>4093</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-969</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pouthier23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tina</given_name>
<surname>Raissi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Lüscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moritz</given_name>
<surname>Gunz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4938</first_page>
						<last_page>4942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-970</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/raissi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiulian</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai-Wei</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>ABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2528</first_page>
						<last_page>2532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-971</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Clément</given_name>
<surname>Gaultier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Goehring</surname>
</person_name>
					</contributors>
					<titles><title>Joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3497</first_page>
						<last_page>3501</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-975</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gaultier23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mitko</given_name>
<surname>Sabev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bistra</given_name>
<surname>Andreeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Gabriel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Gruenke</surname>
</person_name>
					</contributors>
					<titles><title>Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2603</first_page>
						<last_page>2607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-976</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sabev23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Lavechin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaya</given_name>
<surname>Sy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadrien</given_name>
<surname>Titeux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>María Andrea Cruz</given_name>
<surname>Blandón</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4588</first_page>
						<last_page>4592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-978</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lavechin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>Ontology-aware Learning and Evaluation for Audio Tagging</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3799</first_page>
						<last_page>3803</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-979</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youshan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Complex Image Generation SwinTransformer Network for Audio Denoising</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>186</first_page>
						<last_page>190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-980</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joonyong</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiko</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Detai</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1085</first_page>
						<last_page>1089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-981</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/park23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saksham Singh</given_name>
<surname>Kushwaha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Fuentes</surname>
</person_name>
					</contributors>
					<titles><title>A multimodal prototypical approach for unsupervised sound classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>266</first_page>
						<last_page>270</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-982</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kushwaha23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Varun</given_name>
<surname>Belagali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Achuth</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1578</first_page>
						<last_page>1582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-983</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/belagali23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuzhong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>834</first_page>
						<last_page>838</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-984</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Agata</given_name>
<surname>Jakubiak</surname>
</person_name>
					</contributors>
					<titles><title>Whistle-to-text: Automatic recognition of the Silbo Gomero whistled language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3402</first_page>
						<last_page>3406</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-989</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jakubiak23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jari</given_name>
<surname>Kolehmainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yile</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Gourav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashanth</given_name>
<surname>Gurunath Shivakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Bulyko</surname>
</person_name>
					</contributors>
					<titles><title>Personalization for BERT-based Discriminative Speech Recognition Rescoring</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>366</first_page>
						<last_page>370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-990</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kolehmainen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Oberhag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Neudek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Rosenkranz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henning</given_name>
<surname>Puder</surname>
</person_name>
					</contributors>
					<titles><title>Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1075</first_page>
						<last_page>1079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-992</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/oberhag23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Polychronia</given_name>
<surname>Christodoulidou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Nicolaidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Stamovlasis</surname>
</person_name>
					</contributors>
					<titles><title>Vowel reduction by Greek-speaking children: The effect of stress and word length</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4773</first_page>
						<last_page>4777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-995</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/christodoulidou23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Denise</given_name>
<surname>Moussa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Germans</given_name>
<surname>Hirsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Wankerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Riess</surname>
</person_name>
					</contributors>
					<titles><title>Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5057</first_page>
						<last_page>5061</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-996</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/moussa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Sullivan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>AbdelRahim</given_name>
<surname>Elmadany</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Abdul-Mageed</surname>
</person_name>
					</contributors>
					<titles><title>On the Robustness of Arabic Speech Dialect Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5326</first_page>
						<last_page>5330</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1005</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sullivan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alan</given_name>
<surname>Archer-Boyd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4239</first_page>
						<last_page>4243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1007</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/archerboyd23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiting</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanthashree</given_name>
<surname>Mysore Sathyendra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sibo</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuandi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-Ju</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Wiesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name>
					</contributors>
					<titles><title>Model-Internal Slot-triggered Biasing for Domain Expansion in Neural Transducer ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1324</first_page>
						<last_page>1328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1010</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phoenix</given_name>
<surname>Meadowlark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>2-bit Conformer quantization for automatic speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4908</first_page>
						<last_page>4912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1012</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rybakov23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgina</given_name>
<surname>Brown</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christin</given_name>
<surname>Kirchhübel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramiz</given_name>
<surname>Cuthbert</surname>
</person_name>
					</contributors>
					<titles><title>Using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4723</first_page>
						<last_page>4727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1013</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/brown23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daiheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lining</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The MALACH Corpus: Results with End-to-End Architectures and Pretraining</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5097</first_page>
						<last_page>5101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1014</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/picheny23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan Felipe</given_name>
<surname>Montesinos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Michelsanti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gloria</given_name>
<surname>Haro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name>
					</contributors>
					<titles><title>Speech inpainting: Context-based speech synthesis guided by video</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4459</first_page>
						<last_page>4463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1020</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/montesinos23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyang</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Mitchell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiaoqiao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weicheng</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dick</given_name>
<surname>Botteldooren</surname>
</person_name>
					</contributors>
					<titles><title>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>331</first_page>
						<last_page>335</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1021</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hou23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georg</given_name>
<surname>Stemmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paulo</given_name>
<surname>Lopez Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Del Hoyo</given_name>
<surname>Ontiveros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hector A.</given_name>
<surname>Cordourier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1020</first_page>
						<last_page>1024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1023</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/stemmer23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Apiwat</given_name>
<surname>Ditthapron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel O.</given_name>
<surname>Agu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam C.</given_name>
<surname>Lammert</surname>
</person_name>
					</contributors>
					<titles><title>Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2843</first_page>
						<last_page>2847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1026</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ditthapron23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolae Catalin</given_name>
<surname>Ristea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgenii</given_name>
<surname>Indenbom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ando</given_name>
<surname>Saabas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Pärnamaa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jegor</given_name>
<surname>Guzhvin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name>
					</contributors>
					<titles><title>DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3819</first_page>
						<last_page>3823</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1028</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ristea23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Slava</given_name>
<surname>Shechtman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raul</given_name>
<surname>Fernandez</surname>
</person_name>
					</contributors>
					<titles><title>A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4853</first_page>
						<last_page>4857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1032</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shechtman23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianjing</given_name>
<surname>Kuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>May Pik Yu</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nari</given_name>
<surname>Rhee</surname>
</person_name>
					</contributors>
					<titles><title>Tonal coarticulation as a cue for upcoming prosodic boundary</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>102</first_page>
						<last_page>106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1033</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kuang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seong-Gyun</given_name>
<surname>Leem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fulford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jukka-Pekka</given_name>
<surname>Onnela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Gard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1888</first_page>
						<last_page>1892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1034</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/leem23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yassir</given_name>
<surname>Fathullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junteng</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhan</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunxi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mike</given_name>
<surname>Seltzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J. F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Head State Space Model for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>241</first_page>
						<last_page>245</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1036</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fathullah23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hagen</given_name>
<surname>Soltau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Izhak</given_name>
<surname>Shafran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Rastogi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeffrey</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aramys</given_name>
<surname>Miranda</surname>
</person_name>
					</contributors>
					<titles><title>Speech Aware Dialog System Technology Challenge (DSTC11)</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4668</first_page>
						<last_page>4672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1037</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/soltau23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hsiang</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joel Ruben Antony</given_name>
<surname>Moniz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Site</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueyun</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murat</given_name>
<surname>Akbacak</surname>
</person_name>
					</contributors>
					<titles><title>5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>760</first_page>
						<last_page>764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1038</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salah</given_name>
<surname>Zaiem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Essid</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>67</first_page>
						<last_page>71</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1040</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zaiem23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Hedegard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Fröhlich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabian</given_name>
<surname>Tomaschek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carina</given_name>
<surname>Steiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Leemann</surname>
</person_name>
					</contributors>
					<titles><title>Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2558</first_page>
						<last_page>2562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1042</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hedegard23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bashar</given_name>
<surname>Talafha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdul</given_name>
<surname>Waheed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Abdul-Mageed</surname>
</person_name>
					</contributors>
					<titles><title>N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5092</first_page>
						<last_page>5096</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1044</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/talafha23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mikołaj</given_name>
<surname>Pudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Wosik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Cieślak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justyna</given_name>
<surname>Krzywdziak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bozena</given_name>
<surname>Lukasiak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artur</given_name>
<surname>Janicki</surname>
</person_name>
					</contributors>
					<titles><title>MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4054</first_page>
						<last_page>4058</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1049</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pudo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Eisner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed M.</given_name>
<surname>Ali</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Code-switched Text Generation from Parallel Text</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1419</first_page>
						<last_page>1423</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1050</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emily P.</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gina-Anne</given_name>
<surname>Levow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard A.</given_name>
<surname>Wright</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name>
					</contributors>
					<titles><title>An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2573</first_page>
						<last_page>2577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1052</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ahn23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rouditchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Khurana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogerio</given_name>
<surname>Feris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonid</given_name>
<surname>Karlinsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hilde</given_name>
<surname>Kuehne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2268</first_page>
						<last_page>2272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1061</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rouditchenko23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Beck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4094</first_page>
						<last_page>4098</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1062</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Proyag</given_name>
<surname>Pal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Thompson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yogesh</given_name>
<surname>Virkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Mathur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Chronopoulou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcello</given_name>
<surname>Federico</surname>
</person_name>
					</contributors>
					<titles><title>Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>37</first_page>
						<last_page>41</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1063</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Calum</given_name>
<surname>Heggan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Hospedales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Budgett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehrdad</given_name>
<surname>Yaghoobi</surname>
</person_name>
					</contributors>
					<titles><title>MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4399</first_page>
						<last_page>4403</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1064</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/heggan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prerak</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Deleforge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archontis</given_name>
<surname>Politis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>How to (Virtually) Train Your Speaker Localizer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1204</first_page>
						<last_page>1208</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1065</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/srivastava23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malcolm</given_name>
<surname>Chadwick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto Gil C. P.</given_name>
<surname>Ramos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogier</given_name>
<surname>van Dalen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sourav</given_name>
<surname>Bhattacharya</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>804</first_page>
						<last_page>808</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1066</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Mitsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukiya</given_name>
<surname>Hono</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kei</given_name>
<surname>Sawada</surname>
</person_name>
					</contributors>
					<titles><title>UniFLG: Unified Facial Landmark Generator from Text or Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5501</first_page>
						<last_page>5505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1067</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mitsui23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longyue</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyou</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Derek F.</given_name>
<surname>Wong</surname>
</person_name>
					</contributors>
					<titles><title>How Does Pretraining Improve Discourse-Aware Translation?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3899</first_page>
						<last_page>3903</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1068</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongcheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>A Neural Time Alignment Module for End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1374</first_page>
						<last_page>1378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1071</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maeva</given_name>
<surname>Garnier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Le Ferrand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabien</given_name>
<surname>Ringeval</surname>
</person_name>
					</contributors>
					<titles><title>Verbal and nonverbal feedback signals in response to increasing levels of miscommunication</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2698</first_page>
						<last_page>2702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1074</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/garnier23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandrashekhar</given_name>
<surname>Lavania</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjiv</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3612</first_page>
						<last_page>3616</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1075</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lavania23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1104</first_page>
						<last_page>1108</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1076</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cooper23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zih-Ching</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name>
					</contributors>
					<titles><title>How to Estimate Model Transferability of Pre-Trained Speech Models?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>456</first_page>
						<last_page>460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1079</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Ibrahim</given_name>
<surname>Malik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddique</given_name>
<surname>Latif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raja</given_name>
<surname>Jurdak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>646</first_page>
						<last_page>650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1080</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/malik23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhua</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanouil</given_name>
<surname>Benetos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Language-Audio Models as Few-Shot Audio Learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>276</first_page>
						<last_page>280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1082</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Yakovlev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Okhotnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Torgashov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rostislav</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Voevodin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Simonchik</surname>
</person_name>
					</contributors>
					<titles><title>VoxTube: a multilingual speaker recognition dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2238</first_page>
						<last_page>2242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1083</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yakovlev23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pin-Jui</given_name>
<surname>Ku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2453</first_page>
						<last_page>2457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1084</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ku23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pin-Jui</given_name>
<surname>Ku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pin-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3317</first_page>
						<last_page>3321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1086</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salah</given_name>
<surname>Zaiem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youcef</given_name>
<surname>Kemiche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Essid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2873</first_page>
						<last_page>2877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1087</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zaiem23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sefik Emre</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Pärnamaa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaming</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1050</first_page>
						<last_page>1054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1088</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/eskimez23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Darshana</given_name>
<surname>Prisayad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharindu</given_name>
<surname>Fernando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sridha</given_name>
<surname>Sridharan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Denman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clinton</given_name>
<surname>Fookes</surname>
</person_name>
					</contributors>
					<titles><title>Dual Memory Fusion for Multimodal Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4543</first_page>
						<last_page>4547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1090</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/prisayad23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong-Thanh</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama</given_name>
<surname>Doddipatla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Domain Adaptive Self-supervised Training of Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4389</first_page>
						<last_page>4393</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1091</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/do23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Borgstrom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maliha</given_name>
<surname>Jahan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Torres-Carrasquillo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>521</first_page>
						<last_page>525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1094</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/villalba23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eiji</given_name>
<surname>Iimori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3048</first_page>
						<last_page>3052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1095</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/saito23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinod</given_name>
<surname>Subramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namhee</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raymond</given_name>
<surname>Brueckner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nate</given_name>
<surname>Blaylock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henry</given_name>
<surname>O'Connell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis</given_name>
<surname>Sierra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clementina</given_name>
<surname>Ullman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Hildebrand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Laganiere</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Manifest Huntington's Disease Using Vocal Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5032</first_page>
						<last_page>5036</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1096</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/subramanian23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Haik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siamak</given_name>
<surname>Tazari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5541</first_page>
						<last_page>5545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1097</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eiji</given_name>
<surname>Iimori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5561</first_page>
						<last_page>5565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1098</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/saito23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Olivier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>There is more than one kind of robustness: Fooling Whisper with adversarial examples</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4394</first_page>
						<last_page>4398</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1105</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/olivier23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abinay Reddy</given_name>
<surname>Naini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali N.</given_name>
<surname>Salman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Preference Learning Labels by Anchoring on Consecutive Annotations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1898</first_page>
						<last_page>1902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1108</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/naini23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jagabandhu</given_name>
<surname>Mishra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jayadev N</given_name>
<surname>Patil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amartya</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>End to End Spoken Language Diarization with Wav2vec Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>501</first_page>
						<last_page>505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1109</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mishra23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huang-Cheng</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Goncalves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Gyun</given_name>
<surname>Leem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>641</first_page>
						<last_page>645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1113</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chou23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayako</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Arai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2503</first_page>
						<last_page>2507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1116</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/araki23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mirazul</given_name>
<surname>Haque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rutvij</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>SlothSpeech: Denial-of-service Attack Against Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1274</first_page>
						<last_page>1278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1118</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/haque23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyuhong</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkyu</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyoung</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyuwoong</given_name>
<surname>Hwang</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1663</first_page>
						<last_page>1667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1121</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shim23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyue</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinglun</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Fogerty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying Informational Masking due to Masker Intelligibility in Same-talker Speech-in-speech Perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1783</first_page>
						<last_page>1787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1122</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sijia</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>First Language Effects on Second Language Perception: Evidence from English Low-vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4184</first_page>
						<last_page>4188</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1123</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsendsuren</given_name>
<surname>Munkhdalai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pat</given_name>
<surname>Rondon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Golan</given_name>
<surname>Pundak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>221</first_page>
						<last_page>225</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1124</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joun Yeop</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Sung</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seongkyu</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihwan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4334</first_page>
						<last_page>4338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1128</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Goeric</given_name>
<surname>Huybrechts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xilai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadis</given_name>
<surname>Nosrati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1658</first_page>
						<last_page>1662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1129</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huybrechts23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zilin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiuxin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukai</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4034</first_page>
						<last_page>4038</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1130</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mizuki</given_name>
<surname>Nagano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sadao</given_name>
<surname>Hiroya</surname>
</person_name>
					</contributors>
					<titles><title>A stimulus-organism-response model of willingness to buy from advertising speech using voice quality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5202</first_page>
						<last_page>5206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1131</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nagano23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziji</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Kamma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharanya</given_name>
<surname>Eswaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Narayanan</given_name>
<surname>Sadagopan</surname>
</person_name>
					</contributors>
					<titles><title>PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3904</first_page>
						<last_page>3908</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1135</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soham</given_name>
<surname>Deshmukh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elizalde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaming</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Audio Retrieval with WavText5K and CLAP Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2948</first_page>
						<last_page>2952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1136</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deshmukh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Tagliasacchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunpeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xia</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name>
					</contributors>
					<titles><title>Real time spectrogram inversion on mobile phone</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4314</first_page>
						<last_page>4318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1137</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rybakov23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hikaru</given_name>
<surname>Yanagida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name>
					</contributors>
					<titles><title>Influence of Personal Traits on Impressions of One's Own Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5212</first_page>
						<last_page>5216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1139</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yanagida23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tünde</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Holik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duy Duong</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Morandini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine J.</given_name>
<surname>Madill</surname>
</person_name>
					</contributors>
					<titles><title>Comparing first spectral moment of Australian English /s/ between straight and gay voices using three analysis window sizes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2543</first_page>
						<last_page>2547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1142</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/szalay23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahana</given_name>
<surname>Deb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sayan</given_name>
<surname>Nag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayan</given_name>
<surname>Mahapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumitri</given_name>
<surname>Chattopadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aritra</given_name>
<surname>Marik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pijush Kanti</given_name>
<surname>Gayen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankha</given_name>
<surname>Sanyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archi</given_name>
<surname>Banerjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samir</given_name>
<surname>Karmakar</surname>
</person_name>
					</contributors>
					<titles><title>BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3392</first_page>
						<last_page>3396</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1146</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deb23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yishuang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4304</first_page>
						<last_page>4308</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1151</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/guan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan E.</given_name>
<surname>Avila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nigel G.</given_name>
<surname>Ward</surname>
</person_name>
					</contributors>
					<titles><title>Towards Cross-Language Prosody Transfer for Dialog</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2143</first_page>
						<last_page>2147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1152</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/avila23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanting</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3537</first_page>
						<last_page>3541</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1154</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sun23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Ning</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuepeng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jixun</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengxiao</given_name>
<surname>Bi</surname>
</person_name>
					</contributors>
					<titles><title>DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2063</first_page>
						<last_page>2067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1157</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ning23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Younggwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungjun</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiho</given_name>
<surname>Yeom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunjoo</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoodong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanley Jungkyu</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honglak</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3764</first_page>
						<last_page>3768</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1158</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haris</given_name>
<surname>Gulzar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monikka Roslianna</given_name>
<surname>Busto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeharu</given_name>
<surname>Eda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsutoshi</given_name>
<surname>Itoyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Nakadai</surname>
</person_name>
					</contributors>
					<titles><title>miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3277</first_page>
						<last_page>3281</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1162</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gulzar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryotaro</given_name>
<surname>Nagase</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Fukumori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoichi</given_name>
<surname>Yamashita</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4533</first_page>
						<last_page>4537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1163</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nagase23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bang</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suo</given_name>
<surname>Hongbin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulong</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Robust Audio Anti-spoofing Countermeasure with Joint Training of Front-end and Back-end Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4004</first_page>
						<last_page>4008</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1166</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaobin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maokui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shutong</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baoxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiakui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>421</first_page>
						<last_page>425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1167</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenghao</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Kou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tong</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingbo</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Information Magnitude Based Dynamic Sub-sampling for Speech-to-text</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4433</first_page>
						<last_page>4437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1168</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolás</given_name>
<surname>Grágeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Alvarado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Mahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Néstor</given_name>
<surname>Becerra Yoma</surname>
</person_name>
					</contributors>
					<titles><title>Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3657</first_page>
						<last_page>3661</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1169</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/grageda23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhipeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weibin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hengsheng</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Scale Temporal Transformer For Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3652</first_page>
						<last_page>3656</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1170</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhanheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Two Stage Contextual Word Filtering for Context Bias in Unified Streaming and Non-streaming Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3257</first_page>
						<last_page>3261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1171</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Marsic</surname>
</person_name>
					</contributors>
					<titles><title>Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3492</first_page>
						<last_page>3496</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1172</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao-Min</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Robust Prototype Learning for Anomalous Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>261</first_page>
						<last_page>265</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1173</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zeng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tuning Audio Spectrogram Transformer with Task-aware Adapters for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>291</first_page>
						<last_page>295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1174</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hieu-Thi</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4379</first_page>
						<last_page>4383</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1175</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/luong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoheng</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumi</given_name>
<surname>Maiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4404</first_page>
						<last_page>4408</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1176</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shubo</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>DCCRN-KWS: An Audio Bias Based Model for Noise Robust Small-Footprint Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>929</first_page>
						<last_page>933</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1184</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lv23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nithish</given_name>
<surname>Muthuchamy Selvaraj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobao</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adams</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingquan</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Kot</surname>
</person_name>
					</contributors>
					<titles><title>Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>909</first_page>
						<last_page>913</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1189</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/muthuchamyselvaraj23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yangshi</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhuan</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongzhe</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoning</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Improving Chinese Mandarin Speech Recognition Using Semantic Graph Embedding Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1309</first_page>
						<last_page>1313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1192</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Seki</surname>
</person_name>
					</contributors>
					<titles><title>CFVC: Conditional Filtering for Controllable Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2058</first_page>
						<last_page>2062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1193</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tanaka23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwangyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiyang</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2208</first_page>
						<last_page>2212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1194</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/peng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jessica L. L.</given_name>
<surname>Chin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Talevska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Antoniou</surname>
</person_name>
					</contributors>
					<titles><title>Speech-in-Speech Recognition is Modulated by Familiarity to Dialect</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3113</first_page>
						<last_page>3116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1196</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Areej</given_name>
<surname>Buker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huda</given_name>
<surname>Alsofyani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1045</first_page>
						<last_page>1049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1200</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/buker23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingu</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-ki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You Jin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Curriculum Learning for Self-supervised Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4693</first_page>
						<last_page>4697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1202</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/heo23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinmei</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengrun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenguang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Diffusion Probability Model For Cross-domain Speaker Verification Integrating Contrastive Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5336</first_page>
						<last_page>5340</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1205</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/su23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chu Yuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xun</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3137</first_page>
						<last_page>3141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1206</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Astik</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelmoumene</given_name>
<surname>Boumadane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephane</given_name>
<surname>Peillon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gildas</given_name>
<surname>Bleas</surname>
</person_name>
					</contributors>
					<titles><title>An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Sorpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3939</first_page>
						<last_page>3943</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1208</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/biswas23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biao</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>296</first_page>
						<last_page>300</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1210</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li-Jen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Efficient Learning for Text-to-Speech Accent Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4354</first_page>
						<last_page>4358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1212</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakeel</given_name>
<surname>Muhammad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>62</first_page>
						<last_page>66</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1213</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/peng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Range-Based Equal Error Rate for Spoof Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3212</first_page>
						<last_page>3216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1214</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudhakar</given_name>
<surname>P</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sreenivasa K.</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pabitra</given_name>
<surname>Mitra</surname>
</person_name>
					</contributors>
					<titles><title>Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1618</first_page>
						<last_page>1622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1215</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/p23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiamin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H. L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1304</first_page>
						<last_page>1308</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1216</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xie23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Houjun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3202</first_page>
						<last_page>3206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1217</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruixi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Hieu</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1319</first_page>
						<last_page>1323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1221</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minkyu</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ohhyeok</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghyun</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soonshin</given_name>
<surname>Seo</surname>
</person_name>
					</contributors>
					<titles><title>Blank Collapse: Compressing CTC Emission for the Faster Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1349</first_page>
						<last_page>1353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1223</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jung23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3552</first_page>
						<last_page>3556</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1228</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitali</given_name>
<surname>Chakrabarti</surname>
</person_name>
					</contributors>
					<titles><title>Aligning Speech Enhancement for Improving Downstream Classification Performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3874</first_page>
						<last_page>3878</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1232</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xiong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengqi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chutong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Cao</surname>
</person_name>
					</contributors>
					<titles><title>GigaST: A 10,000-hour Pseudo Speech Translation Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2168</first_page>
						<last_page>2172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1233</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ye23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kamo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Target Speech Extraction with Conditional Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>176</first_page>
						<last_page>180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1234</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kamo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>765</first_page>
						<last_page>769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1236</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Faustini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Besnik</given_name>
<surname>Fetahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Castellucci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjie</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rokhlenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shervin</given_name>
<surname>Malmasi</surname>
</person_name>
					</contributors>
					<titles><title>Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3427</first_page>
						<last_page>3431</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1240</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/faustini23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi</given_name>
<surname>Yin</surname>
</person_name>
					</contributors>
					<titles><title>Spot Keywords From Very Noisy and Mixed Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1488</first_page>
						<last_page>1492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1242</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenchun</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minglei</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Group GMM-ResNet for Detection of Synthetic Speech Attacks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3187</first_page>
						<last_page>3191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1249</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lei23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Iwamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Shinozaki</surname>
</person_name>
					</contributors>
					<titles><title>Memory Network-Based End-To-End Neural ES-KMeans for Improved Word Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>486</first_page>
						<last_page>490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1251</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/iwamoto23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chu Yuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xun</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3844</first_page>
						<last_page>3848</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1254</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dohee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jieun</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Intra-ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2203</first_page>
						<last_page>2207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1255</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hariram</given_name>
<surname>Veeramani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natarajan</given_name>
<surname>Balaji Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4608</first_page>
						<last_page>4612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1257</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/johnson23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meena M. C.</given_name>
<surname>Shekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H. L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1459</first_page>
						<last_page>1463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1258</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shekar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1733</first_page>
						<last_page>1737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1263</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zilu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenbin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1065</first_page>
						<last_page>1069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1265</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/guo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Tsubokura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yurie</given_name>
<surname>Iribe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>Relationships Between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2713</first_page>
						<last_page>2717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1267</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tsubokura23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yist Y.</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Tung</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tze Yuang</given_name>
<surname>Chong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>904</first_page>
						<last_page>908</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1272</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajat</given_name>
<surname>Hebbar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rimita</given_name>
<surname>Lahiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lindsay</given_name>
<surname>Butler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lue</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Tager-Flusberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4633</first_page>
						<last_page>4637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1273</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongmei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Zeng</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Multi-Lingual ASR Models for Handling Multiple Talkers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1314</first_page>
						<last_page>1318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1276</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3517</first_page>
						<last_page>3521</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1280</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Igor</given_name>
<surname>Gitman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Laptev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Confidence-based Ensembles of End-to-End Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1414</first_page>
						<last_page>1418</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1281</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gitman23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joonhyung</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hajin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunho</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3367</first_page>
						<last_page>3371</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1282</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5491</first_page>
						<last_page>5495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1285</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yasuda23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaochen</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>A Study on Visualization of Voiceprint Feature</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2233</first_page>
						<last_page>2237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1286</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhifeng</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Ping</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ambrish</given_name>
<surname>Dantrey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bryan</given_name>
<surname>Catanzaro</surname>
</person_name>
					</contributors>
					<titles><title>CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>790</first_page>
						<last_page>794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1287</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kong23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao-Ci</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Jun</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin-Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Di</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>7</first_page>
						<last_page>11</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1292</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yafeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luyao</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Qi</surname>
</person_name>
					</contributors>
					<titles><title>An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2228</first_page>
						<last_page>2232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1294</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Askar</given_name>
<surname>Hamdulla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Visualizing Data Augmentation in Deep Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2243</first_page>
						<last_page>2247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1298</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Huynh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Lun</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Tensor decomposition for minimization of E2E SLU model toward on-device processing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>710</first_page>
						<last_page>714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1299</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kashiwagi23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jizhong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4953</first_page>
						<last_page>4957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1300</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyungchan</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-Wook</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4299</first_page>
						<last_page>4303</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1301</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yoon23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Richeng</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5227</first_page>
						<last_page>5231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1302</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/duan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakeel</given_name>
<surname>Muhammad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3312</first_page>
						<last_page>3316</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1303</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sudo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takatomo</given_name>
<surname>Kano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2943</first_page>
						<last_page>2947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1307</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/matsuura23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanyuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Image-driven Audio-visual Universal Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3729</first_page>
						<last_page>3733</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1309</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yash</given_name>
<surname>Thakran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Acoustic Cues for Multilingual Abuse Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3642</first_page>
						<last_page>3646</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1311</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/thakran23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>386</first_page>
						<last_page>390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1313</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianchen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwen</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tieran</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjun</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guibin</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3147</first_page>
						<last_page>3151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1314</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Berrebbi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>En-Pei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ping</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ho-Lam</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>ML-SUPERB: Multilingual Speech Universal PERformance Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>884</first_page>
						<last_page>888</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1316</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haobin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xulong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>12</first_page>
						<last_page>16</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1317</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Hata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Nakadai</surname>
</person_name>
					</contributors>
					<titles><title>Retraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>491</first_page>
						<last_page>495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1320</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sudo23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shun</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>416</first_page>
						<last_page>420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1321</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/takahashi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Aware Anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2498</first_page>
						<last_page>2502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1323</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kangwook</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungnyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Se-Young</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>316</first_page>
						<last_page>320</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1329</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanbo</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Human Transcription Quality Improvement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3053</first_page>
						<last_page>3057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1330</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khanh-Tung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Truong</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duy Khuong</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoang D.</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan-Son</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Personalization for Robust Voice Pathology Detection in Sound Waves</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1708</first_page>
						<last_page>1712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1332</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tran23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakeel</given_name>
<surname>Muhammad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4479</first_page>
						<last_page>4483</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1333</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sudo23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shashi Kant</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sushant</given_name>
<surname>Hiray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Kukde</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4114</first_page>
						<last_page>4118</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1335</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gupta23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirofumi</given_name>
<surname>Inaguma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Exploration on HuBERT with Multiple Resolution</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3287</first_page>
						<last_page>3291</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1337</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shi23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2253</first_page>
						<last_page>2257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1338</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Nespoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Barreda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jöerg</given_name>
<surname>Bitzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick A.</given_name>
<surname>Naylor</surname>
</person_name>
					</contributors>
					<titles><title>Two-Stage Voice Anonymization for Enhanced Privacy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3854</first_page>
						<last_page>3858</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1341</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nespoli23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Bayes Risk Transducer: Transducer with Controllable Alignment Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4968</first_page>
						<last_page>4972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1342</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tian23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiheng</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feifei</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minjie</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwei</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xionghu</given_name>
<surname>Zhong</surname>
</person_name>
					</contributors>
					<titles><title>Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2723</first_page>
						<last_page>2727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1344</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tzu-Han Zoe</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Lin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juliane</given_name>
<surname>Schubert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Ping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Brown</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Iversen</surname>
</person_name>
					</contributors>
					<titles><title>Similar Hierarchical Representation of Speech and Other Complex Sounds In the Brain and Deep Residual Networks: An MEG Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5182</first_page>
						<last_page>5186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1347</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cheng23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongning</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3347</first_page>
						<last_page>3351</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1349</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haiyang</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Licai</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Cheng</surname>
</person_name>
					</contributors>
					<titles><title>EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3597</first_page>
						<last_page>3601</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1351</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sun23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jixun</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3994</first_page>
						<last_page>3998</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1352</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhavik</given_name>
<surname>Vachhani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dipesh</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rustom</given_name>
<surname>Lawyer</surname>
</person_name>
					</contributors>
					<titles><title>Multi-resolution Approach to Identification of Spoken Languages and To Improve Overall Language Diarization System Using Whisper Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1993</first_page>
						<last_page>1997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1354</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vachhani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeonjong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Reverberation-Controllable Voice Conversion Using Reverberation Time Estimator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2103</first_page>
						<last_page>2107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1356</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/choi23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuhei</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiichi</given_name>
<surname>Hashimoto</surname>
</person_name>
					</contributors>
					<titles><title>Speech-to-Face Conversion Using Denoising Diffusion Probabilistic Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2188</first_page>
						<last_page>2192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1358</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kato23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinghua</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aijun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4748</first_page>
						<last_page>4752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1359</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hu23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruiteng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongwei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhai</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1858</first_page>
						<last_page>1862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1360</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Memory Augmented Lookup Dictionary Based Language Modeling for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>481</first_page>
						<last_page>485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1362</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/feng23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yajie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Interactions Between Target Positive and Negative Information for Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2443</first_page>
						<last_page>2447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1364</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/han23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2438</first_page>
						<last_page>2442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1376</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuang</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1154</first_page>
						<last_page>1158</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1377</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weinan</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaoxun</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1334</first_page>
						<last_page>1338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1378</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Cord-Landwehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cătălin</given_name>
<surname>Zorilă</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama</given_name>
<surname>Doddipatla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>A Teacher-Student Approach for Extracting Informative Speaker Embeddings From Speech Mixtures</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4703</first_page>
						<last_page>4707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1379</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cordlandwehr23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchang</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuomas</given_name>
<surname>Virtanen</surname>
</person_name>
					</contributors>
					<titles><title>Few-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>301</first_page>
						<last_page>305</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1380</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xie23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soowon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Eun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seo-Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1159</first_page>
						<last_page>1163</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1381</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuankun</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haonan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ye</surname>
</person_name>
					</contributors>
					<titles><title>Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2808</first_page>
						<last_page>2812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1383</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xie23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingfa</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huayi</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Prompting for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3108</first_page>
						<last_page>3112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1385</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Remixing-based Unsupervised Source Separation from Scratch</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1678</first_page>
						<last_page>1682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1389</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/saijo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chutong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Ao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2978</first_page>
						<last_page>2982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1390</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/meng23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sang-eun</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeonseok</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-won</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyungjae</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>On Monotonic Aggregation for Open-domain QA</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3432</first_page>
						<last_page>3436</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1391</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/han23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi Xuan</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Navonil</given_name>
<surname>Majumder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soujanya</given_name>
<surname>Poria</surname>
</person_name>
					</contributors>
					<titles><title>Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3914</first_page>
						<last_page>3918</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1392</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tan23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yucheng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4673</first_page>
						<last_page>4677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1397</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cai23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bernhard C.</given_name>
<surname>Geiger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Graph Theory Methods For the Analysis of Pronunciation Variation in Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>596</first_page>
						<last_page>600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1398</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/geiger23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sung Hwan</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeonghun</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min Hyun</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>Towards Single Integrated Spoofing-aware Speaker Verification Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3989</first_page>
						<last_page>3993</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1402</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye-Rin</given_name>
<surname>Jeoung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-Seok</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jehyun</given_name>
<surname>Kyung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3197</first_page>
						<last_page>3201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1404</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jeoung23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoori</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juheon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoseob</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4818</first_page>
						<last_page>4822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1405</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/oh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srijith</given_name>
<surname>Radhakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sumeer Ahmad</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Narsis A.</given_name>
<surname>Kiani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Gomez-Cabrero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper N.</given_name>
<surname>Tegner</surname>
</person_name>
					</contributors>
					<titles><title>A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1958</first_page>
						<last_page>1962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1407</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/radhakrishnan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuto</given_name>
<surname>Matsuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiko</given_name>
<surname>Arimoto</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Laughter and Screaming Using the Attention and CTC Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1025</first_page>
						<last_page>1029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1412</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/matsuda23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carina</given_name>
<surname>Steiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dieter</given_name>
<surname>Studer-Joho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Lanthemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrin</given_name>
<surname>Büchler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Leemann</surname>
</person_name>
					</contributors>
					<titles><title>Sociodemographic and Attitudinal Effects on Dialect Speakers’ Articulation of the Standard Language: Evidence from German-Speaking Switzerland</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3542</first_page>
						<last_page>3546</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1414</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/steiner23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akira</given_name>
<surname>Sasou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of GIF- and SSL-based Features in Pathological-voice Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2893</first_page>
						<last_page>2897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1418</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sasou23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avik</given_name>
<surname>Ray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Compositional Generalization in Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>750</first_page>
						<last_page>754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1419</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ray23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3467</first_page>
						<last_page>3471</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1422</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/meng23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shi-wook</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3944</first_page>
						<last_page>3948</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1425</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sangmin</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June-Woo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Won-Yang</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyerim</given_name>
<surname>Baek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soyoun</given_name>
<surname>Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byungjo</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changwan</given_name>
<surname>Ha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyongpil</given_name>
<surname>Tae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungnyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Se-Young</given_name>
<surname>Yun</surname>
</person_name>
					</contributors>
					<titles><title>Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5436</first_page>
						<last_page>5440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1426</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bae23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zerui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoneng</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xian</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yabin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingyun</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>FunASR: A Fundamental End-to-End Speech Recognition Toolkit</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1593</first_page>
						<last_page>1597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1428</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean-Marie</given_name>
<surname>Lemercier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Tobergte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4024</first_page>
						<last_page>4028</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1429</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lemercier23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiuping</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhen</given_name>
<surname>Ren</surname>
</person_name>
					</contributors>
					<titles><title>A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5416</first_page>
						<last_page>5420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1430</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xiao23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3699</first_page>
						<last_page>3703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1431</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3272</first_page>
						<last_page>3276</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1432</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhu23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name>
					</contributors>
					<titles><title>High Fidelity Speech Enhancement with Band-split RNN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2483</first_page>
						<last_page>2487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1433</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Towards Reference Speech Characterization for Health Applications</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2363</first_page>
						<last_page>2367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1435</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/botelho23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fredrik</given_name>
<surname>Cumlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Schüldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saikat</given_name>
<surname>Chatterjee</surname>
</person_name>
					</contributors>
					<titles><title>DeePMOS: Deep Posterior Mean-Opinion-Score of Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>526</first_page>
						<last_page>530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1436</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liang23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Embedding Articulatory Constraints for Low-resource Speech Recognition Based on Large Pre-trained Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1394</first_page>
						<last_page>1398</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1437</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adria</given_name>
<surname>Mallol-Ragolta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nils</given_name>
<surname>Urbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2358</first_page>
						<last_page>2362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1438</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mallolragolta23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianrui</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Can Contextual Biasing Remain Effective with Whisper and GPT-2?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1289</first_page>
						<last_page>1293</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1440</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sun23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye-Xin</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3834</first_page>
						<last_page>3838</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1441</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lu23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suo</given_name>
<surname>Hongbin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulong</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>Task-Agnostic Structured Pruning of Speech Representation Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>231</first_page>
						<last_page>235</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1442</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Betz</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2538</first_page>
						<last_page>2542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1443</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wagner23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinfeng</given_name>
<surname>Bai</surname>
</person_name>
					</contributors>
					<titles><title>DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2273</first_page>
						<last_page>2277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1444</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bunlong</given_name>
<surname>Lay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Welker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julius</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3809</first_page>
						<last_page>3813</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1445</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lay23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victoria Y. H.</given_name>
<surname>Chua</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei Ting</given_name>
<surname>Woon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyi</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy W. H.</given_name>
<surname>Khong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Dauwels</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzy J.</given_name>
<surname>Styles</surname>
</person_name>
					</contributors>
					<titles><title>MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4109</first_page>
						<last_page>4113</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1446</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chua23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rimita</given_name>
<surname>Lahiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajat</given_name>
<surname>Hebbar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lord</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>So Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3557</first_page>
						<last_page>3561</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1447</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lahiri23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuyi</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingjiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition Transformer with Global Contextual Information Decoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4474</first_page>
						<last_page>4478</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1450</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/qian23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Szakay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name>
					</contributors>
					<titles><title>Creak Prevalence and Prosodic Context in Australian English</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>112</first_page>
						<last_page>116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1454</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/white23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chowdam</given_name>
<surname>Venkata Thirumala Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanuka</given_name>
<surname>Bhattacharjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Belur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atchayaram</given_name>
<surname>Nalini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Multi-class Vowels and Fricatives From Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>146</first_page>
						<last_page>150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1455</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/venkatathirumalakumar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowei</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Giavazzi</surname>
</person_name>
					</contributors>
					<titles><title>Lexical Stress and Velar Palatalization in Italian: A spatio-temporal Interaction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1833</first_page>
						<last_page>1837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1456</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuyue</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruihua</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4828</first_page>
						<last_page>4832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1460</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Kobashikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2663</first_page>
						<last_page>2667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1464</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hojo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengyun</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaofeng</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinqiang</given_name>
<surname>Leng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Improving End-to-End Modeling For Mandarin-English Code-Switching Using Lightweight Switch-Routing Mixture-of-Experts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4224</first_page>
						<last_page>4228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1465</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tan23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangpeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>How ChatGPT is Robust for Spoken Language Understanding?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2163</first_page>
						<last_page>2167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1466</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Markopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Maniati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Vamvoukakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Vardaxoglou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panos</given_name>
<surname>Kakoulidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junkwang</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunu</given_name>
<surname>Jho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inchul</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aimilios</given_name>
<surname>Chalamandaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pirros</given_name>
<surname>Tsiakoulis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyros</given_name>
<surname>Raptis</surname>
</person_name>
					</contributors>
					<titles><title>Generating Multilingual Gender-Ambiguous Text-to-Speech Voices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>621</first_page>
						<last_page>625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1467</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/markopoulos23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viet Dac</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abel</given_name>
<surname>Salinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung</given_name>
<surname>Bui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghyun</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanieh</given_name>
<surname>Deilamsalehy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dernoncourt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thien Huu</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>Boosting Punctuation Restoration with Data Generation and Reinforcement Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2133</first_page>
						<last_page>2137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1468</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lai23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>CoMFLP: Correlation Measure Based Fast Search on ASR Layer Pruning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3282</first_page>
						<last_page>3286</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1469</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Santiago</given_name>
<surname>Cuervo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name>
					</contributors>
					<titles><title>On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1788</first_page>
						<last_page>1792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1476</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cuervo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>341</first_page>
						<last_page>345</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1478</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xin23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theo</given_name>
<surname>Lepage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reda</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4708</first_page>
						<last_page>4712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1479</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lepage23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lindun</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4763</first_page>
						<last_page>4767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1480</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ge23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puja</given_name>
<surname>Bharati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabyasachi</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shayamal Kumar</given_name>
<surname>Das Mandal</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali)</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3068</first_page>
						<last_page>3072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1481</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bharati23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhitao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengying</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Prompt Guided Copy Mechanism for Conversational Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3422</first_page>
						<last_page>3426</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1485</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenzhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yupeng</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuyong</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shidong</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-mode Neural Speech Coding Based on Deep Generative Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>819</first_page>
						<last_page>823</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1490</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xiao23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingchen</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhendong</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuping</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1648</first_page>
						<last_page>1652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1497</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/song23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thanathai</given_name>
<surname>Lertpetchpun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Instance-based Temporal Normalization for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3172</first_page>
						<last_page>3176</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1498</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lertpetchpun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shutong</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maokui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baoxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiakui</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Adaptation with Quality-Aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3482</first_page>
						<last_page>3486</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1502</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/niu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minghan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinglu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaosong</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongyao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hengchao</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daimeng</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shimin</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>WhiSLU: End-to-End Spoken Language Understanding with Whisper</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>770</first_page>
						<last_page>774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1505</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Kitagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taichi</given_name>
<surname>Asami</surname>
</person_name>
					</contributors>
					<titles><title>What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1873</first_page>
						<last_page>1877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1507</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kitagishi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Le</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongxiu</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huibao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2293</first_page>
						<last_page>2297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1508</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogier</given_name>
<surname>van Dalen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto Gil C. P.</given_name>
<surname>Ramos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sourav</given_name>
<surname>Bhattacharya</surname>
</person_name>
					</contributors>
					<titles><title>On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>581</first_page>
						<last_page>585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1510</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/parcollet23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenzhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yupeng</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4044</first_page>
						<last_page>4048</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1511</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yafeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luyao</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5301</first_page>
						<last_page>5305</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1513</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1369</first_page>
						<last_page>1373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1517</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tsunoo23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangze</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3487</first_page>
						<last_page>3491</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1521</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liang23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Model Compression for DNN-based Speaker Verification Using Weight Quantization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1988</first_page>
						<last_page>1992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1524</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>LightClone: Speaker-guided Parallel Subnet Selection for Few-shot Voice Cloning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4868</first_page>
						<last_page>4872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1528</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Matejka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef</given_name>
<surname>Slavíček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mosner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Klčo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Description and Analysis of ABC Submission to NIST LRE 2022</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>511</first_page>
						<last_page>515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1529</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/matejka23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linfeng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Overlap Aware Continuous Speech Separation without Permutation Invariant Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3512</first_page>
						<last_page>3516</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1530</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenz</given_name>
<surname>Diener</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marju</given_name>
<surname>Purin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sten</given_name>
<surname>Sootla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ando</given_name>
<surname>Saabas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Aichner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name>
					</contributors>
					<titles><title>PLCMOS – A Data-driven Non-intrusive Metric for The Evaluation of Packet Loss Concealment Algorithms</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2533</first_page>
						<last_page>2537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1532</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/diener23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Kawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Plata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Czuba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Szymański</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Syga</surname>
</person_name>
					</contributors>
					<titles><title>Improved DeepFake Detection Using Whisper Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4009</first_page>
						<last_page>4013</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1537</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kawa23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anneliese</given_name>
<surname>Kelterer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>(Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-prosodic Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4768</first_page>
						<last_page>4772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1538</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kelterer23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Massa</given_name>
<surname>Baali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ibrahim</given_name>
<surname>Almakky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shady</given_name>
<surname>Shehata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fakhri</given_name>
<surname>Karray</surname>
</person_name>
					</contributors>
					<titles><title>Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1558</first_page>
						<last_page>1562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1541</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/baali23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanuka</given_name>
<surname>Bhattacharjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjali</given_name>
<surname>Jayakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Belur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atchayaram</given_name>
<surname>Nalini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1543</first_page>
						<last_page>1547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1542</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bhattacharjee23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryosuke</given_name>
<surname>Sawata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Murata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhta</given_name>
<surname>Takida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshimitsu</given_name>
<surname>Uesaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Shibuya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shusuke</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Mitsufuji</surname>
</person_name>
					</contributors>
					<titles><title>Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3824</first_page>
						<last_page>3828</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1547</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sawata23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li-Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao-Fei</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2473</first_page>
						<last_page>2477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1548</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Hassan</given_name>
<surname>Vali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>306</first_page>
						<last_page>310</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1549</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vali23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marie</given_name>
<surname>Kunešová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindřich</given_name>
<surname>Matoušek</surname>
</person_name>
					</contributors>
					<titles><title>Neural Speech Synthesis with Enriched Phrase Boundaries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4833</first_page>
						<last_page>4837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1552</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kunesova23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Jiang</surname>
</person_name>
					</contributors>
					<titles><title>Chinese Dialect Recognition Based on Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5341</first_page>
						<last_page>5345</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1554</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gege</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuefeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaojun</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ranjie</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Xue</surname>
</person_name>
					</contributors>
					<titles><title>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>561</first_page>
						<last_page>565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1556</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/qi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingyan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5286</first_page>
						<last_page>5290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1557</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ia_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4698</first_page>
						<last_page>4702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1558</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youqiang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>171</first_page>
						<last_page>175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1562</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zheng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ismail</given_name>
<surname>Khalfaoui-Hassani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Etienne</given_name>
<surname>Labbé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothée</given_name>
<surname>Masquelier</surname>
</person_name>
					</contributors>
					<titles><title>Adapting a ConvNeXt Model to Audio Classification on AudioSet</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4169</first_page>
						<last_page>4173</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1564</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pellegrini23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaying</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Ordered and Binary Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4683</first_page>
						<last_page>4687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1565</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ja_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ambuj</given_name>
<surname>Mehrish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Ramesh Kashyap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Yingting</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Navonil</given_name>
<surname>Majumder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soujanya</given_name>
<surname>Poria</surname>
</person_name>
					</contributors>
					<titles><title>ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4284</first_page>
						<last_page>4288</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1568</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mehrish23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaya</given_name>
<surname>Sy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William N.</given_name>
<surname>Havard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Lavechin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>Measuring Language Development From Child-centered Recordings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4618</first_page>
						<last_page>4622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1569</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sy23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanzhang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhitao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Ouyang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2173</first_page>
						<last_page>2177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1570</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fan23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyungchan</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3023</first_page>
						<last_page>3027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1571</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yoon23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mary</given_name>
<surname>Paterson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Moor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luisa</given_name>
<surname>Cutillo</surname>
</person_name>
					</contributors>
					<titles><title>A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2993</first_page>
						<last_page>2997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1574</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/paterson23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Shinayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name>
					</contributors>
					<titles><title>Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>854</first_page>
						<last_page>858</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1578</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sato23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji Won</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seok Min</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2833</first_page>
						<last_page>2837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1579</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yoon23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngdo</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2743</first_page>
						<last_page>2747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1581</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ahn23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beibei</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Joint Time and Frequency Transformer for Chinese Opera Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3919</first_page>
						<last_page>3923</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1582</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Koizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Yatabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuyuki</given_name>
<surname>Morioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Bacchiani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name>
					</contributors>
					<titles><title>LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5496</first_page>
						<last_page>5500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1584</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/koizumi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eimear</given_name>
<surname>Stanley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>DeMattos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Klementiev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Ozimek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Clarke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitri</given_name>
<surname>Palaz</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2418</first_page>
						<last_page>2422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1591</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/stanley23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongfu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqian</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Automatic Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1009</first_page>
						<last_page>1013</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1592</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesuraj</given_name>
<surname>Bandekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5147</first_page>
						<last_page>5151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1593</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bandekar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shijun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damian</given_name>
<surname>Borth</surname>
</person_name>
					</contributors>
					<titles><title>Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>351</first_page>
						<last_page>355</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1595</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ka_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Premjeet</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Goutam</given_name>
<surname>Saha</surname>
</person_name>
					</contributors>
					<titles><title>A novel frequency warping scale for speech emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3647</first_page>
						<last_page>3651</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1600</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/singh23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhonghua</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shijun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2298</first_page>
						<last_page>2302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1602</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kihyun</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youkyum</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaesung</given_name>
<surname>Huh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Disentangled Representation Learning for Multilingual Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5316</first_page>
						<last_page>5320</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1603</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nam23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olympia</given_name>
<surname>Simantiraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Pantazis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name>
					</contributors>
					<titles><title>The effect of masking noise on listeners’ spectral tilt preferences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3058</first_page>
						<last_page>3062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1604</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/simantiraki23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangyao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Scale Attention for Audio Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3442</first_page>
						<last_page>3446</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1606</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amin</given_name>
<surname>Honarmandi Shandiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name>
					</contributors>
					<titles><title>Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1169</first_page>
						<last_page>1173</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1607</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/toth23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sang-Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Yeong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Seok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4439</first_page>
						<last_page>4443</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1608</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingtao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangzhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liping</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>934</first_page>
						<last_page>938</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1609</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchang</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Few-shot Class-incremental Audio Classification Using Stochastic Classifier</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4174</first_page>
						<last_page>4178</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1610</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Florian</given_name>
<surname>Mai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Zuluaga-Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name>
					</contributors>
					<titles><title>HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2213</first_page>
						<last_page>2217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1611</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mai23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Umar</given_name>
<surname>Farooq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Learning Cross-lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5072</first_page>
						<last_page>5076</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1613</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/farooq23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuenan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Enhance Temporal Relations in Audio Captioning with Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4179</first_page>
						<last_page>4183</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1614</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xie23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J. F.</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3267</first_page>
						<last_page>3271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1616</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanyu</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2898</first_page>
						<last_page>2902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1617</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/meng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin</given_name>
<surname>Sakuma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Fujie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaibo</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2668</first_page>
						<last_page>2672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1618</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sakuma23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mathilde</given_name>
<surname>Hutin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liesbeth</given_name>
<surname>Degand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Allassonnière-Tang</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2563</first_page>
						<last_page>2567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1619</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hutin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingran</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2803</first_page>
						<last_page>2807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1621</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gong23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Pérez Zarazaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zofia</given_name>
<surname>Malisz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-independent neural formant synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5556</first_page>
						<last_page>5560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1622</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/perezzarazaga23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minchuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenfeng</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5037</first_page>
						<last_page>5041</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1623</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xipin</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zirui</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5391</first_page>
						<last_page>5395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1626</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wei23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinzi</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>151</first_page>
						<last_page>155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1627</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/qi23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhilong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2248</first_page>
						<last_page>2252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1630</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xing</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland van</given_name>
<surname>Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danielle</given_name>
<surname>Reuvekamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Intelligibility in Non-native Speech: Comparing Measures Obtained at Different Levels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>964</first_page>
						<last_page>968</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1635</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wei23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusheng</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4893</first_page>
						<last_page>4897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1639</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tian23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiga</given_name>
<surname>Yamane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiko</given_name>
<surname>Yamazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mihiro</given_name>
<surname>Uchida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keita</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Joint Target and Non-Target Speakers ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2903</first_page>
						<last_page>2907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1640</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/masumura23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Doyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyewon</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youna</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3829</first_page>
						<last_page>3833</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1642</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashwini</given_name>
<surname>Dasare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradyoth</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Supritha</given_name>
<surname>Shetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>K T</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>531</first_page>
						<last_page>535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1644</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/dasare23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Imen</given_name>
<surname>Ben-Amor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>O'Brien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-Michel</given_name>
<surname>Bousquet</surname>
</person_name>
					</contributors>
					<titles><title>Describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3207</first_page>
						<last_page>3211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1648</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/benamor23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2718</first_page>
						<last_page>2722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1650</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhao23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kübra</given_name>
<surname>Bodur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roxane</given_name>
<surname>Bertrand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James S.</given_name>
<surname>German</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stéphane</given_name>
<surname>Rauzy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Fredouille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Meunier</surname>
</person_name>
					</contributors>
					<titles><title>Speech reduction: position within French prosodic structure</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>117</first_page>
						<last_page>121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1651</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bodur23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-En</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Disentangled Learning for Memory-Augmented Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2958</first_page>
						<last_page>2962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1652</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chien23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mario</given_name>
<surname>Zusag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurin</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theresa</given_name>
<surname>Bloder</surname>
</person_name>
					</contributors>
					<titles><title>Careful Whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3013</first_page>
						<last_page>3017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1653</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zusag23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name>
					</contributors>
					<titles><title>Transcribing Speech as Spoken and Written Dual Text Using an Autoregressive Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>461</first_page>
						<last_page>465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1655</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ihori23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lixia</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The effect of stress on Mandarin tonal perception in continuous speech for Spanish-speaking learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3073</first_page>
						<last_page>3077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1662</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hao23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhao</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switchboard Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3332</first_page>
						<last_page>3336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1665</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabrina</given_name>
<surname>Milkus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gebhard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Hohmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Muderlak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Schottdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Musil</surname>
</person_name>
					</contributors>
					<titles><title>The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4139</first_page>
						<last_page>4143</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1668</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kathan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sanjana</given_name>
<surname>Sankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Beautemps</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frédéric</given_name>
<surname>Elisei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Perrotin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4978</first_page>
						<last_page>4982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1669</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sankar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xule</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weishan</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haifeng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongqian</given_name>
<surname>Sun</surname>
</person_name>
					</contributors>
					<titles><title>EE-TTS: Emphatic Expressive TTS with Linguistic Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4873</first_page>
						<last_page>4877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1671</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sewade</given_name>
<surname>Ogun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Colotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4878</first_page>
						<last_page>4882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1673</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ogun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruihai</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2118</first_page>
						<last_page>2122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1674</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2183</first_page>
						<last_page>2187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1675</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>O'Brien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrien</given_name>
<surname>Gresse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Baptise</given_name>
<surname>Billaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guilhem</given_name>
<surname>Belda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Differentiating acoustic and physiological features in speech for hypoxia detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5013</first_page>
						<last_page>5017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1678</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/obrien23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heejin</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary Geunbae</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Score-balanced Loss for Multi-aspect Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4998</first_page>
						<last_page>5002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1679</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/do23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yota</given_name>
<surname>Ueda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihiro</given_name>
<surname>Takamune</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>HumanDiffusion: diffusion model using perceptual gradients</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4264</first_page>
						<last_page>4268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1680</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ueda23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo</given_name>
<surname>Molenaar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5232</first_page>
						<last_page>5236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1681</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/molenaar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elena</given_name>
<surname>Ryumina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Ryumin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Markitantov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heysem</given_name>
<surname>Kaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4049</first_page>
						<last_page>4053</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1686</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yewei</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianfeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaowei</given_name>
<surname>Yi</surname>
</person_name>
					</contributors>
					<titles><title>Robust Feature Decoupling in Voice Conversion by Using Locality-Based Instance Normalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5471</first_page>
						<last_page>5475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1691</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinghong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaowei</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianfeng</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>A Compressed Synthetic Speech Detection Method with Compression Feature Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5376</first_page>
						<last_page>5380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1696</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1968</first_page>
						<last_page>1972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1702</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Burridge</surname>
</person_name>
					</contributors>
					<titles><title>Vowel Normalisation in Latent Space for Sociolinguistics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3547</first_page>
						<last_page>3551</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1704</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/burridge23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suzy J.</given_name>
<surname>Styles</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victoria Y. H.</given_name>
<surname>Chua</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei Ting</given_name>
<surname>Woon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy W. H.</given_name>
<surname>Khong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Dauwels</surname>
</person_name>
					</contributors>
					<titles><title>Investigating model performance in language identification: beyond simple error statistics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4129</first_page>
						<last_page>4133</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1707</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/styles23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Fara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orlaith</given_name>
<surname>Hickey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Georgescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Goria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Molimpakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1728</first_page>
						<last_page>1732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1709</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fara23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haotian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hengshun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuling</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangjiang</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2678</first_page>
						<last_page>2682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1717</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23la_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guan-Wei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guan-Ting</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1503</first_page>
						<last_page>1507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1718</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edward L.</given_name>
<surname>Campbell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judith</given_name>
<surname>Dineley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Conde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faith</given_name>
<surname>Matcham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katie M.</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carolin</given_name>
<surname>Oetzmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Simblett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stuart</given_name>
<surname>Bruce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amos A.</given_name>
<surname>Folarin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Til</given_name>
<surname>Wykes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Vairavan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard J. B.</given_name>
<surname>Dobson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Docio-Fernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carmen</given_name>
<surname>Garcia-Mateo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaibhav A.</given_name>
<surname>Narayan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Hotopf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models.</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1738</first_page>
						<last_page>1742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1721</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/campbell23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Svihlik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vojtěch</given_name>
<surname>Illner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Kryze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mário</given_name>
<surname>Sousa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Krack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elina</given_name>
<surname>Tripoliti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Jech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rusz</surname>
</person_name>
					</contributors>
					<titles><title>Relationship between LTAS-based spectral moments and acoustic parameters of hypokinetic dysarthria in Parkinson’s disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1758</first_page>
						<last_page>1762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1722</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/svihlik23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning based Deep Latent Masking for Music Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3709</first_page>
						<last_page>3713</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1723</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc-Tuan</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Hieu</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1938</first_page>
						<last_page>1942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1725</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yip23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Seki</surname>
</person_name>
					</contributors>
					<titles><title>iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4369</first_page>
						<last_page>4373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1726</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kaneko23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yahuan</given_name>
<surname>Cong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haopeng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shichao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunfeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5486</first_page>
						<last_page>5490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1727</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cong23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Jouaiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pippa</given_name>
<surname>Kirby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Vaidyanathan</surname>
</person_name>
					</contributors>
					<titles><title>Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4508</first_page>
						<last_page>4512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1729</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jouaiti23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peiying</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunlu</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junqing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Label Information for Multimodal Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4219</first_page>
						<last_page>4223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1732</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ma_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucía</given_name>
<surname>Gómez-Zaragozá</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Wills</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Marín-Morales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariano</given_name>
<surname>Alcañiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2403</first_page>
						<last_page>2407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1734</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gomezzaragoza23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddarth</given_name>
<surname>Chandrasekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arvind</given_name>
<surname>Ramesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tilak</given_name>
<surname>Purohit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4518</first_page>
						<last_page>4522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1738</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chandrasekar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenhe</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoguang</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangqing</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahao</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhoujun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>740</first_page>
						<last_page>744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1740</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vojtěch</given_name>
<surname>Illner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Krýže</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švihlík</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mário</given_name>
<surname>Sousa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Krack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elina</given_name>
<surname>Tripoliti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Jech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rusz</surname>
</person_name>
					</contributors>
					<titles><title>Which aspects of motor speech disorder are captured by Mel Frequency Cepstral Coefficients? Evidence from the change in STN-DBS conditions in Parkinson’s disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5027</first_page>
						<last_page>5031</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1744</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/illner23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bang</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suo</given_name>
<surname>Hongbin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulong</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>SEF-Net: Speaker Embedding Free Target Speaker Extraction Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3452</first_page>
						<last_page>3456</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1749</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zeng23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Martel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julius</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1673</first_page>
						<last_page>1677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1753</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/martel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiqin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiaochu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3377</first_page>
						<last_page>3381</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1754</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danilo</given_name>
<surname>de Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Navin</given_name>
<surname>Raj Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3632</first_page>
						<last_page>3636</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1758</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/deoliveira23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Monica</given_name>
<surname>Gonzalez-Machorro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Hecker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uwe D.</given_name>
<surname>Reichel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helly N.</given_name>
<surname>Hammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Hoepner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Pedrotti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alisha</given_name>
<surname>Zmutt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hesam</given_name>
<surname>Sagha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan van</given_name>
<surname>Beek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Eyben</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dagmar M.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bert</given_name>
<surname>Arnrich</surname>
</person_name>
					</contributors>
					<titles><title>Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1518</first_page>
						<last_page>1522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1759</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gonzalezmachorro23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shangeth</given_name>
<surname>Rajaa</surname>
</person_name>
					</contributors>
					<titles><title>Improving End-to-End SLU performance with Prosodic Attention and Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1114</first_page>
						<last_page>1118</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1760</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rajaa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qibao</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruohua</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Impact of Back-End Network on Wav2vec 2.0 for Dialect Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5356</first_page>
						<last_page>5360</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1761</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/luo23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2123</first_page>
						<last_page>2127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1762</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shelly</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyanshi</given_name>
<surname>Pal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2608</first_page>
						<last_page>2612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1764</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jain23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Honglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengyun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanjie</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>SDNet: Stream-attention and Dual-feature Learning Network for Ad-hoc Array Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3739</first_page>
						<last_page>3743</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1766</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23na_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marco</given_name>
<surname>Gaido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Papi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Turchi</surname>
</person_name>
					</contributors>
					<titles><title>Joint Speech Translation and Named Entity Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>47</first_page>
						<last_page>51</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1767</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gaido23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emma</given_name>
<surname>Reyner-Fuentes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Rituerto-González</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carmen</given_name>
<surname>Peláez-Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1768</first_page>
						<last_page>1772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1771</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/reynerfuentes23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maxwell</given_name>
<surname>Hope</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charlotte</given_name>
<surname>Ward</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Lilley</surname>
</person_name>
					</contributors>
					<titles><title>Nonbinary American English speakers encode gender in vowel acoustics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4713</first_page>
						<last_page>4717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1772</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hope23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexis Deighton</given_name>
<surname>MacIntyre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Goehring</surname>
</person_name>
					</contributors>
					<titles><title>Effects of spectral degradation on the cortical tracking of the speech envelope</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5187</first_page>
						<last_page>5191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1776</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/macintyre23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanjie</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengyun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Locate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3789</first_page>
						<last_page>3793</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1777</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Janine</given_name>
<surname>Rugayan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2158</first_page>
						<last_page>2162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1778</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rugayan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guanghou</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongmao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunlin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4888</first_page>
						<last_page>4892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1779</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinya</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Chinese EFL Learners’ Perception of English Prosodic Focus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>92</first_page>
						<last_page>96</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1781</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Favaro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyu</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Butala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name>
					</contributors>
					<titles><title>Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2388</first_page>
						<last_page>2392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1784</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/favaro23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meiying</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2098</first_page>
						<last_page>2102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1788</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunnar</given_name>
<surname>Kukk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet-Bac</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claude</given_name>
<surname>Barras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdel</given_name>
<surname>Messaoudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Waad</given_name>
<surname>Ben Kheder</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>516</first_page>
						<last_page>520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1790</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/alumae23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Digvijay Anil</given_name>
<surname>Ingle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayush</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendra</given_name>
<surname>Vepa</surname>
</person_name>
					</contributors>
					<titles><title>Listening To Silences In Contact Center Conversations Using Textual Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2688</first_page>
						<last_page>2692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1791</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ingle23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>JungPhil</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yungyeo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2513</first_page>
						<last_page>2517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1795</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/park23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mutian</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name>
					</contributors>
					<titles><title>Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1109</first_page>
						<last_page>1113</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1799</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/he23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Léa-Marie</given_name>
<surname>Lam-Yee-Mui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas Ondel</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name>
					</contributors>
					<titles><title>Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>87</first_page>
						<last_page>91</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1802</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lamyeemui23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4229</first_page>
						<last_page>4233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1803</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Simon</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Vieting</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Mixture Encoder for Joint Speech Separation and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3527</first_page>
						<last_page>3531</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1815</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/berger23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pailla Balakrishna</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Achyut Mani</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megavath Bharadwaj</given_name>
<surname>Rathod</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejesh</given_name>
<surname>Karavadi</surname>
</person_name>
					</contributors>
					<titles><title>Towards Multi-Lingual Audio Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>356</first_page>
						<last_page>360</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1816</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/behera23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sabine</given_name>
<surname>Gosselke Berthelsen</surname>
</person_name>
					</contributors>
					<titles><title>Adaptation to predictive prosodic cues in non-native standard dialect</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4234</first_page>
						<last_page>4238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1818</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gosselkeberthelsen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eros</given_name>
<surname>Rosello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Gomez-Alanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel M.</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Peinado</surname>
</person_name>
					</contributors>
					<titles><title>A conformer-based classifier for variable-length utterance processing in anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5281</first_page>
						<last_page>5285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1820</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rosello23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mietta</given_name>
<surname>Lennes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minnaleena</given_name>
<surname>Toivola</surname>
</person_name>
					</contributors>
					<titles><title>Pitch distributions in a very large corpus of spontaneous Finnish speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4778</first_page>
						<last_page>4782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1822</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lennes23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taichi</given_name>
<surname>Asami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukinori</given_name>
<surname>Honma</surname>
</person_name>
					</contributors>
					<titles><title>SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2888</first_page>
						<last_page>2892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1823</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ashihara23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Artit</given_name>
<surname>Suwanbandit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Burin</given_name>
<surname>Naowarat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orathai</given_name>
<surname>Sangpetch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4069</first_page>
						<last_page>4073</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1828</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/suwanbandit23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taesik</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josh</given_name>
<surname>Belanich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Somandepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arsha</given_name>
<surname>Nagrani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Eoff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brendan</given_name>
<surname>Jou</surname>
</person_name>
					</contributors>
					<titles><title>LanSER: Language-Model Supported Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2408</first_page>
						<last_page>2412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1832</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gong23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Doukhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Devauchelle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucile</given_name>
<surname>Girard-Monneron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mía</given_name>
<surname>Chávez Ruz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>V.</given_name>
<surname>Chaddouk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabelle</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name>
					</contributors>
					<titles><title>Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5207</first_page>
						<last_page>5211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1835</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/doukhan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Pirlogeanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandru-Lucian</given_name>
<surname>Georgescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title>The SpeeD--ZevoTech submission at DISPLACE 2023</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3572</first_page>
						<last_page>3576</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1839</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pirlogeanu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joshua Jansen</given_name>
<surname>van Vüren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1439</first_page>
						<last_page>1443</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1841</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vanvuren23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ayushi</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sébastien</given_name>
<surname>Le Maguer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Listener sensitivity to deviating obstruents in WaveNet</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1080</first_page>
						<last_page>1084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1843</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pandey23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis</given_name>
<surname>ten Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Bentum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Phonemic competition in end-to-end ASR models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>586</first_page>
						<last_page>590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1846</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tenbosch23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina</given_name>
<surname>Markl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>Everyone has an accent</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4424</first_page>
						<last_page>4427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1847</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/markl23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahsa</given_name>
<surname>Kadkhodaei Elyaderani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahram</given_name>
<surname>Shirani</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-to-Sequence Multi-Modal Speech In-Painting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>829</first_page>
						<last_page>833</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1848</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kadkhodaeielyaderani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debasmita</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Capturing Formality in Speech Across Domains and Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1030</first_page>
						<last_page>1034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1852</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bhattacharya23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seongmin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkyu</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihwa</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>730</first_page>
						<last_page>734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1859</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/park23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eduard</given_name>
<surname>Tulchinskii</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristian</given_name>
<surname>Kuznetsov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laida</given_name>
<surname>Kushnareva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniil</given_name>
<surname>Cherniavskii</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Serguei</given_name>
<surname>Barannikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Piontkovskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Nikolenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Burnaev</surname>
</person_name>
					</contributors>
					<titles><title>Topological Data Analysis for Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>311</first_page>
						<last_page>315</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1861</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tulchinskii23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hallap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Dunbar</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating context-invariance in unsupervised speech representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2973</first_page>
						<last_page>2977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1862</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hallap23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Towards continually learning new languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3262</first_page>
						<last_page>3266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1867</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pham23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenthe</given_name>
<surname>Thienpondt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caroline M.</given_name>
<surname>Speksnijder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name>
					</contributors>
					<titles><title>Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3018</first_page>
						<last_page>3022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1868</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/thienpondt23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4648</first_page>
						<last_page>4652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1873</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/baumann23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eesung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Jajodia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cindy</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divya</given_name>
<surname>Neelagiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taeyeon</given_name>
<surname>Ki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijendra Raj</given_name>
<surname>Apsingekar</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3959</first_page>
						<last_page>3963</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1878</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jasmin</given_name>
<surname>Pöhnlein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicitas</given_name>
<surname>Kleber</surname>
</person_name>
					</contributors>
					<titles><title>The emergence of obstruent-intrinsic f0 and VOT as cues to the fortis/lenis contrast in West Central Bavarian</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1808</first_page>
						<last_page>1812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1879</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pohnlein23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Reed</given_name>
<surname>Blaylock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Beatboxing Kick Drum Kinematics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2583</first_page>
						<last_page>2587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1880</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/blaylock23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Inge</given_name>
<surname>Salomons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eder</given_name>
<surname>del Blanco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva</given_name>
<surname>Navas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inma</given_name>
<surname>Hernáez</surname>
</person_name>
					</contributors>
					<titles><title>Spanish Phone Confusion Analysis for EMG-Based Silent Speech Interfaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1179</first_page>
						<last_page>1183</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1881</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/salomons23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina R</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan L</given_name>
<surname>Preston</surname>
</person_name>
					</contributors>
					<titles><title>Prospective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4558</first_page>
						<last_page>4562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1882</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/benway23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tahiya</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronica</given_name>
<surname>Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Stent</surname>
</person_name>
					</contributors>
					<titles><title>Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2643</first_page>
						<last_page>2647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1885</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chowdhury23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theodoros</given_name>
<surname>Kouzelis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Katsouros</surname>
</person_name>
					</contributors>
					<titles><title>Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1563</first_page>
						<last_page>1567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1887</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kouzelis23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nowshin</given_name>
<surname>Tabassum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tasfia</given_name>
<surname>Tabassum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fardin</given_name>
<surname>Saad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tahiya Sultana</given_name>
<surname>Safa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hasan</given_name>
<surname>Mahmud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Kamrul</given_name>
<surname>Hasan</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3217</first_page>
						<last_page>3221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1888</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tabassum23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandipana</given_name>
<surname>Dowerah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Romain</given_name>
<surname>Serizel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Jouvet</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3849</first_page>
						<last_page>3853</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1890</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/dowerah23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Rathod</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monil</given_name>
<surname>Charola</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshat</given_name>
<surname>Vora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yash</given_name>
<surname>Jogi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant A.</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Whisper Features for Dysarthric Severity-Level Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1523</first_page>
						<last_page>1527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1891</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rathod23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J. F.</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name>
					</contributors>
					<titles><title>Adapting an Unadaptable ASR System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>989</first_page>
						<last_page>993</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1899</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosa</given_name>
<surname>Gonzalez Hautamäki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>785</first_page>
						<last_page>789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1901</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shim23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md Asif</given_name>
<surname>Jalal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Peso Parada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jisi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mete</given_name>
<surname>Ozay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthikeyan</given_name>
<surname>Saravanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myoungji</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung In</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seokyeong</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>780</first_page>
						<last_page>784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1902</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jalal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesca</given_name>
<surname>Carbone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maud</given_name>
<surname>Champagne-Lavau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurélien Said</given_name>
<surname>Housseini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caterina</given_name>
<surname>Petrone</surname>
</person_name>
					</contributors>
					<titles><title>Evaluation of delexicalization methods for research on emotional speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2618</first_page>
						<last_page>2622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1903</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/audibert23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manuele</given_name>
<surname>Rusci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tinne</given_name>
<surname>Tuytelaars</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2768</first_page>
						<last_page>2772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1904</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rusci23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tu Anh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antony</given_name>
<surname>D'Avirro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itai</given_name>
<surname>Gat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maryam</given_name>
<surname>Fazel-Zarani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tal</given_name>
<surname>Remez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jade</given_name>
<surname>Copet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Synnaeve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Hassid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Kreuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4823</first_page>
						<last_page>4827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1905</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nguyen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3804</first_page>
						<last_page>3808</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1910</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shim23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Branco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paulo</given_name>
<surname>Infante</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khiet P.</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3622</first_page>
						<last_page>3626</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1914</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/branco23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michal</given_name>
<surname>Šimek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomáš</given_name>
<surname>Kouba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tereza</given_name>
<surname>Tykalová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rusz</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of acoustic measures of dysphonia in Parkinson's disease and Huntington's disease: Effect of sex and speaking task</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2398</first_page>
						<last_page>2402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1915</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/simek23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Monil</given_name>
<surname>Charola</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aastha</given_name>
<surname>Kachhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant A.</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Whisper Encoder features for Infant Cry Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1773</first_page>
						<last_page>1777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1916</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/charola23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amélie</given_name>
<surname>Elmerich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayin</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelique</given_name>
<surname>Amelot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lise</given_name>
<surname>Crevier-Buchman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Maeda</surname>
</person_name>
					</contributors>
					<titles><title>Combining acoustic and aerodynamic data collection: A perceptual evaluation of acoustic distortions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3078</first_page>
						<last_page>3082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1918</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/elmerich23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dianna</given_name>
<surname>Yee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Lea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaya</given_name>
<surname>Narain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zifang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Tooley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeffrey P.</given_name>
<surname>Bigham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leah</given_name>
<surname>Findlater</surname>
</person_name>
					</contributors>
					<titles><title>Latent Phrase Matching for Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>161</first_page>
						<last_page>165</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1921</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yee23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nika</given_name>
<surname>Jurov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Idsardi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi H.</given_name>
<surname>Feldman</surname>
</person_name>
					</contributors>
					<titles><title>A neural architecture for selective attention to speech features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1778</first_page>
						<last_page>1782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1922</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jurov23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sergio</given_name>
<surname>Burdisso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esaú</given_name>
<surname>Villatoro-Tello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name>
					</contributors>
					<titles><title>Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3617</first_page>
						<last_page>3621</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1923</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/burdisso23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina R</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashish M</given_name>
<surname>Siriwardena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan L</given_name>
<surname>Preston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elaine</given_name>
<surname>Hitchcock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>McAllister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /ɹ/ in Child Speech Sound Disorders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4568</first_page>
						<last_page>4572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1924</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/benway23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sofoklis</given_name>
<surname>Kakouros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katri</given_name>
<surname>Hiovain-Asikainen</surname>
</person_name>
					</contributors>
					<titles><title>North Sámi Dialect Identification with Self-supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5306</first_page>
						<last_page>5310</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1928</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kakouros23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jay</given_name>
<surname>Kejriwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Štefan</given_name>
<surname>Beňuš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lina M.</given_name>
<surname>Rojas-Barahona</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2628</first_page>
						<last_page>2632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1929</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kejriwal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gibson</surname>
</person_name>
					</contributors>
					<titles><title>Using Random Forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3088</first_page>
						<last_page>3092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1931</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gibson23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Mahshie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Larsen</surname>
</person_name>
					</contributors>
					<titles><title>Did you see that? Exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>137</first_page>
						<last_page>140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1933</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mahshie23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Melistas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lefteris</given_name>
<surname>Kapelonis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikos</given_name>
<surname>Antoniou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petros</given_name>
<surname>Mitseas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitris</given_name>
<surname>Sgouropoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theodoros</given_name>
<surname>Giannakopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Features for Alzheimer’s Dementia Detection from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3008</first_page>
						<last_page>3012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1934</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/melistas23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Flynn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Cross-Utterance Context For ASR Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1359</first_page>
						<last_page>1363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1941</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/flynn23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shabnam</given_name>
<surname>Ghaffarzadegan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Bondi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ho-Hsiang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sirajum</given_name>
<surname>Munir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kelly J.</given_name>
<surname>Shields</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samarjit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Aracri</surname>
</person_name>
					</contributors>
					<titles><title>Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1743</first_page>
						<last_page>1747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1946</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ghaffarzadegan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jay</given_name>
<surname>Kejriwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Štefan</given_name>
<surname>Beňuš</surname>
</person_name>
					</contributors>
					<titles><title>Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN)</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2623</first_page>
						<last_page>2627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1947</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kejriwal23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giorgia</given_name>
<surname>Cantisani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amirhossein</given_name>
<surname>Chalehchaleh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Di Liberto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shihab</given_name>
<surname>Shamma</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the cortical tracking of speech and music with sung speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5157</first_page>
						<last_page>5161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1949</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cantisani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarenne</given_name>
<surname>Wallbridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying the perceptual value of lexical and non-lexical channels in speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2708</first_page>
						<last_page>2712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1951</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wallbridge23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Richard</given_name>
<surname>Rose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oscar</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Siohan</surname>
</person_name>
					</contributors>
					<titles><title>Cascaded encoders for fine-tuning ASR models on overlapped speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3457</first_page>
						<last_page>3461</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1952</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rose23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Wesołek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Gulgowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Błaszczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marzena</given_name>
<surname>Żygis</surname>
</person_name>
					</contributors>
					<titles><title>What influences the foreign accent strength? Phonological and grammatical errors in the perception of accentedness</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3098</first_page>
						<last_page>3102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1954</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wesoek23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>720</first_page>
						<last_page>724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1962</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/arora23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Almudévar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis</given_name>
<surname>Vicente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2823</first_page>
						<last_page>2827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1965</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/almudevar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachel</given_name>
<surname>Beeson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1149</first_page>
						<last_page>1153</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1966</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/beeson23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anais</given_name>
<surname>Tran Ngoc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanny</given_name>
<surname>Meunier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3063</first_page>
						<last_page>3067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1967</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tranngoc23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eklavya</given_name>
<surname>Sarkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1189</first_page>
						<last_page>1193</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1968</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sarkar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sourya Dipta</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yash</given_name>
<surname>Vadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Unnam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuldeep</given_name>
<surname>Yadav</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1978</first_page>
						<last_page>1982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1974</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/das23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Minixhofer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating and reducing the distance between synthetic and real speech distributions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2078</first_page>
						<last_page>2082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1978</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/minixhofer23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Claytone</given_name>
<surname>Sikasote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kalinda</given_name>
<surname>Siaminwe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanly</given_name>
<surname>Mwape</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bangiwe</given_name>
<surname>Zulu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mofya</given_name>
<surname>Phiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Phiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Zulu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayumbo</given_name>
<surname>Nyirenda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Anastasopoulos</surname>
</person_name>
					</contributors>
					<titles><title>Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3984</first_page>
						<last_page>3988</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1979</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sikasote23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moreno</given_name>
<surname>La Quatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Vaiani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Colomba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Attanasio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliana</given_name>
<surname>Pastor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Cagliero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Baralis</surname>
</person_name>
					</contributors>
					<titles><title>ITALIC: An Italian Intent Classification Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2153</first_page>
						<last_page>2157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1980</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/koudounas23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prashanth</given_name>
<surname>Gurunath Shivakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jari</given_name>
<surname>Kolehmainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yile</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Bulyko</surname>
</person_name>
					</contributors>
					<titles><title>Distillation Strategies for Discriminative Speech Recognition Rescoring</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4084</first_page>
						<last_page>4088</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1981</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gurunathshivakumar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Paturi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3567</first_page>
						<last_page>3571</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1982</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/paturi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kei</given_name>
<surname>Ota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddarth</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radu</given_name>
<surname>Corcodel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devesh</given_name>
<surname>Jha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Romeres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4663</first_page>
						<last_page>4667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1983</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hori23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrick Cormac</given_name>
<surname>English</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John D.</given_name>
<surname>Kelleher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name>
					</contributors>
					<titles><title>Discovering Phonetic Feature Event Patterns in Transformer Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4733</first_page>
						<last_page>4737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1985</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/english23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oriol</given_name>
<surname>Nieto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dernoncourt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Salamon</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Spoken Language Recognition via Multilabel Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>506</first_page>
						<last_page>510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1986</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nieto23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viet Thanh</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan Thai Hoa</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vu</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thi Thu Trang</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1918</first_page>
						<last_page>1922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1989</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pham23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minxue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amrit</given_name>
<surname>Romana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mimansa</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin</given_name>
<surname>McInnis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1718</first_page>
						<last_page>1722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1990</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/niu23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shivam</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ambika</given_name>
<surname>Kirkland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harm</given_name>
<surname>Lameris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Beskow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name>
					</contributors>
					<titles><title>OverFlow: Putting flows on top of neural transducers for better TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4279</first_page>
						<last_page>4283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1996</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mehta23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula A.</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Hönig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hartmut</given_name>
<surname>Lehfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hillemacher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Classifying Dementia in the Presence of Depression: A Cross-Corpus Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2308</first_page>
						<last_page>2312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1997</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/braun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michal</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tereza</given_name>
<surname>Tykalová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Šimek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomáš</given_name>
<surname>Kouba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rusz</surname>
</person_name>
					</contributors>
					<titles><title>Glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo Parkinson's disease and Huntington's disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1568</first_page>
						<last_page>1572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1998</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/novotny23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tilak</given_name>
<surname>Purohit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Vlasenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Implicit phonetic information modeling for speech emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1883</first_page>
						<last_page>1887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-1999</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/purohit23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kavan</given_name>
<surname>Fatehi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayse</given_name>
<surname>Kucukyilmaz</surname>
</person_name>
					</contributors>
					<titles><title>LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>211</first_page>
						<last_page>215</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2001</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fatehi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2323</first_page>
						<last_page>2327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2002</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kothare23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>57</first_page>
						<last_page>61</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2004</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23oa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mine</given_name>
<surname>Kerpicci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhua</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Visser</surname>
</person_name>
					</contributors>
					<titles><title>Application of Knowledge Distillation to Multi-Task Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2813</first_page>
						<last_page>2817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2011</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kerpicci23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gerda Ana</given_name>
<surname>Melnik-Leroy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gediminas</given_name>
<surname>Navickas</surname>
</person_name>
					</contributors>
					<titles><title>Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1100</first_page>
						<last_page>1103</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2013</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/melnikleroy23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Camp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Kenter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lev</given_name>
<surname>Finkelstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rob</given_name>
<surname>Clark</surname>
</person_name>
					</contributors>
					<titles><title>MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1090</first_page>
						<last_page>1094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2014</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/camp23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Waradon</given_name>
<surname>Phokhinanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Obin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Argentieri</surname>
</person_name>
					</contributors>
					<titles><title>Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3704</first_page>
						<last_page>3708</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2015</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/phokhinanan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Loes</given_name>
<surname>van Bemmel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiara</given_name>
<surname>Pesenti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xue</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Automatic assessments of dysarthric speech: the usability of acoustic-phonetic features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>141</first_page>
						<last_page>145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2017</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/vanbemmel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishal</given_name>
<surname>Sunder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Fosler-Lussier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Kwang J</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1129</first_page>
						<last_page>1133</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2018</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sunder23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roland</given_name>
<surname>Adams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Calbert</given_name>
<surname>Graham</surname>
</person_name>
					</contributors>
					<titles><title>An Acoustic Analysis of Fricative Variation in Three Accents of English</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2593</first_page>
						<last_page>2597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2020</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/adams23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sebastian P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Hönig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>A Stutter Seldom Comes Alone – Cross-Corpus Stuttering Detection as a Multi-label Problem</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1538</first_page>
						<last_page>1542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2026</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bayerl23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Self-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4688</first_page>
						<last_page>4692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2031</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kataria23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>396</first_page>
						<last_page>400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2032</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/peng23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaan</given_name>
<surname>Bijwadia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Text Injection for Capitalization and Turn-Taking Prediction in Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1409</first_page>
						<last_page>1413</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2037</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bijwadia23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manila</given_name>
<surname>Kodali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4134</first_page>
						<last_page>4138</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2038</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kodali23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yongyi</given_name>
<surname>Zang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>Phase perturbation improves channel robustness for speech spoofing countermeasures</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3162</first_page>
						<last_page>3166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2039</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zang23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>391</first_page>
						<last_page>395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2044</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/peng23e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samridhi</given_name>
<surname>Choudhary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siegfried</given_name>
<surname>Kunzmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3292</first_page>
						<last_page>3296</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2045</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuzanna</given_name>
<surname>Miodonska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claartje</given_name>
<surname>Levelt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Mocko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Kręcichwost</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agata</given_name>
<surname>Sage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pawel</given_name>
<surname>Badura</surname>
</person_name>
					</contributors>
					<titles><title>Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3122</first_page>
						<last_page>3126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2046</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/miodonska23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kiran</given_name>
<surname>Praveen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Balaji</given_name>
<surname>Radhakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamini</given_name>
<surname>Sabu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahaboob Ali Basha</given_name>
<surname>Shaik</surname>
</person_name>
					</contributors>
					<titles><title>Language Identification Networks for Multilingual Everyday Recordings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4124</first_page>
						<last_page>4128</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2047</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/praveen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gerard</given_name>
<surname>Sant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Escolano</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Acoustic information in End-to-End Spoken Language Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>52</first_page>
						<last_page>56</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2050</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sant23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Maekaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1399</first_page>
						<last_page>1403</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2051</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwanath Pratap</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1948</first_page>
						<last_page>1952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2052</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/singh23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruilin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gurunandan</given_name>
<surname>Krishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changxi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shree K.</given_name>
<surname>Nayar</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Dereverberation of Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3859</first_page>
						<last_page>3863</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2055</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xu23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Lamb</surname>
</person_name>
					</contributors>
					<titles><title>A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4324</first_page>
						<last_page>4328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2056</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wells23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiuching</given_name>
<surname>Hung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula A.</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomás</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children Based on Speech Intelligibility Using a Machine Learning Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4623</first_page>
						<last_page>4627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2057</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hung23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning for Personality Perception via Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5197</first_page>
						<last_page>5201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2061</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erik</given_name>
<surname>Ekstedt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5481</first_page>
						<last_page>5485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2064</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ekstedt23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kiran</given_name>
<surname>Praveen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Advait Vinay</given_name>
<surname>Dhopeshwarkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Balaji</given_name>
<surname>Radhakrishnan</surname>
</person_name>
					</contributors>
					<titles><title>Prefix Search Decoding for RNN Transducers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4484</first_page>
						<last_page>4488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2065</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/praveen23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Szu-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debjyoti</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutong</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuedong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Language Agnostic Data-Driven Inverse Text Normalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>451</first_page>
						<last_page>455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2066</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hakan</given_name>
<surname>Erdogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Scott</given_name>
<surname>Wisdom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zalán</given_name>
<surname>Borsos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Tagliasacchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neil</given_name>
<surname>Zeghidour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name>
					</contributors>
					<titles><title>TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3462</first_page>
						<last_page>3466</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2069</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/erdogan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neil</given_name>
<surname>Scheidwasser-Clow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karl</given_name>
<surname>El Hajal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Embeddings as Individuality Proxy for Voice Stress Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1838</first_page>
						<last_page>1842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2070</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harm</given_name>
<surname>Lameris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Style: Synthesizing Speech with Pragmatic Functions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3382</first_page>
						<last_page>3386</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2072</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lameris23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruiming</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaustubh</given_name>
<surname>Kalgaonkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Seide</surname>
</person_name>
					</contributors>
					<titles><title>Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3522</first_page>
						<last_page>3526</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2076</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lin23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lena-Marie</given_name>
<surname>Huttner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noël</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin J.</given_name>
<surname>Pickering</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3103</first_page>
						<last_page>3107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2077</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huttner23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1449</first_page>
						<last_page>1453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2078</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paula A.</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomás</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Hönig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos A.</given_name>
<surname>Tobón-Quintero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Aguillón</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Lopera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liliana</given_name>
<surname>Hincapié-Henao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Alzheimer's across Three Languages Using Speech and Language Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1748</first_page>
						<last_page>1752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2079</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pereztoro23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ferdy</given_name>
<surname>Hubers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland van</given_name>
<surname>Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5237</first_page>
						<last_page>5241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2084</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bai23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilaria</given_name>
<surname>Torre</surname>
</person_name>
					</contributors>
					<titles><title>Prosody-controllable Gender-ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1234</first_page>
						<last_page>1238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2086</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/szekely23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sven</given_name>
<surname>Kachel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Pöhlmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Nussbaum</surname>
</person_name>
					</contributors>
					<titles><title>Queer Events, Relationships, and Sports: Does Topic Influence Speakers’ Acoustic Expression of Sexual Orientation?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4269</first_page>
						<last_page>4273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2087</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kachel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vitória S.</given_name>
<surname>Fahed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emer P</given_name>
<surname>Doheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madeleine M</given_name>
<surname>Lowery</surname>
</person_name>
					</contributors>
					<titles><title>Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2738</first_page>
						<last_page>2742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2089</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fahed23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leif</given_name>
<surname>Simmatis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothy</given_name>
<surname>Pommeé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yana</given_name>
<surname>Yunusova</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) Using a Novel Remote Speech Assessment App</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2378</first_page>
						<last_page>2382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2093</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/simmatis23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vahid</given_name>
<surname>Ahmadi Kalkhorani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Time-domain Transformer-based Audiovisual Speaker Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3472</first_page>
						<last_page>3476</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2098</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ahmadikalkhorani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomás</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Londoño-Mora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula A.</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name>
					</contributors>
					<titles><title>Measuring Phonological Precision in Children with Cleft Lip and Palate</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4638</first_page>
						<last_page>4642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2099</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ariasvergara23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2353</first_page>
						<last_page>2357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2100</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/neumann23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2343</first_page>
						<last_page>2347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2101</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23pa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mafuyu</given_name>
<surname>Kitahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Watabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroto</given_name>
<surname>Noguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuyu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayako</given_name>
<surname>Hashimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ai</given_name>
<surname>Mizoguchi</surname>
</person_name>
					</contributors>
					<titles><title>Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1803</first_page>
						<last_page>1807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2103</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kitahara23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soroosh</given_name>
<surname>Tayebi Arasteh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian David</given_name>
<surname>Ríos-Urrego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rusz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>Federated Learning for Secure Development of AI Models for Parkinson’s Disease Detection Using Speech from Different Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5003</first_page>
						<last_page>5007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2108</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tayebiarasteh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anjalie</given_name>
<surname>Field</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prateek</given_name>
<surname>Verma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nay</given_name>
<surname>San</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer L.</given_name>
<surname>Eberhardt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Jurafsky</surname>
</person_name>
					</contributors>
					<titles><title>Developing Speech Processing Pipelines for Police Accountability</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1229</first_page>
						<last_page>1233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2109</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/field23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Tavernor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>656</first_page>
						<last_page>660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2111</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tavernor23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vanessa</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Richburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5441</first_page>
						<last_page>5445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2115</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/richter23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Repp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lara</given_name>
<surname>Muhtz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Heim</surname>
</person_name>
					</contributors>
					<titles><title>Alignment of Beat Gestures and Prosodic Prominence in German</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>107</first_page>
						<last_page>111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2116</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/repp23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miguel</given_name>
<surname>Sarabia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Menyaylenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Toso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Skyler</given_name>
<surname>Seto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zakaria</given_name>
<surname>Aldeneh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shadi</given_name>
<surname>Pirhosseinloo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Zappella</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barry-John</given_name>
<surname>Theobald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Apostoloff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Sheaffer</surname>
</person_name>
					</contributors>
					<titles><title>Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3724</first_page>
						<last_page>3728</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2117</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yochai</given_name>
<surname>Blau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Madmony</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zorik</given_name>
<surname>Gekhman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Genady</given_name>
<surname>Beryozkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>Using Text Injection to Improve Recognition of Personal Identifiers in Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>191</first_page>
						<last_page>195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2118</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/blau23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tobi</given_name>
<surname>Olatunji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejumade</given_name>
<surname>Afonja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bonaventure F. P.</given_name>
<surname>Dossou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atnafu Lambebo</given_name>
<surname>Tonja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris Chinenye</given_name>
<surname>Emezue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amina Mardiyyah</given_name>
<surname>Rufai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahib</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>AfriNames: Most ASR Models "Butcher" African Names</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5077</first_page>
						<last_page>5081</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2122</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/olatunji23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Maison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Some Voices are Too Common: Building Fair Speech Recognition Systems Using the CommonVoice Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4428</first_page>
						<last_page>4432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2124</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/maison23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yile</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashanth</given_name>
<surname>Gurunath Shivakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jari</given_name>
<surname>Kolehmainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Bulyko</surname>
</person_name>
					</contributors>
					<titles><title>Scaling Laws for Discriminative Speech Recognition Rescoring Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>471</first_page>
						<last_page>475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2128</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gu23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Badr M.</given_name>
<surname>Abdullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammed Maqsood</given_name>
<surname>Shaik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2883</first_page>
						<last_page>2887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2131</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/abdullah23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linda</given_name>
<surname>Gerlach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirsty</given_name>
<surname>McDougall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Finnian</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil</given_name>
<surname>Alexander</surname>
</person_name>
					</contributors>
					<titles><title>Voice Twins: Discovering Extremely Similar-sounding, Unrelated Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2553</first_page>
						<last_page>2557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2134</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gerlach23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maurits</given_name>
<surname>Bleeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pawel</given_name>
<surname>Swietojanski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodan</given_name>
<surname>Zhuang</surname>
</person_name>
					</contributors>
					<titles><title>Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>939</first_page>
						<last_page>943</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2136</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bleeker23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minh</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Soleymani</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-preserving Representation Learning for Speech Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2858</first_page>
						<last_page>2862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2138</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tran23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cristian David</given_name>
<surname>Ríos-Urrego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rusz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2368</first_page>
						<last_page>2372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2146</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/riosurrego23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianyu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung Hoon</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carl</given_name>
<surname>Wivagg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanna</given_name>
<surname>Shimizu</surname>
</person_name>
					</contributors>
					<titles><title>Record Deduplication for Entity Distribution Modeling in ASR Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>376</first_page>
						<last_page>380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2147</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phat</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jelske</given_name>
<surname>Dĳkstra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Klabbers</surname>
</person_name>
					</contributors>
					<titles><title>The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5461</first_page>
						<last_page>5465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2148</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/do23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Eisenstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinodkumar</given_name>
<surname>Prabhakaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clara</given_name>
<surname>Rivera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dorottya</given_name>
<surname>Demszky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devyani</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>MD3: The Multi-Dialect Dataset of Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4059</first_page>
						<last_page>4063</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2150</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/eisenstein23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felicia</given_name>
<surname>Schulz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirella</given_name>
<surname>De Sisto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M. Paula</given_name>
<surname>Roncaglia-Denissen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Hendrix</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Perceptual Centers Located at Vowel Onset in German Speech Using Long Short-Term Memory Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1793</first_page>
						<last_page>1797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2154</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/schulz23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phat</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jelske</given_name>
<surname>Dĳkstra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Klabbers</surname>
</person_name>
					</contributors>
					<titles><title>Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5466</first_page>
						<last_page>5470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2158</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/do23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name>
					</contributors>
					<titles><title>O-1: Self-training with Oracle and 1-best Hypothesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>77</first_page>
						<last_page>81</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2166</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/baskar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olivia M.</given_name>
<surname>Murton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abigail E.</given_name>
<surname>Haenssler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc F.</given_name>
<surname>Maffei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathryn P.</given_name>
<surname>Connaghan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4993</first_page>
						<last_page>4997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2168</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/murton23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minh</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufeng</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Soleymani</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>636</first_page>
						<last_page>640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2170</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tran23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul K.</given_name>
<surname>Krug</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel R.</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Solution to the Control Problem of Articulatory Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4329</first_page>
						<last_page>4333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2173</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/krug23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Chivriga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rinita</given_name>
<surname>Roy</surname>
</person_name>
					</contributors>
					<titles><title>Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1473</first_page>
						<last_page>1477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2174</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chivriga23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julitta</given_name>
<surname>Bartolewska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanisław</given_name>
<surname>Kacprzak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4039</first_page>
						<last_page>4043</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2177</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bartolewska23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mikey</given_name>
<surname>Elmers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannah</given_name>
<surname>O'Mahony</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4843</first_page>
						<last_page>4847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2178</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/elmers23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junchen</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5536</first_page>
						<last_page>5540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2179</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lu23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Martínez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dayana</given_name>
<surname>Ribas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>On the Use of High Frequency Information for Voice Pathology Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2383</first_page>
						<last_page>2387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2181</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/martinez23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Krishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesujoba O.</given_name>
<surname>Alabi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>On the N-gram Approximation of Pre-trained Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>371</first_page>
						<last_page>375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2182</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/krishnan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dhanush</given_name>
<surname>Bekal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Gopalakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karel</given_name>
<surname>Mundnich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4079</first_page>
						<last_page>4083</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2183</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bekal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Delphine</given_name>
<surname>Charuau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Béatrice</given_name>
<surname>Vaxelaire</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudolph</given_name>
<surname>Sock</surname>
</person_name>
					</contributors>
					<titles><title>Speech Breathing Behavior During Pauses in Children</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4628</first_page>
						<last_page>4632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2185</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/charuau23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4903</first_page>
						<last_page>4907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2186</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhao23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amel</given_name>
<surname>Issa</surname>
</person_name>
					</contributors>
					<titles><title>Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4753</first_page>
						<last_page>4757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2187</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/issa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Shaique</given_name>
<surname>Solanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Bharadwaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeevan</given_name>
<surname>Kylash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5406</first_page>
						<last_page>5410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2190</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/solanki23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiyang</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>MacWhinney</surname>
</person_name>
					</contributors>
					<titles><title>A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1528</first_page>
						<last_page>1532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2191</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anshu</given_name>
<surname>Bhatia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanchit</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saket</given_name>
<surname>Dingliwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Gopalakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Don’t Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3362</first_page>
						<last_page>3366</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2192</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bhatia23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Khurana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonid</given_name>
<surname>Karlinsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2798</first_page>
						<last_page>2802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2193</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gong23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doug</given_name>
<surname>Habberstad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1219</first_page>
						<last_page>1223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2194</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/neumann23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Papadimitriou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1513</first_page>
						<last_page>1517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2198</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/papadimitriou23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dennis</given_name>
<surname>Hoffmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>O'Reilly</surname>
</person_name>
					</contributors>
					<titles><title>Segmental features of Brazilian (Santa Catarina) Hunsrik</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1818</first_page>
						<last_page>1822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2200</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hoffmann23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Waris</given_name>
<surname>Quamer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Decoupling Segmental and Prosodic Cues of Non-native Speech through Vector Quantization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2083</first_page>
						<last_page>2087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2202</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/quamer23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myra</given_name>
<surname>Sydnor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Becky</given_name>
<surname>Lammers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name>
					</contributors>
					<titles><title>DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1548</first_page>
						<last_page>1552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2203</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23qa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name>
					</contributors>
					<titles><title>A System for Generating Voice Source Signals that Implements the Transformed LF-model Parameter Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4738</first_page>
						<last_page>4742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2204</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23ra_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aakriti</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milind</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anit Kumar</given_name>
<surname>Sahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopinath</given_name>
<surname>Chennupati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Learning When to Trust Which Teacher for Weakly Supervised ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>381</first_page>
						<last_page>385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2205</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/agrawal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1299</first_page>
						<last_page>1303</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2207</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cui23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sankaran</given_name>
<surname>Panchapagesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Turaj Zakizadeh</given_name>
<surname>Shabestary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4019</first_page>
						<last_page>4023</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2209</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/panchapagesan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed Adel</given_name>
<surname>Attia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Tiede</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4504</first_page>
						<last_page>4507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2211</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/attia23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1843</first_page>
						<last_page>1847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2213</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Essery</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name>
					</contributors>
					<titles><title>Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2568</first_page>
						<last_page>2572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2214</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/essery23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiujia</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro M.</given_name>
<surname>Mengibar</surname>
</person_name>
					</contributors>
					<titles><title>Modular Domain Adaptation for Conformer-Based Streaming ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3357</first_page>
						<last_page>3361</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2215</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chouchang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashas Malur</given_name>
<surname>Saidutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rakshith Sharma</given_name>
<surname>Srinivasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Hua</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1638</first_page>
						<last_page>1642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2222</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atharva</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara Abedalmon'em Mohammad</given_name>
<surname>Shatnawi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanan</given_name>
<surname>Aldarmaki</surname>
</person_name>
					</contributors>
					<titles><title>ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5511</first_page>
						<last_page>5515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2224</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kulkarni23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Polák</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Bojar</surname>
</person_name>
					</contributors>
					<titles><title>Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3979</first_page>
						<last_page>3983</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2225</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/polak23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Radfar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paulina</given_name>
<surname>Lyskawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brandon</given_name>
<surname>Trujillo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Zhen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahn</given_name>
<surname>Heymann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Filimonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Susanj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name>
					</contributors>
					<titles><title>Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2198</first_page>
						<last_page>2202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2228</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/radfar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eray</given_name>
<surname>Eren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lee Ngee</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4523</first_page>
						<last_page>4527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2229</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/eren23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaëlle</given_name>
<surname>Laperrière</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Ghannay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bassam</given_name>
<surname>Jabaian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Enrichment Towards Efficient Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>705</first_page>
						<last_page>709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2234</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/laperriere23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Devang</given_name>
<surname>Kulshreshtha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saket</given_name>
<surname>Dingliwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brady</given_name>
<surname>Houston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3302</first_page>
						<last_page>3306</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2235</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kulshreshtha23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwas M.</given_name>
<surname>Shetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steven M.</given_name>
<surname>Lulich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4598</first_page>
						<last_page>4602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2236</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shetty23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyu Derek</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiun-Yu</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuyang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arpit</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tagyoung</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanyun</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4653</first_page>
						<last_page>4657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2238</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ma23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minh Van</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kishan</given_name>
<surname>KC</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toan</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thien Huu</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankit</given_name>
<surname>Chadha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thuy</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3437</first_page>
						<last_page>3441</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2240</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nguyen23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruishan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanlu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dengfeng</given_name>
<surname>Ke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Dual Audio Encoders Based Mandarin Prosodic Boundary Prediction by Using Multi-Granularity Prosodic Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4793</first_page>
						<last_page>4797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2242</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi Mai</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name>
					</contributors>
					<titles><title>STEN-TTS: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4464</first_page>
						<last_page>4468</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2243</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tran23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ursa</given_name>
<surname>Maity</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangxu</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jerry</given_name>
<surname>Prince</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maureen</given_name>
<surname>Stone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>El Fakhri</given_name>
<surname>Georges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonghye</given_name>
<surname>Woo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>Motor Control Similarity Between Speakers Saying “A Souk” Using Inverse Atlas Tongue Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4189</first_page>
						<last_page>4193</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2247</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/maity23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helene</given_name>
<surname>Fung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianmin</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothy</given_name>
<surname>Kwok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Mok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean</given_name>
<surname>Woo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ka Ho</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shensheng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naijun</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ranzo</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoquan</given_name>
<surname>Ke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1713</first_page>
						<last_page>1717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2249</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/meng23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amir</given_name>
<surname>Pouran Ben Veyseh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dernoncourt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thien Huu</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>Combining Heterogeneous Structures for Event Causality Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3934</first_page>
						<last_page>3938</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2250</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pouranbenveyseh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yahan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunghye</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxine</given_name>
<surname>Covello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Azia</given_name>
<surname>Knox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Osbert</given_name>
<surname>Bastani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Weimer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edgar</given_name>
<surname>Dobriban</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Insup</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Parish-Morris</surname>
</person_name>
					</contributors>
					<titles><title>Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4603</first_page>
						<last_page>4607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2251</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Arvan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A. Seza</given_name>
<surname>Doğruöz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Parde</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3929</first_page>
						<last_page>3933</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2252</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/arvan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ram C. M. C.</given_name>
<surname>Shekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okim</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H. L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1923</first_page>
						<last_page>1927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2254</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuta</given_name>
<surname>Nishikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2193</first_page>
						<last_page>2197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2255</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nishikawa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>An Improved End-to-End Audio-Visual Speech Recognition Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3093</first_page>
						<last_page>3097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2256</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aoi</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Kida</surname>
</person_name>
					</contributors>
					<titles><title>Target Vocabulary Recognition Based on Multi-Task Learning with Decomposed Teacher Sequences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1254</first_page>
						<last_page>1258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2257</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ito23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongji</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>924</first_page>
						<last_page>928</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2258</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Allen</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyuan</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aarav</given_name>
<surname>Monga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seoho</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejas</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesse</given_name>
<surname>Thomason</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Speech Recognition for Language-Guided Embodied Agents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1608</first_page>
						<last_page>1612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2262</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haolin</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>De</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglai</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Explicit Intensity Control for Accented Text-to-speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>22</first_page>
						<last_page>26</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2270</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreyan</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Utkarsh</given_name>
<surname>Tyagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Ramaneswaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harshvardhan</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh</given_name>
<surname>Manocha</surname>
</person_name>
					</contributors>
					<titles><title>MMER: Multimodal Multi-task Learning for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1209</first_page>
						<last_page>1213</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2271</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ghosh23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo Andrés</given_name>
<surname>Tamayo Flórez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rubén</given_name>
<surname>Manrique</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernardo</given_name>
<surname>Pereira Nunes</surname>
</person_name>
					</contributors>
					<titles><title>HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1963</first_page>
						<last_page>1967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2272</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tamayoflorez23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xulong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiyu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5456</first_page>
						<last_page>5460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2275</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sun23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Anwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vedanuj</given_name>
<surname>Goswami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4064</first_page>
						<last_page>4068</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2279</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/anwar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taichi</given_name>
<surname>Asami</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>899</first_page>
						<last_page>903</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2280</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/moriya23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aldo</given_name>
<surname>Pastore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dorina</given_name>
<surname>de Jong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciano</given_name>
<surname>Fadiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>D'Ausilio</surname>
</person_name>
					</contributors>
					<titles><title>The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>132</first_page>
						<last_page>136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2283</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yuan23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Escobar-Grisales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomás</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian David</given_name>
<surname>Ríos-Urrego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adolfo M.</given_name>
<surname>García</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1703</first_page>
						<last_page>1707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2287</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/escobargrisales23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soonshin</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Geonmin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You Jin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-ki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minjae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Encoder-decoder Multimodal Speaker Change Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5311</first_page>
						<last_page>5315</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2289</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jung23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guodong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1389</first_page>
						<last_page>1393</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2292</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wang23sa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhouyuan</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsendsuren</given_name>
<surname>Munkhdalai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro M.</given_name>
<surname>Mengibar</surname>
</person_name>
					</contributors>
					<titles><title>Re-investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>556</first_page>
						<last_page>560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2296</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huo23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xilin</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinghao Aaron</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2818</first_page>
						<last_page>2822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2297</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jiang23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wonjune</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deb</given_name>
<surname>Roy</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2303</first_page>
						<last_page>2307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2298</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Henry</given_name>
<surname>Weld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sijia</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqu</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josiah</given_name>
<surname>Poon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soyeon</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>700</first_page>
						<last_page>704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2300</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/weld23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weihua</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuocheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name>
					</contributors>
					<titles><title>Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2523</first_page>
						<last_page>2527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2302</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hong-Sun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoon-Cheol</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Il-Hwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeong-Yeol</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuk-Jae</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Yong</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>FACTSpeech: Speaking a Foreign Language Pronunciation Using Only Your Native Characters</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>606</first_page>
						<last_page>610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2303</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungmin</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jongmo</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungkwon</given_name>
<surname>Beack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngcheol</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Improvement of Deep Neural Network (DNN) Speech Coder Using Parametric and Non-parametric Density Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>859</first_page>
						<last_page>863</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2305</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/byun23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin Rao</given_name>
<surname>Koluguri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>A Compact End-to-End Model with Local and Global Context for Spoken Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5321</first_page>
						<last_page>5325</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2310</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jia23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng-Ping</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhankar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Adapter-Based Extension of Multi-Speaker Text-To-Speech Model for New Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3028</first_page>
						<last_page>3032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2313</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hsieh23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zefei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil</given_name>
<surname>Ramakrishna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Rumshisky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy</given_name>
<surname>Rosenbaum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saleh</given_name>
<surname>Solta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Gupta</surname>
</person_name>
					</contributors>
					<titles><title>Sampling bias in NLU models: Impact and Mitigation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>755</first_page>
						<last_page>759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2314</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingle</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijing</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yubin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala K.</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Deep Speech Synthesis from MRI-Based Articulatory Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5132</first_page>
						<last_page>5136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2316</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/wu23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heeseung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungwon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiheum</given_name>
<surname>Yeom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungroh</given_name>
<surname>Yoon</surname>
</person_name>
					</contributors>
					<titles><title>UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3038</first_page>
						<last_page>3042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2326</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohan</given_name>
<surname>Badlani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rafael</given_name>
<surname>Valle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin J.</given_name>
<surname>Shih</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João Felipe</given_name>
<surname>Santos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Gururani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bryan</given_name>
<surname>Catanzaro</surname>
</person_name>
					</contributors>
					<titles><title>RAD-MMM: Multilingual Multiaccented Multispeaker Text To Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>626</first_page>
						<last_page>630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2330</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/badlani23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachel</given_name>
<surname>Ostrand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor S.</given_name>
<surname>Ferreira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Piorkowski</surname>
</person_name>
					</contributors>
					<titles><title>Rapid Lexical Alignment to a Conversational Agent</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2653</first_page>
						<last_page>2657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2332</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ostrand23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhua</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglai</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3999</first_page>
						<last_page>4003</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2335</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunseop</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee Suk</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhananjaya</given_name>
<surname>Gowda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>SooHwan</given_name>
<surname>Eom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daehyeok</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Harvill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heting</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang D.</given_name>
<surname>Yoo</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2028</first_page>
						<last_page>2032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2336</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yoon23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louise</given_name>
<surname>Ratko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name>
					</contributors>
					<titles><title>Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1823</first_page>
						<last_page>1827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2337</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ratko23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sai Srujana</given_name>
<surname>Buddi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Utkarsh Oggy</given_name>
<surname>Sarawgi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tashweena</given_name>
<surname>Heeramun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Sawnhey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ed</given_name>
<surname>Yanosik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saravana</given_name>
<surname>Rathinam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Adya</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Multimodal Neural Networks for Trigger-less Voice Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2648</first_page>
						<last_page>2652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2341</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/buddi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinjin</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudip</given_name>
<surname>Vhaduri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1194</first_page>
						<last_page>1198</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2342</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cai23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanan</given_name>
<surname>Aldarmaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Ghannam</surname>
</person_name>
					</contributors>
					<titles><title>Diacritic Recognition Performance in Arabic ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>361</first_page>
						<last_page>365</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2344</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/aldarmaki23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cihan</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henry Li</given_name>
<surname>Xinyuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongji</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Duh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4074</first_page>
						<last_page>4078</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2351</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xiao23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yashish M</given_name>
<surname>Siriwardena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzanne</given_name>
<surname>Boyce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Tiede</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liran</given_name>
<surname>Oren</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-independent Speech Inversion for Estimation of Nasalance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4743</first_page>
						<last_page>4747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2352</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/siriwardena23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinit S.</given_name>
<surname>Unni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunita</given_name>
<surname>Sarawagi</surname>
</person_name>
					</contributors>
					<titles><title>Improving RNN-Transducers with Acoustic LookAhead</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4419</first_page>
						<last_page>4423</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2354</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/unni23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Burin</given_name>
<surname>Naowarat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thananchai</given_name>
<surname>Kongthaworn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Word-level Confidence Estimation for CTC Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3297</first_page>
						<last_page>3301</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2355</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/naowarat23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kinan</given_name>
<surname>Martin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Gauthier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Canaan</given_name>
<surname>Breiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roger</given_name>
<surname>Levy</surname>
</person_name>
					</contributors>
					<titles><title>Probing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>251</first_page>
						<last_page>255</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2359</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/martin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gene-Ping</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingming</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongsu</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuzong</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1623</first_page>
						<last_page>1627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2362</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yang23y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shikha</given_name>
<surname>Baghel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreyas</given_name>
<surname>Ramoji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Sidharth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ranjana</given_name>
<surname>H</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prachi</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somil</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pratik</given_name>
<surname>Roy Chowdhuri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaustubh</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swapnil</given_name>
<surname>Padhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepu</given_name>
<surname>Vijayasenan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3562</first_page>
						<last_page>3566</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2367</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/baghel23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jae-Heung</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3769</first_page>
						<last_page>3773</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2369</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/cho23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ram C. M. C.</given_name>
<surname>Shekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Hirschi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Looney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okim</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H. L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>984</first_page>
						<last_page>988</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2371</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shekar23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jehyun</given_name>
<surname>Kyung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-Seok</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Rin</given_name>
<surname>Jeoung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Joint Speech and Emotion Recognition Using Global Style Tokens</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4528</first_page>
						<last_page>4532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2375</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kyung23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akshara</given_name>
<surname>Soman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhi</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5177</first_page>
						<last_page>5181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2378</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/soman23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Won</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heayoung</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4364</first_page>
						<last_page>4368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2379</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jang23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Kanagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>VC-T: Streaming Voice Conversion Based on Neural Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2088</first_page>
						<last_page>2092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2383</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kanagawa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dohee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daeyeol</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>889</first_page>
						<last_page>893</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2389</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel J.</given_name>
<surname>Broughton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lahiru</given_name>
<surname>Samarakoon</surname>
</person_name>
					</contributors>
					<titles><title>Improving End-to-End Neural Diarization Using Conversational Summary Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3157</first_page>
						<last_page>3161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2401</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/broughton23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yerin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myoung-Wan</given_name>
<surname>Koo</surname>
</person_name>
					</contributors>
					<titles><title>DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2048</first_page>
						<last_page>2052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2403</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/choi23f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaousheik</given_name>
<surname>Jayakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vrunda N.</given_name>
<surname>Sukhadia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A</given_name>
<surname>Arunkumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4414</first_page>
						<last_page>4418</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2406</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/jayakumar23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zein</given_name>
<surname>Shaheen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tasnima</given_name>
<surname>Sadekova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulia</given_name>
<surname>Matveeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Shirshova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikhail</given_name>
<surname>Kudinov</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2038</first_page>
						<last_page>2042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2407</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shaheen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuheng</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junqing</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>SVVAD: Personal Voice Activity Detection for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5067</first_page>
						<last_page>5071</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2413</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kang23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hejing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiaoxi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiyang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youde</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>336</first_page>
						<last_page>340</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2416</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxu</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunhe</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael Bi</given_name>
<surname>Mi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deyi</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>GhostRNN: Reducing State Redundancy in RNN with Cheap Operations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>226</first_page>
						<last_page>230</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2417</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhou23g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>A Simple RNN Model for Lightweight, Low-compute and Low-latency Multichannel Speech Enhancement in the Time Domain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2478</first_page>
						<last_page>2482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2418</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pandey23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan</given_name>
<surname>Zuluaga-Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danielius</given_name>
<surname>Visockas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cem</given_name>
<surname>Subakan</surname>
</person_name>
					</contributors>
					<titles><title>CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5291</first_page>
						<last_page>5295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2419</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zuluagagomez23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eungbeom</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunkee</given_name>
<surname>Chae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeheon</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1508</first_page>
						<last_page>1512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2421</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaobiao</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenbo</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>476</first_page>
						<last_page>480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2429</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/liu23w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongheon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dayun</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung-Woo</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>DeFT-AN RT: Real-time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>864</first_page>
						<last_page>868</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2437</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhongjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2423</first_page>
						<last_page>2427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2444</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ia_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iuliia</given_name>
<surname>Nigmatulina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esaú</given_name>
<surname>Villatoro-Tello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Zuluaga-Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Pandia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Ganapathiraju</surname>
</person_name>
					</contributors>
					<titles><title>Implementing Contextual Biasing in GPU Decoder for Online ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4494</first_page>
						<last_page>4498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2449</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/nigmatulina23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min-Sang</given_name>
<surname>Baek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Young</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3744</first_page>
						<last_page>3748</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2451</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/baek23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunya</given_name>
<surname>Kimura</surname>
</person_name>
					</contributors>
					<titles><title>A Generative Framework for Conversational Laughter: Its 'Language Model' and Laughter Sound Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3372</first_page>
						<last_page>3376</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2453</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mori23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Miwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsuhiko</given_name>
<surname>Kai</surname>
</person_name>
					</contributors>
					<titles><title>Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4928</first_page>
						<last_page>4932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2463</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/miwa23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lokesh</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S. Pavankumar</given_name>
<surname>Dubagunta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malolan</given_name>
<surname>Chetlur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pushpak</given_name>
<surname>Jagtap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Ganapathiraju</surname>
</person_name>
					</contributors>
					<titles><title>On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1863</first_page>
						<last_page>1867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2464</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bansal23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prithvi R.R.</given_name>
<surname>Gudepu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jayesh M.</given_name>
<surname>Koroth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamini</given_name>
<surname>Sabu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahaboob Ali Basha</given_name>
<surname>Shaik</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5052</first_page>
						<last_page>5056</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2466</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gudepu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tankala</given_name>
<surname>Pavan Kalyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pushpak</given_name>
<surname>Bhattacharyya</surname>
</person_name>
					</contributors>
					<titles><title>Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4808</first_page>
						<last_page>4812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2469</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/pavankalyan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ranzo</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4958</first_page>
						<last_page>4962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2470</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Per</given_name>
<surname>Fallgren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name>
					</contributors>
					<titles><title>Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2178</first_page>
						<last_page>2182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2473</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fallgren23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pingyue</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2998</first_page>
						<last_page>3002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2474</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/zhang23ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heui-Yeen</given_name>
<surname>Yeen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Ju</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myoung-Wan</given_name>
<surname>Koo</surname>
</person_name>
					</contributors>
					<titles><title>I Learned Error, I Can Fix It! : A Detector-Corrector Structure for ASR Error Calibration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2693</first_page>
						<last_page>2697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2475</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yeen23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeong Ju</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiwon</given_name>
<surname>Ryu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sejoong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>An Analysis of Glottal Features of Chronic Kidney Disease Speech and Its Application to CKD Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1573</first_page>
						<last_page>1577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2478</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mun23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhipeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation Approach for Efficient Internal Language Model Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1339</first_page>
						<last_page>1343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2479</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chen23u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>156</first_page>
						<last_page>160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2481</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/hermann23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul-Ambroise</given_name>
<surname>Duquenne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Holger</given_name>
<surname>Schwenk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoît</given_name>
<surname>Sagot</surname>
</person_name>
					</contributors>
					<titles><title>Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>32</first_page>
						<last_page>36</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2484</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/duquenne23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woo-Jin</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4499</first_page>
						<last_page>4503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2487</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chung23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiulian</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Masked Audio Modeling with CLAP and Multi-Objective Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2763</first_page>
						<last_page>2767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2488</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/xin23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liusong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>MTANet: Multi-band Time-frequency Attention Network for Singing Melody Extraction from Polyphonic Music</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5396</first_page>
						<last_page>5400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2494</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dehua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harold</given_name>
<surname>Chui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Luk</surname>
</person_name>
					</contributors>
					<titles><title>A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3662</first_page>
						<last_page>3666</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2498</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/tao23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Florian L.</given_name>
<surname>Kreyssig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinxi</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leda</given_name>
<surname>Sari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdel-rahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Biased Self-supervised Learning for ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4948</first_page>
						<last_page>4952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2499</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kreyssig23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marek</given_name>
<surname>Sarvaš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomáš</given_name>
<surname>Pavlíček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Macaire</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Ciuba</surname>
</person_name>
					</contributors>
					<titles><title>Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2148</first_page>
						<last_page>2152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2506</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kesiraju23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bao Thang</given_name>
<surname>Ta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh Tu</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nhat Minh</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Hai</given_name>
<surname>Do</surname>
</person_name>
					</contributors>
					<titles><title>Probing Speech Quality Information in ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>541</first_page>
						<last_page>545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2507</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/ta23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zengwei</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangjun</given_name>
<surname>Kuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name>
					</contributors>
					<titles><title>Delay-penalized CTC Implemented Based on Finite State Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1329</first_page>
						<last_page>1333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2508</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yao23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaebin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary Geunbae</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Tracking Must Go On : Dialogue State Tracking with Verified Self-Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4678</first_page>
						<last_page>4682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2513</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lee23k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuma</given_name>
<surname>Okamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>E2E-S2S-VC: End-To-End Sequence-To-Sequence Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2043</first_page>
						<last_page>2047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2518</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/okamoto23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mieszko</given_name>
<surname>Fraś</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Witkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource MNMF with Localization Prior</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3734</first_page>
						<last_page>3738</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2520</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/fras23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>McNeill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rivka</given_name>
<surname>Levitan</surname>
</person_name>
					</contributors>
					<titles><title>An Autoregressive Conversational Dynamics Model for Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4658</first_page>
						<last_page>4662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2525</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mcneill23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liam</given_name>
<surname>Lonergan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neasa</given_name>
<surname>Ní Chiaráin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailbhe</given_name>
<surname>Ní Chasaide</surname>
</person_name>
					</contributors>
					<titles><title>Towards Dialect-inclusive Recognition in a Low-resource Language: Are Balanced Corpora the Answer?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5082</first_page>
						<last_page>5086</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2528</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/lonergan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manila</given_name>
<surname>Kodali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2393</first_page>
						<last_page>2397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2531</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kadiri23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Nan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name>
					</contributors>
					<titles><title>Improving wav2vec2-based Spoken Language Identification by Learning Phonological Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4119</first_page>
						<last_page>4123</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2533</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/shahin23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yishan</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4728</first_page>
						<last_page>4732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2534</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/huang23i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mosner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan "Honza"</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Speech Separation with Cross-Attention and Beamforming</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1693</first_page>
						<last_page>1697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2537</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/mosner23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingxiang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaehyun</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noriko</given_name>
<surname>Nakanishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Prediction of Language Learners' Listenability Using Speech and Text Features Extracted from Listening Drills</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>979</first_page>
						<last_page>983</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2541</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/gao23j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuping</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanchun</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohu</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Compressed MoE ASR Model Based on Knowledge Distillation and Quantization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3337</first_page>
						<last_page>3341</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2544</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yuan23c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking the Visual Cues in Audio-Visual Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3754</first_page>
						<last_page>3758</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2545</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/li23ja_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiqin</given_name>
<surname>Zu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanglei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Cross-utterance Conditioned Coherent Speech Editing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2108</first_page>
						<last_page>2112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2558</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/yu23d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid</given_name>
<surname>Chetia Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Balaji Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1903</first_page>
						<last_page>1907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2561</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/chetiaphukan23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yasufumi</given_name>
<surname>Uezu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sicheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teruki</given_name>
<surname>Toya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name>
					</contributors>
					<titles><title>Consonant-emphasis Method Incorporating Robust Consonant-section Detection to Improve Intelligibility of Bone-conducted speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>849</first_page>
						<last_page>853</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2568</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/uezu23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Spandan</given_name>
<surname>Dey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Premjeet</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Goutam</given_name>
<surname>Saha</surname>
</person_name>
					</contributors>
					<titles><title>Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1953</first_page>
						<last_page>1957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2569</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/dey23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ramanan</given_name>
<surname>Sivaguru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasista Sai</given_name>
<surname>Lodagala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>3033</first_page>
						<last_page>3037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2574</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/sivaguru23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshat</given_name>
<surname>Shrivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1119</first_page>
						<last_page>1123</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2575</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/kim23n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Caitlin</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ragnar</given_name>
<surname>Pálsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luke</given_name>
<surname>O'Brien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kolbrún</given_name>
<surname>Friðriksdóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Bédi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eydís Huld</given_name>
<surname>Magnúsdóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name>
					</contributors>
					<titles><title>Orthography-based Pronunciation Scoring for Better CAPT Feedback</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>1004</first_page>
						<last_page>1008</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2577</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/richter23b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chihiro</given_name>
<surname>Taguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Chiang</surname>
</person_name>
					</contributors>
					<titles><title>Universal Automatic Phonetic Transcription into the International Phonetic Alphabet</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>2548</first_page>
						<last_page>2552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2584</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/taguchi23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fazle Rabbi</given_name>
<surname>Rakib</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Souhardya Saha</given_name>
<surname>Dip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samiul</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nazia</given_name>
<surname>Tasnim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Istiak Hossain</given_name>
<surname>Shihab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Nazmuddoha</given_name>
<surname>Ansary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed Mobassir</given_name>
<surname>Hossen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marsia Haque</given_name>
<surname>Meghla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mamunur</given_name>
<surname>Mamun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Farig</given_name>
<surname>Sadeque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sayma Sultana</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tahsin</given_name>
<surname>Reasat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asif</given_name>
<surname>Sushmit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed Imtiaz</given_name>
<surname>Humayun</surname>
</person_name>
					</contributors>
					<titles><title>OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>879</first_page>
						<last_page>883</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2585</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/rakib23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tahir</given_name>
<surname>Javed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakshi</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vignesh</given_name>
<surname>Nagarajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai</given_name>
<surname>Sundaresan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janki</given_name>
<surname>Nawale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhigyan</given_name>
<surname>Raman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaushal</given_name>
<surname>Bhogale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pratyush</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>Svarah: Evaluating English ASR Systems on Indian Accents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>5087</first_page>
						<last_page>5091</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2588</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/javed23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaushal</given_name>
<surname>Bhogale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai</given_name>
<surname>Sundaresan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhigyan</given_name>
<surname>Raman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tahir</given_name>
<surname>Javed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pratyush</given_name>
<surname>Kumar</surname>
</person_name>
					</contributors>
					<titles><title>Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>20</day>
						<year>2023</year>
					</publication_date>
					<pages>
						<first_page>4384</first_page>
						<last_page>4388</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2023-2589</doi>
						<resource>https://www.isca-archive.org/interspeech_2023/bhogale23_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>