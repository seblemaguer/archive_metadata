<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2021</doi_batch_id>
		<timestamp>1705398665591175</timestamp>
		<depositor>
			<depositor_name>Martin Cooke</depositor_name> 
			<email_address>m.cooke@ikerbasque.org</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant> 
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>Interspeech 2021</conference_name>
				<conference_acronym>interspeech_2021</conference_acronym>
				<conference_date>30 August - 3 September 2021</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>Interspeech 2021</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2021</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2021</doi>
					<timestamp>1705398665591175</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2021/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Pucher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Woltron</surname>
</person_name>
					</contributors>
					<titles><title>Conversion of Airborne to Bone-Conducted Speech with Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1</first_page>
						<last_page>5</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-473</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pucher21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markéta</given_name>
<surname>Řezáčková</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Tihelka</surname>
</person_name>
					</contributors>
					<titles><title>T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>6</first_page>
						<last_page>10</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-546</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rezackova21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Perrotin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hussein El</given_name>
<surname>Amouri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gérard</given_name>
<surname>Bailly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the Extrapolation Capabilities of Neural Vocoders to Extreme Pitch Values</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>11</first_page>
						<last_page>15</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1547</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/perrotin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phat</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jelske</given_name>
<surname>Dijkstra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Klabbers</surname>
</person_name>
					</contributors>
					<titles><title>A Systematic Review and Analysis of Multilingual Data Strategies in Text-to-Speech for Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>16</first_page>
						<last_page>20</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1565</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/do21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanya</given_name>
<surname>Talkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy Pearl</given_name>
<surname>Solomon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas S.</given_name>
<surname>Brungart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefanie E.</given_name>
<surname>Kuchinsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megan M.</given_name>
<surname>Eitel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara M.</given_name>
<surname>Lippa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tracey A.</given_name>
<surname>Brickell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis M.</given_name>
<surname>French</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rael T.</given_name>
<surname>Lange</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F.</given_name>
<surname>Quatieri</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Indicators of Speech Motor Coordination in Adults With and Without Traumatic Brain Injury</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>21</first_page>
						<last_page>25</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1581</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/talkar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>J.C.</given_name>
<surname>Vásquez-Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Fritsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.R.</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai-Doss</surname>
</person_name>
					</contributors>
					<titles><title>On Modeling Glottal Source Information for Phonation Assessment in Parkinson&#38;#8217;s Disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>26</first_page>
						<last_page>30</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1084</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vasquezcorrea21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khalid</given_name>
<surname>Daoudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biswajit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solange Milhé de Saint</given_name>
<surname>Victor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Foubert-Samier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne Pavy-Le</given_name>
<surname>Traon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Rascol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wassilios G.</given_name>
<surname>Meissner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name>
					</contributors>
					<titles><title>Distortion of Voiced Obstruents for Differential Diagnosis Between Parkinson&#38;#8217;s Disease and Multiple System Atrophy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>31</first_page>
						<last_page>35</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-223</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/daoudi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bagher</given_name>
<surname>BabaAli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>A Study into Pre-Training Strategies for Spoken Language Understanding on Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>36</first_page>
						<last_page>40</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1720</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rosanna</given_name>
<surname>Turrisi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arianna</given_name>
<surname>Braccia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Emanuele</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Giulietti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maura</given_name>
<surname>Pugliatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariachiara</given_name>
<surname>Sensi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciano</given_name>
<surname>Fadiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Badino</surname>
</person_name>
					</contributors>
					<titles><title>EasyCall Corpus: A Dysarthric Speech Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>41</first_page>
						<last_page>45</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-549</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/turrisi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Bie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Girin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Leglaive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Alameda-Pineda</surname>
</person_name>
					</contributors>
					<titles><title>A Benchmark of Dynamical Variational Autoencoders Applied to Speech Spectrogram Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>46</first_page>
						<last_page>50</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-256</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bie21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Metehan</given_name>
<surname>Yurt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavan</given_name>
<surname>Kantharaju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sascha</given_name>
<surname>Disch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Niedermeier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto N.</given_name>
<surname>Escalante-B</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veniamin I.</given_name>
<surname>Morgenshtern</surname>
</person_name>
					</contributors>
					<titles><title>Fricative Phoneme Detection Using Deep Neural Networks and its Comparison to Traditional Methods</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>51</first_page>
						<last_page>55</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-645</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yurt21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>RaviShankar</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Identification of F1 and F2 in Speech Using Modified Zero Frequency Filtering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>56</first_page>
						<last_page>60</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1598</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/prasad21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yann</given_name>
<surname>Teytaut</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Roebel</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme-to-Audio Alignment with Recurrent Neural Networks for Speaking and Singing Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>61</first_page>
						<last_page>65</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1676</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/teytaut21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seong-Hu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong-Hwa</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Convolutional Neural Network for Text-Independent Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>66</first_page>
						<last_page>70</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-65</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>Bidirectional Multiscale Feature Aggregation for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>71</first_page>
						<last_page>75</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-111</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Jia</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yih-Wen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Ping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Li</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Cheng</given_name>
<surname>Chan</surname>
</person_name>
					</contributors>
					<titles><title>Improving Time Delay Neural Network Based Speaker Recognition with Convolutional Block and Feature Aggregation Methods</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>76</first_page>
						<last_page>80</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-356</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanfeng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenkai</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Deep CNN Architectures with Variable-Length Training Samples for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>81</first_page>
						<last_page>85</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-559</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tinglong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Binary Neural Network for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>86</first_page>
						<last_page>90</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-600</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youzhi</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Mutual Information Enhanced Training for Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>91</first_page>
						<last_page>95</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1436</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ge</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>Y-Vector: Multiscale Waveform Encoder for Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>96</first_page>
						<last_page>100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1707</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme-Aware and Channel-Wise Attentive Learning for Text Dependent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>101</first_page>
						<last_page>105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2137</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongning</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>106</first_page>
						<last_page>110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2210</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhu21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaotong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuguang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>TacoLPCNet: Fast and Stable TTS by Conditioning LPCNet on Mel Spectrogram Predictions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>111</first_page>
						<last_page>115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-852</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taejun</given_name>
<surname>Bak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Sung</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanbin</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Ik</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>FastPitchFormant: Source-Filter Based Decomposed Modeling for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>116</first_page>
						<last_page>120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-866</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bak21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taiki</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-to-Sequence Learning for Deep Gaussian Process Based Speech Synthesis Using Self-Attention GP Layer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>121</first_page>
						<last_page>125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-896</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nakamura21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoto</given_name>
<surname>Kakegawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunao</given_name>
<surname>Hara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanobu</given_name>
<surname>Abe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic and Prosodic Information Estimation from Texts for Genuine Japanese End-to-End Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>126</first_page>
						<last_page>130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-914</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kakegawa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xudong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaili</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Information Sieve: Content Leakage Reduction in End-to-End Prosody Transfer for Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>131</first_page>
						<last_page>135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1011</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dai21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qingyun</given_name>
<surname>Dou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moquan</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiting</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Deliberation-Based Multi-Pass Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>136</first_page>
						<last_page>140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1405</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dou21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Isaac</given_name>
<surname>Elias</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>R.J.</given_name>
<surname>Skerry-Ryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>141</first_page>
						<last_page>145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1461</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/elias21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiping</given_name>
<surname>Xiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thilo</given_name>
<surname>Koehler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-Based Acoustic Modeling for Streaming Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>146</first_page>
						<last_page>150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1655</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>151</first_page>
						<last_page>155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1757</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jia21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenhao</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lakshmish</given_name>
<surname>Kaushik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanori</given_name>
<surname>Omote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saket</given_name>
<surname>Kumar</surname>
</person_name>
					</contributors>
					<titles><title>Speed up Training with Variable Length Inputs by Efficient Batching Strategies</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>156</first_page>
						<last_page>160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2100</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ge21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhang</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linju</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huifeng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Hao</surname>
</person_name>
					</contributors>
					<titles><title>Funnel Deep Complex U-Net for Phase-Aware Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>161</first_page>
						<last_page>165</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-10</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sun21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiquan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Nicolson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Lan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Convolutional Network with Frequency Dimension Adaptive Attention for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>166</first_page>
						<last_page>170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-46</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changjie</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Contributions of Vowels and Consonant-Vowel Transitions in Understanding Time-Compressed Mandarin Sentences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>171</first_page>
						<last_page>175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-58</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ritujoy</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Nathwani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning for Speech Intelligibility Improvement in Noisy Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>176</first_page>
						<last_page>180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-150</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/biswas21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ayako</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Arai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Remote Experiments Using Crowdsourcing and Laboratory Experiments on Speech Intelligibility</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>181</first_page>
						<last_page>185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-174</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yamamoto21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenzhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Ke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Know Your Enemy, Know Yourself: A Unified Two-Stage Framework for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>186</first_page>
						<last_page>190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-238</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingjian</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Weakly Labelled Data from AudioSet</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>191</first_page>
						<last_page>195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-259</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsun-An</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Improving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>196</first_page>
						<last_page>200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-582</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hsieh21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsun-An</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Plantinga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>201</first_page>
						<last_page>205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-599</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amin</given_name>
<surname>Edraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wai-Yip</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fogerty</surname>
</person_name>
					</contributors>
					<titles><title>A Spectro-Temporal Glimpsing Index (STGI) for Speech Intelligibility Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>206</first_page>
						<last_page>210</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-605</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/edraki21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanhang</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruili</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satwinder</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizhong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Hou</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Learning Based Phone-Fortified Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>211</first_page>
						<last_page>215</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-734</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qiu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khandokar Md.</given_name>
<surname>Nayem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donald S.</given_name>
<surname>Williamson</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Embedding Vectors from a Human Mean-Opinion Score Prediction Model for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>216</first_page>
						<last_page>220</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1844</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nayem21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suren</given_name>
<surname>Jayasuriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>Restoring Degraded Speech via a Modified Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>221</first_page>
						<last_page>225</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1889</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hoang Long</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Renkens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joris</given_name>
<surname>Pelemans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srividya Pranavi</given_name>
<surname>Potharaju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Nalamalapu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murat</given_name>
<surname>Akbacak</surname>
</person_name>
					</contributors>
					<titles><title>User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>226</first_page>
						<last_page>230</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1536</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nguyen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Dialogue Learning for Spoken Conversational Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>231</first_page>
						<last_page>235</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-120</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruolin</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting-Wei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biing-Hwang</given_name>
<surname>Juang</surname>
</person_name>
					</contributors>
					<titles><title>Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>236</first_page>
						<last_page>240</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-138</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/su21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Chiba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichiro</given_name>
<surname>Higashinaka</surname>
</person_name>
					</contributors>
					<titles><title>Dialogue Situation Recognition for Everyday Conversation Using Multimodal Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>241</first_page>
						<last_page>245</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-171</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chiba21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoshihiro</given_name>
<surname>Yamazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Chiba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Nose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akinori</given_name>
<surname>Ito</surname>
</person_name>
					</contributors>
					<titles><title>Neural Spoken-Response Generation Using Prosodic and Linguistic Context for Conversational Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>246</first_page>
						<last_page>250</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-381</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yamazaki21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiyuan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peilin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Transportation Prototypical Network for Few-Shot Intent Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>251</first_page>
						<last_page>255</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-548</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Domain-Specific Multi-Agent Dialog Policy Learning in Multi-Domain Task-Oriented Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>256</first_page>
						<last_page>260</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-887</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Majid</given_name>
<surname>Laali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Durda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeff</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Campbell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging ASR N-Best in Deep Entity Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>261</first_page>
						<last_page>265</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1370</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Spelling Correction Conditioned on Acoustic Feature for Code-Switching Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>266</first_page>
						<last_page>270</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1242</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kathleen</given_name>
<surname>Siminyu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinjian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Anastasopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael R.</given_name>
<surname>Marlo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graham</given_name>
<surname>Neubig</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme Recognition Through Fine Tuning of Phonetic Representations: A Case Study on Luhya Language Varieties</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>271</first_page>
						<last_page>275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1434</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/siminyu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoran</given_name>
<surname>Cvetkovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Speech Acoustic Modelling Using Raw Source and Filter Components</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>276</first_page>
						<last_page>280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-53</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/loweimi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masakiyo</given_name>
<surname>Fujimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>281</first_page>
						<last_page>285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-225</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fujimoto21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ratnarajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenyu</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh</given_name>
<surname>Manocha</surname>
</person_name>
					</contributors>
					<titles><title>IR-GAN: Room Impulse Response Generator for Far-Field Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>286</first_page>
						<last_page>290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-230</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ratnarajah21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Scaling Sparsemax Based Channel Selection for Speech Recognition with ad-hoc Microphone Arrays</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>291</first_page>
						<last_page>295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-419</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng-Ju</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Radfar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurizio</given_name>
<surname>Omologo</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Transformer Transducer for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>296</first_page>
						<last_page>300</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-655</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Shibata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitanya</given_name>
<surname>Narisetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation Methods for End-to-End Speech Recognition on Distant-Talk Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>301</first_page>
						<last_page>305</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-958</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tsunoo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guodong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>306</first_page>
						<last_page>310</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-964</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ma21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Likhomanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiantong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineel</given_name>
<surname>Pratap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paden</given_name>
<surname>Tomasello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Kahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilad</given_name>
<surname>Avidov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Synnaeve</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking Evaluation in ASR: Are Our Models Robust Enough?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>311</first_page>
						<last_page>315</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1758</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/likhomanenko21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Max W.Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Raw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>316</first_page>
						<last_page>320</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2084</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lam21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhesong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xia</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingjian</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bilei</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dick</given_name>
<surname>Botteldooren</surname>
</person_name>
					</contributors>
					<titles><title>Attention-Based Cross-Modal Fusion for Audio-Visual Voice Activity Detection in Musical Video Streams</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>321</first_page>
						<last_page>325</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-37</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hou21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ui-Hyun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Noise-Tolerant Self-Supervised Learning for Audio-Visual Voice Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>326</first_page>
						<last_page>330</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-43</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun-Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pai</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ignacio Lopez</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niranjan</given_name>
<surname>Subrahmanya</surname>
</person_name>
					</contributors>
					<titles><title>Noisy Student-Teacher Training for Robust Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>331</first_page>
						<last_page>335</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-72</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/park21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Osamu</given_name>
<surname>Ichikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaito</given_name>
<surname>Nakano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Nakayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hajime</given_name>
<surname>Shirouzu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel VAD for Transcription of Group Discussion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>336</first_page>
						<last_page>340</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-200</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ichikawa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hengshun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijun</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shifu</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Information Fusion Using Cross-Modal Teacher-Student Learning for Voice Activity Detection in Realistic Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>341</first_page>
						<last_page>345</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-592</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Enrollment-Less Training for Personalized Voice Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>346</first_page>
						<last_page>350</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-731</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/makishima21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Nonaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chee Siang</given_name>
<surname>Leow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akio</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takehito</given_name>
<surname>Utsuro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiromitsu</given_name>
<surname>Nishizaki</surname>
</person_name>
					</contributors>
					<titles><title>Voice Activity Detection for Live Speech of Baseball Game Based on Tandem Connection with Speech/Noise Separation Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>351</first_page>
						<last_page>355</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-792</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nonaka21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Young D.</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagmohan</given_name>
<surname>Chauhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cecilia</given_name>
<surname>Mascolo</surname>
</person_name>
					</contributors>
					<titles><title>FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>356</first_page>
						<last_page>360</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1091</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kwon21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meirong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyuhong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiho</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung-Un</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Transformer-Based Open-Vocabulary Keyword Spotting with Location-Guided Local Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>361</first_page>
						<last_page>365</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1335</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wei21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabhchand</given_name>
<surname>Bhati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>366</first_page>
						<last_page>370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1874</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bhati21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuenan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>A Lightweight Framework for Online Voice Activity Detection in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>371</first_page>
						<last_page>375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1977</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aurélie</given_name>
<surname>Chlébowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Ballier</surname>
</person_name>
					</contributors>
					<titles><title>“See what I mean, huh?” Evaluating Visual Inspection of F0 Tracking in Nasal Grunts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>376</first_page>
						<last_page>380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-129</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chlebowski21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bruce Xiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name>
					</contributors>
					<titles><title>System Performance as a Function of Calibration Methods, Sample Size and Sampling Variability in Likelihood Ratio-Based Forensic Voice Comparison</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>381</first_page>
						<last_page>385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-267</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anne</given_name>
<surname>Bonneau</surname>
</person_name>
					</contributors>
					<titles><title>Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>386</first_page>
						<last_page>390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-601</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bonneau21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titas</given_name>
<surname>Chakraborty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaishali</given_name>
<surname>Patil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>The Four-Way Classification of Stops with Voicing and Aspiration for Non-Native Speech Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>391</first_page>
						<last_page>395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-635</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chakraborty21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saba</given_name>
<surname>Urooj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benazir</given_name>
<surname>Mumtaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarmad</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan ul</given_name>
<surname>Haq</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic and Prosodic Correlates of Emotions in Urdu Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>396</first_page>
						<last_page>400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-910</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/urooj21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nour</given_name>
<surname>Tamim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name>
					</contributors>
					<titles><title>Voicing Contrasts in the Singleton Stops of Palestinian Arabic: Production and Perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>401</first_page>
						<last_page>405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1079</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tamim21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Coy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia J.</given_name>
<surname>Gully</surname>
</person_name>
					</contributors>
					<titles><title>A Comparison of the Accuracy of Dissen and Keshet&#38;#8217;s (2016) DeepFormants and Traditional LPC Methods for Semi-Automatic Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>406</first_page>
						<last_page>410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1487</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/coy21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Jessen</surname>
</person_name>
					</contributors>
					<titles><title>MAP Adaptation Characteristics in Forensic Long-Term Formant Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>411</first_page>
						<last_page>415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1697</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jessen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Justin J.H.</given_name>
<surname>Lo</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Linguistic Speaker Individuality of Long-Term Formant Distributions: Phonetic and Forensic Perspectives</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>416</first_page>
						<last_page>420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1699</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachel</given_name>
<surname>Soo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khia A.</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Molly</given_name>
<surname>Babel</surname>
</person_name>
					</contributors>
					<titles><title>Sound Change in Spontaneous Bilingual Speech: A Corpus Study on the Cantonese n-l Merger in Cantonese-English Bilinguals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>421</first_page>
						<last_page>425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1754</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/soo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wendy</given_name>
<surname>Lalhminghlui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name>
					</contributors>
					<titles><title>Characterizing Voiced and Voiceless Nasals in Mizo</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>426</first_page>
						<last_page>430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2104</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lalhminghlui21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cecilia</given_name>
<surname>Mascolo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iulia</given_name>
<surname>Lefter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heysem</given_name>
<surname>Kaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Stappen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Ottl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panagiotis</given_name>
<surname>Tzirakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chloë</given_name>
<surname>Brown</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagmohan</given_name>
<surname>Chauhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Grammenos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apinan</given_name>
<surname>Hasthanasombat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitris</given_name>
<surname>Spathis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tong</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pietro</given_name>
<surname>Cicuta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leon J.M.</given_name>
<surname>Rothkrantz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joeri A.</given_name>
<surname>Zwerts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jelle</given_name>
<surname>Treep</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Casper S.</given_name>
<surname>Kaandorp</surname>
</person_name>
					</contributors>
					<titles><title>The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation &#38;amp; Primates</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>431</first_page>
						<last_page>435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-19</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/schuller21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rubén</given_name>
<surname>Solera-Ureña</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Rolland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>436</first_page>
						<last_page>440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1702</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/soleraurena21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>P.</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>T.</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>T.</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.C.</given_name>
<surname>Vásquez-Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>P.A.</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.R.</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>The Phonetic Footprint of Covid-19?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>441</first_page>
						<last_page>445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1488</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/klumpp21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaldo</given_name>
<surname>Candido Jr.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo Corso</given_name>
<surname>Fernandes Jr.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcelo</given_name>
<surname>Finger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas Rafael Stefanel</given_name>
<surname>Gris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moacir Antonelli</given_name>
<surname>Ponti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel Peixoto</given_name>
<surname>Pinto da Silva</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning and Data Augmentation Techniques to the COVID-19 Identification Tasks in ComParE 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>446</first_page>
						<last_page>450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1798</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/casanova21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steffen</given_name>
<surname>Illium</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Sedlmeier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claudia-Linnhoff</given_name>
<surname>Popien</surname>
</person_name>
					</contributors>
					<titles><title>Visual Transformers for Primates Classification and Covid Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>451</first_page>
						<last_page>455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-273</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/illium21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name>
					</contributors>
					<titles><title>Deep-Learning-Based Central African Primate Species Classification with MixUp and SpecAugment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>456</first_page>
						<last_page>460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1911</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pellegrini21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steffen</given_name>
<surname>Illium</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claudia</given_name>
<surname>Linnhoff-Popien</surname>
</person_name>
					</contributors>
					<titles><title>A Deep and Recurrent Architecture for Primate Vocalization Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>461</first_page>
						<last_page>465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1274</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/muller21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joeri A.</given_name>
<surname>Zwerts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jelle</given_name>
<surname>Treep</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Casper S.</given_name>
<surname>Kaandorp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Floor</given_name>
<surname>Meewis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amparo C.</given_name>
<surname>Koot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heysem</given_name>
<surname>Kaya</surname>
</person_name>
					</contributors>
					<titles><title>Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>466</first_page>
						<last_page>470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-154</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zwerts21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Rizos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jenna</given_name>
<surname>Lawson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuoda</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duncan</given_name>
<surname>Butler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Rosindell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krystian</given_name>
<surname>Mikolajczyk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristina</given_name>
<surname>Banks-Leite</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Attentive Detection of the Spider Monkey Whinny in the (Actual) Wild</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>471</first_page>
						<last_page>475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1969</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rizos21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>José Vicente</given_name>
<surname>Egas-López</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mercedes</given_name>
<surname>Vetráb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Conflict Escalation and Primates by Using Ensemble X-Vectors and Fisher Vector Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>476</first_page>
						<last_page>480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1173</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/egaslopez21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oxana</given_name>
<surname>Verkholyak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Dresvyanskiy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Dvoynikova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Kotov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Ryumina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alena</given_name>
<surname>Velichko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danila</given_name>
<surname>Mamontov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Minker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>Ensemble-Within-Ensemble Classification for Escalation Prediction from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>481</first_page>
						<last_page>485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1821</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/verkholyak21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Schiller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvan</given_name>
<surname>Mertes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pol van</given_name>
<surname>Rijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeth</given_name>
<surname>André</surname>
</person_name>
					</contributors>
					<titles><title>Analysis by Synthesis: Using an Expressive TTS Model as Feature Extractor for Paralinguistic Speech Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>486</first_page>
						<last_page>490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1587</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/schiller21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chau</given_name>
<surname>Luu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Speaker Attribute Information Using Multi Task Learning for Speaker Verification and Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>491</first_page>
						<last_page>495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-622</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Rybicka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Spine2Net: SpineNet with Res2Net and Time-Squeeze-and-Excitation Blocks for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>496</first_page>
						<last_page>500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1163</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rybicka21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Embeddings by Modeling Channel-Wise Correlations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>501</first_page>
						<last_page>505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1442</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/stafylakis21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weipeng</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Odobez</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Neural Network for Robust Multiple Speaker Embedding Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>506</first_page>
						<last_page>510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1769</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/he21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>ICSpk: Interpretable Complex Speaker Embedding Extractor from Raw Waveform</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>511</first_page>
						<last_page>515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2016</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grégoire</given_name>
<surname>Locqueville</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>d'Alessandro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Kuhnert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claire</given_name>
<surname>Pillot-Loiseau</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Disambiguation Using Chironomic Stylization of Intonation with Native and Non-Native Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>516</first_page>
						<last_page>520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-182</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xiao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aleese</given_name>
<surname>Block</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name>
					</contributors>
					<titles><title>Variation in Perceptual Sensitivity and Compensation for Coarticulation Across Adult and Child Naturally-Produced and TTS Voices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>521</first_page>
						<last_page>525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-228</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/block21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Jalilpour</given_name>
<surname>Monesi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Accou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Francart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Extracting Different Levels of Speech Information from EEG Using an LSTM-Based Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>526</first_page>
						<last_page>530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-336</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/monesi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Word Competition: An Entropy-Based Approach in the DIANA Model of Human Word Comprehension</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>531</first_page>
						<last_page>535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1394</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bosch21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Time-to-Event Models for Analyzing Reaction Time Sequences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>536</first_page>
						<last_page>540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1408</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bosch21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Brand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kimberley</given_name>
<surname>Mulder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Models of Reaction Times in Auditory Lexical Decision: RTonset versus RToffset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>541</first_page>
						<last_page>545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1700</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/brand21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gwantae</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David K.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanseok</given_name>
<surname>Ko</surname>
</person_name>
					</contributors>
					<titles><title>SpecMix : A Mixed Sample Data Augmentation Method for Training with Time-Frequency Domain Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>546</first_page>
						<last_page>550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-103</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>551</first_page>
						<last_page>555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-140</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xu</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>An Effective Mutual Mean Teaching Based Domain Adaptation Method for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>556</first_page>
						<last_page>560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-281</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zheng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ritika</given_name>
<surname>Nandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashank</given_name>
<surname>Shekhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manjunath</given_name>
<surname>Mulimani</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Scene Classification Using Kervolution-Based SubSpectralNet</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>561</first_page>
						<last_page>565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-656</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nandi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harshavardhan</given_name>
<surname>Sundar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Event Specific Attention for Polyphonic Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>566</first_page>
						<last_page>570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-684</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sundar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-An</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>AST: Audio Spectrogram Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>571</first_page>
						<last_page>575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-698</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gong21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soonshin</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Shallow Convolution-Augmented Transformer with Differentiable Neural Computer for Low-Complexity Classification of Variable-Length Acoustic Scene</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>576</first_page>
						<last_page>580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1308</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/seo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helen L.</given_name>
<surname>Bear</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronica</given_name>
<surname>Morfi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanouil</given_name>
<surname>Benetos</surname>
</person_name>
					</contributors>
					<titles><title>An Evaluation of Data Augmentation Methods for Sound Scene Geotagging</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>581</first_page>
						<last_page>585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1837</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bear21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>586</first_page>
						<last_page>590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1975</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hori21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shijing</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanyao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Variational Information Bottleneck for Effective Low-Resource Audio Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>591</first_page>
						<last_page>595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2028</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/si21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soham</given_name>
<surname>Deshmukh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Improving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>596</first_page>
						<last_page>600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2079</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/deshmukh21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Event Detection with Classifier Chains</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>601</first_page>
						<last_page>605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2218</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/komatsu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shu-Chuan</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Fen</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Segment and Tone Production in Continuous Speech of Hearing and Hearing-Impaired Children</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>606</first_page>
						<last_page>610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-757</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tseng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Effect of Carrier Bandwidth on Understanding Mandarin Sentences in Simulated Electric-Acoustic Hearing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>611</first_page>
						<last_page>615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-24</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manthan</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Navaneetha</given_name>
<surname>Gaddam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejas</given_name>
<surname>Umesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Murthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study of Different EMG Features for Acoustics-to-EMG Mapping</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>616</first_page>
						<last_page>620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2240</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sharma21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajish K.</given_name>
<surname>Abraham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>V.</given_name>
<surname>Sivaramakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>N.</given_name>
<surname>Swapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>N.</given_name>
<surname>Manohar</surname>
</person_name>
					</contributors>
					<titles><title>Image-Based Assessment of Jaw Parameters and Jaw Kinematics for Articulatory Simulation: Preliminary Results</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>621</first_page>
						<last_page>625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1155</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/abraham21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianrong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuewei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>An Attention Self-Supervised Contrastive Learning Based Three-Stage Model for Hand Shape Feature Representation in Cued Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>626</first_page>
						<last_page>630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-440</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Judith</given_name>
<surname>Dineley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grace</given_name>
<surname>Lavelle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Leightley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faith</given_name>
<surname>Matcham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Siddi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Teresa</given_name>
<surname>Peñarrubia-María</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katie M.</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alina</given_name>
<surname>Ivan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carolin</given_name>
<surname>Oetzmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Simblett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erin</given_name>
<surname>Dawe-Lane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stuart</given_name>
<surname>Bruce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Stahl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yatharth</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zulqarnain</given_name>
<surname>Rashid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Conde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amos A.</given_name>
<surname>Folarin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josep Maria</given_name>
<surname>Haro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Til</given_name>
<surname>Wykes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard J.B.</given_name>
<surname>Dobson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaibhav A.</given_name>
<surname>Narayan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Hotopf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>The RADAR-CNS Consortium</surname>
</person_name>
					</contributors>
					<titles><title>Remote Smartphone-Based Speech Collection: Acceptance and Barriers in Individuals with Major Depressive Disorder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>631</first_page>
						<last_page>635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1240</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dineley21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarah R.</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin T.</given_name>
<surname>Annand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Dugan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah M.</given_name>
<surname>Schwab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathryn J.</given_name>
<surname>Eary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Swearengen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Stack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzanne</given_name>
<surname>Boyce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael A.</given_name>
<surname>Riley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>T. Douglas</given_name>
<surname>Mast</surname>
</person_name>
					</contributors>
					<titles><title>An Automatic, Simple Ultrasound Biofeedback Parameter for Distinguishing Accurate and Misarticulated Rhotic Syllables</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>636</first_page>
						<last_page>640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1749</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manuel Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aciel</given_name>
<surname>Eshky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Silent versus Modal Multi-Speaker Speech Recognition from Ultrasound and Video</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>641</first_page>
						<last_page>645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-23</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ribeiro21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Ferreira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Curado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>António</given_name>
<surname>Teixeira</surname>
</person_name>
					</contributors>
					<titles><title>RaSSpeR: Radar-Based Silent Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>646</first_page>
						<last_page>650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1413</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ferreira21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beiming</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nordine</given_name>
<surname>Sebkhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arpan</given_name>
<surname>Bhavsar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Omer T.</given_name>
<surname>Inan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Samlan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ted</given_name>
<surname>Mau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Speech Reconstruction for Laryngectomees for Silent Speech Interfaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>651</first_page>
						<last_page>655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1842</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hendrik</given_name>
<surname>Schröter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Rosenkranz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto N.</given_name>
<surname>Escalante-B</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name>
					</contributors>
					<titles><title>LACOPE: Latency-Constrained Pitch Estimation for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>656</first_page>
						<last_page>660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-633</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/schroter21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Fontaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouhei</given_name>
<surname>Sekiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya Arie</given_name>
<surname>Nugraha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuyoshi</given_name>
<surname>Yoshii</surname>
</person_name>
					</contributors>
					<titles><title>Alpha-Stable Autoregressive Fast Multichannel Nonnegative Matrix Factorization for Joint Speech Enhancement and Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>661</first_page>
						<last_page>665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-742</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fontaine21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Microphone Array Generalization for Multichannel Narrowband Deep Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>666</first_page>
						<last_page>670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-944</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyungchan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Sound Source Localization Based on Interchannel Phase Differences in All Frequencies with Spectral Masks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>671</first_page>
						<last_page>675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1178</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/song21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo Pérez</given_name>
<surname>Zarazaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariem Bouafif</given_name>
<surname>Mansali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zied</given_name>
<surname>Lachiri</surname>
</person_name>
					</contributors>
					<titles><title>Cancellation of Local Competing Speaker with Near-Field Localization for Distributed ad-hoc Sensor Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>676</first_page>
						<last_page>680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1329</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zarazaga21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Learning Method to Multi-Channel Active Noise Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>681</first_page>
						<last_page>685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1512</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Simone</given_name>
<surname>Graetzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor J.</given_name>
<surname>Cox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Akeroyd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John F.</given_name>
<surname>Culling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graham</given_name>
<surname>Naylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eszter</given_name>
<surname>Porter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rhoddy Viveros</given_name>
<surname>Muñoz</surname>
</person_name>
					</contributors>
					<titles><title>Clarity-2021 Challenges: Machine Learning Challenges for Advancing Hearing Aid Processing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>686</first_page>
						<last_page>690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1574</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/graetzer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehai</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>Optimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>691</first_page>
						<last_page>695</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1613</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sunit</given_name>
<surname>Sivasankaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominique</given_name>
<surname>Fohr</surname>
</person_name>
					</contributors>
					<titles><title>Explaining Deep Learning Models for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>696</first_page>
						<last_page>700</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1764</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sivasankaran21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weilong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwei</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Minimum-Norm Differential Beamforming for Linear Array with Directional Microphones</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>701</first_page>
						<last_page>705</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1989</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueteng</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhe</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoshuo</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Improving Streaming Transformer Based ASR Under a Framework of Self-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>706</first_page>
						<last_page>710</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1454</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cao21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samik</given_name>
<surname>Sadhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Che-Wei</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri Harish</given_name>
<surname>Mallidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name>
					</contributors>
					<titles><title>wav2vec-C: A Self-Supervised Model for Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>711</first_page>
						<last_page>715</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-717</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sadhu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Electra</given_name>
<surname>Wallington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benji</given_name>
<surname>Kershenbaum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>On the Learning Dynamics of Semi-Supervised Training for ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>716</first_page>
						<last_page>720</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1777</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wallington21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anuroop</given_name>
<surname>Sriram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Likhomanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiantong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineel</given_name>
<surname>Pratap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Kahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ann</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Synnaeve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name>
					</contributors>
					<titles><title>Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>721</first_page>
						<last_page>725</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-236</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hsu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name>
					</contributors>
					<titles><title>Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>726</first_page>
						<last_page>730</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-571</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/higuchi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ananya</given_name>
<surname>Misra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhouyuan</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shefali</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Siddhartha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name>
					</contributors>
					<titles><title>A Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>731</first_page>
						<last_page>735</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-654</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/misra21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammadreza</given_name>
<surname>Ghodsi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinghui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesse</given_name>
<surname>Emond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>736</first_page>
						<last_page>740</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-677</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Likhomanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiantong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Kahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Synnaeve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name>
					</contributors>
					<titles><title>slimIPL: Language-Model-Free Iterative Pseudo-Labeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>741</first_page>
						<last_page>745</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-740</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/likhomanenko21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianghu</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Phonetically Motivated Self-Supervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>746</first_page>
						<last_page>750</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-905</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yue21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Improving RNN-T for Domain Scaling Using Semi-Supervised Training with Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>751</first_page>
						<last_page>755</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1017</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/deng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Scott</given_name>
<surname>Seyfarth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Conversation Factorial Designs for Diarization Error Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>756</first_page>
						<last_page>760</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1864</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/seyfarth21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ross</given_name>
<surname>McGowan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinru</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vince</given_name>
<surname>DiCocco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thejaswi</given_name>
<surname>Muniyappa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name>
					</contributors>
					<titles><title>SmallER: Scaling Neural Entity Resolution for Edge Devices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>761</first_page>
						<last_page>765</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-98</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mcgowan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johann C.</given_name>
<surname>Rocholl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vicky</given_name>
<surname>Zayats</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel D.</given_name>
<surname>Walker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noah B.</given_name>
<surname>Murad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Schneider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel J.</given_name>
<surname>Liebling</surname>
</person_name>
					</contributors>
					<titles><title>Disfluency Detection with Unlabeled Data and Small BERT Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>766</first_page>
						<last_page>770</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-351</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rocholl21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Discriminative Self-Training for Punctuation Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>771</first_page>
						<last_page>775</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-246</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks Using Switching Tokens</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>776</first_page>
						<last_page>780</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1607</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ihori21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Noise Robust Method for Word-Level Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>781</first_page>
						<last_page>785</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1005</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Wintrode</surname>
</person_name>
					</contributors>
					<titles><title>Targeted Keyword Filtering for Accelerated Spoken Topic Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>786</first_page>
						<last_page>790</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1670</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wintrode21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shruti</given_name>
<surname>Palaskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruslan</given_name>
<surname>Salakhutdinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Speech Summarization Through Semantic Concept Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>791</first_page>
						<last_page>795</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1923</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/palaskar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyunjae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaewoong</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunjin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seongho</given_name>
<surname>Joe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngjune L.</given_name>
<surname>Gwon</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Semantic Understanding with Self-Supervised Methods for Abstractive Dialogue Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>796</first_page>
						<last_page>800</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1270</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Włodarczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emer</given_name>
<surname>Gilmartin</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Transition Patterns in Three-Party Conversation: Evidence from English, Estonian and Swedish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>801</first_page>
						<last_page>805</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-199</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wodarczak21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel J.</given_name>
<surname>Broughton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Asif</given_name>
<surname>Jalal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roger K.</given_name>
<surname>Moore</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>806</first_page>
						<last_page>810</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1730</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/broughton21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>811</first_page>
						<last_page>815</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-781</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Yang</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Juan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Voice Conversion Against Neural Spoofing Detectors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>816</first_page>
						<last_page>820</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-948</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ding21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiangheng</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Rizos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>821</first_page>
						<last_page>825</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1253</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/he21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>826</first_page>
						<last_page>830</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1301</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhichao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyong</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongqiang</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wendong</given_name>
<surname>Gan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haitao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hai</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Enriching Source Style Transfer in Recognition-Synthesis Based Non-Parallel Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>831</first_page>
						<last_page>835</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1351</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jheng-hao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yist Y.</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Ming</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>836</first_page>
						<last_page>840</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1356</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Liberatore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>An Exemplar Selection Algorithm for Native-Nonnative Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>841</first_page>
						<last_page>845</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1740</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liberatore21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingbei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xintao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>846</first_page>
						<last_page>850</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1990</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manh</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet Anh</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>Many-to-Many Voice Conversion Based Feature Disentanglement Using Variational Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>851</first_page>
						<last_page>855</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2086</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oubaïda</given_name>
<surname>Chouchane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baptiste</given_name>
<surname>Brossier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge Esteban Gamboa</given_name>
<surname>Gamboa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Lardy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orhan</given_name>
<surname>Ermis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madhu R.</given_name>
<surname>Kamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melek</given_name>
<surname>Önen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Voice Anti-Spoofing Using Secure Multi-Party Computation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>856</first_page>
						<last_page>860</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-983</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chouchane21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ranya</given_name>
<surname>Aloufi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamed</given_name>
<surname>Haddadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Boyle</surname>
</person_name>
					</contributors>
					<titles><title>Configurable Privacy-Preserving Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>861</first_page>
						<last_page>865</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1783</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/aloufi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Scott</given_name>
<surname>Novotney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yile</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Bulyko</surname>
</person_name>
					</contributors>
					<titles><title>Adjunct-Emeritus Distillation for Semi-Supervised Language Model Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>866</first_page>
						<last_page>870</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-27</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/novotney21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jae</given_name>
<surname>Ro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Mathews</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehryar</given_name>
<surname>Mohri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ananda Theertha</given_name>
<surname>Suresh</surname>
</person_name>
					</contributors>
					<titles><title>Communication-Efficient Agnostic Federated Averaging</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>871</first_page>
						<last_page>875</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-153</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ro21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timm</given_name>
<surname>Koppelmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandru</given_name>
<surname>Nelus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lea</given_name>
<surname>Schönherr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dorothea</given_name>
<surname>Kolossa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Feature Extraction for Cloud-Based Wake Word Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>876</first_page>
						<last_page>880</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-262</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/koppelmann21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao-Han Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>881</first_page>
						<last_page>885</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-640</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoxin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Continual Learning for Fake Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>886</first_page>
						<last_page>890</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-794</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ma21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad A.</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Szurley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Mueller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the Vulnerability of End-to-End Automatic Speech Recognition Models to Membership Inference Attacks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>891</first_page>
						<last_page>895</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1188</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shah21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amin</given_name>
<surname>Fazel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixiong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name>
					</contributors>
					<titles><title>SynthASR: Unlocking Synthetic Data for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>896</first_page>
						<last_page>900</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1882</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fazel21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ananya</given_name>
<surname>Muguli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lancelot</given_name>
<surname>Pinto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nirmala</given_name>
<surname>R</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Krishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrirama</given_name>
<surname>Bhat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth Raj</given_name>
<surname>Chetupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreyas</given_name>
<surname>Ramoji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viral</given_name>
<surname>Nanda</surname>
</person_name>
					</contributors>
					<titles><title>DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>901</first_page>
						<last_page>905</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-74</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/muguli21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Madhu R.</given_name>
<surname>Kamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose A.</given_name>
<surname>Gonzalez-Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teresa</given_name>
<surname>Grau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan M.</given_name>
<surname>Espin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Cascioli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiqing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Gomez-Alanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Font</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio M.</given_name>
<surname>Peinado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel M.</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria A.</given_name>
<surname>Zuluaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name>
					</contributors>
					<titles><title>PANACEA Cough Sound-Based Diagnosis of COVID-19 for the DiCOVA 2021 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>906</first_page>
						<last_page>910</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1062</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kamble21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Karas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Recognising Covid-19 from Coughing Using Ensembles of SVMs and LSTMs with Handcrafted and Deep Audio Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>911</first_page>
						<last_page>915</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1267</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/karas21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Isabella</given_name>
<surname>Södergren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maryam Pahlavan</given_name>
<surname>Nodeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prakash Chandra</given_name>
<surname>Chhipa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantina</given_name>
<surname>Nikolaidou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>György</given_name>
<surname>Kovács</surname>
</person_name>
					</contributors>
					<titles><title>Detecting COVID-19 from Audio Recording of Coughs Using Random Forests and Support Vector Machines</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>916</first_page>
						<last_page>920</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2191</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sodergren21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maulik</given_name>
<surname>Madhavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Diagnosis of COVID-19 Using Auditory Acoustic Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>921</first_page>
						<last_page>925</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-497</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/das21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Harvill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yash R.</given_name>
<surname>Wani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Narendra</given_name>
<surname>Ahuja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Beiser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Chestek</surname>
</person_name>
					</contributors>
					<titles><title>Classification of COVID-19 from Cough Using Autoregressive Predictive Coding Pretraining and Spectral Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>926</first_page>
						<last_page>930</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-799</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/harvill21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gauri</given_name>
<surname>Deshpande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>The DiCOVA 2021 Challenge &#38;#8212; An Encoder-Decoder Approach for COVID-19 Recognition from Coughing Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>931</first_page>
						<last_page>935</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-811</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/deshpande21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kotra Venkata Sai</given_name>
<surname>Ritwik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shareef Babu</given_name>
<surname>Kalluri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepu</given_name>
<surname>Vijayasenan</surname>
</person_name>
					</contributors>
					<titles><title>COVID-19 Detection from Spectral Features on the DiCOVA Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>936</first_page>
						<last_page>940</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1031</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ritwik21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adria</given_name>
<surname>Mallol-Ragolta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Cuesta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Gómez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Cough-Based COVID-19 Detection with Contextual Attention Convolutional Neural Networks and Gender Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>941</first_page>
						<last_page>945</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1052</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mallolragolta21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Swapnil</given_name>
<surname>Bhosale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Upasana</given_name>
<surname>Tiwari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupayan</given_name>
<surname>Chakraborty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning of Cough Descriptors for Automatic COVID-19 Preliminary Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>946</first_page>
						<last_page>950</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1249</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bhosale21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Flavio</given_name>
<surname>Avila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir H.</given_name>
<surname>Poorjam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Dognin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ananya</given_name>
<surname>Muguli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth Raj</given_name>
<surname>Chetupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maneesh</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Feature Selection and Explainability for COVID-19 Diagnostics from Cough Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>951</first_page>
						<last_page>955</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2197</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/avila21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Chorowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Ciesielski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarosław</given_name>
<surname>Dzikowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Łańcucki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Opala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Pusz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paweł</given_name>
<surname>Rychlikowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Stypułkowski</surname>
</person_name>
					</contributors>
					<titles><title>Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>971</first_page>
						<last_page>975</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1465</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chorowski21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Chorowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Ciesielski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarosław</given_name>
<surname>Dzikowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Łańcucki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Opala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Pusz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paweł</given_name>
<surname>Rychlikowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Stypułkowski</surname>
</person_name>
					</contributors>
					<titles><title>Aligned Contrastive Predictive Coding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>976</first_page>
						<last_page>980</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1544</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chorowski21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Suter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef</given_name>
<surname>Novak</surname>
</person_name>
					</contributors>
					<titles><title>Neural Text Denormalization for Speech Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>981</first_page>
						<last_page>985</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1814</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/suter21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Joglekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyed Omid</given_name>
<surname>Sadjadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meena</given_name>
<surname>Chandra-Shekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Cieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Fearless Steps Challenge Phase-3 (FSC P3): Advancing SLT for Unseen Channel and Mission Data Across NASA Apollo Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>986</first_page>
						<last_page>990</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2011</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/joglekar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Leykum</surname>
</person_name>
					</contributors>
					<titles><title>Voice Quality in Verbal Irony: Electroglottographic Analyses of Ironic Utterances in Standard Austrian German</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>991</first_page>
						<last_page>995</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-452</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/leykum21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mathilde</given_name>
<surname>Hutin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaru</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adèle</given_name>
<surname>Jatteau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Adda-Decker</surname>
</person_name>
					</contributors>
					<titles><title>Synchronic Fortition in Five Romance Languages? A Large Corpus-Based Study of Word-Initial Devoicing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>996</first_page>
						<last_page>1000</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-939</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hutin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Kraljevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Paola</given_name>
<surname>Bissiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Duckhorn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Constanze</given_name>
<surname>Tschoepe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Wolff</surname>
</person_name>
					</contributors>
					<titles><title>Glottal Stops in Upper Sorbian: A Data-Driven Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1001</first_page>
						<last_page>1005</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1101</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kraljevski21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Ludusan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Włodarczak</surname>
</person_name>
					</contributors>
					<titles><title>Cue Interaction in the Perception of Prosodic Prominence: The Role of Voice Quality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1006</first_page>
						<last_page>1010</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1357</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ludusan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenifer Vega</given_name>
<surname>Rodriguez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Vallée</surname>
</person_name>
					</contributors>
					<titles><title>Glottal Sounds in Korebaju</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1011</first_page>
						<last_page>1014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1417</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rodriguez21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anaïs</given_name>
<surname>Chanclu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Imen Ben</given_name>
<surname>Amor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cédric</given_name>
<surname>Gendrot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Ferragne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Classification of Phonation Types in Spontaneous Speech: Towards a New Workflow for the Characterization of Speakers&#38;#8217; Voice Quality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1015</first_page>
						<last_page>1018</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1765</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chanclu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rob J.J.H. van</given_name>
<surname>Son</surname>
</person_name>
					</contributors>
					<titles><title>Measuring Voice Quality Parameters After Speaker Pseudonymization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1019</first_page>
						<last_page>1023</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-26</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/son21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lars</given_name>
<surname>Steinert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Putze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dennis</given_name>
<surname>Küster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Recognition of Emotional Engagement of People with Dementia</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1024</first_page>
						<last_page>1028</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-567</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/steinert21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Hecker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian B.</given_name>
<surname>Pokorny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin D.</given_name>
<surname>Bartl-Pokorny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uwe</given_name>
<surname>Reichel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Hantke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Eyben</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dagmar M.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bert</given_name>
<surname>Arnrich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Speaking Corona? Human and Machine Recognition of COVID-19 from Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1029</first_page>
						<last_page>1033</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1771</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hecker21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huyen</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralph</given_name>
<surname>Vente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Lupea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah Ita</given_name>
<surname>Levitan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-Prosodic, Lexical and Demographic Cues to Persuasiveness in Competitive Debate Speeches</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1034</first_page>
						<last_page>1038</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1891</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nguyen21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bengt J.</given_name>
<surname>Borgström</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Bayesian Adaptation of PLDA for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1039</first_page>
						<last_page>1043</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-33</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/borgstrom21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danwei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingjian</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mi</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The DKU-Duke-Lenovo System Description for the Fearless Steps Challenge Phase III</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1044</first_page>
						<last_page>1048</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-235</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yafeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>Improved Meta-Learning Training for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1049</first_page>
						<last_page>1053</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-405</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanjie</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunfei</given_name>
<surname>Zi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengwu</given_name>
<surname>Xiong</surname>
</person_name>
					</contributors>
					<titles><title>Variational Information Bottleneck Based Regularization for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1054</first_page>
						<last_page>1058</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-482</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niko</given_name>
<surname>Brümmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Swart</surname>
</person_name>
					</contributors>
					<titles><title>Out of a Hundred Trials, How Many Errors Does Your Speaker Verifier Make?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1059</first_page>
						<last_page>1063</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-541</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/brummer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roza</given_name>
<surname>Chojnacka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Pelecanos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ignacio Lopez</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1064</first_page>
						<last_page>1068</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-646</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chojnacka21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Furong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaisheng</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huijia</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>AntVoice Neural Speaker Embedding System for FFSVC 2020</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1069</first_page>
						<last_page>1073</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-966</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianchen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>Gradient Regularization for Noise-Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1074</first_page>
						<last_page>1078</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1216</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Deep Feature CycleGANs: Speaker Identity Preserving Non-Parallel Microphone-Telephone Domain Adaptation for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1079</first_page>
						<last_page>1083</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1502</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kataria21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Pu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuguang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruirui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oguz</given_name>
<surname>Elibol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name>
					</contributors>
					<titles><title>Scaling Effect of Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1084</first_page>
						<last_page>1088</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1935</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yibo</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Joint Feature Enhancement and Speaker Recognition with Multi-Objective Task-Oriented Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1089</first_page>
						<last_page>1093</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1978</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1094</first_page>
						<last_page>1098</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1980</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Nautsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Anonymisation Using the McAdams Coefficient</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1099</first_page>
						<last_page>1103</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1070</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/patino21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiyu</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lidong</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Stream Gated and Pyramidal Temporal Convolutional Neural Networks for Audio-Visual Speech Separation in Multi-Talker Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1104</first_page>
						<last_page>1108</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-366</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1109</first_page>
						<last_page>1113</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-481</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianjun</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingwei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Residual Echo and Noise Cancellation with Feature Attention Module and Multi-Domain Loss Function</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1114</first_page>
						<last_page>1118</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-538</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiyun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>MIMO Self-Attentive RNN Beamformer for Multi-Speaker Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1119</first_page>
						<last_page>1123</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-570</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ritwik</given_name>
<surname>Giri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikant</given_name>
<surname>Venkataramani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Valin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umut</given_name>
<surname>Isik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arvindh</given_name>
<surname>Krishnaswamy</surname>
</person_name>
					</contributors>
					<titles><title>Personalized PercepNet: Real-Time, Low-Complexity Target Voice Separation and Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1124</first_page>
						<last_page>1128</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-694</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/giri21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yochai</given_name>
<surname>Yemini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ethan</given_name>
<surname>Fetaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haggai</given_name>
<surname>Maron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Gannot</surname>
</person_name>
					</contributors>
					<titles><title>Scene-Agnostic Multi-Microphone Speech Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1129</first_page>
						<last_page>1133</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-889</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yemini21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keitaro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryosuke</given_name>
<surname>Sawata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shusuke</given_name>
<surname>Takahashi</surname>
</person_name>
					</contributors>
					<titles><title>Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding Vectors Based on Regular Simplex</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1134</first_page>
						<last_page>1138</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1029</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tanaka21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Learning Approach to Multi-Channel and Multi-Microphone Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1139</first_page>
						<last_page>1143</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1508</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yueyue</given_name>
<surname>Na</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziteng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biao</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fu</surname>
</person_name>
					</contributors>
					<titles><title>Joint Online Multichannel Acoustic Echo Cancellation, Speech Dereverberation and Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1144</first_page>
						<last_page>1148</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1950</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/na21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kamo</surname>
</person_name>
					</contributors>
					<titles><title>Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1149</first_page>
						<last_page>1153</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2253</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sato21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anwesha</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhayjeet</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Illa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Estimating Articulatory Movements in Speech Production with Transformer Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1154</first_page>
						<last_page>1158</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1375</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/udupa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1159</first_page>
						<last_page>1163</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-300</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alfredo Esquivel</given_name>
<surname>Jaramillo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper Kjær</given_name>
<surname>Nielsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mads Græsbøll</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Speech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1164</first_page>
						<last_page>1168</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-47</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jaramillo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1169</first_page>
						<last_page>1173</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1066</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luo21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Noise Robust Pitch Stylization Using Minimum Mean Absolute Error Criterion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1174</first_page>
						<last_page>1178</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1307</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yarra21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Lin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Y.-W. Peter</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>An Attribute-Aligned Strategy for Learning Speech Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1179</first_page>
						<last_page>1183</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1341</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdolreza Sabzi</given_name>
<surname>Shahrebabaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name>
					</contributors>
					<titles><title>Raw Speech-to-Articulatory Inversion by Temporal Filtering and Decimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1184</first_page>
						<last_page>1188</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1429</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shahrebabaki21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Lilley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>H. Timothy</given_name>
<surname>Bunnell</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Training of a DNN-Based Formant Tracker</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1189</first_page>
						<last_page>1193</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1690</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lilley21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shu-wen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-Han</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yung-Sung</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-I Jeff</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kushal</given_name>
<surname>Lakhotia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yist Y.</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy T.</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guan-Ting</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tzu-Hsien</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ko-tik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da-Rong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zili</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuyan</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>SUPERB: Speech Processing Universal PERformance Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1194</first_page>
						<last_page>1198</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1775</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yang21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Synchronising Speech Segments with Musical Beats in Mandarin and English Singing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1199</first_page>
						<last_page>1203</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1841</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Peplinski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joel</given_name>
<surname>Shor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sachin</given_name>
<surname>Joglekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jake</given_name>
<surname>Garrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shwetak</given_name>
<surname>Patel</surname>
</person_name>
					</contributors>
					<titles><title>FRILL: A Non-Semantic Speech Embedding for Mobile Devices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1204</first_page>
						<last_page>1208</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2070</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peplinski21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Mori</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Contour Separation from Overlapping Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1209</first_page>
						<last_page>1213</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2164</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mori21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vamsi Krishna</given_name>
<surname>Ithapu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name>
					</contributors>
					<titles><title>Do Sound Event Representations Generalize to Other Audio Tasks? A Case Study in Audio Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1214</first_page>
						<last_page>1218</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-347</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kumar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Baolin</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenguang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianfeng</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation for Spoken Language Understanding via Pretrained Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1219</first_page>
						<last_page>1223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-117</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peng21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Radfar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siegfried</given_name>
<surname>Kunzmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>FANS: Fusing ASR and NLU for On-Device SLU</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1224</first_page>
						<last_page>1228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-793</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/radfar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiran</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nihal</given_name>
<surname>Potdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anderson R.</given_name>
<surname>Avila</surname>
</person_name>
					</contributors>
					<titles><title>Sequential End-to-End Intent and Slot Label Classification and Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1229</first_page>
						<last_page>1233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1569</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cao21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Muralidharan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joel Ruben Antony</given_name>
<surname>Moniz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weicheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Pulman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megan</given_name>
<surname>Barnes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingjing</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Acero</surname>
</person_name>
					</contributors>
					<titles><title>DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1234</first_page>
						<last_page>1238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1877</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/muralidharan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ting-Wei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruolin</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biing-Hwang</given_name>
<surname>Juang</surname>
</person_name>
					</contributors>
					<titles><title>A Context-Aware Hierarchical BERT Fusion Network for Multi-Turn Dialog Act Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1239</first_page>
						<last_page>1243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-95</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Pre-Training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1244</first_page>
						<last_page>1248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-234</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quynh</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judith</given_name>
<surname>Gaspers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniil</given_name>
<surname>Sorokin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Lehnen</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Temporal Performance Drop of Deployed Production Spoken Language Understanding Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1249</first_page>
						<last_page>1253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-580</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/do21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jatin</given_name>
<surname>Ganhotra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Kwang J.</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sachindra</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Dialog History into End-to-End Spoken Language Understanding Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1254</first_page>
						<last_page>1258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1460</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ganhotra21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ting</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongxuan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1259</first_page>
						<last_page>1263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1463</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/han21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alissa</given_name>
<surname>Ostapenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Viswanathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Dalmia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1264</first_page>
						<last_page>1268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1537</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/arora21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hengxin</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuaijiang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoning</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Data Augmentation for End-to-End Mandarin Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1269</first_page>
						<last_page>1273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1162</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sun21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhikai</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1274</first_page>
						<last_page>1278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1075</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gong21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunzheng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruchao</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Low Resource German ASR with Untranscribed Data Spoken by Non-Native Children &#38;#8212; INTERSPEECH 2021 Shared Task SPAPL System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1279</first_page>
						<last_page>1283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1974</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angad</given_name>
<surname>Chandorkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mason</given_name>
<surname>Chua</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsendsuren</given_name>
<surname>Munkhdalai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Françoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>Robust Continuous On-Device Personalization for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1284</first_page>
						<last_page>1288</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-318</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sim21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shashi</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakti P.</given_name>
<surname>Rath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Pandey</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Normalization Using Joint Variational Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1289</first_page>
						<last_page>1293</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-467</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kumar21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaopeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengfei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongqin</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1294</first_page>
						<last_page>1298</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1104</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsz Kin</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayumi</given_name>
<surname>Ohta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigehiko</given_name>
<surname>Schamoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Riezler</surname>
</person_name>
					</contributors>
					<titles><title>On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1299</first_page>
						<last_page>1303</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1679</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lam21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heting</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junrui</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaizhi</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Cross-Lingual Phonetic Recognition with External Language Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1304</first_page>
						<last_page>1308</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1843</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoli</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Rapid Speaker Adaptation for Conformer Transducer: Attention and Bias Are All You Need</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1309</first_page>
						<last_page>1313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1884</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nilaksh</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Sunkara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duen Horng</given_name>
<surname>Chau</surname>
</person_name>
					</contributors>
					<titles><title>Best of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1314</first_page>
						<last_page>1318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1888</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/das21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Extending Pronunciation Dictionary with Automatically Detected Word Mispronunciations to Improve PAII&#38;#8217;s System for Interspeech 2021 Non-Native Child English Close Track ASR Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1319</first_page>
						<last_page>1323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2053</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tingle</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yichen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>CVC: Contrastive Learning for Non-Parallel Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1324</first_page>
						<last_page>1328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-137</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Huai</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Feng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1329</first_page>
						<last_page>1333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-208</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sefik Emre</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Dimitriadis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Kumatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Gmyr</surname>
</person_name>
					</contributors>
					<titles><title>One-Shot Voice Conversion with Speaker-Agnostic StarGAN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1334</first_page>
						<last_page>1338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-221</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/eskimez21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Koshizuka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hidefumi</given_name>
<surname>Ohmura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouichi</given_name>
<surname>Katsurada</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Tuning Pre-Trained Voice Conversion Model for Adding New Target Speakers with Limited Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1339</first_page>
						<last_page>1343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-244</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/koshizuka21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Disong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liqun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu Ting</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1344</first_page>
						<last_page>1348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-283</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yinghao Aaron</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali</given_name>
<surname>Zare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>StarGANv2-VC: A Diverse, Unsupervised, Non-Parallel Framework for Natural-Sounding Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1349</first_page>
						<last_page>1353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-319</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srishti</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Narang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brejesh</given_name>
<surname>Lall</surname>
</person_name>
					</contributors>
					<titles><title>Normalization Driven Zero-Shot Multi-Speaker Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1354</first_page>
						<last_page>1358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-441</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kumar21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoki</given_name>
<surname>Sakamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akira</given_name>
<surname>Taniguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tadahiro</given_name>
<surname>Taniguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name>
					</contributors>
					<titles><title>StarGAN-VC+ASR: StarGAN-Based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1359</first_page>
						<last_page>1363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-492</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sakamoto21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuexin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunquan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingyuan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edwin R.</given_name>
<surname>Hancock</surname>
</person_name>
					</contributors>
					<titles><title>Two-Pathway Style Embedding for Arbitrary Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1364</first_page>
						<last_page>1368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-506</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yufei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengzhu</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wang</given_name>
<surname>Shuai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenchuan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weibin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Non-Parallel Any-to-Many Voice Conversion by Replacing Speaker Statistics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1369</first_page>
						<last_page>1373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-557</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohai</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Voice Conversion with a Cycle Consistency Loss on Linguistic Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1374</first_page>
						<last_page>1378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-687</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongqiang</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Improving Robustness of One-Shot Voice Conversion with Deep Discriminative Speaker Encoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1379</first_page>
						<last_page>1383</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2132</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/du21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Szakay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing an Automatic Creaky Voice Detection Method for Australian English Speaking Females</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1384</first_page>
						<last_page>1388</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-711</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/white21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proctor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Szakay</surname>
</person_name>
					</contributors>
					<titles><title>A Comparison of Acoustic Correlates of Voice Quality Across Different Recording Devices: A Cautionary Tale</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1389</first_page>
						<last_page>1393</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-729</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/penney21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Sfakianaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George P.</given_name>
<surname>Kafentzis</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Voice Function Characteristics of Greek Speakers with Hearing Loss Using Automatic Glottal Source Feature Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1394</first_page>
						<last_page>1398</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-870</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sfakianaki21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Huckvale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catinca</given_name>
<surname>Buciuleac</surname>
</person_name>
					</contributors>
					<titles><title>Automated Detection of Voice Disorder in the Saarbr&#38;#252;cken Voice Database: Effects of Pathology Subset and Audio Materials</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1399</first_page>
						<last_page>1403</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1507</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huckvale21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steven M.</given_name>
<surname>Lulich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita R.</given_name>
<surname>Patel</surname>
</person_name>
					</contributors>
					<titles><title>Accelerometer-Based Measurements of Voice Quality in Children During Semi-Occluded Vocal Tract Exercise with a Narrow Straw in Air</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1404</first_page>
						<last_page>1408</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1918</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lulich21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amrit</given_name>
<surname>Romana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Roberts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noelle</given_name>
<surname>Carlozzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer Ann</given_name>
<surname>Miner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Praveen</given_name>
<surname>Dayalu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Coordination for Speech Motor Tracking in Huntington Disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1409</first_page>
						<last_page>1413</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-688</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/perez21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos A.</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Efren</given_name>
<surname>Aragón</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>María E.</given_name>
<surname>Hdez-Díaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc S. de</given_name>
<surname>Bodt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roman</given_name>
<surname>Cmejla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marina</given_name>
<surname>Englert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mara</given_name>
<surname>Behlau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Dysphonia Severity as a Function of Roughness and Breathiness Ratings in the GRBAS Scale</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1414</first_page>
						<last_page>1418</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1540</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ferrer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikolay</given_name>
<surname>Karpov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Denisenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fedor</given_name>
<surname>Minkin</surname>
</person_name>
					</contributors>
					<titles><title>Golos: Russian Dataset for Speech Research</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1419</first_page>
						<last_page>1423</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-462</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/karpov21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samik</given_name>
<surname>Sadhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Radically Old Way of Computing Spectra: Applications in End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1424</first_page>
						<last_page>1428</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-643</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sadhu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ragheb</given_name>
<surname>Al-Ghezi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aku</given_name>
<surname>Rouhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raili</given_name>
<surname>Hildén</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised End-to-End ASR for Low Resource L2 Swedish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1429</first_page>
						<last_page>1433</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1710</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/alghezi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrick K.</given_name>
<surname>O’Neill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somshubra</given_name>
<surname>Majumdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vahid</given_name>
<surname>Noroozi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuekai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleksii</given_name>
<surname>Kuchaiev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuliya</given_name>
<surname>Dovzhenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keenan</given_name>
<surname>Freyberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael D.</given_name>
<surname>Shulman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georg</given_name>
<surname>Kucsko</surname>
</person_name>
					</contributors>
					<titles><title>SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1434</first_page>
						<last_page>1438</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1860</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/oneill21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Solène</given_name>
<surname>Evain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcely Zanon</given_name>
<surname>Boito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sina</given_name>
<surname>Alisamir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyi</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Dinarelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Allauzen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Lecouteux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Portet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solange</given_name>
<surname>Rossato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabien</given_name>
<surname>Ringeval</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Didier</given_name>
<surname>Schwab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1439</first_page>
						<last_page>1443</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-556</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/evain21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Šturm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radek</given_name>
<surname>Skarnitzl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomáš</given_name>
<surname>Nechanský</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Accommodation in Face-to-Face and Telephone Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1444</first_page>
						<last_page>1448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-130</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sturm21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Josiane</given_name>
<surname>Riverin-Coutlée</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Conceição</given_name>
<surname>Cunha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enkeleida</given_name>
<surname>Kapia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Harrington</surname>
</person_name>
					</contributors>
					<titles><title>Dialect Features in Heterogeneous and Homogeneous Gheg Speaking Communities</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1449</first_page>
						<last_page>1453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1090</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/riverincoutlee21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alena</given_name>
<surname>Witzlack-Makarevich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lilja</given_name>
<surname>Saeboe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saudah</given_name>
<surname>Namyalo</surname>
</person_name>
					</contributors>
					<titles><title>An Exploration of the Acoustic Space of Rhotics and Laterals in Ruruuli</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1454</first_page>
						<last_page>1458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1328</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zellers21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kubra</given_name>
<surname>Bodur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sweeney</given_name>
<surname>Branje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgane</given_name>
<surname>Peirolo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingrid</given_name>
<surname>Tiscareno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James S.</given_name>
<surname>German</surname>
</person_name>
					</contributors>
					<titles><title>Domain-Initial Strengthening in Turkish: Acoustic Cues to Prosodic Hierarchy in Stop Consonants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1459</first_page>
						<last_page>1463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2230</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bodur21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Zmolikova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Desh</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Auxiliary Loss Function for Target Speech Extraction and Recognition with Weak Supervision Based on Speaker Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1464</first_page>
						<last_page>1468</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-986</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zmolikova21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Borsdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Universal Speaker Extraction in the Presence and Absence of Target Speakers for Speech of One and Two Talkers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1469</first_page>
						<last_page>1473</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1939</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/borsdorf21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Mateju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frantisek</given_name>
<surname>Kynych</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Cerva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindrich</given_name>
<surname>Zdansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiri</given_name>
<surname>Malek</surname>
</person_name>
					</contributors>
					<titles><title>Using X-Vectors for Speech Activity Detection in Broadcast Streams</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1474</first_page>
						<last_page>1478</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-192</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mateju21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Salvati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlo</given_name>
<surname>Drioli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gian Luca</given_name>
<surname>Foresti</surname>
</person_name>
					</contributors>
					<titles><title>Time Delay Estimation for Speaker Localization Using CNN-Based Parametrized GCC-PHAT Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1479</first_page>
						<last_page>1483</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-988</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/salvati21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Midia</given_name>
<surname>Yousefi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Speaker Counting in a Cocktail Party Scenario Using Attention-Guided Convolutional Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1484</first_page>
						<last_page>1488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-331</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yousefi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola García</given_name>
<surname>Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Dauwels</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy W.H.</given_name>
<surname>Khong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzy J.</given_name>
<surname>Styles</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Language Diarization for Bilingual Code-Switching Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1489</first_page>
						<last_page>1493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-82</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raphaël</given_name>
<surname>Duroselle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Jouvet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Illina</surname>
</person_name>
					</contributors>
					<titles><title>Modeling and Training Strategies for Language Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1494</first_page>
						<last_page>1498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-277</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/duroselle21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>A Weight Moving Average Based Alternate Decoupled Learning Algorithm for Long-Tailed Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1499</first_page>
						<last_page>1503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-776</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keqi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1504</first_page>
						<last_page>1508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1186</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/deng21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyun</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Exploring wav2vec 2.0 on Speaker Verification and Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1509</first_page>
						<last_page>1513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1280</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>G.</given_name>
<surname>Ramesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C. Shiva</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Phonotactic Representations for Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1514</first_page>
						<last_page>1518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1310</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ramesh21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jicheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Tung</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>E2E-Based Multi-Task Learning Approach to Joint Speech and Accent Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1519</first_page>
						<last_page>1523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1495</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moakala</given_name>
<surname>Tzudir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shikha</given_name>
<surname>Baghel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.R. Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Excitation Source Feature Based Dialect Identification in Ao &#38;#8212; A Low Resource Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1524</first_page>
						<last_page>1528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1672</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tzudir21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreya</given_name>
<surname>Khare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anuj</given_name>
<surname>Diwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunita</given_name>
<surname>Sarawagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samarth</given_name>
<surname>Bharadwaj</surname>
</person_name>
					</contributors>
					<titles><title>Low Resource ASR: The Surprising Effectiveness of High Resource Transliteration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1529</first_page>
						<last_page>1533</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2062</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/khare21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1534</first_page>
						<last_page>1538</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1664</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/feng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin van</given_name>
<surname>Niekerk</surname>
</person_name>
					</contributors>
					<titles><title>Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1539</first_page>
						<last_page>1543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-50</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kamper21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongwei</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wubo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miao</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1544</first_page>
						<last_page>1548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-391</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jiang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christiaan</given_name>
<surname>Jacobs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Transfer of Acoustic Word Embeddings Improves When Training on Languages Related to the Target Zero-Resource Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1549</first_page>
						<last_page>1553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-461</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jacobs21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin van</given_name>
<surname>Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leanne</given_name>
<surname>Nortje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Baas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1554</first_page>
						<last_page>1558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1182</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/niekerk21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shun</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1559</first_page>
						<last_page>1563</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1340</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/takahashi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Maekaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Rudnicky</surname>
</person_name>
					</contributors>
					<titles><title>Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1564</first_page>
						<last_page>1568</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1503</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/maekaku21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xia</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amila</given_name>
<surname>Gamage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Terry</given_name>
<surname>Hanley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingting</given_name>
<surname>Mu</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Indicators of Vulnerability from Short Speech Segments Using Acoustic and Textual Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1569</first_page>
						<last_page>1573</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1525</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cui21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Dunbar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Bernard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Hamilakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tu Anh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maureen de</given_name>
<surname>Seyssel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patricia</given_name>
<surname>Rozé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgane</given_name>
<surname>Rivière</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugene</given_name>
<surname>Kharitonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>The Zero Resource Speech Challenge 2021: Spoken Language Modelling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1574</first_page>
						<last_page>1578</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1755</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dunbar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gautham Krishna</given_name>
<surname>Gudur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satheesh Kumar</given_name>
<surname>Perepu</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Federated Learning with New Classes for Audio Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1579</first_page>
						<last_page>1583</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2264</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gudur21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rouditchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angie</given_name>
<surname>Boggust</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhiraj</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hilde</given_name>
<surname>Kuehne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rameswar</given_name>
<surname>Panda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogerio</given_name>
<surname>Feris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Torralba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>AVLnet: Learning Audio-Visual Language Representations from Instructional Videos</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1584</first_page>
						<last_page>1588</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1312</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rouditchenko21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gyeong-Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Woo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanbin</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Ji</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Ik</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1589</first_page>
						<last_page>1593</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-239</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Maniati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Markopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Vamvoukakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June Sig</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyoungmin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aimilios</given_name>
<surname>Chalamandaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pirros</given_name>
<surname>Tsiakoulis</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Low Resource Speaker Adaptation Using Phonological Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1594</first_page>
						<last_page>1598</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-327</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/maniati21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyue</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haitong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1599</first_page>
						<last_page>1603</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-474</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenchuan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weibin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Voice Conversion with Disentangled Universal Linguistic Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1604</first_page>
						<last_page>1608</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-552</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yang21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengchen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenfeng</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingying</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>EfficientSing: A Chinese Singing Voice Synthesis System Using Duration-Free Acoustic Model and HiFi-GAN Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1609</first_page>
						<last_page>1613</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-771</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Detai</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker Consistency Loss for Text-To-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1614</first_page>
						<last_page>1618</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-897</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zengqiang</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haozhe</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Cross-Speaker Style Transfer for Multi-Language Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1619</first_page>
						<last_page>1623</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1265</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ege</given_name>
<surname>Kesim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Engin</given_name>
<surname>Erzin</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Contributions of Speech and Facial Landmarks for Talking Head Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1624</first_page>
						<last_page>1628</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1585</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kesim21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shijing</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenqi</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinghua</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Speech2Video: Cross-Modal Distillation for Speech to Video Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1629</first_page>
						<last_page>1633</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1996</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/si21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungu</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1634</first_page>
						<last_page>1638</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-36</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gang-Xuan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Wei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yen-Ju</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun-Shien</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>QISTA-Net-Audio: Audio Super-Resolution via Non-Convex &#38;#8467;_q-Norm Minimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1639</first_page>
						<last_page>1643</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-670</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lizhong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xue</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxing</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngo</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwang Pyo</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>X-net: A Joint Scale Down and Scale Up Method for Voice Call</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1644</first_page>
						<last_page>1648</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-812</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kexun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changliang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1649</first_page>
						<last_page>1653</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-892</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoxin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name>
					</contributors>
					<titles><title>Half-Truth: A Partially Fake Audio Detection Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1654</first_page>
						<last_page>1658</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-930</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhusan</given_name>
<surname>Chettri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosa González</given_name>
<surname>Hautamäki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>Data Quality as Predictor of Voice Anti-Spoofing Generalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1659</first_page>
						<last_page>1663</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1180</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chettri21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngju</given_name>
<surname>Cheon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soojoong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangwook</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inseon</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>Coded Speech Enhancement Using Neural Network-Based Vector-Quantized Residual Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1664</first_page>
						<last_page>1668</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1204</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cheon21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Drude</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahn</given_name>
<surname>Heymann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Schwarz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Valin</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Opus Compression for Far-Field Automatic Speech Recognition with a Fixed Bitrate Budget</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1669</first_page>
						<last_page>1673</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1214</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/drude21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Siegert</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Prosodic Variations on Accidental Triggers of a Commercial Voice Assistant</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1674</first_page>
						<last_page>1678</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1354</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/siegert21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adam</given_name>
<surname>Gabryś</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunlong</given_name>
<surname>Jiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viacheslav</given_name>
<surname>Klimkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name>
					</contributors>
					<titles><title>Improving the Expressiveness of Neural Vocoding with Non-Affine Normalizing Flows</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1679</first_page>
						<last_page>1683</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1555</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gabrys21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gauri P.</given_name>
<surname>Prajapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dipesh K.</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preet P.</given_name>
<surname>Amin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant A.</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Voice Privacy Through x-Vector and CycleGAN-Based Anonymization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1684</first_page>
						<last_page>1688</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1573</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/prajapati21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaustubh</given_name>
<surname>Kalgaonkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gil</given_name>
<surname>Keren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Didi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name>
					</contributors>
					<titles><title>A Two-Stage Approach to Speech Bandwidth Extension</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1689</first_page>
						<last_page>1693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1941</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungmin</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngcheol</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jongmo</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungkwon</given_name>
<surname>Beack</surname>
</person_name>
					</contributors>
					<titles><title>Development of a Psychoacoustic Loss Function for the Deep Neural Network	(DNN)-Based Speech Coder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1694</first_page>
						<last_page>1698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2151</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/byun21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Stoidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Cavallaro</surname>
</person_name>
					</contributors>
					<titles><title>Protecting Gender and Identity with Disentangled Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1699</first_page>
						<last_page>1703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2163</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/stoidis21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yahya</given_name>
<surname>Aldholmi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rawan</given_name>
<surname>Aldhafyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asma</given_name>
<surname>Alqahtani</surname>
</person_name>
					</contributors>
					<titles><title>Perception of Standard Arabic Synthetic Speech Rate</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1704</first_page>
						<last_page>1707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-39</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/aldholmi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Kishiyama</surname>
</person_name>
					</contributors>
					<titles><title>The Influence of Parallel Processing on Illusory Vowels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1708</first_page>
						<last_page>1712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-89</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kishiyama21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anupama</given_name>
<surname>Chingacham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vera</given_name>
<surname>Demberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1713</first_page>
						<last_page>1717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-306</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chingacham21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olympia</given_name>
<surname>Simantiraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name>
					</contributors>
					<titles><title> SpeechAdjuster: A Tool for Investigating Listener Preferences and Speech Intelligibility</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1718</first_page>
						<last_page>1722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-324</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/simantiraki21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Susumu</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuta</given_name>
<surname>Ide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teppei</given_name>
<surname>Nakano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>VocalTurk: Exploring Feasibility of Crowdsourced Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1723</first_page>
						<last_page>1727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-464</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/saito21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Aging and Age-Related Hearing Loss on Talker Discrimination</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1728</first_page>
						<last_page>1732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-682</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanlu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Relationships Between Perceptual Distinctiveness, Articulatory Complexity and Functional Load in Speech Communication</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1733</first_page>
						<last_page>1737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-721</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Camryn</given_name>
<surname>Terblanche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia J.</given_name>
<surname>Gully</surname>
</person_name>
					</contributors>
					<titles><title>Human Spoofing Detection Performance on Degraded Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1738</first_page>
						<last_page>1742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1225</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/terblanche21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marieke</given_name>
<surname>Einfeldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Sevastjanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Zahner-Ritter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Kazak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bettina</given_name>
<surname>Braun</surname>
</person_name>
					</contributors>
					<titles><title>Reliable Estimates of Interpretable Cue Effects with Active Learning in Psycholinguistic Research</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1743</first_page>
						<last_page>1747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1524</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/einfeldt21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puneet</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishesh</given_name>
<surname>Kaushik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Balasubramanian</given_name>
<surname>Raman</surname>
</person_name>
					</contributors>
					<titles><title>Towards the Explainability of Multimodal Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1748</first_page>
						<last_page>1752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1718</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kumar21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Biao</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoxing</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Dobel</surname>
</person_name>
					</contributors>
					<titles><title>Primacy of Mouth over Eyes: Eye Movement Evidence from Audiovisual Mandarin Lexical Tones and Vowels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1753</first_page>
						<last_page>1756</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1741</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zeng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Makio</given_name>
<surname>Kashino</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Impact of Spectral and Temporal Degradation on End-to-End Automatic Speech Recognition Performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1757</first_page>
						<last_page>1761</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2091</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ashihara21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thai-Son</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stüker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Super-Human Performance in Online Low-Latency Recognition of Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1762</first_page>
						<last_page>1766</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1114</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nguyen21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikas</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupesh R.</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Softmax Architecture for Streaming Multilingual End-to-End ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1767</first_page>
						<last_page>1771</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1298</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/joshi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahaveer</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gil</given_name>
<surname>Keren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yatharth</given_name>
<surname>Saraf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1772</first_page>
						<last_page>1776</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1566</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/le21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rami</given_name>
<surname>Botros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cyril</given_name>
<surname>Allauzen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Variani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quoc-Nam</given_name>
<surname>Le-The</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anmol</given_name>
<surname>Gulati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahui</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diamantino</given_name>
<surname>Caseiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pat</given_name>
<surname>Rondon</surname>
</person_name>
					</contributors>
					<titles><title>An Efficient Streaming Non-Recurrent On-Device End-to-End Model with Improvements to Rare-Word Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1777</first_page>
						<last_page>1781</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-206</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sainath21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Multi-Talker Speech Recognition with Joint Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1782</first_page>
						<last_page>1786</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-207</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taichi</given_name>
<surname>Asami</surname>
</person_name>
					</contributors>
					<titles><title>Streaming End-to-End Speech Recognition for Hybrid RNN-T/Attention Architecture</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1787</first_page>
						<last_page>1791</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-437</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/moriya21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Schwarz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Sklyar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Wiesler</surname>
</person_name>
					</contributors>
					<titles><title>Improving RNN-T ASR Accuracy Using Context Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1792</first_page>
						<last_page>1796</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-542</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/schwarz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufeng</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junfeng</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>HMM-Free Encoder Pre-Training for Streaming RNN Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1797</first_page>
						<last_page>1801</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-586</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Haws</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Exposure Bias in Training Recurrent Neural Network Transducers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1802</first_page>
						<last_page>1806</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-587</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cui21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thibault</given_name>
<surname>Doutre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Siohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangliang</given_name>
<surname>Cao</surname>
</person_name>
					</contributors>
					<titles><title>Bridging the Gap Between Streaming and Non-Streaming ASR Systems by Distilling Ensembles of CTC and RNN-T Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1807</first_page>
						<last_page>1811</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-637</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/doutre21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tongzhou</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1812</first_page>
						<last_page>1816</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-720</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/audhkhasi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hirofumi</given_name>
<surname>Inaguma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1817</first_page>
						<last_page>1821</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1110</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/inaguma21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name>
					</contributors>
					<titles><title>Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1822</first_page>
						<last_page>1826</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1693</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/moritz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwangyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Mode Transformer Transducer with Stochastic Future Context</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1827</first_page>
						<last_page>1831</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1953</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinlei</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiguang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>A Causal U-Net Based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1832</first_page>
						<last_page>1836</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1457</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ren21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiran</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuepeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shidong</given_name>
<surname>Shang</surname>
</person_name>
					</contributors>
					<titles><title>A Partitioned-Block Frequency-Domain Adaptive Kalman Filter for Stereophonic Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1837</first_page>
						<last_page>1841</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-135</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhu21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taihui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiran</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Independent Vector Analysis Using Semi-Supervised Nonnegative Matrix Factorization as a Source Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1842</first_page>
						<last_page>1846</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-146</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiangyu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name>
					</contributors>
					<titles><title>Improving Channel Decorrelation for Multi-Channel Target Speech Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1847</first_page>
						<last_page>1851</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-298</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/han21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinjiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Inplace Gated Convolutional Recurrent Neural Network for Dual-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1852</first_page>
						<last_page>1856</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-899</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>R.G. Prithvi</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M.K.</given_name>
<surname>Jayesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurenjan</given_name>
<surname>Purushothaman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M.A. Basha</given_name>
<surname>Shaik</surname>
</person_name>
					</contributors>
					<titles><title>SRIB-LEAP Submission to Far-Field Multi-Channel Speech Enhancement Challenge for Video Conferencing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1857</first_page>
						<last_page>1861</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1111</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/raj21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weilong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiguang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwei</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Multi-Channel Speech Enhancement Based on Neural Network Masking with Attention Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1862</first_page>
						<last_page>1866</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2266</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xue21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominique</given_name>
<surname>Fohr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Illina</surname>
</person_name>
					</contributors>
					<titles><title>BERT-Based Semantic Model for Rescoring N-Best Speech Recognition List</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1867</first_page>
						<last_page>1871</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-313</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fohr21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karel</given_name>
<surname>Beneš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Text Augmentation for Language Models in High Error Recognition Scenario</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1872</first_page>
						<last_page>1876</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-627</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/benes21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingbo</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Thulke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gerstenberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khoa Viet</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>On Sampling-Based Training Criteria for Neural Language Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1877</first_page>
						<last_page>1881</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1067</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gao21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Janne</given_name>
<surname>Pylkkönen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antti</given_name>
<surname>Ukkonen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juho</given_name>
<surname>Kilpikoski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samu</given_name>
<surname>Tamminen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannes</given_name>
<surname>Heikinheimo</surname>
</person_name>
					</contributors>
					<titles><title>Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1882</first_page>
						<last_page>1886</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1191</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pylkkonen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Cieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Fiumara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Wright</surname>
</person_name>
					</contributors>
					<titles><title>Using Games to Augment Corpora for Language Recognition and Confusability</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1887</first_page>
						<last_page>1891</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1611</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cieri21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gianni</given_name>
<surname>Fenu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirko</given_name>
<surname>Marras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giacomo</given_name>
<surname>Medda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giacomo</given_name>
<surname>Meloni</surname>
</person_name>
					</contributors>
					<titles><title>Fair Voice Biometrics: Impact of Demographic Imbalance on Group Fairness in Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1892</first_page>
						<last_page>1896</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1857</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fenu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leying</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1897</first_page>
						<last_page>1901</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2119</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul-Gauthier</given_name>
<surname>Noé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Mohammadamini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Driss</given_name>
<surname>Matrouf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Nautsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1902</first_page>
						<last_page>1906</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1712</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/noe21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amrit</given_name>
<surname>Romana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Bandon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Gutierrez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Roberts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinson&#38;#8217;s Disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1907</first_page>
						<last_page>1911</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1694</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/romana21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robin</given_name>
<surname>Vaysse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jérôme</given_name>
<surname>Farinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corine</given_name>
<surname>Astésano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Régine</given_name>
<surname>André-Obrecht</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Extraction of Speech Rhythm Descriptors for Speech Intelligibility Assessment in the Context of Head and Neck Cancers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1912</first_page>
						<last_page>1916</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1736</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vaysse21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinzi</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Speech Disorder Classification Using Extended Factorized Hierarchical Variational Auto-Encoders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1917</first_page>
						<last_page>1921</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2180</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qi21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikram C.</given_name>
<surname>Mathad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tristan J.</given_name>
<surname>Mahr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy</given_name>
<surname>Scherer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathy</given_name>
<surname>Chapman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine C.</given_name>
<surname>Hustad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>The Impact of Forced-Alignment Errors on Automatic Pronunciation Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1922</first_page>
						<last_page>1926</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1403</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mathad21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Esaú</given_name>
<surname>Villatoro-Tello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S. Pavankumar</given_name>
<surname>Dubagunta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Fritsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriela</given_name>
<surname>Ramírez-de-la-Rosa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Late Fusion of the Available Lexicon and Raw Waveform-Based Acoustic Modeling for Depression and Dementia Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1927</first_page>
						<last_page>1931</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1288</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/villatorotello21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amin Honarmandi</given_name>
<surname>Shandiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Markó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name>
					</contributors>
					<titles><title>Neural Speaker Embeddings for Ultrasound-Based Silent Speech Interfaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1932</first_page>
						<last_page>1936</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1466</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shandiz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jatin</given_name>
<surname>Lamba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Abhishek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jayaprakash</given_name>
<surname>Akula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Dabral</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Ramakrishnan</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Learning for Audio-Visual Video Parsing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1937</first_page>
						<last_page>1941</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2135</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lamba21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Darren</given_name>
<surname>Cook</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miri</given_name>
<surname>Zilka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Maskell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurence</given_name>
<surname>Alison</surname>
</person_name>
					</contributors>
					<titles><title>A Psychology-Driven Computational Analysis of Political Interviews</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1942</first_page>
						<last_page>1946</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2249</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cook21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Santoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Yamada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoji</given_name>
<surname>Makino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenkichi</given_name>
<surname>Ishizuka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takekatsu</given_name>
<surname>Hiramura</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1947</first_page>
						<last_page>1951</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-411</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/santoso21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alif</given_name>
<surname>Silpachai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivana</given_name>
<surname>Rehman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taylor Anne</given_name>
<surname>Barriuso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Levis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Chukharev-Hudilainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Voice Type and Task on L2 Learners&#38;#8217; Awareness of Pronunciation Errors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1952</first_page>
						<last_page>1956</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-701</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/silpachai21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alla</given_name>
<surname>Menshikova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniil</given_name>
<surname>Kocharov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Kachkovskaia</surname>
</person_name>
					</contributors>
					<titles><title>Lexical Entrainment and Intra-Speaker Variability in Cooperative Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1957</first_page>
						<last_page>1961</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1441</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/menshikova21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shamila</given_name>
<surname>Nasreen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Hough</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Purver</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Alzheimer&#38;#8217;s Disease Using Interactional and Acoustic Features from Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1962</first_page>
						<last_page>1966</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1526</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nasreen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Burke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Cornish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doug</given_name>
<surname>Habberstad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alaa</given_name>
<surname>Sakallah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Markuson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seemran</given_name>
<surname>Kansara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Afik</given_name>
<surname>Faerman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasmine</given_name>
<surname>Bensidi-Slimane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Fry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saige</given_name>
<surname>Portera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Suendermann-Oeft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Pautler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carly</given_name>
<surname>Demopoulos</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Interplay Between Affective, Phonatory and Motoric Subsystems in Autism Spectrum Disorder Using a Multimodal Dialogue Agent</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1967</first_page>
						<last_page>1971</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1796</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kothare21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos Toshinori</given_name>
<surname>Ishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiken</given_name>
<surname>Shintani</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Eye Gaze Reasons and Gaze Aversions During Three-Party Conversations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1972</first_page>
						<last_page>1976</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2134</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ishi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Feng</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1977</first_page>
						<last_page>1981</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1929</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoqiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanqing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Light-Weight Contextual Spelling Correction Model for Customizing Transducer-Based Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1982</first_page>
						<last_page>1986</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-379</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ning</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boxin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhouhan</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating External POS Tagger for Punctuation Restoration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1987</first_page>
						<last_page>1991</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1708</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vasileios</given_name>
<surname>Papadourakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurizio</given_name>
<surname>Omologo</surname>
</person_name>
					</contributors>
					<titles><title>Phonetically Induced Subwords for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1992</first_page>
						<last_page>1996</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1787</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/papadourakis21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Courtney</given_name>
<surname>Mansfield</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gina-Anne</given_name>
<surname>Levow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard A.</given_name>
<surname>Wright</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting Parity of Human vs. Machine Conversational Speech Transcription</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>1997</first_page>
						<last_page>2001</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1908</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mansfield21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name>
					</contributors>
					<titles><title>Lookup-Table Recurrent Language Models for Long Tail Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2002</first_page>
						<last_page>2006</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-340</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Andrés-Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dario</given_name>
<surname>Albesano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puming</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Vozila</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2007</first_page>
						<last_page>2011</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-443</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/andresferrer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiushi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>H. Lilian</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Token-Level Supervised Contrastive Learning for Punctuation Restoration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2012</first_page>
						<last_page>2016</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-661</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuerui</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongyu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanfu</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>BART Based Semantic Correction for Mandarin Automatic Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2017</first_page>
						<last_page>2021</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-739</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingfeng</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Class-Based Neural Network Language Model for Second-Pass Rescoring in ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2022</first_page>
						<last_page>2026</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1080</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dai21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Haws</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name>
					</contributors>
					<titles><title>Improving Customization of Neural Transducers by Mitigating Acoustic Mismatch of Synthesized Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2027</first_page>
						<last_page>2031</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1656</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kurata21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mandana</given_name>
<surname>Saebi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ernest</given_name>
<surname>Pusateri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaksha</given_name>
<surname>Meghawat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe Van</given_name>
<surname>Gysel</surname>
</person_name>
					</contributors>
					<titles><title>A Discriminative Entity-Aware Language Model for Virtual Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2032</first_page>
						<last_page>2036</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1767</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/saebi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahdi</given_name>
<surname>Namazifar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Malik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li Erran</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gokhan</given_name>
<surname>Tur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilek Hakkani</given_name>
<surname>Tür</surname>
</person_name>
					</contributors>
					<titles><title>Correcting Automated and Manual Speech Transcription Errors Using Warped Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2037</first_page>
						<last_page>2041</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-591</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/namazifar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Varun</given_name>
<surname>Nagaraja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Feng</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Encoder Transducer: A Flexible Solution for Trading Off Accuracy for Latency</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2042</first_page>
						<last_page>2046</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1272</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shi21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiqi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deyi</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boxing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Domain-Aware Self-Attention for Multi-Domain Neural Machine Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2047</first_page>
						<last_page>2051</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1477</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>André</given_name>
<surname>Merboldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wilfried</given_name>
<surname>Michel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Librispeech Transducer Model with Internal Language Model Prior Correction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2052</first_page>
						<last_page>2056</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1510</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zeyer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sepand</given_name>
<surname>Mavandadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>A Deliberation-Based Joint Acoustic and Text Decoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2057</first_page>
						<last_page>2061</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-165</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mavandadi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>On the Limit of English Conversational Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2062</first_page>
						<last_page>2066</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-211</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tuske21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keyu</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name>
					</contributors>
					<titles><title>Deformable TDNN with Adaptive Receptive Fields for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2067</first_page>
						<last_page>2071</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-387</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/an21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2077</first_page>
						<last_page>2081</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-478</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/you21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chi-Hang</given_name>
<surname>Leong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Han</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name>
					</contributors>
					<titles><title>Online Compressive Transformer for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2082</first_page>
						<last_page>2086</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-545</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/leong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>End to End Transformer-Based Contextual Speech Recognition Based on Pointer Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2087</first_page>
						<last_page>2091</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-774</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yotaro</given_name>
<surname>Kubo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel Adriaan Unico</given_name>
<surname>Bacchiani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Llion</given_name>
<surname>Jones</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2092</first_page>
						<last_page>2096</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-775</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/karita21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name>
					</contributors>
					<titles><title>Advanced Long-Context End-to-End Speech Recognition Using Context-Expanded Transformers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2097</first_page>
						<last_page>2101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1643</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hori21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md. Akmal</given_name>
<surname>Haidar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehdi</given_name>
<surname>Rezagholizadeh</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-Based ASR Incorporating Time-Reduction Layer and Fine-Tuning with Self-Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2102</first_page>
						<last_page>2106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1743</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/haidar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Flexi-Transducer: Optimizing Latency, Accuracy and Compute for Multi-Domain On-Device Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2107</first_page>
						<last_page>2111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1921</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mahadeokar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Przemyslaw</given_name>
<surname>Falkowski-Gilski</surname>
</person_name>
					</contributors>
					<titles><title>Difference in Perceived Speech Signal Quality Assessment Among Monolingual and Bilingual Teenage Students</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2112</first_page>
						<last_page>2116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-16</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/falkowskigilski21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Schymura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benedikt</given_name>
<surname>Bönninghoff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dorothea</given_name>
<surname>Kolossa</surname>
</person_name>
					</contributors>
					<titles><title>PILOT: Introducing Transformers for Probabilistic Sound Event Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2117</first_page>
						<last_page>2121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-124</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/schymura21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masahito</given_name>
<surname>Togami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name>
					</contributors>
					<titles><title>Sound Source Localization with Majorization Minimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2122</first_page>
						<last_page>2126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-126</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/togami21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Mittag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Babak</given_name>
<surname>Naderi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Assmaa</given_name>
<surname>Chehadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2127</first_page>
						<last_page>2131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-299</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mittag21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Babak</given_name>
<surname>Naderi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name>
					</contributors>
					<titles><title>Subjective Evaluation of Noise Suppression Algorithms in Crowdsourcing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2132</first_page>
						<last_page>2136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-343</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/naderi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sifan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>JingWei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Lou</surname>
</person_name>
					</contributors>
					<titles><title>Reliable Intensity Vector Selection for Multi-Source Direction-of-Arrival Estimation Using a Single Acoustic Vector Sensor</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2137</first_page>
						<last_page>2141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-375</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/geng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunlei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2142</first_page>
						<last_page>2146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-659</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Toma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Salvati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlo</given_name>
<surname>Drioli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gian Luca</given_name>
<surname>Foresti</surname>
</person_name>
					</contributors>
					<titles><title>CNN-Based Processing of Acoustic and Radio Frequency Signals for Speaker Localization from MAVs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2147</first_page>
						<last_page>2151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-886</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/toma21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katsutoshi</given_name>
<surname>Itoyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiya</given_name>
<surname>Morimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shungo</given_name>
<surname>Masaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryosuke</given_name>
<surname>Kojima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nishida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Nakadai</surname>
</person_name>
					</contributors>
					<titles><title>Assessment of von Mises-Bernoulli Deep Neural Network in Sound Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2152</first_page>
						<last_page>2156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1050</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/itoyama21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongliang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nengheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Feature Fusion by Attention Networks for Robust DOA Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2157</first_page>
						<last_page>2161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1051</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoufeng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaojie</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>Far-Field Speaker Localization and Adaptive GLMB Tracking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2162</first_page>
						<last_page>2166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1160</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vivek Sivaraman</given_name>
<surname>Narayanaswamy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jayaraman J.</given_name>
<surname>Thiagarajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Spanias</surname>
</person_name>
					</contributors>
					<titles><title>On the Design of Deep Priors for Unsupervised Audio Restoration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2167</first_page>
						<last_page>2171</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1890</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/narayanaswamy21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiguang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xionghu</given_name>
<surname>Zhong</surname>
</person_name>
					</contributors>
					<titles><title>Cram&#38;#233;r-Rao Lower Bound for DOA Estimation with an Array of Directional Microphones in Reverberant Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2172</first_page>
						<last_page>2176</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2267</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaeseong</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dalhyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gyuhyeon</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Geumbyeol</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gyeongsu</given_name>
<surname>Chae</surname>
</person_name>
					</contributors>
					<titles><title>GAN Vocoder: Multi-Resolution Discriminator Is All You Need</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2177</first_page>
						<last_page>2181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-41</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/you21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Cong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name>
					</contributors>
					<titles><title>Glow-WaveGAN: Learning Speech Representations from GAN-Based Variational Auto-Encoder for High Fidelity Flow-Based Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2182</first_page>
						<last_page>2186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-414</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Reo</given_name>
<surname>Yoneyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Unified Source-Filter GAN: Unified Source-Filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2187</first_page>
						<last_page>2191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-517</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yoneyama21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuki</given_name>
<surname>Mizuta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Harmonic WaveGAN: GAN-Based Speech Waveform Generation Model with Harmonic Structure Discriminator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2192</first_page>
						<last_page>2196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-583</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mizuta21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Fre-GAN: Adversarial Frequency-Consistent Audio Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2197</first_page>
						<last_page>2201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-845</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhyeok</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Sung</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taejun</given_name>
<surname>Bak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Ik</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2202</first_page>
						<last_page>2206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-971</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yang21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Won</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaesam</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bongwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juntae</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2207</first_page>
						<last_page>2211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1016</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammed Salah</given_name>
<surname>Al-Radhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Csaba</given_name>
<surname>Zainkó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Géza</given_name>
<surname>Németh</surname>
</person_name>
					</contributors>
					<titles><title>Continuous Wavelet Vocoder-Based Decomposition of Parametric Speech Waveform Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2212</first_page>
						<last_page>2216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1600</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/alradhi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrick Lumban</given_name>
<surname>Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>High-Fidelity and Low-Latency Universal Neural Vocoder Based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2217</first_page>
						<last_page>2221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1984</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tobing21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengxi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2222</first_page>
						<last_page>2226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2173</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min-Jae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>High-Fidelity Parallel WaveGAN with Multi-Band Harmonic-Plus-Noise Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2227</first_page>
						<last_page>2231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-976</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hwang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junkun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingbo</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renjie</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>SpecRec: An Alternative Solution for Improving End-to-End Speech-to-Text Translation via Spectrogram Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2232</first_page>
						<last_page>2236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-733</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Colin</given_name>
<surname>Cherry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naveen</given_name>
<surname>Arivazhagan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Padfield</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Krikun</surname>
</person_name>
					</contributors>
					<titles><title>Subtitle Translation as Markup Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2237</first_page>
						<last_page>2241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-744</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cherry21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Conneau</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Self- and Semi-Supervised Learning for Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2242</first_page>
						<last_page>2246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1912</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatao</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name>
					</contributors>
					<titles><title>CoVoST 2 and Massively Multilingual Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2247</first_page>
						<last_page>2251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2027</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yao-Fei</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>AlloST: Low-Resource Speech Translation Without Source Transcription</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2252</first_page>
						<last_page>2256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-526</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cheng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johanes</given_name>
<surname>Effendi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Weakly-Supervised Speech-to-Text Mapping with Visually Connected Non-Parallel Speech-Text Data Using Cyclic Partially-Aligned Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2257</first_page>
						<last_page>2261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-970</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/effendi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hirotaka</given_name>
<surname>Tokuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuhito</given_name>
<surname>Sudoh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Transcribing Paralinguistic Acoustic Cues to Target Language Text in Transformer-Based Speech-to-Text Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2262</first_page>
						<last_page>2266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1020</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tokuyama21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Translation via Cross-Modal Progressive Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2267</first_page>
						<last_page>2271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1065</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ye21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuka</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuhito</given_name>
<surname>Sudoh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>ASR Posterior-Based Loss for Multi-Task End-to-End Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2272</first_page>
						<last_page>2276</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1105</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ko21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Pérez-González-de-Martos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Iranzo-Sánchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrià Giménez</given_name>
<surname>Pastor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Jorge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan-Albert</given_name>
<surname>Silvestre-Cerdà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Civera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Sanchis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfons</given_name>
<surname>Juan</surname>
</person_name>
					</contributors>
					<titles><title>Towards Simultaneous Machine Interpretation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2277</first_page>
						<last_page>2281</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-201</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/perezgonzalezdemartos21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Martucci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mauro</given_name>
<surname>Cettolo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Turchi</surname>
</person_name>
					</contributors>
					<titles><title>Lexical Modeling of ASR Errors for Robust Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2282</first_page>
						<last_page>2286</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-265</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/martucci21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piyush</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Kuznetsova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donald S.</given_name>
<surname>Williamson</surname>
</person_name>
					</contributors>
					<titles><title>Optimally Encoding Inductive Biases into the Transformer Improves End-to-End Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2287</first_page>
						<last_page>2291</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2007</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vyas21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tejaswini</given_name>
<surname>Ananthanarayana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lipisha</given_name>
<surname>Chaudhary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ifeoma</given_name>
<surname>Nwogu</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Feature Scaling and Fusion on Sign Language Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2292</first_page>
						<last_page>2296</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1863</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ananthanarayana21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Alenin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Okhotnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rostislav</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Torgashov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Shigabeev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Simonchik</surname>
</person_name>
					</contributors>
					<titles><title>The ID R&#38;amp;D System Description for Short-Duration Speaker Verification Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2297</first_page>
						<last_page>2301</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1553</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/alenin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenthe</given_name>
<surname>Thienpondt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brecht</given_name>
<surname>Desplanques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2302</first_page>
						<last_page>2306</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1570</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/thienpondt21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Gusev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alisa</given_name>
<surname>Vinogradova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergei</given_name>
<surname>Astapov</surname>
</person_name>
					</contributors>
					<titles><title>SdSVC Challenge 2021: Tips and Tricks to Boost the Short-Duration Speaker Verification System Performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2307</first_page>
						<last_page>2311</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1737</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gusev21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woo Hyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Team02 Text-Independent Speaker Verification System for SdSV Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2312</first_page>
						<last_page>2316</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-249</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Our Learned Lessons from Cross-Lingual Speaker Verification: The CRMI-DKU System Description for the Short-Duration Speaker Verification Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2317</first_page>
						<last_page>2321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-398</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of IMU&#38;amp;Elevoc Submission for the Short-Duration Speaker Verification Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2322</first_page>
						<last_page>2326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-743</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiqian</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>The Sogou System for Short-Duration Speaker Verification Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2327</first_page>
						<last_page>2331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-965</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhikai</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>The SJTU System for Short-Duration Speaker Verification Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2332</first_page>
						<last_page>2336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2136</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/han21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Desh</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Reformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2351</first_page>
						<last_page>2355</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-323</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/raj21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Graph Attention Networks for Anti-Spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2356</first_page>
						<last_page>2360</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-993</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tak21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victoria</given_name>
<surname>Mingote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Log-Likelihood-Ratio Cost Function as Objective Loss for Speaker Verification Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2361</first_page>
						<last_page>2365</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1085</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mingote21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Effective Phase Encoding for End-To-End Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2366</first_page>
						<last_page>2370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2025</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peng21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ha</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2371</first_page>
						<last_page>2375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-608</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nguyen21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Macháček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matúš</given_name>
<surname>Žilinec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Bojar</surname>
</person_name>
					</contributors>
					<titles><title>Lost in Interpreting: Speech Translation from Source or Interpreter?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2376</first_page>
						<last_page>2380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2232</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/machacek21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Baptiste</given_name>
<surname>Pouthier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Pilati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leela K.</given_name>
<surname>Gudupudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Bouveyron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederic</given_name>
<surname>Precioso</surname>
</person_name>
					</contributors>
					<titles><title>Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-Based Multimodal Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2381</first_page>
						<last_page>2385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-80</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pouthier21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarenne</given_name>
<surname>Wallbridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>It&#38;#8217;s Not What You Said, it&#38;#8217;s How You Said it: Discriminative Perception of Speech as a Multichannel Communication System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2386</first_page>
						<last_page>2390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1658</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wallbridge21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thilo</given_name>
<surname>Michael</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Mittag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Bütow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Extending the Fullband E-Model Towards Background Noise, Bursty Packet Loss, and Conversational Degradations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2391</first_page>
						<last_page>2395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-314</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/michael21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Symonds</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Spong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steven R.</given_name>
<surname>Ness</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Tzanetakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2396</first_page>
						<last_page>2400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-616</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bergler21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wim</given_name>
<surname>Boes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Audiovisual Transfer Learning for Audio Tagging and Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2401</first_page>
						<last_page>2405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-695</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/boes21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Nessler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paolo</given_name>
<surname>Prandoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Mainar</surname>
</person_name>
					</contributors>
					<titles><title>Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2406</first_page>
						<last_page>2410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1685</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nessler21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreea-Maria</given_name>
<surname>Oncescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A. Sophia</given_name>
<surname>Koepke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João F.</given_name>
<surname>Henriques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeynep</given_name>
<surname>Akata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Albanie</surname>
</person_name>
					</contributors>
					<titles><title>Audio Retrieval with Natural Language Queries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2411</first_page>
						<last_page>2415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2227</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/oncescu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Giollo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deniz</given_name>
<surname>Gunceler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Willett</surname>
</person_name>
					</contributors>
					<titles><title>Bootstrap an End-to-End ASR System by Multilingual Training, Transfer Learning, Text-to-Text Mapping and Synthetic Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2416</first_page>
						<last_page>2420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-198</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/giollo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuan-Nam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stüker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Weight Factorization for Multilingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2421</first_page>
						<last_page>2425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-216</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pham21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Conneau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Cross-Lingual Representation Learning for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2426</first_page>
						<last_page>2430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-329</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/conneau21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoaki</given_name>
<surname>Hayakawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chee Siang</given_name>
<surname>Leow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akio</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takehito</given_name>
<surname>Utsuro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiromitsu</given_name>
<surname>Nishizaki</surname>
</person_name>
					</contributors>
					<titles><title>Language and Speaker-Independent Feature Transformation for End-to-End Multilingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2431</first_page>
						<last_page>2435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-390</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hayakawa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Krishna D.</given_name>
<surname>N</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pinyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bruno</given_name>
<surname>Bozza</surname>
</person_name>
					</contributors>
					<titles><title>Using Large Self-Supervised Models for Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2436</first_page>
						<last_page>2440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-631</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/n21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mari Ganesh</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jom</given_name>
<surname>Kuriakose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anand</given_name>
<surname>Thyagachandran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Kumar</given_name>
<surname>A</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Seth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lodagala V.S.V. Durga</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saish</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anusha</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema A.</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Dual Script E2E Framework for Multilingual and Code-Switching ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2441</first_page>
						<last_page>2445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-978</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kumar21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anuj</given_name>
<surname>Diwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rakesh</given_name>
<surname>Vaideeswaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanket</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankita</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasa</given_name>
<surname>Raghavan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreya</given_name>
<surname>Khare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinit</given_name>
<surname>Unni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akash</given_name>
<surname>Rajpuria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kalika</given_name>
<surname>Bali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivek</given_name>
<surname>Seshadri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunayana</given_name>
<surname>Sitaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samarth</given_name>
<surname>Bharadwaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jai</given_name>
<surname>Nanavati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raoul</given_name>
<surname>Nanavati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Sankaranarayanan</surname>
</person_name>
					</contributors>
					<titles><title>MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2446</first_page>
						<last_page>2450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1339</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/diwan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Genta Indra</given_name>
<surname>Winata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangsen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caiming</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steven</given_name>
<surname>Hoi</surname>
</person_name>
					</contributors>
					<titles><title>Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2451</first_page>
						<last_page>2455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1390</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/winata21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiran Praveen</given_name>
<surname>T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikas</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Pandey</surname>
</person_name>
					</contributors>
					<titles><title>SRI-B End-to-End System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2456</first_page>
						<last_page>2460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1578</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sailor21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinjian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juncheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Phone Recognition with Compositional Phonetics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2461</first_page>
						<last_page>2465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1803</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Abdelali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name>
					</contributors>
					<titles><title>Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2466</first_page>
						<last_page>2470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1809</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chowdhury21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Dalmia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Differentiable Allophone Graphs for Language-Universal Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2471</first_page>
						<last_page>2475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1944</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yan21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent P.</given_name>
<surname>Martin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Rouas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Boyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Philip</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition Systems Errors for Objective Sleepiness Detection Through Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2476</first_page>
						<last_page>2480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-291</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/martin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jon</given_name>
<surname>Gillick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wesley</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kimiko</given_name>
<surname>Ryokai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Bamman</surname>
</person_name>
					</contributors>
					<titles><title>Robust Laughter Detection in Noisy Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2481</first_page>
						<last_page>2485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-353</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gillick21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mizuki</given_name>
<surname>Nagano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sadao</given_name>
<surname>Hiroya</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Emotional State on Estimation of Willingness to Buy from Advertising Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2486</first_page>
						<last_page>2490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-827</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nagano21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huda</given_name>
<surname>Alsofyani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>Stacked Recurrent Neural Networks for Speech-Based Inference of Attachment Condition in School Age Children</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2491</first_page>
						<last_page>2495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-904</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/alsofyani21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nujud</given_name>
<surname>Aloshban</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Esposito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>Language or Paralanguage, This is the Problem: Comparing Depressed and Non-Depressed Speakers Through the Analysis of Gated Multimodal Units</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2496</first_page>
						<last_page>2500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-928</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/aloshban21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aniruddha</given_name>
<surname>Tammewar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandra</given_name>
<surname>Cervone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Riccardi</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Carrier Recognition from Personal Narratives</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2501</first_page>
						<last_page>2505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1100</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tammewar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Scott</given_name>
<surname>Condron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Clarke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Klementiev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniela</given_name>
<surname>Morse-Kopp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jack</given_name>
<surname>Parry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitri</given_name>
<surname>Palaz</surname>
</person_name>
					</contributors>
					<titles><title>Non-Verbal Vocalisation and Laughter Detection Using Sequence-to-Sequence Models and Multi-Label Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2506</first_page>
						<last_page>2510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1159</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/condron21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>TDCA-Net: Time-Domain Channel Attention Network for Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2511</first_page>
						<last_page>2515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1176</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cai21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Visual Speech for Obstructive Sleep Apnea Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2516</first_page>
						<last_page>2520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1717</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/botelho21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hector A. Cordourier</given_name>
<surname>Maruri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sinem</given_name>
<surname>Aslan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georg</given_name>
<surname>Stemmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nese</given_name>
<surname>Alyuz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lama</given_name>
<surname>Nachman</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Contextual Voice Changes in Remote Meetings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2521</first_page>
						<last_page>2525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1932</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/maruri21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Speech Based Depression Severity Level Classification Using a Multi-Stage Dilated CNN-LSTM Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2526</first_page>
						<last_page>2530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1967</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/seneviratne21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ho-Gyeong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Joong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoshik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae Gyoon</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunho</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Ju</given_name>
<surname>Hwang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2531</first_page>
						<last_page>2535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1169</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Macoskey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Learning a Neural Diff for Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2536</first_page>
						<last_page>2540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1575</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/macoskey21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Stochastic Attention Head Removal: A Simple and Effective Method for Improving Transformer Based ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2541</first_page>
						<last_page>2545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-280</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiabin</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tieran</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Model-Agnostic Fast Adaptive Multi-Objective Balancing Algorithm for Multilingual Automatic Speech Recognition Model Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2546</first_page>
						<last_page>2550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-355</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xue21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heng-Jui</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin-shan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Towards Lifelong Learning of End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2551</first_page>
						<last_page>2555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-563</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Leal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Farris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manasa</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Self-Adaptive Distillation for Multilingual Speech Recognition: Leveraging Student Independence</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2556</first_page>
						<last_page>2560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-614</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/leal21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinghui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesse</given_name>
<surname>Emond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>Regularizing Word Segmentation by Creating Misspellings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2561</first_page>
						<last_page>2565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-648</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name>
					</contributors>
					<titles><title>Multitask Training with Text Data for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2566</first_page>
						<last_page>2570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-683</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianzhao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongxia</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Emitting Word Timings with HMM-Free End-to-End System in Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2571</first_page>
						<last_page>2575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-894</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oguz</given_name>
<surname>Elibol</surname>
</person_name>
					</contributors>
					<titles><title>Scaling Laws for Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2576</first_page>
						<last_page>2580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1644</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/droppo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jayadev</given_name>
<surname>Billa</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Non-Target Language Resources to Improve ASR Performance in a Target Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2581</first_page>
						<last_page>2585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1657</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/billa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Fasoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mauricio</given_name>
<surname>Serrano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naigang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swagath</given_name>
<surname>Venkataramani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kailash</given_name>
<surname>Gopalakrishnan</surname>
</person_name>
					</contributors>
					<titles><title>4-Bit Quantization of LSTM-Based Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2586</first_page>
						<last_page>2590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1962</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fasoli21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Okamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name>
					</contributors>
					<titles><title>Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2591</first_page>
						<last_page>2595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2043</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/masumura21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoli</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2596</first_page>
						<last_page>2600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2075</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/meng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongcheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Variable Frame Rate Acoustic Models Using Minimum Error Reinforcement Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2601</first_page>
						<last_page>2605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2198</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jiang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Constantijn</given_name>
<surname>Kaland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Gordon</surname>
</person_name>
					</contributors>
					<titles><title>How f0 and Phrase Position Affect Papuan Malay Word Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2606</first_page>
						<last_page>2610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-6</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kaland21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna Bothe</given_name>
<surname>Jespersen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Šturm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Míša</given_name>
<surname>Hejná</surname>
</person_name>
					</contributors>
					<titles><title>On the Feasibility of the Danish Model of Intonational Transcription: Phonetic Evidence from Jutlandic Danish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2611</first_page>
						<last_page>2615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-190</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jespersen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adrien</given_name>
<surname>Méli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Ballier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Achille</given_name>
<surname>Falaise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Henderson</surname>
</person_name>
					</contributors>
					<titles><title>An Experiment in Paratone Detection in a Prosodically Annotated EAP Spoken Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2616</first_page>
						<last_page>2620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-294</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/meli21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Wagner</surname>
</person_name>
					</contributors>
					<titles><title>ProsoBeast Prosody Annotation Tool</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2621</first_page>
						<last_page>2625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-304</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gerazov21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Trang</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name>
					</contributors>
					<titles><title>Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2626</first_page>
						<last_page>2630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-373</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tran21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roger Cheng-yen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-fan</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueh-chin</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Targeted and Targetless Neutral Tones in Taiwanese Southern Min</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2631</first_page>
						<last_page>2635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-434</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mária</given_name>
<surname>Gósy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kálmán</given_name>
<surname>Abari</surname>
</person_name>
					</contributors>
					<titles><title>The Interaction of Word Complexity and Word Duration in an Agglutinative Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2636</first_page>
						<last_page>2640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-594</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gosy21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ho-hsien</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shao-ren</given_name>
<surname>Lyu</surname>
</person_name>
					</contributors>
					<titles><title>Taiwan Min Nan (Taiwanese) Checked Tones Sound Change</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2641</first_page>
						<last_page>2645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-672</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pan21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moritz</given_name>
<surname>Jakob</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bettina</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Zahner-Ritter</surname>
</person_name>
					</contributors>
					<titles><title>In-Group Advantage in the Perception of Emotions: Evidence from Three Varieties of German</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2646</first_page>
						<last_page>2650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1172</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jakob21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name>
					</contributors>
					<titles><title>The LF Model in the Frequency Domain for Glottal Airflow Modelling Without Aliasing Distortion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2651</first_page>
						<last_page>2655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1625</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gobl21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alvaro Iturralde</given_name>
<surname>Zurita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sijia</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Parsing Speech for Grouping and Prominence, and the Typology of Rhythm</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2656</first_page>
						<last_page>2660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1684</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wagner21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benazir</given_name>
<surname>Mumtaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Canzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miriam</given_name>
<surname>Butt</surname>
</person_name>
					</contributors>
					<titles><title>Prosody of Case Markers in Urdu</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2661</first_page>
						<last_page>2665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1776</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mumtaz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brynhildur</given_name>
<surname>Stefansdottir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Burroni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Tilsen</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Characteristics of Icelandic Voiced Fricative Lenition: Gradience, Categoricity, and Speaker/Gesture-Specific Effects</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2666</first_page>
						<last_page>2670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1903</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/stefansdottir21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khia A.</given_name>
<surname>Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging the Uniformity Framework to Examine Crosslinguistic Similarity for Long-Lag Stops in Spontaneous Cantonese-English Bilingual Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2671</first_page>
						<last_page>2675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1780</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/johnson21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aswin</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunwoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minje</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Speech Enhancement Through Self-Supervised Data Augmentation and Purification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2676</first_page>
						<last_page>2680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1868</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sivaraman21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark R.</given_name>
<surname>Saddler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Francl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jenelle</given_name>
<surname>Feather</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaizhi</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josh H.</given_name>
<surname>McDermott</surname>
</person_name>
					</contributors>
					<titles><title>Speech Denoising with Auditory Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2681</first_page>
						<last_page>2685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1973</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/saddler21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sefik Emre</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zirun</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2686</first_page>
						<last_page>2690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-220</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/eskimez21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongxiang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Stage Progressive Speech Enhancement Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2691</first_page>
						<last_page>2695</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-520</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oscar</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dung N.</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhito</given_name>
<surname>Koishida</surname>
</person_name>
					</contributors>
					<titles><title>Single-Channel Speech Enhancement Using Learnable Loss Mixup</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2696</first_page>
						<last_page>2700</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-859</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chang21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao-Qi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Maximum Likelihood Approach to SNR-Progressive Learning Using Generalized Gaussian Distribution for LSTM-Based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2701</first_page>
						<last_page>2705</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-922</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikas</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashi</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakti P.</given_name>
<surname>Rath</surname>
</person_name>
					</contributors>
					<titles><title>Whisper Speech Enhancement Using Joint Variational Autoencoder for Improved Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2706</first_page>
						<last_page>2710</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-953</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/agrawal21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youna</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minjae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Seok</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>DEMUCS-Mobile : On-Device Lightweight Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2711</first_page>
						<last_page>2715</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1025</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Madhav Mahesh</given_name>
<surname>Kashyap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anuj</given_name>
<surname>Tambwekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishnamoorthy</given_name>
<surname>Manohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Natarajan</surname>
</person_name>
					</contributors>
					<titles><title>Speech Denoising Without Clean Training Data: A Noise2Noise Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2716</first_page>
						<last_page>2720</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1130</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kashyap21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Improved Speech Enhancement Using a Complex-Domain GAN with Fused Time-Domain and Time-Frequency Domain Constraints</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2721</first_page>
						<last_page>2725</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1134</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xudong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Topology-Enhanced Generative Adversarial Networks (GANs)</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2726</first_page>
						<last_page>2730</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1411</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suliang</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunxin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mei</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Learning Speech Structure to Improve Time-Frequency Masks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2731</first_page>
						<last_page>2735</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1859</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eesung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeji</given_name>
<surname>Seo</surname>
</person_name>
					</contributors>
					<titles><title>SE-Conformer: Time-Domain Speech Enhancement Using Conformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2736</first_page>
						<last_page>2740</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2207</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thananchai</given_name>
<surname>Kongthaworn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Burin</given_name>
<surname>Naowarat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Spectral and Latent Speech Representation Distortion for TTS Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2741</first_page>
						<last_page>2745</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2258</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kongthaworn21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cassia</given_name>
<surname>Valentini-Botinhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Detection and Analysis of Attention Errors in Sequence-to-Sequence Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2746</first_page>
						<last_page>2750</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-286</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/valentinibotinhao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohola</given_name>
<surname>Zandie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad H.</given_name>
<surname>Mahoor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Madsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eshrat S.</given_name>
<surname>Emamian</surname>
</person_name>
					</contributors>
					<titles><title>RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2751</first_page>
						<last_page>2755</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-341</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zandie21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoji</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>AISHELL-3: A Multi-Speaker Mandarin TTS Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2756</first_page>
						<last_page>2760</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-755</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shi21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Eng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C.T. Justine</given_name>
<surname>Hui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Hioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine I.</given_name>
<surname>Watson</surname>
</person_name>
					</contributors>
					<titles><title>Comparing Speech Enhancement Techniques for Voice Adaptation-Based Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2761</first_page>
						<last_page>2765</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-800</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/eng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenye</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinglin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongjie</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2766</first_page>
						<last_page>2770</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1148</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cui21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sai Sirisha</given_name>
<surname>Rallabandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Bharadwaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Babak</given_name>
<surname>Naderi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Perception of Social Speaker Characteristics in Synthetic Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2771</first_page>
						<last_page>2775</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1229</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rallabandi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Bakhturina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Hi-Fi Multi-Speaker English TTS Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2776</first_page>
						<last_page>2780</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1599</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bakhturina21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chien-yu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Tsung</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yist Y.</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Utilizing Self-Supervised Representations for MOS Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2781</first_page>
						<last_page>2785</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2013</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tseng21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saida</given_name>
<surname>Mussakhojayeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aigerim</given_name>
<surname>Janaliyeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Almas</given_name>
<surname>Mirzakhmetov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huseyin Atakan</given_name>
<surname>Varol</surname>
</person_name>
					</contributors>
					<titles><title>KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2786</first_page>
						<last_page>2790</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2124</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mussakhojayeva21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Confidence Intervals for ASR-Based TTS Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2791</first_page>
						<last_page>2795</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2203</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/taylor21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandan K.A.</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harishchandra</given_name>
<surname>Dubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhito</given_name>
<surname>Koishida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Nair</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishak</given_name>
<surname>Gopal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannes</given_name>
<surname>Gamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Aichner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>INTERSPEECH 2021 Deep Noise Suppression Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2796</first_page>
						<last_page>2800</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1609</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/reddy21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenzhe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxue</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guochen</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Simultaneous Denoising and Dereverberation Framework with Target Decoupling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2801</first_page>
						<last_page>2805</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1137</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Strake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>Deep Noise Suppression with Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2806</first_page>
						<last_page>2810</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-936</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohuai</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongsheng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2811</first_page>
						<last_page>2815</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-296</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/le21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shubo</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shimin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2816</first_page>
						<last_page>2820</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1482</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lv21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kanghao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>DBNet: A Dual-Branch Network Architecture Processing on Spectrum and Waveform for Single-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2821</first_page>
						<last_page>2825</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1042</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinlei</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiguang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Low-Delay Speech Enhancement Using Perceptually Motivated Target and Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2826</first_page>
						<last_page>2830</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1410</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koen</given_name>
<surname>Oostermeijer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2831</first_page>
						<last_page>2835</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-668</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/oostermeijer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolae-Cătălin</given_name>
<surname>Ristea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radu Tudor</given_name>
<surname>Ionescu</surname>
</person_name>
					</contributors>
					<titles><title>Self-Paced Ensemble Learning for Speech and Audio Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2836</first_page>
						<last_page>2840</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-155</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ristea21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Kojima</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for Streaming Transformer&#38;#8211;Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2841</first_page>
						<last_page>2845</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-175</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kojima21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timo</given_name>
<surname>Lohrenz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2846</first_page>
						<last_page>2850</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-555</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lohrenz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salah</given_name>
<surname>Zaiem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Essid</surname>
</person_name>
					</contributors>
					<titles><title>Conditional Independence for Pretext Task Selection in Self-Supervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2851</first_page>
						<last_page>2855</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1027</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zaiem21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Zeineldeen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Glushko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wilfried</given_name>
<surname>Michel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Methods to Improve Language Model Integration for Attention-Based Encoder-Decoder ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2856</first_page>
						<last_page>2860</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1255</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zeineldeen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Apoorv</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2861</first_page>
						<last_page>2865</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1683</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vyas21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Clément Le</given_name>
<surname>Moine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Obin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Roebel</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Attentive Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2866</first_page>
						<last_page>2870</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-573</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/moine21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seong-Gyun</given_name>
<surname>Leem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fulford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jukka-Pekka</given_name>
<surname>Onnela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Gard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2871</first_page>
						<last_page>2875</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1438</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/leem21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>M3: MultiModal Masking Applied to Sentiment Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2876</first_page>
						<last_page>2880</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1739</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/georgiou21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Electra</given_name>
<surname>Wallington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>The CSTR System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2881</first_page>
						<last_page>2885</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1035</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/klejch21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Zeineldeen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuoyun</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2886</first_page>
						<last_page>2890</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1623</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>André</given_name>
<surname>Merboldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2891</first_page>
						<last_page>2895</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1671</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abbas</given_name>
<surname>Khosravani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Lazaridis</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Dialectal Variation for Swiss German Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2896</first_page>
						<last_page>2900</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1735</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/khosravani21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Egorova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hari Krishna</given_name>
<surname>Vydana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Out-of-Vocabulary Words Detection with Attention and CTC Alignments in an End-to-End ASR System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2901</first_page>
						<last_page>2905</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1756</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/egorova21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mousmita</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Desh</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongji</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruizhe</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Supreet</given_name>
<surname>Preet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moris</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zikra</given_name>
<surname>Iqbal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nagendra</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Trmal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola García</given_name>
<surname>Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Training Hybrid Models on Noisy Transliterated Transcripts for Code-Switched Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2906</first_page>
						<last_page>2910</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2127</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wiesner21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland van</given_name>
<surname>Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fleur</given_name>
<surname>Boogmans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mario</given_name>
<surname>Ganzeboom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Speech Intelligibility of Dysarthric Speech: Human Scores and Acoustic-Phonetic Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2911</first_page>
						<last_page>2915</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1189</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xue21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Young-Kyung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rimita</given_name>
<surname>Lahiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Nasir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>So Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somer</given_name>
<surname>Bishop</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lord</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth S.</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Short Term Dynamic Speech Features for Understanding Behavioral Traits of Children with Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2916</first_page>
						<last_page>2920</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2111</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Waldemar</given_name>
<surname>Jęśko</surname>
</person_name>
					</contributors>
					<titles><title>Vocalization Recognition of People with Profound Intellectual and Multiple Disabilities (PIMD) Using Machine Learning Algorithms</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2921</first_page>
						<last_page>2925</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1239</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jesko21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Barbara Gili</given_name>
<surname>Fivela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincenzo</given_name>
<surname>Sallustio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvia</given_name>
<surname>Pede</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danilo</given_name>
<surname>Patrocinio</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Complexity, Speech Accuracy and Intelligibility Assessment of Italian Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2926</first_page>
						<last_page>2930</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1862</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fivela21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Si-Ioi</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cymie Wing-Yee</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Consonant Errors in Disordered Speech Based on Consonant-Vowel Segment Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2931</first_page>
						<last_page>2935</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1305</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ng21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adam</given_name>
<surname>Hair</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie J.</given_name>
<surname>Ballard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Posterior-Based Mispronunciation Detection on Field-Collected Recordings from Child Speech Therapy Sessions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2936</first_page>
						<last_page>2940</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-69</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hair21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>O’Malley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Cognitive Impairment Using Sentence Representation Vectors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2941</first_page>
						<last_page>2945</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-915</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mirheidari21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristina</given_name>
<surname>McKean</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elaine</given_name>
<surname>Ashton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yvonne</given_name>
<surname>Wren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swapnil</given_name>
<surname>Gadgil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rebecca</given_name>
<surname>Bright</surname>
</person_name>
					</contributors>
					<titles><title>Parental Spoken Scaffolding and Narrative Skills in Crowd-Sourced Storytelling Samples of Young Children</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2946</first_page>
						<last_page>2950</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1297</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yue21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tong</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorena</given_name>
<surname>Qendro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cecilia</given_name>
<surname>Mascolo</surname>
</person_name>
					</contributors>
					<titles><title>Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2951</first_page>
						<last_page>2955</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1320</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xia21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Disong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liqun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu Ting</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2956</first_page>
						<last_page>2960</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2139</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanuka</given_name>
<surname>Bhattacharjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jhansi</given_name>
<surname>Mallela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Belur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nalini</given_name>
<surname>Atchayaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradeep</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dipanjan</given_name>
<surname>Gope</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Source and Vocal Tract Cues for Speech-Based Classification of Patients with Parkinson&#38;#8217;s Disease and Healthy Subjects</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2961</first_page>
						<last_page>2965</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2008</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bhattacharjee21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>R’mani</given_name>
<surname>Haulcy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>CLAC: A Speech Corpus of Healthy English Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2966</first_page>
						<last_page>2970</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1810</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/haulcy21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leanne</given_name>
<surname>Nortje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Direct Multimodal Few-Shot Learning of Speech and Images</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2971</first_page>
						<last_page>2975</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-49</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nortje21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ramon</given_name>
<surname>Sanabria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Austin</given_name>
<surname>Waters</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Baldridge</surname>
</person_name>
					</contributors>
					<titles><title>Talk, Don&#38;#8217;t Write: A Study of Direct Speech-Based Image Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2976</first_page>
						<last_page>2980</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-96</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sanabria21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaili</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>A Fast Discrete Two-Step Learning Hashing for Scalable Cross-Modal Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2981</first_page>
						<last_page>2985</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-287</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhao21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianrong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyue</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuewei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2986</first_page>
						<last_page>2990</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-432</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kayode</given_name>
<surname>Olaleye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Attention-Based Keyword Localisation in Speech Using Visual Grounding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2991</first_page>
						<last_page>2995</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-435</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/olaleye21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khazar</given_name>
<surname>Khorrami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name>
					</contributors>
					<titles><title>Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>2996</first_page>
						<last_page>3000</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-496</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/khorrami21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bao-Cai</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Lip-Reading with Hierarchical Pyramidal Convolution and Self-Attention for Image Sequences with No Word Boundaries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3001</first_page>
						<last_page>3005</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-723</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rouditchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angie</given_name>
<surname>Boggust</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hilde</given_name>
<surname>Kuehne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rameswar</given_name>
<surname>Panda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogerio</given_name>
<surname>Feris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Cascaded Multilingual Audio-Visual Learning from Videos</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3006</first_page>
						<last_page>3010</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1352</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rouditchenko21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Mira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3011</first_page>
						<last_page>3015</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1360</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ma21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Richard</given_name>
<surname>Rose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Siohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshuman</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Otavio</given_name>
<surname>Braga</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Audio-Visual Speech Recognition for Overlapping Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3016</first_page>
						<last_page>3020</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1621</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rose21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongqin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Multi-Talker Speech Recognition in a Cocktail Party</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3021</first_page>
						<last_page>3025</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2128</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sanyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangzhan</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Ultra Fast Speech Separation Model with Teacher Student Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3026</first_page>
						<last_page>3030</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-142</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murtiza</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashwani</given_name>
<surname>Koul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Nathwani</surname>
</person_name>
					</contributors>
					<titles><title>Group Delay Based Re-Weighted Sparse Recovery Algorithms for Robust and High-Resolution Source Separation in DOA Framework</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3031</first_page>
						<last_page>3035</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-164</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ali21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hakan</given_name>
<surname>Erdogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Continuous Speech Separation Using Speaker Inventory for Long Recording</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3036</first_page>
						<last_page>3040</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-338</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/han21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weitao</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengbei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangrui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Crossfire Conditional Generative Adversarial Networks for Singing Voice Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3041</first_page>
						<last_page>3045</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-433</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yuan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Separation Using Orthogonal Representation in Complex and Real Time-Frequency Domain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3046</first_page>
						<last_page>3050</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-504</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Nakagome</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masahito</given_name>
<surname>Togami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Efficient and Stable Adversarial Learning Using Unpaired Data for Unsupervised Multichannel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3051</first_page>
						<last_page>3055</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-523</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nakagome21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sung-Feng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun-Po</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da-Rong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gene-Ping</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3056</first_page>
						<last_page>3060</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-763</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan-Lin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Huai</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Dual-Path Filter Network: Speaker-Aware Modeling for Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3061</first_page>
						<last_page>3065</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-858</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Practical Aspects of Single Channel Speech Separation for ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3066</first_page>
						<last_page>3070</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-921</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>Implicit Filter-and-Sum Network for End-to-End Multi-Channel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3071</first_page>
						<last_page>3075</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1158</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luo21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuohuang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Generalized Spatio-Temporal RNN Beamformer for Target Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3076</first_page>
						<last_page>3080</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-430</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi Chieh</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunjung</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chul</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Neural Diarization: From Transformer to Conformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3081</first_page>
						<last_page>3085</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1909</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Three-Class Overlapped Speech Detection Using a Convolutional Recurrent Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3086</first_page>
						<last_page>3090</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-149</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jung21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xucheng</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Online Speaker Diarization Equipped with Discriminative Modeling and Guided Inference</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3091</first_page>
						<last_page>3095</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-261</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola García</given_name>
<surname>Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Training with Pseudo-Labeling for End-To-End Neural Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3096</first_page>
						<last_page>3100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-384</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/takashima21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You Jin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Speaker Embeddings for Speaker Diarisation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3101</first_page>
						<last_page>3105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-448</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kwon21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Xuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maokui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shu-Tong</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Scenario-Dependent Speaker Diarization for DIHARD-III Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3106</first_page>
						<last_page>3110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-516</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name>
					</contributors>
					<titles><title>End-To-End Speaker Segmentation for Overlap-Aware Resegmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3111</first_page>
						<last_page>3115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-560</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bredin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yawen</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola García</given_name>
<surname>Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name>
					</contributors>
					<titles><title>Online Streaming End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3116</first_page>
						<last_page>3120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-708</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xue21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Or Haim</given_name>
<surname>Anidjar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itshak</given_name>
<surname>Lapidot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Hajaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amit</given_name>
<surname>Dvir</surname>
</person_name>
					</contributors>
					<titles><title>A Thousand Words are Worth More Than One Recording: Word-Embedding Based Speaker Change Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3121</first_page>
						<last_page>3125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-87</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/anidjar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kosuke</given_name>
<surname>Futamata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeongseon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name>
					</contributors>
					<titles><title>Phrase Break Prediction with Bidirectional Encoder Representations in Japanese Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3126</first_page>
						<last_page>3130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-252</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/futamata21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iván</given_name>
<surname>Vallés-Pérez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Roth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Beringer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name>
					</contributors>
					<titles><title>Improving Multi-Speaker TTS Prosody Variance with a Residual Encoder and Normalizing Flows</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3131</first_page>
						<last_page>3135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-562</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vallesperez21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenpeng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Rich Prosody Diversity Modelling with Phone-Level Mixture Density Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3136</first_page>
						<last_page>3140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-802</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/du21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme Duration Modeling Using Speech Rhythm-Based Speaker Embeddings for Multi-Speaker Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3141</first_page>
						<last_page>3145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-826</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fujita21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shichao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haopeng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunfeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Grained Prosody Modeling in Neural Speech Synthesis Using ToBI Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3146</first_page>
						<last_page>3150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-883</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zou21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mayank</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yogesh</given_name>
<surname>Virkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcello</given_name>
<surname>Federico</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Enyedi</surname>
</person_name>
					</contributors>
					<titles><title>Intra-Sentential Speaking Rate Control in Neural Text-To-Speech for Automatic Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3151</first_page>
						<last_page>3155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1012</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sharma21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Applying the Information Bottleneck Principle to Prosodic Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3156</first_page>
						<last_page>3160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1049</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvan</given_name>
<surname>Mertes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Milling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Stappen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Wiest</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeth</given_name>
<surname>André</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>A Prototypical Network Approach for Evaluating Generated Emotional Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3161</first_page>
						<last_page>3165</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1123</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/baird21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsukasa</given_name>
<surname>Yoshinaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Tada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Nozaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akiyoshi</given_name>
<surname>Iida</surname>
</person_name>
					</contributors>
					<titles><title>A Simplified Model for the Vocal Tract of [s] with Inclined Incisors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3166</first_page>
						<last_page>3170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-231</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yoshinaga21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Arai</surname>
</person_name>
					</contributors>
					<titles><title>Vocal-Tract Models to Visualize the Airstream of Human Breath and Droplets While Producing Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3171</first_page>
						<last_page>3175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-449</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/arai21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Tanji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hidefumi</given_name>
<surname>Ohmura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouichi</given_name>
<surname>Katsurada</surname>
</person_name>
					</contributors>
					<titles><title>Using Transposed Convolution for Articulatory-to-Acoustic Conversion from Real-Time MRI Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3176</first_page>
						<last_page>3180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-906</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tanji21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rafia</given_name>
<surname>Inaam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsukasa</given_name>
<surname>Yoshinaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Arai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Yokoyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akiyoshi</given_name>
<surname>Iida</surname>
</person_name>
					</contributors>
					<titles><title>Comparison Between Lumped-Mass Modeling and Flow Simulation of the Reed-Type Artificial Vocal Fold</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3181</first_page>
						<last_page>3185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-929</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/inaam21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Werner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susanne</given_name>
<surname>Fuchs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Trouvain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>Inhalations in Speech: Acoustic and Physiological Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3186</first_page>
						<last_page>3190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1262</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/werner21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel van</given_name>
<surname>Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul Konstantin</given_name>
<surname>Krug</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santitham</given_name>
<surname>Prom-on</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Model-Based Exploration of Linking Between Vowel Articulatory Space and Acoustic Space</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3191</first_page>
						<last_page>3195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1422</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mikey</given_name>
<surname>Elmers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Werner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beeke</given_name>
<surname>Muhlack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Trouvain</surname>
</person_name>
					</contributors>
					<titles><title>Take a Breath: Respiratory Sounds Improve Recollection in Synthetic Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3196</first_page>
						<last_page>3200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1496</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/elmers21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taijing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Lammert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Parrell</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3201</first_page>
						<last_page>3205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1746</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshie</given_name>
<surname>Matsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Yatabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ken-Ichi</given_name>
<surname>Sakakibara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minoru</given_name>
<surname>Tsuzaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanori</given_name>
<surname>Morise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name>
					</contributors>
					<titles><title>Mixture of Orthogonal Sequences Made from Extended Time-Stretched Pulses Enables Measurement of Involuntary Voice Fundamental Frequency Response to Pitch Perturbation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3206</first_page>
						<last_page>3210</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2073</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kawahara21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Contextualized Attention-Based Knowledge Transfer for Spoken Conversational Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3211</first_page>
						<last_page>3215</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-110</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/you21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenying</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zimu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lothar</given_name>
<surname>Thiele</surname>
</person_name>
					</contributors>
					<titles><title>Injecting Descriptive Meta-Information into Pre-Trained Language Models with Hypernetworks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3216</first_page>
						<last_page>3220</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-229</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/duan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahdin</given_name>
<surname>Rohmatillah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name>
					</contributors>
					<titles><title>Causal Confusion Reduction for Robust Multi-Domain Dialogue Policy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3221</first_page>
						<last_page>3225</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-534</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rohmatillah21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Fujie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Katayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin</given_name>
<surname>Sakuma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Timing Generating Networks: Neural Network Based Precise Turn-Taking Timing Prediction in Multiparty Conversation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3226</first_page>
						<last_page>3230</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-874</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fujie21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kehan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zezhong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyang</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haiqing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Human-to-Human Conversation Dataset for Learning Fine-Grained Turn-Taking Action</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3231</first_page>
						<last_page>3235</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-994</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mukuntha Narayanan</given_name>
<surname>Sundararaman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayush</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendra</given_name>
<surname>Vepa</surname>
</person_name>
					</contributors>
					<titles><title>PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3236</first_page>
						<last_page>3240</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1582</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sundararaman21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongyin</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Garima</given_name>
<surname>Lalwani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Joint Retrieval-Extraction Training for Evidence-Aware Dialog Response Selection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3241</first_page>
						<last_page>3245</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1689</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luo21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Shenoy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Sunkara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Long Context NLM for ASR Rescoring in Conversational Agents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3246</first_page>
						<last_page>3250</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1849</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shenoy21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binling</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Zhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Oriental Language Recognition (OLR) 2020: Summary and Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3251</first_page>
						<last_page>3255</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2171</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raphaël</given_name>
<surname>Duroselle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Jouvet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Illina</surname>
</person_name>
					</contributors>
					<titles><title>Language Recognition on Unknown Conditions: The LORIA-Inria-MULTISPEECH System for AP20-OLR Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3256</first_page>
						<last_page>3260</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-276</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/duroselle21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianlong</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shouyi</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dawei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wang</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dandan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiyu</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Multi-Scale Convolution for Dialect Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3261</first_page>
						<last_page>3265</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-56</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kong21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ding</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuaishuai</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinkang</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Dialect Identification System with Transfer Learning from a Multilingual Automatic Speech Recognition Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3266</first_page>
						<last_page>3270</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-374</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongqin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuting</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Language Recognition Based on Unsupervised Pretrained Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3271</first_page>
						<last_page>3275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-807</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Additive Phoneme-Aware Margin Softmax Loss for Language Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3276</first_page>
						<last_page>3280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1167</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nataly</given_name>
<surname>Jahchan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florentin</given_name>
<surname>Barbier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariyanidevi Dharma</given_name>
<surname>Gita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khaled</given_name>
<surname>Khelif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Estelle</given_name>
<surname>Delpech</surname>
</person_name>
					</contributors>
					<titles><title>Towards an Accent-Robust Approach for ATC Communications Transcription</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3281</first_page>
						<last_page>3285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-333</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jahchan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Igor</given_name>
<surname>Szöke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Kocour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karel</given_name>
<surname>Veselý</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Detecting English Speech in the Air Traffic Control Voice Communication</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3286</first_page>
						<last_page>3290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1033</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/szoke21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Ohneiser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyyed Saeed</given_name>
<surname>Sarfjoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hartmut</given_name>
<surname>Helmke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shruthi</given_name>
<surname>Shetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Kleinert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiko</given_name>
<surname>Ehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Šarūnas</given_name>
<surname>Murauskas</surname>
</person_name>
					</contributors>
					<titles><title>Robust Command Recognition for Lithuanian Air Traffic Control Tower Utterances</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3291</first_page>
						<last_page>3295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-935</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ohneiser21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan</given_name>
<surname>Zuluaga-Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iuliia</given_name>
<surname>Nigmatulina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amrutha</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karel</given_name>
<surname>Veselý</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Kocour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Szöke</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Semi-Supervised Learning: An Approach to Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3296</first_page>
						<last_page>3300</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1373</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zuluagagomez21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Kocour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karel</given_name>
<surname>Veselý</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Blatt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Zuluaga</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Szöke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name>
					</contributors>
					<titles><title>Boosting of Contextual Information in ASR for Air-Traffic Call-Sign Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3301</first_page>
						<last_page>3305</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1619</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kocour21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jodie</given_name>
<surname>Gauvain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Gauvain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name>
					</contributors>
					<titles><title>Modeling the Effect of Military Oxygen Masks on Speech Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3306</first_page>
						<last_page>3310</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1650</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/elie21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinicius</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karyna</given_name>
<surname>Isaieva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justine</given_name>
<surname>Leclere</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-André</given_name>
<surname>Vuissoz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yves</given_name>
<surname>Laprie</surname>
</person_name>
					</contributors>
					<titles><title>Towards the Prediction of the Vocal Tract Shape from the Sequence of Phonemes to be Articulated</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3325</first_page>
						<last_page>3329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-184</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ribeiro21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rémi</given_name>
<surname>Blandin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Arnela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Félix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Baptiste</given_name>
<surname>Doc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of the Finite Element Method, the Multimodal Method and the Transmission-Line Model for the Computation of Vocal Tract Transfer Functions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3330</first_page>
						<last_page>3334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-975</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/blandin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sina</given_name>
<surname>Zarrieß</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joana</given_name>
<surname>Cholin</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Time Pressure and Spontaneity on Phonotactic Innovations in German Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3335</first_page>
						<last_page>3339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1539</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wagner21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salvador</given_name>
<surname>Medina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Tiede</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Hauptmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iain</given_name>
<surname>Matthews</surname>
</person_name>
					</contributors>
					<titles><title>Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3340</first_page>
						<last_page>3344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1732</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/medina21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc-Antoine</given_name>
<surname>Georges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Girin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Schwartz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name>
					</contributors>
					<titles><title>Learning Robust Speech Representation with an Articulatory-Regularized Variational Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3345</first_page>
						<last_page>3349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1604</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/georges21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heather</given_name>
<surname>Weston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura L.</given_name>
<surname>Koenig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susanne</given_name>
<surname>Fuchs</surname>
</person_name>
					</contributors>
					<titles><title>Changes in Glottal Source Parameter Values with Light to Moderate Physical Load</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3350</first_page>
						<last_page>3354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1881</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/weston21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Hassan</given_name>
<surname>Vali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Optimized Multi-Stage Vector Quantization of Spectral Envelopes for Speech and Audio Coding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3355</first_page>
						<last_page>3359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-867</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vali21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Santhan Kumar Reddy</given_name>
<surname>Nareddula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subrahmanyam</given_name>
<surname>Gorthi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama Krishna Sai S.</given_name>
<surname>Gorthi</surname>
</person_name>
					</contributors>
					<titles><title>Fusion-Net: Time-Frequency Information Fusion Y-Network for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3360</first_page>
						<last_page>3364</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1184</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nareddula21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ľuboš</given_name>
<surname>Marcinek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Stone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rebecca</given_name>
<surname>Millman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Gaydecki</surname>
</person_name>
					</contributors>
					<titles><title>N-MTTL SI Model: Non-Intrusive Multi-Task Transfer Learning-Based Speech Intelligibility Prediction Model with Scenery Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3365</first_page>
						<last_page>3369</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1878</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/marcinek21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Rudnicky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard M.</given_name>
<surname>Stern</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Context in Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3370</first_page>
						<last_page>3374</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1840</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xia21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenbiao</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongqin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zitao</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3375</first_page>
						<last_page>3379</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-158</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Einari</given_name>
<surname>Vaaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sari</given_name>
<surname>Ahlqvist-Björkroth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Drossos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3380</first_page>
						<last_page>3384</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-303</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vaaras21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Sentiment Analysis with Temporal Modality Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3385</first_page>
						<last_page>3389</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-487</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qian21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mani Kumar</given_name>
<surname>T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enrique</given_name>
<surname>Sanchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Tzimiropoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Giesbrecht</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michel</given_name>
<surname>Valstar</surname>
</person_name>
					</contributors>
					<titles><title>Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3390</first_page>
						<last_page>3394</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-610</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/t21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yelin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-Hao</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth S.</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3395</first_page>
						<last_page>3399</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-666</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Pepino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Riera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Recognition from Speech Using wav2vec 2.0 Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3400</first_page>
						<last_page>3404</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-703</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pepino21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiawang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoxiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Graph Isomorphism Network for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3405</first_page>
						<last_page>3409</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1154</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pooja</given_name>
<surname>Kumawat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurobinda</given_name>
<surname>Routray</surname>
</person_name>
					</contributors>
					<titles><title>Applying TDNN Architectures for Analyzing Duration Dependencies on Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3410</first_page>
						<last_page>3414</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2168</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kumawat21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Keesing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun Sing</given_name>
<surname>Koh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Witbrock</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3415</first_page>
						<last_page>3419</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2217</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/keesing21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Brusco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Pre-Trained Language Model for Speech Sentiment Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3420</first_page>
						<last_page>3424</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1723</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shon21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenxin</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Shinozaki</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Domain Speech Recognition with Unsupervised Character-Level Distribution Matching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3425</first_page>
						<last_page>3429</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-57</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hou21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoli</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3430</first_page>
						<last_page>3434</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-102</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kanda21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>On Minimum Word Error Rate Training of the Hybrid Autoregressive Transducer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3435</first_page>
						<last_page>3439</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-161</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshuman</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hasim</given_name>
<surname>Sak</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Streaming ASR Model Delay with Self Alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3440</first_page>
						<last_page>3444</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-322</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anuj</given_name>
<surname>Diwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3445</first_page>
						<last_page>3449</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-644</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/diwan21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation Based Training of Universal ASR Source Models for Cross-Lingual Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3450</first_page>
						<last_page>3454</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-796</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fukuda21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Swayambhu Nath</given_name>
<surname>Ray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pegah</given_name>
<surname>Ghahremani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghavendra</given_name>
<surname>Bilgi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milind</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harish</given_name>
<surname>Arsikere</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name>
					</contributors>
					<titles><title>Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3455</first_page>
						<last_page>3459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-836</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ray21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyun</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangliang</given_name>
<surname>Cao</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Targeted Universal Adversarial Perturbations to End-to-End ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3460</first_page>
						<last_page>3464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1668</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lu21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miguel</given_name>
<surname>Del Rio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Delworth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Westerman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nishchal</given_name>
<surname>Bhandari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Palakapilly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quinten</given_name>
<surname>McNamara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miguel</given_name>
<surname>Jetté</surname>
</person_name>
					</contributors>
					<titles><title>Earnings-21: A Practical Benchmark for ASR in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3465</first_page>
						<last_page>3469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1915</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/delrio21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eric</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Improving Multilingual Transformer Transducer Models by Reducing Language Confusions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3470</first_page>
						<last_page>3474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1949</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sun21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasser</given_name>
<surname>Hifny</surname>
</person_name>
					</contributors>
					<titles><title>Arabic Code-Switching Speech Recognition Using Monolingual Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3475</first_page>
						<last_page>3479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2231</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ali21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aviad</given_name>
<surname>Eisenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boaz</given_name>
<surname>Schwartz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Gannot</surname>
</person_name>
					</contributors>
					<titles><title>Online Blind Audio Source Separation Using Recursive Expectation-Maximization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3480</first_page>
						<last_page>3484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-662</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/eisenberg21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>Empirical Analysis of Generalized Iterative Speech Separation Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3485</first_page>
						<last_page>3489</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1161</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luo21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thilo von</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Graph-PIT: Generalized Permutation Invariant Training for Continuous Separation of Arbitrary Numbers of Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3490</first_page>
						<last_page>3494</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1177</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/neumann21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jisi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cătălin</given_name>
<surname>Zorilă</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama</given_name>
<surname>Doddipatla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3495</first_page>
						<last_page>3499</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1243</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge Bennasar</given_name>
<surname>Vázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Learning of New Sound Classes for Target Sound Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3500</first_page>
						<last_page>3504</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1369</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/delcroix21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>Binaural Speech Separation of Moving Speakers With Preserved Spatial Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3505</first_page>
						<last_page>3509</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1372</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/han21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shell Xu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Rifat</given_name>
<surname>Arefin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet-Nhat</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alish</given_name>
<surname>Dipani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xaq</given_name>
<surname>Pitkow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas Savas</given_name>
<surname>Tolias</surname>
</person_name>
					</contributors>
					<titles><title>AvaTr: One-Shot Speaker Extraction with Transformers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3510</first_page>
						<last_page>3514</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1378</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hu21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurjya</given_name>
<surname>Sarkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanouil</given_name>
<surname>Benetos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Sandler</surname>
</person_name>
					</contributors>
					<titles><title>Vocal Harmony Separation Using Time-Domain Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3515</first_page>
						<last_page>3519</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1531</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sarkar21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Maciejewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Verification-Based Evaluation of Single-Channel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3520</first_page>
						<last_page>3524</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1924</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/maciejewski21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tian</given_name>
<surname>Lan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilan</given_name>
<surname>Lyu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Refuoe</given_name>
<surname>Mokhosi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxin</given_name>
<surname>Tai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Improved Speech Separation with Time-and-Frequency Cross-Domain Feature Selection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3525</first_page>
						<last_page>3529</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2246</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengyun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiqian</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongtao</given_name>
<surname>Sha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Robust Speaker Extraction Network Based on Iterative Refined Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3530</first_page>
						<last_page>3534</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2250</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/deng21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wupeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Neural Speaker Extraction with Speaker-Speech Cross-Attention Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3535</first_page>
						<last_page>3539</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2260</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rémi</given_name>
<surname>Rigal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacques</given_name>
<surname>Chodorowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoît</given_name>
<surname>Zerr</surname>
</person_name>
					</contributors>
					<titles><title>Deep Audio-Visual Speech Separation Based on Facial Motion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3540</first_page>
						<last_page>3544</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1560</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rigal21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prachi</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajat</given_name>
<surname>Varma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkat</given_name>
<surname>Krishnamohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth Raj</given_name>
<surname>Chetupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>LEAP Submission for the Third DIHARD Diarization Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3545</first_page>
						<last_page>3549</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-728</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/singh21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weilong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Suo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwei</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Spatial-Acoustic Features for Overlapping Speech Detection in Multiparty Meetings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3550</first_page>
						<last_page>3554</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-747</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maokui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Desh</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zili</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Target-Speaker Voice Activity Detection with Improved i-Vector Estimation for Unknown Number of Speaker</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3555</first_page>
						<last_page>3559</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-750</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/he21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nauman</given_name>
<surname>Dawalatabad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Grondin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jenthe</given_name>
<surname>Thienpondt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brecht</given_name>
<surname>Desplanques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hwidong</given_name>
<surname>Na</surname>
</person_name>
					</contributors>
					<titles><title>ECAPA-TDNN Embeddings for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3560</first_page>
						<last_page>3564</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-941</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dawalatabad21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name>
					</contributors>
					<titles><title>Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3565</first_page>
						<last_page>3569</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1004</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kinoshita21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neville</given_name>
<surname>Ryant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prachi</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkat</given_name>
<surname>Krishnamohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajat</given_name>
<surname>Varma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenneth</given_name>
<surname>Church</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Cieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name>
					</contributors>
					<titles><title>The Third DIHARD Diarization Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3570</first_page>
						<last_page>3574</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1208</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ryant21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsun-Yat</given_name>
<surname>Leung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lahiru</given_name>
<surname>Samarakoon</surname>
</person_name>
					</contributors>
					<titles><title>Robust End-to-End Speaker Diarization with Conformer and Additive Margin Penalty</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3575</first_page>
						<last_page>3579</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1377</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/leung21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>O’Brien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anaïs</given_name>
<surname>Chanclu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Anonymous Speaker Clusters: Making Distinctions Between Anonymised Speech Recordings with Clustering Interface</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3580</first_page>
						<last_page>3584</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1588</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/obrien21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kiran</given_name>
<surname>Karra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>McCree</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization Using Two-Pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3585</first_page>
						<last_page>3589</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1807</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/karra21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenhou</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chendong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Federated Learning with Dynamic Transformer for Text to Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3590</first_page>
						<last_page>3594</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2039</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huu-Kim</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kihyuk</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Jae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>LiteTTS: A Lightweight Mel-Spectrogram-Free Text-to-Wave Synthesizer Based on Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3595</first_page>
						<last_page>3599</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-188</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nguyen21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chuanxin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dacheng</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yucheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjun</given_name>
<surname>Zeng</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3600</first_page>
						<last_page>3604</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-189</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tang21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Myeonghun</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeongju</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Jun</given_name>
<surname>Cheon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byoung Jin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3605</first_page>
						<last_page>3609</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-469</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jeong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jae-Sung</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taejun</given_name>
<surname>Bak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Sun</given_name>
<surname>Joo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3610</first_page>
						<last_page>3614</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-471</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bae21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adam</given_name>
<surname>Polyak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jade</given_name>
<surname>Copet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugene</given_name>
<surname>Kharitonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kushal</given_name>
<surname>Lakhotia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3615</first_page>
						<last_page>3619</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-475</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/polyak21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Penny</given_name>
<surname>Karanasou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Karlapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Joly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ammar</given_name>
<surname>Abbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Slangen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name>
					</contributors>
					<titles><title>A Learned Conditional Prior for the VAE Acoustic Space of a TTS System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3620</first_page>
						<last_page>3624</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-528</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/karanasou21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dipjyoti</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sankar</given_name>
<surname>Mukherjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Pantazis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>A Universal Multi-Speaker Multi-Style Text-to-Speech via Disentangled Representation Learning Based on R&#38;#233;nyi Divergence Minimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3625</first_page>
						<last_page>3629</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-660</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/paul21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-Hung</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Huai</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Relational Data Selection for Data Augmentation of Speaker-Dependent Multi-Band MelGAN Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3630</first_page>
						<last_page>3634</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-806</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyunseung</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3635</first_page>
						<last_page>3639</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-831</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chung21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shilun</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fenglong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Triple M: A Practical Text-to-Speech Synthesis System with Multi-Guidance Attention and Multi-Band Multi-Time LPCNet</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3640</first_page>
						<last_page>3644</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-851</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Shulby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eren</given_name>
<surname>Gölge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas Michael</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederico Santos de</given_name>
<surname>Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaldo</given_name>
<surname>Candido Jr.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anderson da Silva</given_name>
<surname>Soares</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra Maria</given_name>
<surname>Aluisio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moacir Antonelli</given_name>
<surname>Ponti</surname>
</person_name>
					</contributors>
					<titles><title>SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3645</first_page>
						<last_page>3649</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1774</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/casanova21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ian</given_name>
<surname>Palmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rouditchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Barbu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Katz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3650</first_page>
						<last_page>3654</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-245</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/palmer21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Salesky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Bremerman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roldano</given_name>
<surname>Cattoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Turchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas W.</given_name>
<surname>Oard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Post</surname>
</person_name>
					</contributors>
					<titles><title>The Multilingual TEDx Corpus for Speech Recognition and Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3655</first_page>
						<last_page>3659</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-11</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/salesky21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Picone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinjian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathleen</given_name>
<surname>Siminyu</surname>
</person_name>
					</contributors>
					<titles><title>Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3660</first_page>
						<last_page>3664</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1435</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mortensen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yihui</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luyao</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubo</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukai</given_name>
<surname>Jv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingdong</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3665</first_page>
						<last_page>3669</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1397</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guoguo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuzhou</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guan-Bo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayu</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Trmal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingjie</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuaijiang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuchen</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3670</first_page>
						<last_page>3674</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1965</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>You Jin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soyeon</given_name>
<surname>Choe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoohwan</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Look Who&#38;#8217;s Talking: Active Speaker Detection in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3675</first_page>
						<last_page>3679</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2041</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie J.</given_name>
<surname>Ballard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Burnham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharmakulasingam</given_name>
<surname>Sirojan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadi</given_name>
<surname>Mehmood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominique</given_name>
<surname>Estival</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elise</given_name>
<surname>Baker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanne</given_name>
<surname>Arciuli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titia</given_name>
<surname>Benders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Demuth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chloé</given_name>
<surname>Diskin-Holdaway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chwee Beng</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>AusKidTalk: An Auditory-Visual Corpus of 3- to 12-Year-Old Australian Children&#38;#8217;s Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3680</first_page>
						<last_page>3684</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2000</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ahmed21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Per</given_name>
<surname>Fallgren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name>
					</contributors>
					<titles><title>Human-in-the-Loop Efficiency Analysis for Binary Classification in Edyson</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3685</first_page>
						<last_page>3689</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-45</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fallgren21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elena</given_name>
<surname>Ryumina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oxana</given_name>
<surname>Verkholyak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>Annotation Confidence vs. Training Sample Size: Trade-Off Solution for Partially-Continuous Categorical Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3690</first_page>
						<last_page>3694</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1636</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ryumina21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gonçal V.</given_name>
<surname>Garcés Díaz-Munío</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan-Albert</given_name>
<surname>Silvestre-Cerdà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Jorge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrià Giménez</given_name>
<surname>Pastor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Iranzo-Sánchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pau</given_name>
<surname>Baquero-Arnal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nahuel</given_name>
<surname>Roselló</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Pérez-González-de-Martos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Civera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Sanchis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfons</given_name>
<surname>Juan</surname>
</person_name>
					</contributors>
					<titles><title>Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3695</first_page>
						<last_page>3699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1905</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/garcesdiazmunio21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Parul</given_name>
<surname>Kapoor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudrabha</given_name>
<surname>Mukhopadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sindhu B.</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Namboodiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C.V.</given_name>
<surname>Jawahar</surname>
</person_name>
					</contributors>
					<titles><title>Towards Automatic Speech to Sign Language Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3700</first_page>
						<last_page>3704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1094</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kapoor21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Won Ik</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seok Min</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunchang</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>kosp2e: Korean Speech to English Translation Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3705</first_page>
						<last_page>3709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1040</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cho21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiong</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3710</first_page>
						<last_page>3714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1259</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruchao</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>An Improved Single Step Non-Autoregressive Transformer for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3715</first_page>
						<last_page>3719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1955</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fan21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3720</first_page>
						<last_page>3724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2155</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/guo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edwin G.</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chan</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the Limits of Non-Autoregressive Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3725</first_page>
						<last_page>3729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-337</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ng21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander H.</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-An</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3730</first_page>
						<last_page>3734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-349</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jumon</given_name>
<surname>Nozaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name>
					</contributors>
					<titles><title>Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3735</first_page>
						<last_page>3739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-911</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nozaki21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Motoi</given_name>
<surname>Omachi</surname>
</person_name>
					</contributors>
					<titles><title>Toward Streaming ASR with Non-Autoregressive Insertion-Based Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3740</first_page>
						<last_page>3744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1131</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fujita21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaesong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingu</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Layer Pruning on Demand with Intermediate CTC</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3745</first_page>
						<last_page>3749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1171</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Song</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beibei</given_name>
<surname>Ouyang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuchuan</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dexin</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time End-to-End Monaural Multi-Speaker Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3750</first_page>
						<last_page>3754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1449</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Streaming End-to-End ASR Based on Blockwise Non-Autoregressive Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3755</first_page>
						<last_page>3759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1556</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stanislav</given_name>
<surname>Beliaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>TalkNet: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3760</first_page>
						<last_page>3764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1770</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/beliaev21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Norouzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chan</surname>
</person_name>
					</contributors>
					<titles><title>WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3765</first_page>
						<last_page>3769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1897</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Align-Denoise: Single-Pass Non-Autoregressive Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3770</first_page>
						<last_page>3774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1906</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>VAENAR-TTS: Variational Auto-Encoder Based Non-AutoRegressive Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3775</first_page>
						<last_page>3779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2121</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lu21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fasih</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia de la</given_name>
<surname>Fuente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Davida</given_name>
<surname>Fromm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>MacWhinney</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3780</first_page>
						<last_page>3784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1220</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>P.A.</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.P.</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>T.</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.C.</given_name>
<surname>Vásquez-Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>P.</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M.</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.R.</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K.</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Influence of the Interviewer on the Automatic Assessment of Alzheimer&#38;#8217;s Disease in the Context of the ADReSSo Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3785</first_page>
						<last_page>3789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1589</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pereztoro21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youxiang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Obyat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohui</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John A.</given_name>
<surname>Batsis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert M.</given_name>
<surname>Roth</surname>
</person_name>
					</contributors>
					<titles><title>WavBERT: Exploiting Semantic and Non-Semantic Speech Using Wav2vec and BERT for Dementia Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3790</first_page>
						<last_page>3794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-332</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhu21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lara</given_name>
<surname>Gauder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Pepino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Riera</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer Disease Recognition Using Speech-Based Embeddings From Pre-Trained Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3795</first_page>
						<last_page>3799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-753</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gauder21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aparna</given_name>
<surname>Balagopalan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jekaterina</given_name>
<surname>Novikova</surname>
</person_name>
					</contributors>
					<titles><title>Comparing Acoustic-Based Approaches for Alzheimer&#38;#8217;s Disease Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3800</first_page>
						<last_page>3804</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-759</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/balagopalan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefeng</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Wiechmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elma</given_name>
<surname>Kerz</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer&#38;#8217;s Disease Detection from Spontaneous Speech Through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3805</first_page>
						<last_page>3809</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1415</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qiao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer M.</given_name>
<surname>Harris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer C.</given_name>
<surname>Thompson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie S.</given_name>
<surname>Snowden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Using the Outputs of Different Automatic Speech Recognition Paradigms for Acoustic- and BERT-Based Alzheimer&#38;#8217;s Dementia Detection Through Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3810</first_page>
						<last_page>3814</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1519</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pan21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zafi Sherhan</given_name>
<surname>Syed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad Shehram Shah</given_name>
<surname>Syed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Lech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Pirogova</surname>
</person_name>
					</contributors>
					<titles><title>Tackling the ADRESSO Challenge 2021: The MUET-RMIT System for Alzheimer&#38;#8217;s Dementia Recognition from Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3815</first_page>
						<last_page>3819</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1572</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/syed21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Morteza</given_name>
<surname>Rohanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Hough</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Purver</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer&#38;#8217;s Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3820</first_page>
						<last_page>3824</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1633</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rohanian21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raghavendra</given_name>
<surname>Pappagari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaejin</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection and Assessment of Alzheimer Disease Using Speech and Language Technologies in Low-Resource Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3825</first_page>
						<last_page>3829</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1850</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pappagari21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jieping</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengyi</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayu</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Alzheimer&#38;#8217;s Disease Using Spontaneous Speech Only</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3830</first_page>
						<last_page>3834</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2002</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ning</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yupeng</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongru</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K.P.</given_name>
<surname>Subbalakshmi</surname>
</person_name>
					</contributors>
					<titles><title>Modular Multi-Modal Attention Network for Alzheimer&#38;#8217;s Disease Detection Using Patient Audio and Language Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3835</first_page>
						<last_page>3839</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2024</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carl</given_name>
<surname>Quillen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dushyant</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Goderre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>José</given_name>
<surname>Laínez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ljubomir</given_name>
<surname>Milanović</surname>
</person_name>
					</contributors>
					<titles><title>Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-Field Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3840</first_page>
						<last_page>3844</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1190</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gong21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>R.</given_name>
<surname>Gretter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Matassoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>D.</given_name>
<surname>Falavigna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A.</given_name>
<surname>Misra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C.W.</given_name>
<surname>Leong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L.</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>ETLT 2021: Shared Task on Automatic Speech Recognition for Non-Native Children&#38;#8217;s Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3845</first_page>
						<last_page>3849</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1237</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gretter21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lars</given_name>
<surname>Rumberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Ehlert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ulrike</given_name>
<surname>Lüdtke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörn</given_name>
<surname>Ostermann</surname>
</person_name>
					</contributors>
					<titles><title>Age-Invariant Training for End-to-End Child Speech Recognition Using Adversarial Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3850</first_page>
						<last_page>3854</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1241</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rumberg21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuele</given_name>
<surname>Cornell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Matassoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Squartini</surname>
</person_name>
					</contributors>
					<titles><title>Learning to Rank Microphones for Distant Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3855</first_page>
						<last_page>3859</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1315</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cornell21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucile</given_name>
<surname>Gelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Pinquier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgane</given_name>
<surname>Daniel</surname>
</person_name>
					</contributors>
					<titles><title>Simulating Reading Mistakes for Child Speech Transformer-Based Phone Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3860</first_page>
						<last_page>3864</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2202</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gelin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brooke</given_name>
<surname>Stephenson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Girin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3865</first_page>
						<last_page>3869</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-275</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/stephenson21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pol van</given_name>
<surname>Rijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvan</given_name>
<surname>Mertes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Schiller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter M.C.</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Larrouy-Maestri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeth</given_name>
<surname>André</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nori</given_name>
<surname>Jacoby</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Emotional Prototypes in a High Dimensional TTS Latent Space</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3870</first_page>
						<last_page>3874</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1538</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rijn21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Devang S. Ram</given_name>
<surname>Mohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivian</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian Huey</given_name>
<surname>Teh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Torresquintero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher G.R.</given_name>
<surname>Wallis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marlene</given_name>
<surname>Staib</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Foglianti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiameng</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3875</first_page>
						<last_page>3879</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1583</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mohan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Torresquintero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian Huey</given_name>
<surname>Teh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher G.R.</given_name>
<surname>Wallis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marlene</given_name>
<surname>Staib</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devang S. Ram</given_name>
<surname>Mohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivian</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Foglianti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiameng</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>ADEPT: A Dataset for Evaluating Prosody Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3880</first_page>
						<last_page>3884</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1610</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/torresquintero21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nguyen Thi Thu</given_name>
<surname>Trang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nguyen Hoang</given_name>
<surname>Ky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>d'Alessandro</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3885</first_page>
						<last_page>3889</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-125</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/trang21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaked</given_name>
<surname>Dovrat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliya</given_name>
<surname>Nachmani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Wolf</surname>
</person_name>
					</contributors>
					<titles><title>Many-Speakers Single Channel Speech Separation with Optimal Permutation Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3890</first_page>
						<last_page>3894</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-493</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dovrat21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mieszko</given_name>
<surname>Fraś</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Witkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Combating Reverberation in NTF-Based Speech Separation Using a Sub-Source Weighted Multichannel Wiener Filter and Linear Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3895</first_page>
						<last_page>3899</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1230</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fras21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Strauss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jouni</given_name>
<surname>Paulus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Torcoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Edler</surname>
</person_name>
					</contributors>
					<titles><title>A Hands-On Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3900</first_page>
						<last_page>3904</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1418</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/strauss21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Borsdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3905</first_page>
						<last_page>3909</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1552</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/borsdorf21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kimiko</given_name>
<surname>Tsukada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Yurong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joo-Yeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong-Im</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Hajek</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Linguistic Perception of the Japanese Singleton/Geminate Contrast: Korean, Mandarin and Mongolian Compared</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3910</first_page>
						<last_page>3914</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-21</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tsukada21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szymon</given_name>
<surname>Zaporowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Beringer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alicja</given_name>
<surname>Serafinowicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bozena</given_name>
<surname>Kostek</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3915</first_page>
						<last_page>3919</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-86</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/korzekwa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bettina</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Dehé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marieke</given_name>
<surname>Einfeldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniela</given_name>
<surname>Wochner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Zahner-Ritter</surname>
</person_name>
					</contributors>
					<titles><title>Testing Acoustic Voice Quality Classification Across Languages and Speech Styles</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3920</first_page>
						<last_page>3924</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-315</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/braun21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qianyutong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kexin</given_name>
<surname>Lyu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zening</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Acquisition of Prosodic Focus Marking by Three- to Six-Year-Old Children Learning Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3925</first_page>
						<last_page>3928</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-316</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maryam Sadat</given_name>
<surname>Mirzaei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kourosh</given_name>
<surname>Meshgi</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Listening Difficulty Detection for L2 Learners Through Moderating ASR Resources</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3929</first_page>
						<last_page>3933</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-372</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mirzaei21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>F0 Patterns of L2 English Speech by Mandarin Chinese Learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3934</first_page>
						<last_page>3938</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-581</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ding21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Neural Network-Based Noise Compensation Method for Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3939</first_page>
						<last_page>3943</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-843</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacek</given_name>
<surname>Kudera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Georgis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tania</given_name>
<surname>Avgustinova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Distance and Surprisal in Multilingual Priming: Evidence from Slavic</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3944</first_page>
						<last_page>3948</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1003</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kudera21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Preliminary Study on Discourse Prosody Encoding in L1 and L2 English Spontaneous Narratives</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3949</first_page>
						<last_page>3953</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1082</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minglin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wai-Kim</given_name>
<surname>Leung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Transformer Based End-to-End Mispronunciation Detection and Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3954</first_page>
						<last_page>3958</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1467</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Calbert</given_name>
<surname>Graham</surname>
</person_name>
					</contributors>
					<titles><title>L1 Identification from L2 Speech Using Neural Spectrogram Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3959</first_page>
						<last_page>3963</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1545</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/graham21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miran</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Byrd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth S.</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Real-Time MRI for Illuminating Linguistic Velum Action</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3964</first_page>
						<last_page>3968</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1823</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/oh21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zirui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Segmental Alignment of English Syllables with Singleton and Cluster Onsets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3969</first_page>
						<last_page>3973</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-187</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Míša</given_name>
<surname>Hejná</surname>
</person_name>
					</contributors>
					<titles><title>Exploration of Welsh English Pre-Aspiration: How Wide-Spread is it?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3974</first_page>
						<last_page>3978</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-685</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hejna21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beeke</given_name>
<surname>Muhlack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikey</given_name>
<surname>Elmers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiner</given_name>
<surname>Drenhaus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Trouvain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marjolein van</given_name>
<surname>Os</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Werner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margarita</given_name>
<surname>Ryzhova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting Recall Effects of Filler Particles in German and English</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3979</first_page>
						<last_page>3983</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1056</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/muhlack21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chunyu</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peggy</given_name>
<surname>Mok</surname>
</person_name>
					</contributors>
					<titles><title>How Reliable Are Phonetic Data Collected Remotely? Comparison of Recording Devices and Environments on Acoustic Measurements</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3984</first_page>
						<last_page>3988</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1122</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ge21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-fan</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueh-chin</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>A Cross-Dialectal Comparison of Apical Vowels in Beijing Mandarin, Northeastern Mandarin and Southwestern Mandarin: An EMA and Ultrasound Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3989</first_page>
						<last_page>3993</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1326</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/huang21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oihane</given_name>
<surname>Muxika</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marianne</given_name>
<surname>Pouplier</surname>
</person_name>
					</contributors>
					<titles><title>Dissecting the Aero-Acoustic Parameters of Open Articulatory Transitions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3994</first_page>
						<last_page>3998</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1379</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gibson21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amelia J.</given_name>
<surname>Gully</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying Vocal Tract Shape Variation and its Acoustic Impact: A Geometric Morphometric Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>3999</first_page>
						<last_page>4003</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1400</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gully21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Guevara-Rukoz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Peperkamp</surname>
</person_name>
					</contributors>
					<titles><title>Speech Perception and Loanword Adaptations: The Case of Copy-Vowel Epenthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4004</first_page>
						<last_page>4008</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1481</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/guevararukoz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe-chen</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajka</given_name>
<surname>Smiljanic</surname>
</person_name>
					</contributors>
					<titles><title>Speakers Coarticulate Less When Facing Real and Imagined Communicative Difficulties: An Analysis of Read and Spontaneous Speech from the LUCID Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4009</first_page>
						<last_page>4013</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1640</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/guo21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Einar</given_name>
<surname>Meister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lya</given_name>
<surname>Meister</surname>
</person_name>
					</contributors>
					<titles><title>Developmental Changes of Vowel Acoustics in Adolescents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4014</first_page>
						<last_page>4018</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1649</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/meister21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sonia</given_name>
<surname>d'Apolito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara Gili</given_name>
<surname>Fivela</surname>
</person_name>
					</contributors>
					<titles><title>Context and Co-Text Influence on the Accuracy Production of Italian L2 Non-Native Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4019</first_page>
						<last_page>4023</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1724</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dapolito21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wilbert</given_name>
<surname>Heeringa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hans Van de</given_name>
<surname>Velde</surname>
</person_name>
					</contributors>
					<titles><title>A New Vowel Normalization for Sociophonetics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4024</first_page>
						<last_page>4028</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1846</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/heeringa21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rosey</given_name>
<surname>Billington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hywel</given_name>
<surname>Stoakes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nick</given_name>
<surname>Thieberger</surname>
</person_name>
					</contributors>
					<titles><title>The Pacific Expansion: Optimizing Phonetic Transcription of Archival Corpora</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4029</first_page>
						<last_page>4033</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2167</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/billington21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>FSR: Accelerating the Inference Process of Transducer-Based Models by Applying Fast-Skip Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4034</first_page>
						<last_page>4038</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1367</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tian21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>Mitrofanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariya</given_name>
<surname>Korenevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Podluzhny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Khokhlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Laptev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Ilin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Korenevsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Medennikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Romanenko</surname>
</person_name>
					</contributors>
					<titles><title>LT-LM: A Novel Non-Autoregressive Language Model for Single-Shot Lattice Rescoring</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4039</first_page>
						<last_page>4043</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1716</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mitrofanov21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cyril</given_name>
<surname>Allauzen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Variani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Riley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Hybrid Seq-2-Seq ASR Design for On-Device and Server Applications</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4044</first_page>
						<last_page>4048</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-658</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/allauzen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hirofumi</given_name>
<surname>Inaguma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>VAD-Free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4049</first_page>
						<last_page>4053</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1107</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/inaguma21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuoyuan</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhendong</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Lei</surname>
</person_name>
					</contributors>
					<titles><title>WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4054</first_page>
						<last_page>4058</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1983</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4059</first_page>
						<last_page>4063</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1992</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tanaka21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mun-Hak</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Deep Neural Network Calibration for E2E Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4064</first_page>
						<last_page>4068</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-176</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiujia</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangliang</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Residual Energy-Based Models for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4069</first_page>
						<last_page>4073</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-690</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiujia</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangliang</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McGraw</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Learning for End-to-End ASR Word and Utterance Confidence with Deletion Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4074</first_page>
						<last_page>4078</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1207</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qiu21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Ollerenshaw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Asif</given_name>
<surname>Jalal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Insights on Neural Representations for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4079</first_page>
						<last_page>4083</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1516</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ollerenshaw21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amber</given_name>
<surname>Afshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kshitiz</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-Level Confidence Classifier for ASR Utterance Accuracy and Application to Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4084</first_page>
						<last_page>4088</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1666</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/afshan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Learning of Disentangled Speech Content and Style Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4089</first_page>
						<last_page>4093</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1936</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tjandra21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunbi</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hwa-Yeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong-Hwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Label Embedding for Chinese Grapheme-to-Phoneme Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4094</first_page>
						<last_page>4098</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-885</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/choi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haiteng</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>PDF: Polyphone Disambiguation in Chinese by Using FLAT</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4099</first_page>
						<last_page>4103</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1087</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Improving Polyphone Disambiguation for Mandarin Chinese by Combining Mix-Pooling Strategy and Window-Based Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4104</first_page>
						<last_page>4108</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1232</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Congyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Polyphone Disambiguation in Mandarin Chinese with Semi-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4109</first_page>
						<last_page>4113</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-502</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shi21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing-Feng</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>A Neural-Network-Based Approach to Identifying Speakers in Novels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4114</first_page>
						<last_page>4118</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-609</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>UnitNet-Based Hybrid Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4119</first_page>
						<last_page>4123</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1092</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sashi</given_name>
<surname>Novitasari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Dynamically Adaptive Machine Speech Chain Inference for TTS in Noisy Environment: Listen and Speak Louder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4124</first_page>
						<last_page>4128</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-946</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/novitasari21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haozhe</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengqiang</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>LinearSpeech: Parallel Text-to-Speech with Linear Complexity</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4129</first_page>
						<last_page>4133</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1192</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noa</given_name>
<surname>Mansbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny Hershkovitch</given_name>
<surname>Neiterman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amos</given_name>
<surname>Azaria</surname>
</person_name>
					</contributors>
					<titles><title>An Agent for Competing with Humans in a Deceptive Game Based on Vocal Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4134</first_page>
						<last_page>4138</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-83</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mansbach21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Fakhry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyi</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaclyn</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunvant</given_name>
<surname>Chaudhari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asriel</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Branch Deep Learning Network for Automated Detection of COVID-19</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4139</first_page>
						<last_page>4143</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-378</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fakhry21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youxuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongze</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shugong</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4144</first_page>
						<last_page>4148</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-438</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ma21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hira</given_name>
<surname>Dhamyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayesha</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ihsan Ayyub</given_name>
<surname>Qazi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agha Ali</given_name>
<surname>Raza</surname>
</person_name>
					</contributors>
					<titles><title>Fake Audio Detection in Resource-Constrained Settings Using Microfeatures</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4149</first_page>
						<last_page>4153</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-524</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dhamyal21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianhao</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Parada-Cabaleiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meishu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Coughing-Based Recognition of Covid-19 with Spatial Attentive ConvLSTM Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4154</first_page>
						<last_page>4158</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-630</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yan21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soumava</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gurunath Reddy</given_name>
<surname>M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sreenivasa</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Partha Pratim</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for Singing Voice Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4159</first_page>
						<last_page>4163</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-636</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/paul21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryu</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Komatani</surname>
</person_name>
					</contributors>
					<titles><title>Age Estimation with Speech-Age Model for Heterogeneous Speech Datasets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4164</first_page>
						<last_page>4168</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-861</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/takeda21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kah Kuan</given_name>
<surname>Teh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>Open-Set Audio Classification with Limited Training Resources Based on Augmentation Enhanced Variational Auto-Encoder GAN with Detection-Classification Joint Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4169</first_page>
						<last_page>4173</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1142</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/teh21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Fukumori</surname>
</person_name>
					</contributors>
					<titles><title>Deep Spectral-Cepstral Fusion for Shouted and Normal Speech Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4174</first_page>
						<last_page>4178</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1245</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/fukumori21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shikha</given_name>
<surname>Baghel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mrinmoy</given_name>
<surname>Bhattacharjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.R. Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prithwijit</given_name>
<surname>Guha</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Shouted Speech Segments in Indian News Debates</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4179</first_page>
						<last_page>4183</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1592</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/baghel21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Vuong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahsa</given_name>
<surname>Elyasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaurav</given_name>
<surname>Bharaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Generalized Spoofing Detection Inspired from Audio Generation Artifacts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4184</first_page>
						<last_page>4188</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1705</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gao21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiguang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Tung</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xionghu</given_name>
<surname>Zhong</surname>
</person_name>
					</contributors>
					<titles><title>Overlapped Speech Detection Based on Spectral and Spatial Feature Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4189</first_page>
						<last_page>4193</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2138</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Badr M.</given_name>
<surname>Abdullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marius</given_name>
<surname>Mosbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iuliia</given_name>
<surname>Zaitova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4194</first_page>
						<last_page>4198</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-678</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/abdullah21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radhika</given_name>
<surname>Arava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xibin</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thahir</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>AbdelHady</surname>
</person_name>
					</contributors>
					<titles><title>Paraphrase Label Alignment for Voice Application Retrieval in Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4199</first_page>
						<last_page>4203</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-97</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gao21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rikhye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ding</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McGraw</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Keyphrase Detection Using Speaker and Environment Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4204</first_page>
						<last_page>4208</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-204</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/rikhye21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonil</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Sigtia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Adya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pramod</given_name>
<surname>Simha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pranay</given_name>
<surname>Dighe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra</given_name>
<surname>Dhir</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4209</first_page>
						<last_page>4213</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1428</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/garg21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Mazumder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colby</given_name>
<surname>Banbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josh</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pete</given_name>
<surname>Warden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay Janapa</given_name>
<surname>Reddi</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Keyword Spotting in Any Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4214</first_page>
						<last_page>4218</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1966</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mazumder21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Text Anchor Based Metric Learning for Small-Footprint Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4219</first_page>
						<last_page>4223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-136</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yangbin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianping</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4224</first_page>
						<last_page>4228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-147</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongyub</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeongil</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeong Cheol</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taesun</given_name>
<surname>Whang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunhwa</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunggyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaechoon</given_name>
<surname>Jo</surname>
</person_name>
					</contributors>
					<titles><title>Auxiliary Sequence Labeling Tasks for Disfluency Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4229</first_page>
						<last_page>4233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-400</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu Ting</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Energy-Friendly Keyword Spotting System Using Add-Based Convolution</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4234</first_page>
						<last_page>4238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-458</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhou21g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinping</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The 2020 Personalized Voice Trigger Challenge: Open Datasets, Evaluation Metrics, Baseline System and Results</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4239</first_page>
						<last_page>4243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-602</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jia21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingsong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyu</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qijie</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Wei</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Auto-KWS 2021 Challenge: Task, Datasets, and Baselines</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4244</first_page>
						<last_page>4248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-817</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Axel</given_name>
<surname>Berg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>O’Connor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miguel Tairum</given_name>
<surname>Cruz</surname>
</person_name>
					</contributors>
					<titles><title>Keyword Transformer: A Self-Attention Model for Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4249</first_page>
						<last_page>4253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1286</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/berg21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abhijeet</given_name>
<surname>Awasthi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Kilgour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Rom</surname>
</person_name>
					</contributors>
					<titles><title>Teaching Keyword Spotters to Spot New Keywords with Limited Examples</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4254</first_page>
						<last_page>4258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1395</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/awasthi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4259</first_page>
						<last_page>4263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-702</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>An Initial Investigation for Detecting Partially Spoofed Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4264</first_page>
						<last_page>4268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-738</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenchuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingchun</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Siamese Network with wav2vec Feature for Spoofing Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4269</first_page>
						<last_page>4273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-847</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xie21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingliang</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas Fang</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Database Replay Detection in Terminal-Dependent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4274</first_page>
						<last_page>4278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-960</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cheng21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The Effect of Silence and Dual-Band Fusion in Anti-Spoofing System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4279</first_page>
						<last_page>4283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1281</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Pairing Weak with Strong: Twin Models for Defending Against Adversarial Attack on Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4284</first_page>
						<last_page>4288</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1343</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peng21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hefei</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leichao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junrui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baiyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Attention-Based Convolutional Neural Network for ASV Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4289</first_page>
						<last_page>4293</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1404</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ling21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Voting for the Right Answer: Adversarial Defense for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4294</first_page>
						<last_page>4298</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1452</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wu21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Nautsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Delgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4299</first_page>
						<last_page>4303</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1522</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kinnunen21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Representation Learning to Classify and Detect Adversarial Attacks Against Speaker and Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4304</first_page>
						<last_page>4308</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1759</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/villalba21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>You</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ge</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>An Empirical Study on Channel Effects for Synthetic Voice Spoofing Countermeasure Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4309</first_page>
						<last_page>4313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1820</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Channel-Wise Gated Res2Net: Towards Robust Detection of Synthetic Speech Attacks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4314</first_page>
						<last_page>4318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2125</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wanying</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michele</given_name>
<surname>Panariello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4319</first_page>
						<last_page>4323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1187</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ge21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kay</given_name>
<surname>Peterson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Audrey</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>OpenASR20: An Open Challenge for Automatic Speech Recognition of Conversational Telephone Speech in Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4324</first_page>
						<last_page>4328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1930</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peterson21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Multitask Adaptation with Lattice-Free MMI for Multi-Genre Speech Recognition of Low Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4329</first_page>
						<last_page>4333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1778</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/madikeri21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiu-shi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-hui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>An Improved Wav2Vec 2.0 Pre-Training Approach Using Enhanced Local Dependency Modeling for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4334</first_page>
						<last_page>4338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-67</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhu21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hung-Pang</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Jia</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Ping</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Systems for Low-Resource Speech Recognition Tasks in Open Automatic Speech Recognition and Formosa Speech Recognition Challenges</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4339</first_page>
						<last_page>4343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-358</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ambyera</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guan-Bo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guixin</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinghao</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The TNT Team System Descriptions of Cantonese and Mongolian for IARPA OpenASR20</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4344</first_page>
						<last_page>4348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1063</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhao21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Kong</surname>
</person_name>
					</contributors>
					<titles><title>Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4349</first_page>
						<last_page>4353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1086</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/alumae21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ethan</given_name>
<surname>Morris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robbie</given_name>
<surname>Jimerson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Prud’hommeaux</surname>
</person_name>
					</contributors>
					<titles><title>One Size Does Not Fit All in Resource-Constrained ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4354</first_page>
						<last_page>4358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1970</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/morris21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Gimeno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Representation Learning for Speech Activity Detection in the Fearless Steps Challenge 2021</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4359</first_page>
						<last_page>4363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-309</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gimeno21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Vuong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard M.</given_name>
<surname>Stern</surname>
</person_name>
					</contributors>
					<titles><title>The Application of Learnable STRF Kernels to the 2021 Fearless Steps Phase-03 SAD Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4364</first_page>
						<last_page>4368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-651</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vuong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyyed Saeed</given_name>
<surname>Sarfjoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name>
					</contributors>
					<titles><title>Speech Activity Detection Based on Multilingual Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4369</first_page>
						<last_page>4373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1058</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sarfjoo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jarrod</given_name>
<surname>Luckenbaugh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Abplanalp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachel</given_name>
<surname>Gonzalez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fulford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Gard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Voice Activity Detection with Teacher-Student Domain Emulation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4374</first_page>
						<last_page>4378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1234</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/luckenbaugh21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Omid</given_name>
<surname>Ghahabi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Fischer</surname>
</person_name>
					</contributors>
					<titles><title>EML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4379</first_page>
						<last_page>4382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1456</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ghahabi21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuba</given_name>
<surname>Łopatka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katarzyna</given_name>
<surname>Kaszuba-Miotke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Klinke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paweł</given_name>
<surname>Trella</surname>
</person_name>
					</contributors>
					<titles><title>Device Playback Augmentation with Echo Cancellation for Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4383</first_page>
						<last_page>4387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1316</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/opatka21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alican</given_name>
<surname>Gok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Batuhan</given_name>
<surname>Gundogdu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murat</given_name>
<surname>Saraclar</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Open Vocabulary Keyword Search</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4388</first_page>
						<last_page>4392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1399</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yusuf21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danny</given_name>
<surname>Merkx</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan L.</given_name>
<surname>Frank</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Sentence Similarity: Size does not Always Matter</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4393</first_page>
						<last_page>4397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1464</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/merkx21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luboš</given_name>
<surname>Šmídl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef V.</given_name>
<surname>Psutka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleš</given_name>
<surname>Pražák</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Term Detection and Relevance Score Estimation Using Dot-Product of Pronunciation Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4398</first_page>
						<last_page>4402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1704</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/svec21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>François</given_name>
<surname>Buet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Yvon</surname>
</person_name>
					</contributors>
					<titles><title>Toward Genre Adapted Closed Captioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4403</first_page>
						<last_page>4407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1762</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/buet21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shira</given_name>
<surname>Calamaro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bozena</given_name>
<surname>Kostek</surname>
</person_name>
					</contributors>
					<titles><title>Weakly-Supervised Word-Level Pronunciation Error Detection in Non-Native English Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4408</first_page>
						<last_page>4412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-38</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/korzekwa21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoli</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speaker-Attributed ASR with Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4413</first_page>
						<last_page>4417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-101</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kanda21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hagen</given_name>
<surname>Soltau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Izhak</given_name>
<surname>Shafran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent El</given_name>
<surname>Shafey</surname>
</person_name>
					</contributors>
					<titles><title>Understanding Medical Conversations: Rich Transcription, Confidence Scores &#38;amp; Information Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4418</first_page>
						<last_page>4422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-691</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/soltau21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jazmín</given_name>
<surname>Vidal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cyntia</given_name>
<surname>Bonomi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcelo</given_name>
<surname>Sancinetti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name>
					</contributors>
					<titles><title>Phone-Level Pronunciation Scoring for Spanish Speakers Learning English Using a GOP-DNN System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4423</first_page>
						<last_page>4427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-745</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/vidal21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoshuo</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueteng</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Explore wav2vec 2.0 for Mispronunciation Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4428</first_page>
						<last_page>4432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-777</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xu21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shintaro</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name>
					</contributors>
					<titles><title>Lexical Density Analysis of Word Productions in Japanese English Using Acoustic Word Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4433</first_page>
						<last_page>4437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-853</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ando21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Deep Feature Transfer Learning for Automatic Pronunciation Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4438</first_page>
						<last_page>4442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-931</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huayun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy F.</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4443</first_page>
						<last_page>4447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1258</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linkai</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binghuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dengfeng</given_name>
<surname>Ke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4448</first_page>
						<last_page>4452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1344</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peng21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elma</given_name>
<surname>Kerz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name>
					</contributors>
					<titles><title>The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4453</first_page>
						<last_page>4457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1402</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/qiao21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4458</first_page>
						<last_page>4462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1981</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tanaka21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ronald</given_name>
<surname>Cumbal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Birger</given_name>
<surname>Moell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>José</given_name>
<surname>Lopes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olov</given_name>
<surname>Engwall</surname>
</person_name>
					</contributors>
					<titles><title>&#38;#8220;You don&#38;#8217;t understand me!&#38;#8221;: Comparing ASR Results for L1 and L2 Speakers of Swedish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4463</first_page>
						<last_page>4467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2140</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cumbal21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Bakhturina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyle</given_name>
<surname>Gorman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>NeMo Inverse Text Normalization: From Development to Production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4468</first_page>
						<last_page>4472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1571</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Satsuki</given_name>
<surname>Naijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akinori</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Nose</surname>
</person_name>
					</contributors>
					<titles><title>Improvement of Automatic English Pronunciation Assessment with Small Number of Utterances Using Sentence Speakability</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4473</first_page>
						<last_page>4477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1132</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/naijo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fasih</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name>
					</contributors>
					<titles><title>Affect Recognition Through Scalogram and Multi-Resolution Cochleagram Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4478</first_page>
						<last_page>4482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1761</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/haider21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiawang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoxiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Speech Emotion Recognition Framework for Better Discrimination of Confusions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4483</first_page>
						<last_page>4487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-718</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruichen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition via Multi-Level Cross-Modal Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4488</first_page>
						<last_page>4492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-785</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Fujioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinghua</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4493</first_page>
						<last_page>4497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-809</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ito21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deboshree</given_name>
<surname>Bose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Parametric Distributions to Model Numerical Emotion Labels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4498</first_page>
						<last_page>4502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1000</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/bose21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Metric Learning Based Feature Representation with Gated Fusion Model for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4503</first_page>
						<last_page>4507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1133</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gao21e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingyu</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renjie</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenneth</given_name>
<surname>Church</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition with Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4508</first_page>
						<last_page>4512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1852</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cai21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Generalized Dilated CNN Models for Depression Detection Using Inverted Vocal Tract Variables</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4513</first_page>
						<last_page>4517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1960</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/seneviratne21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhua</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guang</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuezhu</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengdao</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4518</first_page>
						<last_page>4522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2004</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaodong</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruiguo</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Time-Frequency Representation Learning with Graph Convolutional Network for Dialogue-Level Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4523</first_page>
						<last_page>4527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2067</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gonçalo</given_name>
<surname>Mordido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthijs</given_name>
<surname>Van keirsbilck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Keller</surname>
</person_name>
					</contributors>
					<titles><title>Compressing 1D Time-Channel Separable Convolutions Using Sparse Random Ternary Matrices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4528</first_page>
						<last_page>4532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-141</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mordido21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengli</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobo</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Weakly Supervised Construction of ASR Systems from Massive Video Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4533</first_page>
						<last_page>4537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-7</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cheng21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Byeonggeun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyung</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkyu</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dooyong</given_name>
<surname>Sung</surname>
</person_name>
					</contributors>
					<titles><title>Broadcasted Residual Learning for Efficient Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4538</first_page>
						<last_page>4542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-383</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rupak Vignesh</given_name>
<surname>Swaminathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name>
					</contributors>
					<titles><title>CoDERT: Distilling Encoder Representations with Co-Learning for Transducer-Based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4543</first_page>
						<last_page>4547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-797</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/swaminathan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>Extremely Low Footprint End-to-End ASR System for Smart Device</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4548</first_page>
						<last_page>4552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-819</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gao21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Dissecting User-Perceived Latency of On-Device E2E Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4553</first_page>
						<last_page>4557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1887</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shangguan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Macoskey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinru</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Amortized Neural Networks for Low-Latency Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4558</first_page>
						<last_page>4562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-712</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/macoskey21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rami</given_name>
<surname>Botros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>David</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Guzman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Tied &#38;amp; Reduced RNN-T Decoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4563</first_page>
						<last_page>4567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-212</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/botros21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jangho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyung</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nojun</given_name>
<surname>Kwak</surname>
</person_name>
					</contributors>
					<titles><title>PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4568</first_page>
						<last_page>4572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-248</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Varun</given_name>
<surname>Nagaraja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Venkatesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikas</given_name>
<surname>Chandra</surname>
</person_name>
					</contributors>
					<titles><title>Collaborative Training of Acoustic Encoders for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4573</first_page>
						<last_page>4577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-354</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/nagaraja21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4578</first_page>
						<last_page>4582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-415</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>The Energy and Carbon Footprint of Training End-to-End Speech Recognizers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4583</first_page>
						<last_page>4587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-456</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/parcollet21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Ravichandran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Graph-Based Label Propagation for Semi-Supervised Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4588</first_page>
						<last_page>4592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1209</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruirui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chelsea J.-T.</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeya</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongda</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oguz</given_name>
<surname>Elibol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4593</first_page>
						<last_page>4597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-3</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Sarni</surname>
</person_name>
					</contributors>
					<titles><title>A Generative Model for Duration-Dependent Score Calibration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4598</first_page>
						<last_page>4602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-114</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cumani21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Pelecanos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ignacio Lopez</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4603</first_page>
						<last_page>4607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-641</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pelecanos21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Speaker Verification for Single and Multi-Talker Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4608</first_page>
						<last_page>4612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-681</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kataria21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Padfield</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel J.</given_name>
<surname>Liebling</surname>
</person_name>
					</contributors>
					<titles><title>Chronological Self-Training for Real-Time Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4613</first_page>
						<last_page>4617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-822</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/padfield21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Runqiu</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liuping</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Margin Circle Loss for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4618</first_page>
						<last_page>4622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1043</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xiao21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>O’Brien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Meunier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alain</given_name>
<surname>Ghio</surname>
</person_name>
					</contributors>
					<titles><title>Presentation Matters: Evaluating Speaker Identification Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4623</first_page>
						<last_page>4627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1211</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/obrien21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fuchuan</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Error Correction for Speaker Embedding Learning with Noisy Labels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4628</first_page>
						<last_page>4632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2021</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tong21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dexin</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Zhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>An Integrated Framework for Two-Pass Personalized Voice Trigger</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4633</first_page>
						<last_page>4637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2161</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aiswarya Vinod</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hira</given_name>
<surname>Dhamyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Masked Proxy Loss for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4638</first_page>
						<last_page>4642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2190</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lian21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyumin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daeyoung</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4643</first_page>
						<last_page>4647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-838</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lee21h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4648</first_page>
						<last_page>4652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1236</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/liu21p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarath</given_name>
<surname>Sivaprasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saiteja</given_name>
<surname>Kosgi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Gandhi</surname>
</person_name>
					</contributors>
					<titles><title>Emotional Prosody Control for Speech Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4653</first_page>
						<last_page>4657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-307</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/sivaprasad21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Cong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Na</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name>
					</contributors>
					<titles><title>Controllable Context-Aware Conversational Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4658</first_page>
						<last_page>4662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-412</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cong21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minchan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Jun</given_name>
<surname>Cheon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byoung Jin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Jin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Expressive Text-to-Speech Using Style Tag</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4663</first_page>
						<last_page>4667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-465</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/kim21n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuzi</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tie-Yan</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Text to Speech for Spontaneous Style</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4668</first_page>
						<last_page>4672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-584</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yan21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingbei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Multi-Scale Style Control for Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4673</first_page>
						<last_page>4677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-947</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/li21r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shifeng</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4678</first_page>
						<last_page>4682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-979</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pan21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4683</first_page>
						<last_page>4687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1129</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/tan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaochun</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank K.</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4688</first_page>
						<last_page>4692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1407</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/an21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Slava</given_name>
<surname>Shechtman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raul</given_name>
<surname>Fernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Sorin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Haws</surname>
</person_name>
					</contributors>
					<titles><title>Synthesis of Expressive Speaking Styles with Limited Training Data in a Multi-Speaker, Prosody-Controllable Sequence-to-Sequence Architecture</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4693</first_page>
						<last_page>4697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1446</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/shechtman21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mai Hoang</given_name>
<surname>Dao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thinh Hung</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dat Quoc</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>Intent Detection and Slot Filling for Vietnamese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4698</first_page>
						<last_page>4702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-618</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/dao21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haitao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengqing</given_name>
<surname>Zong</surname>
</person_name>
					</contributors>
					<titles><title>Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4703</first_page>
						<last_page>4707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-55</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/lin21k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Judith</given_name>
<surname>Gaspers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quynh</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniil</given_name>
<surname>Sorokin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Lehnen</surname>
</person_name>
					</contributors>
					<titles><title>The Impact of Intent Distribution Mismatch on Semi-Supervised Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4708</first_page>
						<last_page>4712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-335</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gaspers21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yidi</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maulik</given_name>
<surname>Madhavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4713</first_page>
						<last_page>4717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-402</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jiang21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nick J.C.</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yandan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haimei</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejun</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-Trained DNN-HMM-Based Acoustic-Phonetic Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4718</first_page>
						<last_page>4722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-501</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ia_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sujeong</given_name>
<surname>Cha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangrui</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>My</given_name>
<surname>Phung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Kwang J.</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edmilson</given_name>
<surname>Morais</surname>
</person_name>
					</contributors>
					<titles><title>Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4723</first_page>
						<last_page>4727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-788</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cha21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianwei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Cross-Lingual Spoken Language Understanding Model with Multilingual Pretraining</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4728</first_page>
						<last_page>4732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-818</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hamidreza</given_name>
<surname>Saghir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samridhi</given_name>
<surname>Choudhary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sepehr</given_name>
<surname>Eghbali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clement</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Factorization-Aware Training of Transformers for Natural Language Understanding on the Edge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4733</first_page>
						<last_page>4737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1816</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/saghir21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Saxon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samridhi</given_name>
<surname>Choudhary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph P.</given_name>
<surname>McKenna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Spoken Language Understanding for Generalized Voice Assistants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4738</first_page>
						<last_page>4742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1826</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/saxon21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soyeon Caren</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqu</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huichun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henry</given_name>
<surname>Weld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josiah</given_name>
<surname>Poon</surname>
</person_name>
					</contributors>
					<titles><title>Bi-Directional Joint Neural Networks for Intent Classification and Slot Filling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4743</first_page>
						<last_page>4747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2044</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/han21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ando</given_name>
<surname>Saabas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Parnamaa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Loide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sten</given_name>
<surname>Sootla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marju</given_name>
<surname>Purin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannes</given_name>
<surname>Gamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karsten</given_name>
<surname>Sorensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Aichner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>INTERSPEECH 2021 Acoustic Echo Cancellation Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4748</first_page>
						<last_page>4752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1870</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cutler21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Pfeifenberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Zoehrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franz</given_name>
<surname>Pernkopf</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Echo Cancellation with Cross-Domain Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4753</first_page>
						<last_page>4757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-85</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/pfeifenberger21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shimin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubo</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4758</first_page>
						<last_page>4762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1359</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/zhang21ia_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ernst</given_name>
<surname>Seidel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Franzen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Strake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>Y2-Net FCRN for Acoustic Echo and Noise Suppression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4763</first_page>
						<last_page>4767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1590</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/seidel21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Renhua</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linjuan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Echo Cancellation Using Deep Complex Neural Network with Nonlinear Magnitude Compression and Phase Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4768</first_page>
						<last_page>4772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2022</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/peng21f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amir</given_name>
<surname>Ivry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baruch</given_name>
<surname>Berdugo</surname>
</person_name>
					</contributors>
					<titles><title>Nonlinear Acoustic Echo Cancellation with Deep Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4773</first_page>
						<last_page>4777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-722</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/ivry21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert L.</given_name>
<surname>MacDonald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pan-Pan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Cattiau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rus</given_name>
<surname>Heywood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Cave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katie</given_name>
<surname>Seaver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marilyn A.</given_name>
<surname>Ladewig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimmy</given_name>
<surname>Tobin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael P.</given_name>
<surname>Brenner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Nelson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Tomanek</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition of Disordered Speech: Personalized Models Outperforming Human Listeners on Short Phrases</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4778</first_page>
						<last_page>4782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1384</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/green21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Suendermann-Oeft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Pautler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Indu</given_name>
<surname>Navar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aria</given_name>
<surname>Anvar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jochen</given_name>
<surname>Kumm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raquel</given_name>
<surname>Norel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ernest</given_name>
<surname>Fraenkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander V.</given_name>
<surname>Sherman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James D.</given_name>
<surname>Berry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary L.</given_name>
<surname>Pattee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Utility of Multimodal Conversational Technology and Audiovisual Analytic Measures for the Assessment and Monitoring of Amyotrophic Lateral Sclerosis at Scale</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4783</first_page>
						<last_page>4787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1801</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/neumann21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Handling Acoustic Variation in Dysarthric Speech Recognition Systems Through Model Combination</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4788</first_page>
						<last_page>4792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2212</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/hermann21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zi</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4793</first_page>
						<last_page>4797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-60</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/geng21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarah E.</given_name>
<surname>Gutz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannah P.</given_name>
<surname>Rowe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>Speaking with a KN95 Face Mask: ASR Performance and Speaker Compensation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4798</first_page>
						<last_page>4802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-99</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/gutz21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Data Augmentation for Disordered Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4803</first_page>
						<last_page>4807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-168</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/jin21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rukiye</given_name>
<surname>Ruzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Variational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4808</first_page>
						<last_page>4812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-173</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/xie21b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Disong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lifa</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4813</first_page>
						<last_page>4817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-285</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/wang21ja_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabian Ritter</given_name>
<surname>Gutierrez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zi</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian Parametric and Architectural Domain Adaptation of LF-MMI Trained TDNNs for Elderly and Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4818</first_page>
						<last_page>4822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-289</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/deng21d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shanqing</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisie</given_name>
<surname>Lillianfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katie</given_name>
<surname>Seaver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael P.</given_name>
<surname>Brenner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Nelson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>D.</given_name>
<surname>Sculley</surname>
</person_name>
					</contributors>
					<titles><title>A Voice-Activated Switch for Persons with Motor and Speech Impairments: Isolated-Vowel Spotting Using Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4823</first_page>
						<last_page>4827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-330</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/cai21c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xia</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzheng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan</given_name>
<surname>Doshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Conformer Parrotron: A Faster and Stronger End-to-End Speech Conversion and Recognition Model for Atypical Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4828</first_page>
						<last_page>4832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-676</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/chen21w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert L.</given_name>
<surname>MacDonald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pan-Pan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Cattiau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rus</given_name>
<surname>Heywood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Cave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katie</given_name>
<surname>Seaver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marilyn A.</given_name>
<surname>Ladewig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimmy</given_name>
<surname>Tobin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael P.</given_name>
<surname>Brenner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip C.</given_name>
<surname>Nelson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Tomanek</surname>
</person_name>
					</contributors>
					<titles><title>Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4833</first_page>
						<last_page>4837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-697</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/macdonald21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eun Jung</given_name>
<surname>Yeo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Severity Classification of Korean Dysarthric Speech Using Phoneme-Level Pronunciation Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4838</first_page>
						<last_page>4842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1353</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/yeo21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Subhashini</given_name>
<surname>Venugopalan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joel</given_name>
<surname>Shor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manoj</given_name>
<surname>Plakal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimmy</given_name>
<surname>Tobin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Tomanek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael P.</given_name>
<surname>Brenner</surname>
</person_name>
					</contributors>
					<titles><title>Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4843</first_page>
						<last_page>4847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-1913</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/venugopalan21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zifang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Lea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Tooley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Darren</given_name>
<surname>Botten</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashwini</given_name>
<surname>Palekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrinath</given_name>
<surname>Thelapurath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sachin</given_name>
<surname>Kajarekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jefferey</given_name>
<surname>Bigham</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and Tuning of a Voice Assistant System for Dysfluent Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>30</day>
						<year>2021</year>
					</publication_date>
					<pages>
						<first_page>4848</first_page>
						<last_page>4852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2021-2006</doi>
						<resource>https://www.isca-archive.org/interspeech_2021/mitra21_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>