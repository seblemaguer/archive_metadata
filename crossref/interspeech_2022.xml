<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2022</doi_batch_id>
		<timestamp>1705398673789527</timestamp>
		<depositor>
			<depositor_name>Martin Cooke</depositor_name> 
			<email_address>m.cooke@ikerbasque.org</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant> 
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>Interspeech 2022</conference_name>
				<conference_acronym>interspeech_2022</conference_acronym>
				<conference_date>18-22 September 2022</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>Interspeech 2022</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2022</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2022</doi>
					<timestamp>1705398673789527</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2022/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Variani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Riley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cyril</given_name>
<surname>Allauzen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tongzhou</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>On Adaptive Weight Interpolation of the Hybrid Autoregressive Transducer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1646</first_page>
						<last_page>1650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-4</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/variani22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jumon</given_name>
<surname>Nozaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenkichi</given_name>
<surname>Ishizuka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiichi</given_name>
<surname>Hashimoto</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end Speech-to-Punctuated-Text Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1811</first_page>
						<last_page>1815</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-5</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nozaki22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaesung</given_name>
<surname>Tae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeongju</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taesu</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>EdiTTS: Score-based Editing for Controllable Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>421</first_page>
						<last_page>425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-6</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tae22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Multi-Talker ASR with Token-Level Serialized Output Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3774</first_page>
						<last_page>3778</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-7</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kanda22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre-Michel</given_name>
<surname>Bousquet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mickael</given_name>
<surname>Rouvier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Francois</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Reliability criterion based on learning-phase entropy for speaker recognition with neural network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>281</first_page>
						<last_page>285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-8</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bousquet22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rikke</given_name>
<surname>Bundgaard-Nielsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brett</given_name>
<surname>Baker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olga</given_name>
<surname>Maxwell</surname>
</person_name>
					</contributors>
					<titles><title>Native phonotactic interference in L2 vowel processing: Mouse-tracking reveals cognitive conflicts during identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5223</first_page>
						<last_page>5227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-12</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2608</first_page>
						<last_page>2612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-13</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aaqib</given_name>
<surname>Saeed</surname>
</person_name>
					</contributors>
					<titles><title>Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>516</first_page>
						<last_page>520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-17</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saeed22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hai-tao</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1963</first_page>
						<last_page>1967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-18</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunkyung</given_name>
<surname>Yoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeonseop</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taehyeong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chul</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Online Learning of Open-set Speaker Identification by Active User-registration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5065</first_page>
						<last_page>5069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-25</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yoo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeremy Heng Meng</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huayun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Variations of multi-task learning for spoken language assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4456</first_page>
						<last_page>4460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-28</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyunghyun</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Towards Disentangled Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3603</first_page>
						<last_page>3607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-30</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peyser22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rosanna</given_name>
<surname>Turrisi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Badino</surname>
</person_name>
					</contributors>
					<titles><title>Interpretable dysarthric speaker adaptation based on optimal-transport</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>26</first_page>
						<last_page>30</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-36</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/turrisi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyun</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cyril</given_name>
<surname>Allauzen</surname>
</person_name>
					</contributors>
					<titles><title>E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4995</first_page>
						<last_page>4999</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-38</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaokai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keke</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Coupled Discriminant Subspace Alignment for Cross-database Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4695</first_page>
						<last_page>4699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-40</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolás</given_name>
<surname>Schmidt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordi</given_name>
<surname>Pons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marius</given_name>
<surname>Miron</surname>
</person_name>
					</contributors>
					<titles><title>PodcastMix: A dataset for separating music and speech in podcasts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>231</first_page>
						<last_page>235</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-41</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/schmidt22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunhao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaen</given_name>
<surname>Liang</surname>
</person_name>
					</contributors>
					<titles><title>Selective Pseudo-labeling and Class-wise Discriminative Fusion for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1496</first_page>
						<last_page>1500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-42</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haixin</given_name>
<surname>Guan</surname>
</person_name>
					</contributors>
					<titles><title>PercepNet+: A Phase and SNR Aware PercepNet for Real-Time Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>916</first_page>
						<last_page>920</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-43</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ge22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seungu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4401</first_page>
						<last_page>4405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-45</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/han22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyunjae</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonbin</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang Hoon</given_name>
<surname>Woo</surname>
</person_name>
					</contributors>
					<titles><title>SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1</first_page>
						<last_page>5</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-46</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cho22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuna</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Jun</given_name>
<surname>Baek</surname>
</person_name>
					</contributors>
					<titles><title>Keyword Spotting with Synthetic Data using Heterogeneous Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1397</first_page>
						<last_page>1401</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-47</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinsheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanzhao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heyang</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongmao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengxiao</given_name>
<surname>Bi</surname>
</person_name>
					</contributors>
					<titles><title>Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4242</first_page>
						<last_page>4246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-48</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikko</given_name>
<surname>Strom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haidar</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wael</given_name>
<surname>Hamza</surname>
</person_name>
					</contributors>
					<titles><title>Squashed Weight Distribution for Low Bit Quantization of Deep Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3953</first_page>
						<last_page>3957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-50</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/strom22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1566</first_page>
						<last_page>1570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-52</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanbin</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Sun</given_name>
<surname>Joo</surname>
</person_name>
					</contributors>
					<titles><title>Enhancement of Pitch Controllability using Timbre-Preserving Pitch Augmentation in FastPitch</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>6</first_page>
						<last_page>10</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-55</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bae22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ioannis</given_name>
<surname>Tsiamas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerard I.</given_name>
<surname>Gállego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>José A. R.</given_name>
<surname>Fonollosa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marta R.</given_name>
<surname>Costa-jussà</surname>
</person_name>
					</contributors>
					<titles><title>SHAS: Approaching optimal Segmentation for End-to-End Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>106</first_page>
						<last_page>110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-59</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tsiamas22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiantong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name>
					</contributors>
					<titles><title>Simple and Effective Zero-shot Cross-lingual Phoneme Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2113</first_page>
						<last_page>2117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-60</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Byeonggeun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jangho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunsin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juntae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyung</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2393</first_page>
						<last_page>2397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-61</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edward</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xulong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Tiny-Sepformer: A Tiny Time-Domain Transformer Network For Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5313</first_page>
						<last_page>5317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-66</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/luo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hokuto</given_name>
<surname>Munakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryu</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Komatani</surname>
</person_name>
					</contributors>
					<titles><title>Training Data Generation with DOA-based Selecting and Remixing for Unsupervised Training of Deep Separation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>861</first_page>
						<last_page>865</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-69</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/munakata22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiahui</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanghao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>Speaker recognition-assisted robust audio deepfake detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4202</first_page>
						<last_page>4206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-72</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arne-Lukas</given_name>
<surname>Fietkau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Stone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>Relationship between the acoustic time intervals and tongue movements of German diphthongs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>734</first_page>
						<last_page>738</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-73</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fietkau22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weidong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianxin</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>346</first_page>
						<last_page>350</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-74</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kamil</given_name>
<surname>Deja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariadna</given_name>
<surname>Sanchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Roth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marius</given_name>
<surname>Cotescu</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Evaluation of Speaker Similarity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2348</first_page>
						<last_page>2352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-75</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/deja22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Mateju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frantisek</given_name>
<surname>Kynych</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Cerva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiri</given_name>
<surname>Malek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindrich</given_name>
<surname>Zdansky</surname>
</person_name>
					</contributors>
					<titles><title>Overlapped Speech Detection in Broadcast Streams Using X-vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4606</first_page>
						<last_page>4610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-81</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mateju22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apu</given_name>
<surname>Kapadia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donald</given_name>
<surname>Williamson</surname>
</person_name>
					</contributors>
					<titles><title>Preventing sensitive-word recognition using self-supervised learning to preserve user-privacy for automatic speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4207</first_page>
						<last_page>4211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-85</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Young</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Rin</given_name>
<surname>Jeoung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Improved CNN-Transformer using Broadcasted Residual Learning for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2223</first_page>
						<last_page>2227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-88</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/choi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingqiong</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>Mandarin nasal place assimilation revisited: an acoustic study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5228</first_page>
						<last_page>5232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-89</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/luo22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zifeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Aware Mixture of Mixtures Training for Weakly Supervised Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5318</first_page>
						<last_page>5322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-96</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouei</given_name>
<surname>Yamaoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobutaka</given_name>
<surname>Ono</surname>
</person_name>
					</contributors>
					<titles><title>Joint Optimization of Sampling Rate Offsets Based on Entire Signal Relationship Among Distributed Microphones</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>704</first_page>
						<last_page>708</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-97</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/masuyama22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name>
					</contributors>
					<titles><title>Robust Self-Supervised Audio-Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2118</first_page>
						<last_page>2122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-99</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kitamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Kunimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeaki</given_name>
<surname>Amano</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Evaluation of Penetrating Voices through a Semantic Differential Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3063</first_page>
						<last_page>3067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-100</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kitamura22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Kunihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanbo</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noriko</given_name>
<surname>Nakanishi</surname>
</person_name>
					</contributors>
					<titles><title>Gradual Improvements Observed in Learners' Perception and Production of L2 Sounds Through Continuing Shadowing Practices on a Daily Basis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1303</first_page>
						<last_page>1307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-101</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kunihara22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tao</given_name>
<surname>Rui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ouchi</given_name>
<surname>Kazushige</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangdong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Couple learning for semi-supervised sound event detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2398</first_page>
						<last_page>2402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-103</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rui22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Stan</surname>
</person_name>
					</contributors>
					<titles><title>The ZevoMOS entry to VoiceMOS Challenge 2022</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4516</first_page>
						<last_page>4520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-105</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/stan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Deep residual spiking neural network for keyword spotting in low-resource settings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3023</first_page>
						<last_page>3027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-107</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Czempin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Diekmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Froghyar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Böttinger</surname>
</person_name>
					</contributors>
					<titles><title>Does Audio Deepfake Detection Generalize?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2783</first_page>
						<last_page>2787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-108</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/muller22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngdo</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Joo</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Corpus Speech Emotion Recognition for Unseen Corpus Using Corpus-Wise Weights in Classification Loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>131</first_page>
						<last_page>135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-111</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ahn22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Baas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Voice Conversion Can Improve ASR in Very Low-Resource Settings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3513</first_page>
						<last_page>3517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-112</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baas22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Feinberg</surname>
</person_name>
					</contributors>
					<titles><title>VoiceLab: Software for Fully Reproducible Automated Voice Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>351</first_page>
						<last_page>355</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-113</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/feinberg22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Peso Parada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnieszka</given_name>
<surname>Dobrowolska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthikeyan</given_name>
<surname>Saravanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mete</given_name>
<surname>Ozay</surname>
</person_name>
					</contributors>
					<titles><title>pMCT: Patched Multi-Condition Training for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3779</first_page>
						<last_page>3783</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-117</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pesoparada22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joel</given_name>
<surname>Shor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhashini</given_name>
<surname>Venugopalan</surname>
</person_name>
					</contributors>
					<titles><title>TRILLsson: Distilled Universal Paralinguistic Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>356</first_page>
						<last_page>360</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-118</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shor22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Ciccarelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarred</given_name>
<surname>Barber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Nair</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Challenges and Opportunities in Multi-device Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>709</first_page>
						<last_page>713</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-119</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ciccarelli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Sliding Window Modeling for Abstractive Meeting Summarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5150</first_page>
						<last_page>5154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-121</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan</given_name>
<surname>Vu Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maori</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Akagi</surname>
</person_name>
					</contributors>
					<titles><title>Speak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>171</first_page>
						<last_page>175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-124</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vuho22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youjin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngki</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the limits of raw waveform speaker recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2228</first_page>
						<last_page>2232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-126</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jung22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Diekmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Williams</surname>
</person_name>
					</contributors>
					<titles><title>Attacker Attribution of Audio Deepfakes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2788</first_page>
						<last_page>2792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-129</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/muller22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deyi</given_name>
<surname>Tuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>426</first_page>
						<last_page>430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-131</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nabarun</given_name>
<surname>Goswami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Harada</surname>
</person_name>
					</contributors>
					<titles><title>SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1203</first_page>
						<last_page>1207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-133</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/goswami22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nik</given_name>
<surname>Vaessen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>van Leeuwen</surname>
</person_name>
					</contributors>
					<titles><title>Training speaker recognition systems with limited data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4760</first_page>
						<last_page>4764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-135</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vaessen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ananya</given_name>
<surname>Ayasi</surname>
</person_name>
					</contributors>
					<titles><title>Oktoechos Classification in Liturgical Music Using SBU-LSTM/GRU</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2403</first_page>
						<last_page>2407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-136</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rajan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woohyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abderrahim</given_name>
<surname>Fathan</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end framework for spoof-aware speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4362</first_page>
						<last_page>4366</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-139</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woohyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abderrahim</given_name>
<surname>Fathan</surname>
</person_name>
					</contributors>
					<titles><title>Mixup regularization strategies for spoofing countermeasure system</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3734</first_page>
						<last_page>3738</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-140</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5050</first_page>
						<last_page>5054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-141</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/feng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woohyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abderrahim</given_name>
<surname>Fathan</surname>
</person_name>
					</contributors>
					<titles><title>MIM-DG: Mutual information minimization-based domain generalization for speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3674</first_page>
						<last_page>3678</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-142</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arun</given_name>
<surname>Babu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kushal</given_name>
<surname>Lakhotia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiantong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naman</given_name>
<surname>Goyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kritika</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>von Platen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yatharth</given_name>
<surname>Saraf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Conneau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name>
					</contributors>
					<titles><title>XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2278</first_page>
						<last_page>2282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-143</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/babu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ting-Wei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I-Fan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name>
					</contributors>
					<titles><title>Learning to rank with BERT-based confidence models in ASR rescoring</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1651</first_page>
						<last_page>1655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-145</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iona</given_name>
<surname>Gessinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Cultural Comparison of Gradient Emotion Perception: Human vs. Alexa TTS Voices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4970</first_page>
						<last_page>4974</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-146</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gessinger22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shahar</given_name>
<surname>Lutati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliya</given_name>
<surname>Nachmani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Wolf</surname>
</person_name>
					</contributors>
					<titles><title>SepIt: Approaching a Single Channel Speech Separation Bound</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5323</first_page>
						<last_page>5327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-149</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lutati22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Perez</given_name>
<surname>Ogayo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graham</given_name>
<surname>Neubig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>W Black</surname>
</person_name>
					</contributors>
					<titles><title>Building African Voices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1263</first_page>
						<last_page>1267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-152</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ogayo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuangqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingjian</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Full-band and Sub-band Fusion Network for Real Time Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>921</first_page>
						<last_page>925</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-153</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>LI</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Global Signal-to-noise Ratio Estimation Based on Multi-subband Processing Using Convolutional Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>361</first_page>
						<last_page>365</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-154</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju-Ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Extended U-Net for Speaker Verification in Noisy Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>590</first_page>
						<last_page>594</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-155</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Hybrid Continuity Loss to Reduce Over-Suppression for Time-domain Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1786</first_page>
						<last_page>1790</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-157</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guochen</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>TaylorBeamformer: Learning All-Neural Beamformer for Multi-Channel Speech Enhancement from Taylor’s Approximation Theory</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5413</first_page>
						<last_page>5417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-159</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Da-Hee</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>FiLM Conditioning with Enhanced Feature to the Transformer-based End-to-End Noisy Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4098</first_page>
						<last_page>4102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-161</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kristina</given_name>
<surname>Tesch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nils-Hendrik</given_name>
<surname>Mohrmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>On the Role of Spatial, Spectral, and Temporal Processing for DNN-based Non-linear Multi-channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2908</first_page>
						<last_page>2912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-162</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tesch22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoran</given_name>
<surname>Cvetkovic</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>31</first_page>
						<last_page>35</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-163</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yue22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ronglai</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Local Context-aware Self-attention for Continuous Sign Language Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4810</first_page>
						<last_page>4814</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-164</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zuo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>Global RNN Transducer Models For Multi-dialect Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3138</first_page>
						<last_page>3142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-165</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fukuda22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>On the Use of Deep Mask Estimation Module for Neural Source Separation Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5328</first_page>
						<last_page>5332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-174</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zifeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Target Confusion in End-to-end Speaker Extraction: Analysis and Approaches</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5333</first_page>
						<last_page>5337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-176</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sebastião</given_name>
<surname>Quintas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Mauclair</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Pinquier</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Speech Intelligibility using Consonant Similarity for Head and Neck Cancer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3608</first_page>
						<last_page>3612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-182</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/quintas22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ameya</given_name>
<surname>Agaskar</surname>
</person_name>
					</contributors>
					<titles><title>Practical Over-the-air Perceptual AcousticWatermarking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>714</first_page>
						<last_page>718</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-183</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/agaskar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Constantijn</given_name>
<surname>Kaland</surname>
</person_name>
					</contributors>
					<titles><title>Bending the string: intonation contour length as a correlate of macro-rhythm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5233</first_page>
						<last_page>5237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-185</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kaland22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luke</given_name>
<surname>Prananta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bence</given_name>
<surname>Halpern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>The Effectiveness of Time Stretching for Enhancing Dysarthric Speech for Improved Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>36</first_page>
						<last_page>40</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-190</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/prananta22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Van Gysel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirko</given_name>
<surname>Hannemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ernest</given_name>
<surname>Pusateri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youssef</given_name>
<surname>Oualil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Oparin</surname>
</person_name>
					</contributors>
					<titles><title>Space-Efficient Representation of Entity-centric Query Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>679</first_page>
						<last_page>683</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-193</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vangysel22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zalan</given_name>
<surname>Borsos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Sharifi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Tagliasacchi</surname>
</person_name>
					</contributors>
					<titles><title>SpeechPainter: Text-conditioned Speech Inpainting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>431</first_page>
						<last_page>435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-194</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/borsos22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoyi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dick</given_name>
<surname>Botteldooren</surname>
</person_name>
					</contributors>
					<titles><title>CT-SAT: Contextual Transformer for Sequential Audio Tagging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4147</first_page>
						<last_page>4151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-196</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hou22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vera</given_name>
<surname>Bernhard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Schwab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Philippe</given_name>
<surname>Goldman</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Stress Detection in Isolated English Words for Computer-Assisted Pronunciation Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3143</first_page>
						<last_page>3147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-197</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bernhard22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naokazu</given_name>
<surname>Uchida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Homma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Makoto</given_name>
<surname>Iwayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasuhiro</given_name>
<surname>Sogawa</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Offensive Replies in Open Domain Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1076</first_page>
						<last_page>1080</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-200</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/uchida22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Harvill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang D.</given_name>
<surname>Yoo</surname>
</person_name>
					</contributors>
					<titles><title>Frame-Level Stutter Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2843</first_page>
						<last_page>2847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-204</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/harvill22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyue</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>YU</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haitong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4247</first_page>
						<last_page>4251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-205</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longfei</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Shinozaki</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Learning with Multi-Target Contrastive Coding for Non-Native Acoustic Modeling of Mispronunciation Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4312</first_page>
						<last_page>4316</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-207</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashwinkumar</given_name>
<surname>Ganesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Campbell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name>
					</contributors>
					<titles><title>L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4317</first_page>
						<last_page>4321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-209</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Young</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Rin</given_name>
<surname>Jeoung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>HYU Submission for the SASV Challenge 2022: Reforming Speaker Embeddings with Spoofing-Aware Conditioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2873</first_page>
						<last_page>2877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-210</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/choi22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honoka</given_name>
<surname>Tamaru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayako</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>Speech intelligibility of simulated hearing loss sounds and its prediction using the Gammachirp Envelope Similarity Index (GESI)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3929</first_page>
						<last_page>3933</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-211</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/irino22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Chang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chuan</given_name>
<surname>Steven</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yen-Cheng</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ren</given_name>
<surname>Yeh</surname>
</person_name>
					</contributors>
					<titles><title>g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1926</first_page>
						<last_page>1930</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-216</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linh The</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nguyen Luong</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Doan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manh</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dat Quoc</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>A High-Quality and Large-Scale Dataset for English-Vietnamese Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1726</first_page>
						<last_page>1730</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-218</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nguyen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengdong</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiadi</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Far-Field Speaker Verification with Large-Scale Ad-hoc Microphone Arrays</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3679</first_page>
						<last_page>3683</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-219</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xue</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changchun</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Embedding Recurrent Layers with Dual-Path Strategy in a Variant of Convolutional Network for Speaker-Independent Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5338</first_page>
						<last_page>5342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-220</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name>
					</contributors>
					<titles><title>An Improved Deliberation Network with Text Pre-training for Code-Switching Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3854</first_page>
						<last_page>3858</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-221</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minchan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeonghun</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byoung Jin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunghwan</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joun Yeop</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>788</first_page>
						<last_page>792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-225</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robin</given_name>
<surname>Algayres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adel</given_name>
<surname>Nabli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoît</given_name>
<surname>Sagot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2123</first_page>
						<last_page>2127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-226</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/algayres22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khaled</given_name>
<surname>Koutini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamid</given_name>
<surname>Eghbal-zadeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerhard</given_name>
<surname>Widmer</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Training of Audio Transformers with Patchout</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2753</first_page>
						<last_page>2757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-227</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/koutini22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Song</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ken</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baoxiang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>436</first_page>
						<last_page>440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-229</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Beam-Guided TasNet: An Iterative Speech Separation Framework with Multi-Channel Output</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>866</first_page>
						<last_page>870</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-230</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Diego</given_name>
<surname>Aguirre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nigel</given_name>
<surname>Ward</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan E.</given_name>
<surname>Avila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heike</given_name>
<surname>Lehnert-LeHouillier</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Models for Detecting Off-Putting Speaking Styles</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2303</first_page>
						<last_page>2307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-232</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/aguirre22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ariadna</given_name>
<surname>Sanchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Falai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orazio</given_name>
<surname>Angelini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kayoko</given_name>
<surname>Yanagisawa</surname>
</person_name>
					</contributors>
					<titles><title>Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2963</first_page>
						<last_page>2967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-233</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sanchez22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Sadeghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Magron</surname>
</person_name>
					</contributors>
					<titles><title>A Sparsity-promoting Dictionary Model for Variational Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>366</first_page>
						<last_page>370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-237</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sadeghi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishal</given_name>
<surname>Sunder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Fosler-Lussier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Kwang</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2683</first_page>
						<last_page>2687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-239</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sunder22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ting-Wei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biing</given_name>
<surname>Juang</surname>
</person_name>
					</contributors>
					<titles><title>Induce Spoken Dialog Intents via Deep Unsupervised Context Contrastive Clustering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1081</first_page>
						<last_page>1085</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-240</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Falai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariadna</given_name>
<surname>Sanchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orazio</given_name>
<surname>Angelini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kayoko</given_name>
<surname>Yanagisawa</surname>
</person_name>
					</contributors>
					<titles><title>Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2353</first_page>
						<last_page>2357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-242</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sepand</given_name>
<surname>Mavandadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Deliberation by Text-Only and Semi-Supervised Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4940</first_page>
						<last_page>4944</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-243</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dacheng</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanxin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanqing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoqiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yucheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1571</first_page>
						<last_page>1575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-245</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolaea Catalin</given_name>
<surname>Ristea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radu Tudor</given_name>
<surname>Ionescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fahad Shahbaz</given_name>
<surname>Khan</surname>
</person_name>
					</contributors>
					<titles><title>SepTr: Separable Transformer for Audio Spectrogram Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4103</first_page>
						<last_page>4107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-249</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ristea22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haaris</given_name>
<surname>Mehmood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnieszka</given_name>
<surname>Dobrowolska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthikeyan</given_name>
<surname>Saravanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mete</given_name>
<surname>Ozay</surname>
</person_name>
					</contributors>
					<titles><title>FedNST: Federated Noisy Student Training for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1001</first_page>
						<last_page>1005</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-252</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mehmood22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>521</first_page>
						<last_page>525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-253</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kanda22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kenta</given_name>
<surname>Udagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2968</first_page>
						<last_page>2972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-257</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/udagawa22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Mitsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyu</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kei</given_name>
<surname>Sawada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukiya</given_name>
<surname>Hono</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiko</given_name>
<surname>Nankaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keiichi</given_name>
<surname>Tokuda</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2328</first_page>
						<last_page>2332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-259</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mitsui22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sashi</given_name>
<surname>Novitasari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name>
					</contributors>
					<titles><title>Improving ASR Robustness in Noisy Condition Through VAD Integration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3784</first_page>
						<last_page>3788</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-260</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/novitasari22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binu Nisal</given_name>
<surname>Abeysinghe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesin</given_name>
<surname>James</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Watson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Marattukalam</surname>
</person_name>
					</contributors>
					<titles><title>Visualising Model Training via Vowel Space for Text-To-Speech Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>511</first_page>
						<last_page>515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-264</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/abeysinghe22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Kishiyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuyu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Hirose</surname>
</person_name>
					</contributors>
					<titles><title>One-step models in pitch perception: Experimental evidence from Japanese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1841</first_page>
						<last_page>1845</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-265</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kishiyama22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinpeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuzhou</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanbo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guoguo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The THUEE System Description for the IARPA OpenASR21 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4855</first_page>
						<last_page>4859</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-269</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name>
					</contributors>
					<titles><title>Independence-based Joint Dereverberation and Separation with Neural Source Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>236</first_page>
						<last_page>240</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-271</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saijo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manh</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet Anh</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>FlowVocoder: A small Footprint Neural Vocoder based Normalizing Flow for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1576</first_page>
						<last_page>1580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-272</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/luong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name>
					</contributors>
					<titles><title>Spatial Loss for Unsupervised Multi-channel Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>241</first_page>
						<last_page>245</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-274</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saijo22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanqing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruiqing</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1581</first_page>
						<last_page>1585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-277</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koharu</given_name>
<surname>Horii</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meiko</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kengo</given_name>
<surname>Ohta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryota</given_name>
<surname>Nishimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Spontaneous Speech Recognition Using Disfluency Labeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4108</first_page>
						<last_page>4112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-281</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/horii22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuohuang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donald</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Shen</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on the Band Importance of Phase-aware Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4651</first_page>
						<last_page>4655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-284</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chamara</given_name>
<surname>Kasun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung Soo</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagath</given_name>
<surname>Rajapakse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiping</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guang-Bin</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Discriminative Adversarial Learning for Speaker Independent Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4975</first_page>
						<last_page>4979</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-285</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kasun22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingming</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Tuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>AdaVocoder: Adaptive Vocoder for Custom Voice</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1586</first_page>
						<last_page>1590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-288</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yuan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Suo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>PRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1431</first_page>
						<last_page>1435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-289</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zheng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeqian</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiu-shi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>MingHui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ZhouWang</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>A Complementary Joint Training Approach Using Unpaired Speech and Text A Complementary Joint Training Approach Using Unpaired Speech and Text</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2613</first_page>
						<last_page>2617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-291</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/du22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Saeki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>793</first_page>
						<last_page>797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-294</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saeki22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Mitsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kei</given_name>
<surname>Sawada</surname>
</person_name>
					</contributors>
					<titles><title>MSR-NV: Neural Vocoder Using Multiple Sampling Rates</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>798</first_page>
						<last_page>802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-295</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mitsui22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanxin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengdong</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>An Anchor-Free Detector for Continuous Speech Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3228</first_page>
						<last_page>3232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-296</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Saeki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiko</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoko</given_name>
<surname>Tanji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4406</first_page>
						<last_page>4410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-298</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saeki22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junghun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoojin</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihie</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>136</first_page>
						<last_page>140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-299</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Nishimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5155</first_page>
						<last_page>5159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-300</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saito22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Koizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Yatabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Bacchiani</surname>
</person_name>
					</contributors>
					<titles><title>SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>803</first_page>
						<last_page>807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-301</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/koizumi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Koizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sankaran</given_name>
<surname>Panchapagesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Bacchiani</surname>
</person_name>
					</contributors>
					<titles><title>SNRi Target Training for Joint Speech Enhancement and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1173</first_page>
						<last_page>1177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-302</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/koizumi22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsukasa</given_name>
<surname>Yoshinaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kikuo</given_name>
<surname>Maekawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akiyoshi</given_name>
<surname>Iida</surname>
</person_name>
					</contributors>
					<titles><title>Variability in Production of Non-Sibilant Fricative [ç] in /hi/</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>620</first_page>
						<last_page>624</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-303</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yoshinaga22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Speaker consistency loss and step-wise optimization for semi-supervised joint training of TTS and ASR using unpaired text data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>526</first_page>
						<last_page>530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-304</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/makishima22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yutaro</given_name>
<surname>Sanada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takumi</given_name>
<surname>Nakagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuichiro</given_name>
<surname>Wada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kosaku</given_name>
<surname>Takanashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiichi</given_name>
<surname>Tokuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Kanamori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomonori</given_name>
<surname>Yamada</surname>
</person_name>
					</contributors>
					<titles><title>Deep Self-Supervised Learning of Speech Denoising from Noisy Speeches</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1178</first_page>
						<last_page>1182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-306</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sanada22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songbin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jigang</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1501</first_page>
						<last_page>1505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-307</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sangjun</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kihyun</given_name>
<surname>Choo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joohyung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton V.</given_name>
<surname>Porov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantin</given_name>
<surname>Osipov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June Sig</given_name>
<surname>Sung</surname>
</person_name>
					</contributors>
					<titles><title>Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>808</first_page>
						<last_page>812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-310</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/park22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yael</given_name>
<surname>Segal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kasia</given_name>
<surname>Hitczenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Goldrick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Buchwald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Roberts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>DDKtor: Automatic Diadochokinetic Speech Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4611</first_page>
						<last_page>4615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-311</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/segal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Corentin</given_name>
<surname>Puffay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jana</given_name>
<surname>Van Canneyt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Vanthornhout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Francart</surname>
</person_name>
					</contributors>
					<titles><title>Relating the fundamental frequency of speech with EEG using a dilated convolutional network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4038</first_page>
						<last_page>4042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-315</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/puffay22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuma</given_name>
<surname>Iwamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rintaro</given_name>
<surname>Ikeshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeru</given_name>
<surname>Katagiri</surname>
</person_name>
					</contributors>
					<titles><title>How bad are artifacts?: Analyzing the impact of speech enhancement errors on ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5418</first_page>
						<last_page>5422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-318</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/iwamoto22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jucai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingwei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingbiao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruidong</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanping</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weizhen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yapeng</given_name>
<surname>Mao</surname>
</person_name>
					</contributors>
					<titles><title>The CLIPS System for 2022 Spoofing-Aware Speaker Verification Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4367</first_page>
						<last_page>4370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-320</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Proszewska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Beringer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Sáez-Trigueros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Merritt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelhamid</given_name>
<surname>Ezzerg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name>
					</contributors>
					<titles><title>GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2973</first_page>
						<last_page>2977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-322</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/proszewska22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fumio</given_name>
<surname>Nihei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Ishii</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukiko</given_name>
<surname>Nakano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyosuke</given_name>
<surname>Nishida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Fukayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takao</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Dialogue Acts Aided Important Utterance Detection Based on Multiparty and Multimodal Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1086</first_page>
						<last_page>1090</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-324</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nihei22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yasuhito</given_name>
<surname>Ohsugi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itsumi</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyosuke</given_name>
<surname>Nishida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sen</given_name>
<surname>Yoshida</surname>
</person_name>
					</contributors>
					<titles><title>Japanese ASR-Robust Pre-trained Language Model with Pseudo-Error Sentences Generated by Grapheme-Phoneme Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2688</first_page>
						<last_page>2692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-327</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ohsugi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Einari</given_name>
<surname>Vaaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manu</given_name>
<surname>Airaksinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in Clustering-Based Active Learning for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1143</first_page>
						<last_page>1147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-329</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vaaras22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lars</given_name>
<surname>Rumberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Gebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Ehlert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maren</given_name>
<surname>Wallbaum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena</given_name>
<surname>Bornholt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörn</given_name>
<surname>Ostermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ulrike</given_name>
<surname>Lüdtke</surname>
</person_name>
					</contributors>
					<titles><title>kidsTALC: A Corpus of 3- to 11-year-old German Children’s Connected Natural Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5160</first_page>
						<last_page>5164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-330</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rumberg22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lars</given_name>
<surname>Rumberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Gebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Ehlert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ulrike</given_name>
<surname>Lüdtke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörn</given_name>
<surname>Ostermann</surname>
</person_name>
					</contributors>
					<titles><title>Improving Phonetic Transcriptions of Children’s Speech by Pronunciation Modelling with Constrained CTC-Decoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1357</first_page>
						<last_page>1361</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-332</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rumberg22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Byeongseon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Accent Estimation Method Based on Multi-Task Learning for Japanese Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1931</first_page>
						<last_page>1935</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-334</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/park22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kak</given_name>
<surname>Soky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenhui</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Simultaneous Translation for Enhancing Transcription of Low-resource Language via Cross Attention Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1362</first_page>
						<last_page>1366</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-343</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/soky22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiřı́</given_name>
<surname>Martı́nek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Cerisara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Kral</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Lenc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef</given_name>
<surname>Baloun</surname>
</person_name>
					</contributors>
					<titles><title>Weak supervision for Question Type Detection with large language models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3283</first_page>
						<last_page>3287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-345</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/martnek22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sanae</given_name>
<surname>Matsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyoji</given_name>
<surname>Iwamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reiko</given_name>
<surname>Mazuka</surname>
</person_name>
					</contributors>
					<titles><title>Development of allophonic realization until adolescence: A production study of the affricate-fricative variation of /z/ among Japanese children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>739</first_page>
						<last_page>743</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-346</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/matsui22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sebastian Peter</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Roccabruna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tommaso</given_name>
<surname>Ciulli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morena</given_name>
<surname>Danieli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Riccardi</surname>
</person_name>
					</contributors>
					<titles><title>What can Speech and Language Tell us About the Working Alliance in Psychotherapy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2443</first_page>
						<last_page>2447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-347</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bayerl22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lizhong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sijun</given_name>
<surname>Bi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianqian</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuyue</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Human Sound Classification based on Feature Fusion Method with Air and Bone Conducted Signal</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1506</first_page>
						<last_page>1510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-348</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengyuan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxiao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Guo</surname>
</person_name>
					</contributors>
					<titles><title>RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1591</first_page>
						<last_page>1595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-349</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valentin</given_name>
<surname>Pelloin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Hervé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoit</given_name>
<surname>Favre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Camelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>LAURENT</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3453</first_page>
						<last_page>3457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-352</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pelloin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marcely</given_name>
<surname>Zanon Boito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1278</first_page>
						<last_page>1282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-353</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zanonboito22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy</given_name>
<surname>Khong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzy</given_name>
<surname>Styles</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>PHO-LID: A Unified Model Incorporating Acoustic-Phonetic and Phonotactic Information for Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2233</first_page>
						<last_page>2237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-354</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Lyu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huijia</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Ant Multilingual Recognition System for OLR 2021 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3684</first_page>
						<last_page>3688</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-355</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lyu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Weise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Heismann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4815</first_page>
						<last_page>4819</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-356</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/weise22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Munhak</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Eon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-Seok</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanhee</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haeyoung</given_name>
<surname>Kwon</surname>
</person_name>
					</contributors>
					<titles><title>Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>56</first_page>
						<last_page>60</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-362</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri</given_name>
<surname>Karlapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Penny</given_name>
<surname>Karanasou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Łajszczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed</given_name>
<surname>Ammar Abbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ray</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arent</given_name>
<surname>van Korlaar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Slangen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name>
					</contributors>
					<titles><title>CopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many Fine-Grained Prosody Transfer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3363</first_page>
						<last_page>3367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-367</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/karlapati22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marise</given_name>
<surname>Neijman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Femke</given_name>
<surname>Hof</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noelle</given_name>
<surname>Oosterom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Pfau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bertus</given_name>
<surname>van Rooy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rob J.J.H.</given_name>
<surname>van Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel M.W.M.</given_name>
<surname>van den Brekel</surname>
</person_name>
					</contributors>
					<titles><title>Compensation in Verbal and Nonverbal Communication after Total Laryngectomy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3613</first_page>
						<last_page>3617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-369</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/neijman22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maureen</given_name>
<surname>de Seyssel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Lavechin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Wisniewski</surname>
</person_name>
					</contributors>
					<titles><title>Probing phoneme, language and speaker information in unsupervised speech representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1402</first_page>
						<last_page>1406</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-373</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/deseyssel22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shinimol</given_name>
<surname>Salim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed</given_name>
<surname>Shahnawazuddin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Waquar</given_name>
<surname>Ahmad</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speaker Verification System for Dysarthria Patients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5070</first_page>
						<last_page>5074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-375</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/salim22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fernau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Hillmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nils</given_name>
<surname>Feldhus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name>
					</contributors>
					<titles><title>Towards Automated Dialog Personalization using MBTI Personality Indicators</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1968</first_page>
						<last_page>1972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-376</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fernau22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>YUHANG</given_name>
<surname>HE</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Markham</surname>
</person_name>
					</contributors>
					<titles><title>SoundDoA: Learn Sound Source Direction of Arrival and Semantics from Sound Raw Waveforms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2408</first_page>
						<last_page>2412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-378</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/he22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed</given_name>
<surname>Ammar Abbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Łajszczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Joly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Karlapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Penny</given_name>
<surname>Karanasou</surname>
</person_name>
					</contributors>
					<titles><title>Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3368</first_page>
						<last_page>3372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-379</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/makarov22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary Geunbae</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>SF-DST: Few-Shot Self-Feeding Reading Comprehension Dialogue State Tracking with Auxiliary Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1233</first_page>
						<last_page>1237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-380</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adrien</given_name>
<surname>Pupier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximin</given_name>
<surname>Coavoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Lecouteux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jerome</given_name>
<surname>Goulian</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Dependency Parsing of Spoken French</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1816</first_page>
						<last_page>1820</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-381</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pupier22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Geoffrey T.</given_name>
<surname>Frost</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant</given_name>
<surname>Theron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>TB or not TB? Acoustic cough analysis for tuberculosis classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2448</first_page>
						<last_page>2452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-383</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/frost22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Syed</given_name>
<surname>Ammar Abbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Merritt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Karlapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewa</given_name>
<surname>Muszynska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Slangen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elia</given_name>
<surname>Gatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name>
					</contributors>
					<titles><title>Expressive, Variable, and Controllable Duration Modelling in TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4546</first_page>
						<last_page>4550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-384</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ammarabbas22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oralie</given_name>
<surname>Cattan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Ghannay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Servan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Rosset</surname>
</person_name>
					</contributors>
					<titles><title>Benchmarking Transformers-based models on French Spoken Language Understanding tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1238</first_page>
						<last_page>1242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-385</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cattan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Bîrlădeanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Minnis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Reactive Attachment Disorder Through Turn-Taking Analysis in Clinical Child-Caregiver Sessions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1407</first_page>
						<last_page>1410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-387</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/birladeanu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raul</given_name>
<surname>Fernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Haws</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guy</given_name>
<surname>Lorberbom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slava</given_name>
<surname>Shechtman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Sorin</surname>
</person_name>
					</contributors>
					<titles><title>Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5488</first_page>
						<last_page>5492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-388</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fernandez22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Golan</given_name>
<surname>Pundak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsendsuren</given_name>
<surname>Munkhdalai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name>
					</contributors>
					<titles><title>On-the-fly ASR Corrections with Audio Exemplars</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3148</first_page>
						<last_page>3152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-389</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pundak22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shalini</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Content-Context Factorized Representations for Automated Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>61</first_page>
						<last_page>65</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-390</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hagen</given_name>
<surname>Soltau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Izhak</given_name>
<surname>Shafran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent El</given_name>
<surname>Shafey</surname>
</person_name>
					</contributors>
					<titles><title>RNN Transducers for Named Entity Recognition with constraints on alignment for understanding medical conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1901</first_page>
						<last_page>1905</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-391</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/soltau22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michel Cardoso</given_name>
<surname>Meneses</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rafael Bérgamo</given_name>
<surname>Holanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis Vasconcelos</given_name>
<surname>Peres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriela Dantas</given_name>
<surname>Rocha</surname>
</person_name>
					</contributors>
					<titles><title>SiDi KWS: A Large-Scale Multilingual Dataset for Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4616</first_page>
						<last_page>4620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-394</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meneses22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kimiko</given_name>
<surname>Tsukada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yurong</given_name>
<surname>Yurong</surname>
</person_name>
					</contributors>
					<titles><title>Non-native Perception of Japanese Singleton/Geminate Contrasts: Comparison of Mandarin and Mongolian Speakers Differing in Japanese Experience</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3068</first_page>
						<last_page>3072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-397</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tsukada22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyun</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Data Selection via Discrete Speech Representation for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3393</first_page>
						<last_page>3397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-399</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Olivier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>Recent improvements of ASR models in the face of adversarial attacks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4113</first_page>
						<last_page>4117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-400</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/olivier22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Parvaneh</given_name>
<surname>Janbakhshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ina</given_name>
<surname>Kodrasi</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial-Free Speaker Identity-Invariant Representation Learning for Automatic Dysarthric Speech Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2138</first_page>
						<last_page>2142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-402</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/janbakhshi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Nishimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3373</first_page>
						<last_page>3377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-403</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nishimura22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pranay</given_name>
<surname>Manocha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyu</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Finkelstein</surname>
</person_name>
					</contributors>
					<titles><title>Audio Similarity is Unreliable as a Proxy for Audio Quality</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3553</first_page>
						<last_page>3557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-405</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/manocha22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pranay</given_name>
<surname>Manocha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjali</given_name>
<surname>Menon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel Degene</given_name>
<surname>Gebru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vamsi Krishna</given_name>
<surname>Ithapu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Calamia</surname>
</person_name>
					</contributors>
					<titles><title>SAQAM: Spatial Audio Quality Assessment Metric</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>649</first_page>
						<last_page>653</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-406</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/manocha22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pranay</given_name>
<surname>Manocha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name>
					</contributors>
					<titles><title>Speech Quality Assessment through MOS using Non-Matching References</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>654</first_page>
						<last_page>658</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-407</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/manocha22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dhanush</given_name>
<surname>Bekal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Acoustic Barge-In Classification for Spoken Dialog Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1091</first_page>
						<last_page>1095</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-408</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bekal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreyas</given_name>
<surname>Seshadri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuomo</given_name>
<surname>Raitio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Castellani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangchuan</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Emphasis Control for Parallel Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3378</first_page>
						<last_page>3382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-411</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/seshadri22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengchen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1006</first_page>
						<last_page>1010</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-412</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Fasoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mauricio</given_name>
<surname>Serrano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swagath</given_name>
<surname>Venkataramani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kailash</given_name>
<surname>Gopalakrishnan</surname>
</person_name>
					</contributors>
					<titles><title>Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2038</first_page>
						<last_page>2042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-413</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fasoli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Haws</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name>
					</contributors>
					<titles><title>VQ-T: RNN Transducers using Vector-Quantized Prediction Network States</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1656</first_page>
						<last_page>1660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-414</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shi22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mutian</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingzhou</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Soong</surname>
</person_name>
					</contributors>
					<titles><title>Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>441</first_page>
						<last_page>445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-420</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/he22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saida</given_name>
<surname>Mussakhojayeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huseyin</given_name>
<surname>Atakan Varol</surname>
</person_name>
					</contributors>
					<titles><title>KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1367</first_page>
						<last_page>1371</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-421</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mussakhojayeva22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyuhong</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonyong</given_name>
<surname>Sung</surname>
</person_name>
					</contributors>
					<titles><title>Similarity and Content-based Phonetic Self Attention for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4118</first_page>
						<last_page>4122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-422</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shim22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yijie</given_name>
<surname>Lou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Pu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianfeng</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinbo</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>A Deep One-Class Learning Method for Replay Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4765</first_page>
						<last_page>4769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-427</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lou22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruiyu</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiyuan</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Layer Similarity Knowledge Distillation for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>926</first_page>
						<last_page>930</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-429</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cheng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicola</given_name>
<surname>Pia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kishan</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Korse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Multrus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Fuchs</surname>
</person_name>
					</contributors>
					<titles><title>NESC: Robust Neural End-2-End Speech Coding with GANs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4212</first_page>
						<last_page>4216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-430</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pia22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongjie</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>WenWu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1511</first_page>
						<last_page>1515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-433</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaitao</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teng</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bixia</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiqiang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luna</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liping</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qun</given_name>
<surname>Lou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongsheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xudong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lili</given_name>
<surname>Qiu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Hypernasality Estimation with Automatic Speech Recognition in Cleft Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4820</first_page>
						<last_page>4824</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-438</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/song22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Saeki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Detai</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wataru</given_name>
<surname>Nakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4521</first_page>
						<last_page>4525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-439</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saeki22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Kunihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanbo</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noriko</given_name>
<surname>Nakanishi</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Learners' Listening Breakdown with Oral Dictation and Its Use to Model Listening Skill Improvement Exclusively Through Shadowing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4461</first_page>
						<last_page>4465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-440</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kunihara22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>DDS: A new device-degraded speech dataset for speech enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2913</first_page>
						<last_page>2917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-441</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan Vu</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quoc Huy</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Akagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name>
					</contributors>
					<titles><title>Vector-quantized Variational Autoencoder for Phase-aware Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>176</first_page>
						<last_page>180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-443</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ho22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wataru</given_name>
<surname>Nakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoko</given_name>
<surname>Tanji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2358</first_page>
						<last_page>2362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-444</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/takamichi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changchun</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Multi-source wideband DOA estimation method by frequency focusing and error weighting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5423</first_page>
						<last_page>5427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-445</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haodong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gongshen</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4770</first_page>
						<last_page>4774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-446</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuta</given_name>
<surname>Taniguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsuneo</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiro</given_name>
<surname>Tamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keiji</given_name>
<surname>Yasuda</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2813</first_page>
						<last_page>2817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-448</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/taniguchi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zewang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4252</first_page>
						<last_page>4256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-454</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phil</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2043</first_page>
						<last_page>2047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-461</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sun22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mariia</given_name>
<surname>Lesnichaia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veranika</given_name>
<surname>Mikhailava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Bogach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iurii</given_name>
<surname>Lezhenin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Blake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Pyshkin</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Accented English Using CNN Model Trained on Amplitude Mel-Spectrograms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3669</first_page>
						<last_page>3673</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-462</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lesnichaia22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>O'Brien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Meunier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alain</given_name>
<surname>Ghio</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the effects of modified speech on perceptual speaker identification performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3073</first_page>
						<last_page>3077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-463</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/obrien22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feifei</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiguang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwei</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Spectro-Temporal SubNet for Real-Time Monaural Speech Denoising and Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>931</first_page>
						<last_page>935</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-468</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xiong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Norm-constrained Score-level Ensemble for Spoofing Aware Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4371</first_page>
						<last_page>4375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-470</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinlong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Acoustic-to-Articulatory Inversion with Variable Vocal Tract Anatomy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4656</first_page>
						<last_page>4660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-477</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sun22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Attentive Feature Fusion for Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>286</first_page>
						<last_page>290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-478</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Dual Path Embedding Learning for Speaker Verification with Triplet Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>291</first_page>
						<last_page>295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-481</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhendong</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingchen</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuoyuan</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuping</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Niu</surname>
</person_name>
					</contributors>
					<titles><title>WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1661</first_page>
						<last_page>1665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-483</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>DF-ResNet: Boosting Speaker Verification Performance with Depth-First Design</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>296</first_page>
						<last_page>300</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-484</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Ruida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Shuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ma</given_name>
<surname>Chenguang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liang</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Rectangle Loss for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>301</first_page>
						<last_page>305</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-486</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ruida22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Achyut</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konark</given_name>
<surname>Paul</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Self Attention-Based Residual Network for Environmental Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1516</first_page>
						<last_page>1520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-488</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tripathi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenpeng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1596</first_page>
						<last_page>1600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-489</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/du22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carmen</given_name>
<surname>Llamas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Kettig</surname>
</person_name>
					</contributors>
					<titles><title>Eliciting and evaluating likelihood ratios for speaker recognition by human listeners under forensically realistic channel-mismatched conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5238</first_page>
						<last_page>5242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-490</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hughes22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feifei</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongfu</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinwei</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Joint Estimation of Direction-of-Arrival and Distance for Arrays with Directional Sensors based on Sparse Bayesian Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>871</first_page>
						<last_page>875</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-497</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xiong22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hieu</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Animesh</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Susanj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lokesh</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation via Module Replacing for Automatic Speech Recognition with Recurrent Neural Network Transducer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4436</first_page>
						<last_page>4440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-500</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixiu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rim</given_name>
<surname>Helaoui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Reforgiato Recupero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Riboni</surname>
</person_name>
					</contributors>
					<titles><title>Towards Automated Counselling Decision-Making: Remarks on Therapist Action Forecasting on the AnnoMI Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1906</first_page>
						<last_page>1910</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-506</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan-Lin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5343</first_page>
						<last_page>5347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-509</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdul Hameed</given_name>
<surname>Azeemi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ihsan Ayyub</given_name>
<surname>Qazi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agha Ali</given_name>
<surname>Raza</surname>
</person_name>
					</contributors>
					<titles><title>Dataset Pruning for Resource-constrained Spoofed Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>416</first_page>
						<last_page>420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-514</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/azeemi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juncheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhui</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-Yao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1521</first_page>
						<last_page>1525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-515</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruizhe</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sherif</given_name>
<surname>Abdulatif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>CMGAN: Conformer-based Metric GAN for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>936</first_page>
						<last_page>940</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-517</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bruce Xiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name>
					</contributors>
					<titles><title>Reducing uncertainty at the score-to-LR stage in likelihood ratio-based forensic voice comparison using automatic speaker recognition systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5243</first_page>
						<last_page>5247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-518</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuan Po</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Kuan</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2193</first_page>
						<last_page>2197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-519</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ho-Hsiang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Fuentes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prem</given_name>
<surname>Seetharaman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Pablo</given_name>
<surname>Bello</surname>
</person_name>
					</contributors>
					<titles><title>How to Listen? Rethinking Visual Sound Localization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>876</first_page>
						<last_page>880</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-520</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soha</given_name>
<surname>Nossier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Wall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mansour</given_name>
<surname>Moniri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cornelius</given_name>
<surname>Glackin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nigel</given_name>
<surname>Cannings</surname>
</person_name>
					</contributors>
					<titles><title>Convolutional Recurrent Smart Speech Enhancement Architecture for Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5428</first_page>
						<last_page>5432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-522</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nossier22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Axel</given_name>
<surname>Berg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>O'Connor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kalle</given_name>
<surname>Åström</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magnus</given_name>
<surname>Oskarsson</surname>
</person_name>
					</contributors>
					<titles><title>Extending GCC-PHAT using Shift Equivariant Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1791</first_page>
						<last_page>1795</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-524</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/berg22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanhang</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizhong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satwinder</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruili</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CyclicAugment: Speech Data Random Augmentation with Cosine Annealing Scheduler for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3859</first_page>
						<last_page>3863</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-526</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chi-Chang</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-Hung</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chen</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chu-Song</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>NASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1183</first_page>
						<last_page>1187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-527</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huy</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Mean Opinion Score Estimation with Temporal Modulation Features on Gammatone Filterbank for Speech Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4526</first_page>
						<last_page>4530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-528</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nguyen22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arindam</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Fuhs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deblin</given_name>
<surname>Bagchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Farahani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monika</given_name>
<surname>Woszczyna</surname>
</person_name>
					</contributors>
					<titles><title>Low-resource Low-footprint Wake-word Detection using Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3739</first_page>
						<last_page>3743</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-529</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ghosh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sung-Lin</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Autoregressive Co-Training for Learning Discrete Speech Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5000</first_page>
						<last_page>5004</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-530</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yeh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Word-wise Sparse Attention for Multimodal Sentiment Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1973</first_page>
						<last_page>1977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-532</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/qian22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglai</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5493</first_page>
						<last_page>5497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-534</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengxi</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caixia</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Prompt-based Re-ranking Language Model for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3864</first_page>
						<last_page>3868</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-536</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nie22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Jurgens</surname>
</person_name>
					</contributors>
					<titles><title>ByT5 model for massively multilingual grapheme-to-phoneme conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>446</first_page>
						<last_page>450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-538</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jounghee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pilsung</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4945</first_page>
						<last_page>4949</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-547</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joosung</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>141</first_page>
						<last_page>145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-551</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Parameter Sharing in Multilingual Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1731</first_page>
						<last_page>1735</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-552</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Bellows</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothy W.</given_name>
<surname>Leishman</surname>
</person_name>
					</contributors>
					<titles><title>Effect of Head Orientation on Speech Directivity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>246</first_page>
						<last_page>250</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-553</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bellows22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Satwik</given_name>
<surname>Dutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah Anne</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob C.</given_name>
<surname>Reyna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rebecca Elizabeth</given_name>
<surname>Hacker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dwight W.</given_name>
<surname>Irvin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay F.</given_name>
<surname>Buzhardt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Challenges remain in Building ASR for Spontaneous Preschool Children Speech in Naturalistic Educational Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4322</first_page>
						<last_page>4326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-555</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dutta22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin-Chun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin-Lin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoming</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingshuai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinchuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunfeng</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Le</given_name>
<surname>Gan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>De-Chuan</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Avoid Overfitting User Specific Information in Federated Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3869</first_page>
						<last_page>3873</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-558</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jieun</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hae-Sung</given_name>
<surname>Jeon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jieun</given_name>
<surname>Kiaer</surname>
</person_name>
					</contributors>
					<titles><title>Use of prosodic and lexical cues for disambiguating wh-words in Korean</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>81</first_page>
						<last_page>85</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-561</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/song22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shanshan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>306</first_page>
						<last_page>310</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-563</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seiya</given_name>
<surname>Kawano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muteki</given_name>
<surname>Arioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akishige</given_name>
<surname>Yuguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenta</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Yoshino</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Persuasive Dialogue Corpus using Teleoperated Android</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2308</first_page>
						<last_page>2312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-565</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kawano22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Turn-Taking Prediction for Natural Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1821</first_page>
						<last_page>1825</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-566</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tarun</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuan Duc</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tran The</given_name>
<surname>Anh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1978</first_page>
						<last_page>1982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-567</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gupta22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guru</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Stambler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shyam</given_name>
<surname>Upadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manaal</given_name>
<surname>Faruqui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Intended Query Detection using E2E Modeling for Continued Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1826</first_page>
						<last_page>1830</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-569</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liumeng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Na</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2548</first_page>
						<last_page>2552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-570</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xue22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>SiCheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Methawee</given_name>
<surname>Tantrawenith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haolin</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aolan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaizhen</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xintao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2553</first_page>
						<last_page>2557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-571</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arjit</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pranay Reddy</given_name>
<surname>Samala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maneesh</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>SPLICEOUT: A Simple and Efficient Audio Augmentation Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2678</first_page>
						<last_page>2682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-572</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jain22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shushan</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yumei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Low-complex and Highly-performed Binary Residual Neural Network for Small-footprint Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3233</first_page>
						<last_page>3237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-573</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puneet</given_name>
<surname>Mathur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dernoncourt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan Hung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiuxiang</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ani</given_name>
<surname>Nenkova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vlad</given_name>
<surname>Morariu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh</given_name>
<surname>Manocha</surname>
</person_name>
					</contributors>
					<titles><title>DocLayoutTTS: Dataset and Baselines for Layout-informed Document-level Neural Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>451</first_page>
						<last_page>455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-574</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mathur22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryu</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Nakadai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Komatani</surname>
</person_name>
					</contributors>
					<titles><title>Empirical Sampling from Latent Utterance-wise Evidence Model for Missing Data ASR based on Neural Encoder-Decoder Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3789</first_page>
						<last_page>3793</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-576</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/takeda22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yule</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junshi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongpeng</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xiang</surname>
</person_name>
					</contributors>
					<titles><title>FlowCPCVC: A Contrastive Predictive Coding Supervised Flow Framework for Any-to-Any Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2558</first_page>
						<last_page>2562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-577</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyeon-Kyeong</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyewon</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1871</first_page>
						<last_page>1875</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-580</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juntae</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeehye</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4123</first_page>
						<last_page>4127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-581</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name>
					</contributors>
					<titles><title>An overview of discourse clicks in Central Swedish</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3423</first_page>
						<last_page>3427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-583</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zellers22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Santoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Yamada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenkichi</given_name>
<surname>Ishizuka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiichi</given_name>
<surname>Hashimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoji</given_name>
<surname>Makino</surname>
</person_name>
					</contributors>
					<titles><title>Performance Improvement of Speech Emotion Recognition by Neutral Speech Detection Using Autoencoder and Intermediate Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4700</first_page>
						<last_page>4704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-584</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/santoso22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroto</given_name>
<surname>Noguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanae</given_name>
<surname>Matsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Watabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuyu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayako</given_name>
<surname>Hashimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ai</given_name>
<surname>Mizoguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mafuyu</given_name>
<surname>Kitahara</surname>
</person_name>
					</contributors>
					<titles><title>VOT and F0 perturbations for the realization of voicing contrast in Tohoku Japanese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3428</first_page>
						<last_page>3432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-587</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/noguchi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang-Rui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Class-Aware Distribution Alignment based Unsupervised Domain Adaptation for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3689</first_page>
						<last_page>3693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-591</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gholamreza</given_name>
<surname>Haffari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Shareghi</surname>
</person_name>
					</contributors>
					<titles><title>M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>111</first_page>
						<last_page>115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-592</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guan-Ting</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2198</first_page>
						<last_page>2202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-600</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lin22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-Ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-seo</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>Two Methods for Spoofing-Aware Speaker Verification: Multi-Layer Perceptron Score Fusion Model and Integrated Embedding Projector</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2878</first_page>
						<last_page>2882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-602</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/heo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Shchekotov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel K.</given_name>
<surname>Andreev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Ivanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aibek</given_name>
<surname>Alanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Vetrov</surname>
</person_name>
					</contributors>
					<titles><title>FFC-SE: Fast Fourier Convolution for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1188</first_page>
						<last_page>1192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-603</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shchekotov22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yufei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weibin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Internal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1666</first_page>
						<last_page>1670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-606</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>UniKW-AT: Unified Keyword Spotting and Audio Tagging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3238</first_page>
						<last_page>3242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-607</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dinkel22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinsheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qicong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhichao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqi</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5498</first_page>
						<last_page>5502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-610</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guan-Ting</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yung-Sung</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ho-Lam</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shu-wen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsuan-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuyan Annie</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin-shan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5165</first_page>
						<last_page>5169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-612</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lin22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Talia</given_name>
<surname>Ben Simon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Kreuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faten</given_name>
<surname>Awwad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob T.</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Correcting Mispronunciations in Speech using Spectrogram Inpainting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1208</first_page>
						<last_page>1212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-615</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bensimon22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huayi</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Phoneme-based Modeling for Text-independent Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4775</first_page>
						<last_page>4779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-617</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyuan</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Model Compression by Iterative Pruning with Knowledge Distillation and Its Application to Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>941</first_page>
						<last_page>945</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-619</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wei22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guangyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaitao</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuzi</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanqing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>456</first_page>
						<last_page>460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-621</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weixin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Fully Automatic Balance between Directivity Factor and White Noise Gain for Large-scale Microphone Arrays in Diffuse Noise Fields</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5433</first_page>
						<last_page>5437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-625</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Badr M.</given_name>
<surname>Abdullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Form and Meaning: A Multi-Task Learning Model for Acoustic Word Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1876</first_page>
						<last_page>1880</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-626</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/abdullah22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhikai</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Transfer and Distillation from Autoregressive to Non-Autoregessive Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2618</first_page>
						<last_page>2622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-632</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyang</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge distillation for In-memory keyword spotting model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4128</first_page>
						<last_page>4132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-633</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/song22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuwu</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4705</first_page>
						<last_page>4709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-637</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wataru</given_name>
<surname>Nakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>Predicting VQVAE-based Character Acting Style from Quotation-Annotated Text for Audiobook Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4551</first_page>
						<last_page>4555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-638</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nakata22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Darshana</given_name>
<surname>Priyasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andi</given_name>
<surname>Partovi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sridha</given_name>
<surname>Sridharan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maryam</given_name>
<surname>Kashefpoor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharindu</given_name>
<surname>Fernando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Denman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clinton</given_name>
<surname>Fookes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Kaye</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Heart Failure Through Voice Analysis using Self-Supervised Mode-Based Memory Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2848</first_page>
						<last_page>2852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-643</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/priyasad22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuansheng</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guochen</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>565</first_page>
						<last_page>569</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-644</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leying</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Enroll-Aware Attentive Statistics Pooling for Target Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>311</first_page>
						<last_page>315</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-645</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Na</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weng</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1436</first_page>
						<last_page>1440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-648</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/qin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guolong</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diyuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>External Text Based Data Augmentation for Low-Resource Speech Recognition in the Constrained Condition of OpenASR21 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4860</first_page>
						<last_page>4864</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-649</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Kai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da-Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han-Jia</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>De-Chuan</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Generalized Few-Shot Learning with Prototype-Based Co-Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>531</first_page>
						<last_page>535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-652</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zongfeng</given_name>
<surname>Quan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nick J.C.</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>FFM: A Frame Filtering Mechanism To Accelerate Inference Speed For Conformer In Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3153</first_page>
						<last_page>3157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-656</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/quan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sunmook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Il-Youp</given_name>
<surname>Kwak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungsang</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>Overlapped Frequency-Distributed Network: Frequency-Aware Voice Spoofing Countermeasure</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3558</first_page>
						<last_page>3562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-657</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/choi22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingjing</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Spoken Language Understanding with Cross-Modal Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2693</first_page>
						<last_page>2697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-658</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinlong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Inference of Physiologically Meaningful Articulatory Trajectories with VocalTractLab</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4661</first_page>
						<last_page>4665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-659</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sun22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyuan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongjun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>ASR Error Correction with Constrained Decoding on Operation Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3874</first_page>
						<last_page>3878</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-660</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christin</given_name>
<surname>Kirchhübel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgina</given_name>
<surname>Brown</surname>
</person_name>
					</contributors>
					<titles><title>Spoofed speech from the perspective of a forensic phonetician</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1308</first_page>
						<last_page>1312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-661</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kirchhubel22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anuroop</given_name>
<surname>Sriram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name>
					</contributors>
					<titles><title>Wav2Vec-Aug: Improved self-supervised training with limited data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4950</first_page>
						<last_page>4954</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-667</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sriram22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoxue</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Ke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Bifurcation and Reunion: A Loss-Guided Two-Stage Approach for Monaural Speech Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2503</first_page>
						<last_page>2507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-668</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/luo22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linjuan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuquan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renhua</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A deep complex multi-frame filtering network for stereophonic acoustic echo cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2508</first_page>
						<last_page>2512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-669</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cheng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amir</given_name>
<surname>Ivry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baruch</given_name>
<surname>Berdugo</surname>
</person_name>
					</contributors>
					<titles><title>Objective Metrics to Evaluate Residual-Echo Suppression During Double-Talk in the Stereophonic Case</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5348</first_page>
						<last_page>5352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-673</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ivry22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Improving Target Sound Extraction with Timestamp Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1526</first_page>
						<last_page>1530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-676</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingjian</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Online Target Speaker Voice Activity Detection for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1441</first_page>
						<last_page>1445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-677</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junhao</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Swithboard Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2128</first_page>
						<last_page>2132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-678</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jincen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ru</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>371</first_page>
						<last_page>375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-679</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>DENG</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boyang</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Confidence Score Based Conformer Speaker Adaptation for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2623</first_page>
						<last_page>2627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-680</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/deng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiqiao</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kongyang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junpeng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongcheng</given_name>
<surname>Fu</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Multi-task Learning Based Gender Recognition and Age Estimation for Class-imbalanced Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1983</first_page>
						<last_page>1987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-682</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zheng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Cong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name>
					</contributors>
					<titles><title>Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2563</first_page>
						<last_page>2567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-684</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lei22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaofei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruihua</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian-Yun</given_name>
<surname>Nie</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Context-aware Style Representation for Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5503</first_page>
						<last_page>5507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-686</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chelsea</given_name>
<surname>Krantsevich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriela</given_name>
<surname>Stegmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shira</given_name>
<surname>Hahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name>
					</contributors>
					<titles><title>Are reported accuracies in the clinical speech machine learning literature overoptimistic?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2453</first_page>
						<last_page>2457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-691</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/berisha22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Or</given_name>
<surname>Tal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moshe</given_name>
<surname>Mandel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Kreuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name>
					</contributors>
					<titles><title>A Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1193</first_page>
						<last_page>1197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-695</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boyang</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Two-pass Decoding and Cross-adaptation Based System Combination of End-to-end Conformer and Hybrid TDNN ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3158</first_page>
						<last_page>3162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-696</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cui22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>junfeng</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wanyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufeng</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Bring dialogue-context into RNN-T for streaming ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2048</first_page>
						<last_page>2052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-697</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hou22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuyi</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehua</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingjiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Coarse-Grained Attention Fusion With Joint Training Framework for Complex Speech Enhancement and End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3794</first_page>
						<last_page>3798</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-698</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhuang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joel</given_name>
<surname>Rixen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Renz</surname>
</person_name>
					</contributors>
					<titles><title>QDPN - Quasi-dual-path Network for single-channel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5353</first_page>
						<last_page>5357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-700</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rixen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanyu</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anh Tuan</given_name>
<surname>Luu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhuo</given_name>
<surname>Dong</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1988</first_page>
						<last_page>1992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-703</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wei22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keqi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanli</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1671</first_page>
						<last_page>1675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-707</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaituo</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1676</first_page>
						<last_page>1680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-709</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bai22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zi</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Conformer Based Elderly Speech Recognition System for Alzheimer’s Disease Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4825</first_page>
						<last_page>4829</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-712</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Min</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Distilling a Pretrained Language Model to a Multilingual ASR Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2203</first_page>
						<last_page>2207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-716</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/choi22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingxuan</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runyan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehui</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanli</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Improving Recognition of Out-of-vocabulary Words in E2E Code-switching ASR by Fusing Speech Generation Methods</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3163</first_page>
						<last_page>3167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-719</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ye22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Decoupled Federated Learning for ASR with Non-IID Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2628</first_page>
						<last_page>2632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-720</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan Him Nok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dehua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harold</given_name>
<surname>Chui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Luk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolette Wing Tung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koonkan</given_name>
<surname>Fung</surname>
</person_name>
					</contributors>
					<titles><title>Durational Patterning at Discourse Boundaries in Relation to Therapist Empathy in Psychotherapy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5248</first_page>
						<last_page>5252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-722</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zi</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Exploring linguistic feature and model combination for speech recognition based automatic AD detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3328</first_page>
						<last_page>3332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-723</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulei</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhilan</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuangjian</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4152</first_page>
						<last_page>4156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-726</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehui</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runyan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingxuan</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaohui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingqing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1736</first_page>
						<last_page>1740</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-729</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>He</given_name>
<surname>Mengnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenxing</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhang</given_name>
<surname>Ruixiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gong</given_name>
<surname>Caixia</surname>
</person_name>
					</contributors>
					<titles><title>Improving GAN-based vocoder for fast and high-quality speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1601</first_page>
						<last_page>1605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-730</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mengnan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niko</given_name>
<surname>Brummer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Swart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mosner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldrich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Probabilistic Spherical Discriminant Analysis: An Alternative to PLDA for length-normalized embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1446</first_page>
						<last_page>1450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-731</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/brummer22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felix</given_name>
<surname>Weninger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Gaudesi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Akmal</given_name>
<surname>Haidar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicola</given_name>
<surname>Ferri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Andrés-Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puming</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Conformer with dual-mode chunked attention for joint online and offline ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2053</first_page>
						<last_page>2057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-733</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/weninger22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolás</given_name>
<surname>Serrano</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Domain mismatch in Self-supervised speech pre-training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3028</first_page>
						<last_page>3032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-736</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baskar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaesung</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhyeok</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taejun</given_name>
<surname>Bak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Sun</given_name>
<surname>Joo</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural Non-Autoregressive Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>813</first_page>
						<last_page>817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-737</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bae22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Running</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangtao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingle</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edith C. H.</given_name>
<surname>Ngai</surname>
</person_name>
					</contributors>
					<titles><title>Radio2Speech: High Quality Speech Recovery from Radio Frequency Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4666</first_page>
						<last_page>4670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-738</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenhui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Pan</surname>
</person_name>
					</contributors>
					<titles><title>Single-channel speech enhancement using Graph Fourier Transform</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>946</first_page>
						<last_page>950</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-740</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Scene Classification Based on Multi-modal Graph Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4157</first_page>
						<last_page>4161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-741</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lei22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Speaker Verification Using Dynamic Loss-Gate and Label Correction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4780</first_page>
						<last_page>4784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-742</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/han22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin</given_name>
<surname>Sakuma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Fujie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Response Timing Estimation for Spoken Dialog System using Dialog Act Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4486</first_page>
						<last_page>4490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-746</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sakuma22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1011</first_page>
						<last_page>1015</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-748</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosi</given_name>
<surname>Shrem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Kreuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Formant Estimation and Tracking using Probabilistic Heat-Maps</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3563</first_page>
						<last_page>3567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-749</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shrem22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Maouche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brij Mohan Lal</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Vauquier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurélien</given_name>
<surname>Bellet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Tommasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Speech Privacy with Slicing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5025</first_page>
						<last_page>5029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-752</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/maouche22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murchana</given_name>
<surname>Baruah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bonny</given_name>
<surname>Banerjee</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition via Generation using an Attention-based Variational Recurrent Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4710</first_page>
						<last_page>4714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-753</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baruah22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shun</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianchao</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengru</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>Conformer Space Neural Architecture Search for Multi-Task Audio Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5358</first_page>
						<last_page>5362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-755</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Lenglet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Perrotin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gérard</given_name>
<surname>Bailly</surname>
</person_name>
					</contributors>
					<titles><title>Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>11</first_page>
						<last_page>15</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-759</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lenglet22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anderson R.</given_name>
<surname>Avila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khalil</given_name>
<surname>Bibi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui Heng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinlin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Low-bit Shift Network for End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2698</first_page>
						<last_page>2702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-760</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/avila22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixun</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>DENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3799</first_page>
						<last_page>3803</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-763</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guo22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiujuan</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunlong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-grained based Attention Network for Semi-supervised Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1531</first_page>
						<last_page>1535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-767</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hu22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rodolfo</given_name>
<surname>Zevallos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Núria</given_name>
<surname>Bel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillermo</given_name>
<surname>Cámbara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Farrús</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordi</given_name>
<surname>Luque</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation for Low-Resource Quechua ASR Improvement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3518</first_page>
						<last_page>3522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-770</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zevallos22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sanli</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keqi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingxuan</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation For CTC-based Speech Recognition Via Consistent Acoustic Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2633</first_page>
						<last_page>2637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-775</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tian22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valentin</given_name>
<surname>Gabeur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul Hongsuck</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arsha</given_name>
<surname>Nagrani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karteek</given_name>
<surname>Alahari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cordelia</given_name>
<surname>Schmid</surname>
</person_name>
					</contributors>
					<titles><title>AVATAR: Unconstrained Audiovisual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2818</first_page>
						<last_page>2822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-776</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gabeur22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yehoshua</given_name>
<surname>Dissen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Kreuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4013</first_page>
						<last_page>4017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-777</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dissen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zilu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongfu</given_name>
<surname>Ye</surname>
</person_name>
					</contributors>
					<titles><title>Joint Optimization of the Module and Sign of the Spectral Real Part Based on CRN for Speech Denoising.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>951</first_page>
						<last_page>955</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-778</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guo22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huaying</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiulian</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xue</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Towards Error-Resilient Neural Speech Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4217</first_page>
						<last_page>4221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-779</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xue22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ya-Hsin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Nung</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3458</first_page>
						<last_page>3462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-781</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peilin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dading</given_name>
<surname>Chong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingcheng</given_name>
<surname>Zeng</surname>
</person_name>
					</contributors>
					<titles><title>Calibrate and Refine! A Novel and Agile Framework for ASR Error Robust Intent Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1096</first_page>
						<last_page>1100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-786</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaesong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Memory-Efficient Training of RNN-Transducer with Sampled Softmax</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4441</first_page>
						<last_page>4445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-787</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Webber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel K.</given_name>
<surname>Lo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isaac L.</given_name>
<surname>Bleaman</surname>
</person_name>
					</contributors>
					<titles><title>REYD – The First Yiddish Text-to-Speech Dataset and System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2363</first_page>
						<last_page>2367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-789</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/webber22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hae-Sung</given_name>
<surname>Jeon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Nichols</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Prosodic Variation in British English Varieties using ProPer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1313</first_page>
						<last_page>1317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-792</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jeon22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Everson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anissa</given_name>
<surname>Gladney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Dialect Density Estimation for African American English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1283</first_page>
						<last_page>1287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-796</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/johnson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Chinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Skoglund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandan K. A.</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Ragano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Hines</surname>
</person_name>
					</contributors>
					<titles><title>Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4531</first_page>
						<last_page>4535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-799</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chinen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Yatabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ken-Ichi</given_name>
<surname>Sakakibara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kitamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Banno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanori</given_name>
<surname>Morise</surname>
</person_name>
					</contributors>
					<titles><title>An objective test tool for pitch extractors' response attributes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>659</first_page>
						<last_page>663</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-800</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kawahara22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theo</given_name>
<surname>Lepage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reda</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Label-Efficient Self-Supervised Speaker Verification With Information Maximization and Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4018</first_page>
						<last_page>4022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-802</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lepage22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junteng</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiyi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Shangguan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Seide</surname>
</person_name>
					</contributors>
					<titles><title>Federated Domain Adaptation for ASR with Full Self-Supervision</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>536</first_page>
						<last_page>540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-803</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jia22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Farhat</given_name>
<surname>Jabeen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Betz</surname>
</person_name>
					</contributors>
					<titles><title>Hesitations in Urdu/Hindi: Distribution and Properties of Fillers &#38; Silences</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4491</first_page>
						<last_page>4495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-805</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jabeen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Attentive Recurrent Network for Low-Latency Active Noise Control</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>956</first_page>
						<last_page>960</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-811</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Flemotomos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Clustering with Role Induced Constraints for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5075</first_page>
						<last_page>5079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-814</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/flemotomos22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junrui</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heting</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaizhi</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>461</first_page>
						<last_page>465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-816</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ni22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Janine</given_name>
<surname>Rugayan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name>
					</contributors>
					<titles><title>Semantically Meaningful Metrics for Norwegian ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2283</first_page>
						<last_page>2287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-817</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rugayan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Okan</given_name>
<surname>Köpüklü</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Taseska</surname>
</person_name>
					</contributors>
					<titles><title>ResectNet: An Efficient Architecture for Voice Activity Detection on Mobile Devices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5363</first_page>
						<last_page>5367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-820</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kopuklu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tohru</given_name>
<surname>Nagano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name>
					</contributors>
					<titles><title>Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2638</first_page>
						<last_page>2642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-821</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cui22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saket</given_name>
<surname>Dingliwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Shenoy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sravan</given_name>
<surname>Bodapati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi Teja</given_name>
<surname>Gadde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>684</first_page>
						<last_page>688</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-824</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dingliwal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuomo</given_name>
<surname>Raitio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petko</given_name>
<surname>Petkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangchuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammed</given_name>
<surname>Shifas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Davis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1936</first_page>
						<last_page>1940</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-825</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/raitio22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wilfried</given_name>
<surname>Michel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Training of Neural Transducer for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2058</first_page>
						<last_page>2062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-829</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenhua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Decoupled Pronunciation and Prosody Modeling in Meta-Learning-based Multilingual Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4257</first_page>
						<last_page>4261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-831</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bence</given_name>
<surname>Halpern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating bias against non-native accents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3168</first_page>
						<last_page>3172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-836</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liam</given_name>
<surname>Lonergan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neasa Ní</given_name>
<surname>Chiaráin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailbhe Ní</given_name>
<surname>Chasaide</surname>
</person_name>
					</contributors>
					<titles><title>Cross-dialect lexicon optimisation for an endangered language ASR system: the case of Irish</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4865</first_page>
						<last_page>4869</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-838</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lonergan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seong-Hwan</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>WonKee</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong-Hyeok</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1243</first_page>
						<last_page>1247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-839</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/heo22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timm</given_name>
<surname>Koppelmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Becker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandru</given_name>
<surname>Nelus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rene</given_name>
<surname>Glitza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lea</given_name>
<surname>Schönherr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>Clustering-based Wake Word Detection in Privacy-aware Acoustic Sensor Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>719</first_page>
						<last_page>723</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-842</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/koppelmann22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khalid</given_name>
<surname>Daoudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biswajit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solange</given_name>
<surname>Milhé de Saint Victor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Foubert-Samier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margherita</given_name>
<surname>Fabbri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Pavy-Le Traon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Rascol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wassilios G.</given_name>
<surname>Meissner</surname>
</person_name>
					</contributors>
					<titles><title>A comparative study on vowel articulation in Parkinson's disease and multiple system atrophy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2248</first_page>
						<last_page>2252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-845</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/daoudi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Barnhill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Perrin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>ORCA-WHISPER: An Automatic Killer Whale Sound Type Generation Toolkit Using Deep Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2413</first_page>
						<last_page>2417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-846</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bergler22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rubén</given_name>
<surname>Pérez Ramón</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Luisa</given_name>
<surname>Garcia Lecumberri</surname>
</person_name>
					</contributors>
					<titles><title>Generating iso-accented stimuli for second language research: methodology and a dataset for Spanish-accented English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1846</first_page>
						<last_page>1850</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-850</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/perezramon22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xufeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingmu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linjun</given_name>
<surname>Cai</surname>
</person_name>
					</contributors>
					<titles><title>Mandarin Lombard Grid: a Lombard-grid-like corpus of Standard Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3078</first_page>
						<last_page>3082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-854</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rikhye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>O’Malley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McGraw</surname>
</person_name>
					</contributors>
					<titles><title>Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3744</first_page>
						<last_page>3748</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-856</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ding22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenglin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Zhuang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hai</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Automated Detection of Wilson’s Disease Based on Improved Mel-frequency Cepstral Coefficients with Signal Decomposition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2143</first_page>
						<last_page>2147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-859</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandan</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishak</given_name>
<surname>Gopal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harishchandra</given_name>
<surname>Dubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergiy</given_name>
<surname>Matusevych</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Aichner</surname>
</person_name>
					</contributors>
					<titles><title>MusicNet: Compact Convolutional Neural Network for Real-time Background Music Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4162</first_page>
						<last_page>4166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-864</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/reddy22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min-Kyung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial and Sequential Training for Cross-lingual Prosody Transfer TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4556</first_page>
						<last_page>4560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-865</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bin</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>Deep speaker embedding with frame-constrained training strategy for speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1451</first_page>
						<last_page>1455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-867</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Hung</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Memory-Efficient Multi-Step Speech Enhancement with Neural ODE</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>961</first_page>
						<last_page>965</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-868</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuehai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianyi</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end Mispronunciation Detection with Simulated Error Distance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4327</first_page>
						<last_page>4331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-870</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Waibel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive multilingual speech recognition with pretrained models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3879</first_page>
						<last_page>3883</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-872</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pham22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai</given_name>
<surname>Zhen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hieu Duy</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raviteja</given_name>
<surname>Chinta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Susanj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tariq</given_name>
<surname>Afzal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network Accelerator with On-Device Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3033</first_page>
						<last_page>3037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-874</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengfei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhao</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaoping</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangjing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaguang</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changbin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinfeng</given_name>
<surname>Bai</surname>
</person_name>
					</contributors>
					<titles><title>TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1741</first_page>
						<last_page>1745</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-877</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuehai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianyi</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>BiCAPT: Bidirectional Computer-Assisted Pronunciation Training with Normalizing Flows</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4332</first_page>
						<last_page>4336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-878</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anoop</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pankaj Kumar</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Illa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Venkatapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhrangshu</given_name>
<surname>Nandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pritam</given_name>
<surname>Varma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Dwarakanath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aram</given_name>
<surname>Galstyan</surname>
</person_name>
					</contributors>
					<titles><title>Learning Under Label Noise for Robust Spoken Language Understanding systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3463</first_page>
						<last_page>3467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-880</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kumar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amber</given_name>
<surname>Afshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Attention-based conditioning methods using variable frame rate for style-robust speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2333</first_page>
						<last_page>2337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-882</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/afshan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amber</given_name>
<surname>Afshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Learning from human perception to improve automatic speaker verification in style-mismatched conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2338</first_page>
						<last_page>2342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-883</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/afshan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name>
					</contributors>
					<titles><title>Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4785</first_page>
						<last_page>4789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-885</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shi22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusheng</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Transport-Oriented Feature Aggregation for Speaker Embedding Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>316</first_page>
						<last_page>320</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-886</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tian22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan-Hao</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shifeng</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>SoftSpeech: Unsupervised Duration Model in FastSpeech 2</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1606</first_page>
						<last_page>1610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-887</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chung-Soo</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chamara</given_name>
<surname>Kasun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil</given_name>
<surname>Sivadas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagath</given_name>
<surname>Rajapakse</surname>
</person_name>
					</contributors>
					<titles><title>Recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>744</first_page>
						<last_page>748</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-888</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ahn22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mufan</given_name>
<surname>Sang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>321</first_page>
						<last_page>325</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-892</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohai</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>MA</given_name>
<surname>Zejun</surname>
</person_name>
					</contributors>
					<titles><title>Using Fluency Representation Learned from Sequential Raw Features for Improving Non-native Fluency Scoring</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4337</first_page>
						<last_page>4341</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-896</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruihua</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tie-Yan</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2568</first_page>
						<last_page>2572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-901</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Valin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Mustafa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Montgomery</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothy B.</given_name>
<surname>Terriberry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Klingbeil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paris</given_name>
<surname>Smaragdis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arvindh</given_name>
<surname>Krishnaswamy</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Packet Loss Concealment With Mixed Generative and Predictive Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>570</first_page>
						<last_page>574</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-903</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/valin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ZHENYU</given_name>
<surname>WANG</surname>
</person_name>
					</contributors>
					<titles><title>Audio Anti-spoofing Using Simple Attention Module and Joint Optimization Based on Additive Angular Margin Loss and Meta-learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>376</first_page>
						<last_page>380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-904</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hansen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun Kyung</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manami</given_name>
<surname>Hirayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaomi</given_name>
<surname>Kato</surname>
</person_name>
					</contributors>
					<titles><title>Perceived prominence and downstep in Japanese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1318</first_page>
						<last_page>1321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-908</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hwang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4870</first_page>
						<last_page>4874</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-909</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Subramani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Valin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umut</given_name>
<surname>Isik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paris</given_name>
<surname>Smaragdis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arvindh</given_name>
<surname>Krishnaswamy</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>818</first_page>
						<last_page>822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-912</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/subramani22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linjun</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xufeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyang</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>CS-CTCSCONV1D: Small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>326</first_page>
						<last_page>330</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-913</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cai22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyun</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenlin</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linhao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Token-level Speaker Change Detection Using Speaker Difference and Speech Content via Continuous Integrate-and-fire</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3749</first_page>
						<last_page>3753</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-914</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-level Acoustic Feature Extraction Framework for Transformer Based End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3173</first_page>
						<last_page>3177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-915</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Meta Auxiliary Learning for Low-resource Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2703</first_page>
						<last_page>2707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-916</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Spoofing-Aware Speaker Verification by Multi-Level Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4357</first_page>
						<last_page>4361</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-920</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Byeonggeun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inseop</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyung</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4621</first_page>
						<last_page>4625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-921</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tho Nguyen Duc</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>The Chuong</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vu</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Huu</given_name>
<surname>Bui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung Quoc</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>466</first_page>
						<last_page>470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-922</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tran22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunlei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>LAE: Language-Aware Encoder for Monolingual and Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3178</first_page>
						<last_page>3182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-923</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tian22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yooncheol</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilhwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongsun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeongyeol</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumi</given_name>
<surname>Maiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>TriniTTS: Pitch-controllable End-to-end TTS without External Aligner</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>16</first_page>
						<last_page>20</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-925</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ju22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Askar</given_name>
<surname>Hamdulla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Reliable Visualization for Deep Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>331</first_page>
						<last_page>335</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-926</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hoang</given_name>
<surname>Thi Thu Uyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nguyen Anh</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Duc Huy</surname>
</person_name>
					</contributors>
					<titles><title>Vietnamese Capitalization and Punctuation Recovery Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3884</first_page>
						<last_page>3888</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-931</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/thithuuyen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keqi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name>
					</contributors>
					<titles><title>Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1746</first_page>
						<last_page>1750</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-933</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/deng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaeuk</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>One-Shot Speaker Adaptation Based on Initialization by Generative Adversarial Networks for TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2978</first_page>
						<last_page>2982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-934</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Si-Ioi</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cymie Wing-Yee</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2853</first_page>
						<last_page>2857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-935</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Furu</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2643</first_page>
						<last_page>2647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-936</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeewoo</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyoung</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Bucy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungseock</given_name>
<surname>Joo</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Emotional Intensity in Political Debates via Non-verbal Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3983</first_page>
						<last_page>3987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-938</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yoon22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaobin</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiran</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weifeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>KaraTuner: Towards End-to-End Natural Pitch Correction for Singing Voice in Karaoke</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4262</first_page>
						<last_page>4266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-939</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhuang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seunghan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debasmit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janghoon</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyoungwoo</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungrack</given_name>
<surname>Yun</surname>
</person_name>
					</contributors>
					<titles><title>Domain Agnostic Few-shot Learning for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>595</first_page>
						<last_page>599</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-940</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longfei</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenqing</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiyi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Shinozaki</surname>
</person_name>
					</contributors>
					<titles><title>Augmented Adversarial Self-Supervised Learning for Early-Stage Alzheimer's Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>541</first_page>
						<last_page>545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-943</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingxuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Interrelate Training and Searching: A Unified Online Clustering Framework for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1456</first_page>
						<last_page>1460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-944</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>ESSumm: Extractive Speech Summarization from Untranscribed Meeting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3243</first_page>
						<last_page>3247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-945</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seunghan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeonggeun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inseop</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simyung</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Keyword Spotting through Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1881</first_page>
						<last_page>1885</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-947</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sangwook</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep Reddy</given_name>
<surname>Kothinti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mounya</given_name>
<surname>Elhilali</surname>
</person_name>
					</contributors>
					<titles><title>Temporal coding with magnitude-phase regularization for sound event detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1536</first_page>
						<last_page>1540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-950</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/park22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-Long</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1611</first_page>
						<last_page>1615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-952</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guo22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungmin</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jongmo</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungkwon</given_name>
<surname>Beack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngcheol</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Optimization of Deep Neural Network (DNN) Speech Coder Using a Multi Time Scale Perceptual Loss Function</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4411</first_page>
						<last_page>4415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-955</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/byun22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsiang-Yun Sherry</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasudha</given_name>
<surname>Kowtha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph Yitan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erdrin</given_name>
<surname>Azemi</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4715</first_page>
						<last_page>4719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-957</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mitra22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baishun</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhao</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Knowledge Distillation For Robust Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2708</first_page>
						<last_page>2712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-958</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>Convolutional Recurrent Neural Network with Auxiliary Stream for Robust Variable-Length Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2418</first_page>
						<last_page>2422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-959</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heyang</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinsheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongmao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengxiao</given_name>
<surname>Bi</surname>
</person_name>
					</contributors>
					<titles><title>Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker SVS by Learning from Singing Teacher</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4267</first_page>
						<last_page>4271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-960</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xue22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Gated Convolutional Fusion for Time-Domain Target Speaker Extraction Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5368</first_page>
						<last_page>5372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-961</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minseung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungchan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sein</given_name>
<surname>Cheong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>iDeepMMSE: An improved deep learning approach to MMSE speech and noise power spectrum estimation for speech enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>181</first_page>
						<last_page>185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-964</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>EDITnet: A Lightweight Network for Unsupervised Domain Adaptation in Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3694</first_page>
						<last_page>3698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-967</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>The VoiceMOS Challenge 2022</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4536</first_page>
						<last_page>4540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-970</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Alicehajic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name>
					</contributors>
					<titles><title>The discrimination of [zi]-[dʑi] by Japanese listeners and the prospective phonologization of /zi/</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1322</first_page>
						<last_page>1326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-973</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/alicehajic22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Rho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhyeok</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Hwan</given_name>
<surname>Ko</surname>
</person_name>
					</contributors>
					<titles><title>NAS-VAD: Neural Architecture Search for Voice Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3754</first_page>
						<last_page>3758</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-975</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rho22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Training of Sequential Neural Beamformer Using Coarsely-separated and Non-separated Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>251</first_page>
						<last_page>255</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-976</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saijo22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4272</first_page>
						<last_page>4276</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-978</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guo22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiheng</given_name>
<surname>Ouyang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ping</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>Small Footprint Neural Networks for Acoustic Direction of Arrival Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>881</first_page>
						<last_page>885</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-979</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ouyang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Biswaranjan</given_name>
<surname>Pattanayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gayadhar</given_name>
<surname>Pradhan</surname>
</person_name>
					</contributors>
					<titles><title>Significance of single frequency filter for the development of children’s KWS system</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3183</first_page>
						<last_page>3187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-980</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pattanayak22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuo</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Furu</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Speech Pre-training with Acoustic Piece</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2648</first_page>
						<last_page>2652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-981</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ren22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoci</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ningqian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yajie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenhua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Discrete Word-Level Style Variations into Non-Autoregressive Acoustic Models for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5508</first_page>
						<last_page>5512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-984</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Desheng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinkang</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4720</first_page>
						<last_page>4724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-985</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hu22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungchan</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>FluentTTS: Text-dependent Fine-grained Style Control for Multi-style TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4561</first_page>
						<last_page>4565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-988</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tae Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin Rao</given_name>
<surname>Koluguri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Multi-scale Speaker Diarization with Dynamic Scale Weighting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5080</first_page>
						<last_page>5084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-991</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/park22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Ping</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-Chun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung-Feng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Few Shot Cross-Lingual TTS Using Transferable Phoneme Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4566</first_page>
						<last_page>4570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-994</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianchao</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengru</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5373</first_page>
						<last_page>5377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-995</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marcel</given_name>
<surname>de Korte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaebok</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aki</given_name>
<surname>Kunikoshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adaeze</given_name>
<surname>Adigwe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Klabbers</surname>
</person_name>
					</contributors>
					<titles><title>Data-augmented cross-lingual synthesis in a teacher-student framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2368</first_page>
						<last_page>2372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-9995</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dekorte22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ShiLiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2063</first_page>
						<last_page>2067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-9996</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gao22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuo-Hsuan</given_name>
<surname>Hung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan-Hsin</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Tien</given_name>
<surname>Chiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chii-Wann</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Boosting Self-Supervised Embeddings for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>186</first_page>
						<last_page>190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10002</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hung22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixia</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weigong</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>The effect of backward noise on lexical tone discrimination in Mandarin-speaking amusics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2148</first_page>
						<last_page>2152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10004</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanyao</given_name>
<surname>Bian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>GuangZhi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Prosody Annotation with Pre-Trained Text-Speech Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5513</first_page>
						<last_page>5517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10005</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dai22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiumin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vince</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heguang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Bidichandani</surname>
</person_name>
					</contributors>
					<titles><title>A Language Agnostic Multilingual Streaming On-Device ASR System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3188</first_page>
						<last_page>3192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10006</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Conneau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>von Platen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Lozhkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Cherry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clara</given_name>
<surname>Rivera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mihir</given_name>
<surname>Kale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daan</given_name>
<surname>van Esch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vera</given_name>
<surname>Axelrod</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simran</given_name>
<surname>Khanuja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Clark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orhan</given_name>
<surname>Firat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Ruder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Riesa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin</given_name>
<surname>Johnson</surname>
</person_name>
					</contributors>
					<titles><title>XTREME-S: Evaluating Cross-lingual Speech Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3248</first_page>
						<last_page>3252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10007</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/conneau22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianqian</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shicong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Xiang</surname>
</person_name>
					</contributors>
					<titles><title>BIT-MI Deep Learning-based Model to Non-intrusive Speech Quality Assessment Challenge in Online Conferencing Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3288</first_page>
						<last_page>3292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10010</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qianqian</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengpeng</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qibing</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1781</first_page>
						<last_page>1785</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10011</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dong22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Myunghun</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoi Rin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Asymmetric Proxy Loss for Multi-View Acoustic Word Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5170</first_page>
						<last_page>5174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10013</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jung22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixia</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weigong</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting visuo-spatial processing in individuals with congenital amusia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4830</first_page>
						<last_page>4834</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10014</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fan22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqing</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Finer-grained Modeling units-based Meta-Learning for Low-resource Tibetan Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2133</first_page>
						<last_page>2137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10015</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/qin22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changsheng</given_name>
<surname>Quan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multichannel Speech Separation with Narrow-band Conformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5378</first_page>
						<last_page>5382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10018</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/quan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sanyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangzhan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Furu</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3699</first_page>
						<last_page>3703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10019</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>M K</given_name>
<surname>Jayesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mukesh</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Praneeth</given_name>
<surname>Vonteddu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahaboob Ali Basha</given_name>
<surname>Shaik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Transformer Networks for Non-Intrusive Speech Quality Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4078</first_page>
						<last_page>4082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10020</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jayesh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuanji</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglu</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>Unifying Cosine and PLDA Back-ends for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>336</first_page>
						<last_page>340</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10021</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohai</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiqi</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>A Transfer and Multi-Task Learning based Approach for MOS Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5438</first_page>
						<last_page>5442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10022</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tian22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seorim</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungwook</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngcheol</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Monoaural Speech Enhancement Using a Nested U-Net with Two-Level Skip Connections</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>191</first_page>
						<last_page>195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10025</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hwang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Claus</given_name>
<surname>Larsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Koch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Multi-Task Deep Learning for Noise-Robust Voice Activity Detection with Low Algorithmic Delay</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3759</first_page>
						<last_page>3763</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10033</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/larsen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianjun</given_name>
<surname>Hao</surname>
</person_name>
					</contributors>
					<titles><title>GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>966</first_page>
						<last_page>970</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10034</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nian</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>RCT: Random consistency training for semi-supervised sound event detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1541</first_page>
						<last_page>1545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10037</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuning</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangzheng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huazhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Muskits: an End-to-end Music Processing Toolkit for Singing Voice Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4277</first_page>
						<last_page>4281</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10039</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shi22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinmeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>971</first_page>
						<last_page>975</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10041</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lester Phillip</given_name>
<surname>Violeta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Self-supervised Pretraining Frameworks for Pathological Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>41</first_page>
						<last_page>45</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10043</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/violeta22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alon</given_name>
<surname>Levkovitch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliya</given_name>
<surname>Nachmani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Wolf</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2983</first_page>
						<last_page>2987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10045</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/levkovitch22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Karakasidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Comparison and Analysis of New Curriculum Criteria for End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>66</first_page>
						<last_page>70</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10046</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/karakasidis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Leemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Péter</given_name>
<surname>Jeszenszky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carina</given_name>
<surname>Steiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Lanthemann</surname>
</person_name>
					</contributors>
					<titles><title>Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1851</first_page>
						<last_page>1855</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10048</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/leemann22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhui</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peilin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuping</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>ECAPA-TDNN Based Depression Detection from Clinical Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3333</first_page>
						<last_page>3337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10051</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mayank</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tarun</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenny</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raffay</given_name>
<surname>Hamid</surname>
</person_name>
					</contributors>
					<titles><title>CNN-based Audio Event Recognition for Automated Violence Classification and Rating for Prime Video Content</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2758</first_page>
						<last_page>2762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10053</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sharma22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luwen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanyao</given_name>
<surname>Bian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2573</first_page>
						<last_page>2577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10054</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiongqiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Scoring of Large-Margin Embeddings for Speaker Verification: Cosine or PLDA?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>600</first_page>
						<last_page>604</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10055</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Xin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Audio Pyramid Transformer with Domain Adaption for Weakly Supervised Sound Event Detection and Audio Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1546</first_page>
						<last_page>1550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10057</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghuveer</given_name>
<surname>Peri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition on Federated Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5055</first_page>
						<last_page>5059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10060</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/feng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingbei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanyao</given_name>
<surname>Bian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5518</first_page>
						<last_page>5522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10061</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirofumi</given_name>
<surname>Inaguma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sei</given_name>
<surname>Ueno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinsuke</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Non-autoregressive Error Correction for CTC-based ASR with Phone-conditioned Masked LM</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3889</first_page>
						<last_page>3893</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10062</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/futami22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jae-Hong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chae-Won</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin-Seong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woo Kyeong</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeonghan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>CTRL: Continual Representation Learning to Transfer Information of Pre-trained for WAV2VEC 2.0</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3398</first_page>
						<last_page>3402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10063</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Chaubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sparsh</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susmita</given_name>
<surname>Ghose</surname>
</person_name>
					</contributors>
					<titles><title>Improved Relation Networks for End-to-End Speaker Verification and Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5085</first_page>
						<last_page>5089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10064</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chaubey22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Jiang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3804</first_page>
						<last_page>3808</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10066</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wei22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhanheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Minimizing Sequential Confusion Error in Speech Command Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3193</first_page>
						<last_page>3197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10067</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>iCNN-Transformer: An improved CNN-Transformer with Channel-spatial Attention and Keyword Prediction for Automated Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4167</first_page>
						<last_page>4171</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10073</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mao</given_name>
<surname>Saeki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kotoka</given_name>
<surname>Miyagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Fujie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shungo</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoichi</given_name>
<surname>Matsuyama</surname>
</person_name>
					</contributors>
					<titles><title>Confusion Detection for Adaptive Conversational Strategies of An Oral Proficiency Assessment Interview Agent</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3988</first_page>
						<last_page>3992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10075</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saeki22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Speaker- and Phone-aware Convolutional Transformer Network for Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2513</first_page>
						<last_page>2517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10077</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/han22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Kawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Plata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Syga</surname>
</person_name>
					</contributors>
					<titles><title>Attack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4023</first_page>
						<last_page>4027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10078</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kawa22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>MOS Prediction Network for Non-intrusive Speech Quality Assessment in Online Conferencing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3293</first_page>
						<last_page>3297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10081</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xue</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiulian</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaying</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Scale Vector Quantization for Scalable Neural Speech Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4222</first_page>
						<last_page>4226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10084</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jiang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Akagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation Using McAdams-Coefficient-Based Speaker Anonymization for Fake Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>664</first_page>
						<last_page>668</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10088</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pouriya</given_name>
<surname>Amini Digehsara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João Vítor</given_name>
<surname>Possamai de Menezes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Bärhold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Schaffer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Plettemeier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>A user-friendly headset for radar-based silent speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4835</first_page>
						<last_page>4839</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10090</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/aminidigehsara22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louise</given_name>
<surname>Coppieters de Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name>
					</contributors>
					<titles><title>Low-Level Physiological Implications of End-to-End Learning for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>749</first_page>
						<last_page>753</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10093</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/coppietersdegibson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingyun</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haitao</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>ASR-Robust Natural Language Understanding on ASR-GLUE dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1101</first_page>
						<last_page>1105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10097</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/feng22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Arai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miho</given_name>
<surname>Yamada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megumi</given_name>
<surname>Okusawa</surname>
</person_name>
					</contributors>
					<titles><title>Syllable sequence of /a/+/ta/ can be heard as /atta/ in Japanese with visual or tactile cues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3083</first_page>
						<last_page>3087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10099</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/arai22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ragheb</given_name>
<surname>Al-Ghezi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katja</given_name>
<surname>Voskoboinik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>Strömbergsson</surname>
</person_name>
					</contributors>
					<titles><title>wav2vec2-based Speech Rating System for Children with Speech Sound Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3618</first_page>
						<last_page>3622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10103</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/getman22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Muckenhirn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Safin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hakan</given_name>
<surname>Erdogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>de Chaumont Quitry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Tagliasacchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Scott</given_name>
<surname>Wisdom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name>
					</contributors>
					<titles><title>CycleGAN-based Unpaired Speech Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>196</first_page>
						<last_page>200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10104</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/muckenhirn22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mao-Kui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Audio-Visual Neural Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1461</first_page>
						<last_page>1465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10106</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/he22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emil</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xenia</given_name>
<surname>Kogler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franz</given_name>
<surname>Pernkopf</surname>
</person_name>
					</contributors>
					<titles><title>Homophone Disambiguation Profits from Durational Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3198</first_page>
						<last_page>3202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10109</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/schuppler22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoyasu</given_name>
<surname>Komori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Mishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiko</given_name>
<surname>Kawai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Mochizuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoei</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Text-Only Domain Adaptation Based on Intermediate CTC</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2208</first_page>
						<last_page>2212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10114</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sato22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lev</given_name>
<surname>Finkelstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norman</given_name>
<surname>Casagrande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun-an</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Kenter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Petelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Clark</surname>
</person_name>
					</contributors>
					<titles><title>Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4571</first_page>
						<last_page>4575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10115</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/finkelstein22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brooke</given_name>
<surname>Stephenson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Girin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name>
					</contributors>
					<titles><title>BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3383</first_page>
						<last_page>3387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10116</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/stephenson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noriyuki</given_name>
<surname>Matsunaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamato</given_name>
<surname>Ohtani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Spoken-Text-Style Transfer with Conditional Variational Autoencoder and Content Word Storage</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4576</first_page>
						<last_page>4580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10118</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yoshioka22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Langheinrich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Stone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>Glottal inverse filtering based on articulatory synthesis and deep learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1327</first_page>
						<last_page>1331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10119</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/langheinrich22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoquan</given_name>
<surname>KE</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen M.</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Selection of Discriminative Features for Dementia Detection in Cantonese-Speaking People</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2153</first_page>
						<last_page>2157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10122</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ke22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xian</given_name>
<surname>LI</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>ATST: Audio Representation Learning with Teacher-Student Transformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4172</first_page>
						<last_page>4176</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10126</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyeonuk</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Hu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeong-Yun</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong-Hwa</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2763</first_page>
						<last_page>2767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10127</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nam22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yookyung</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Younggun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suhee</given_name>
<surname>Jo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeongtae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taesu</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2313</first_page>
						<last_page>2317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10131</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shin22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cassia</given_name>
<surname>Valentini-Botinhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Watts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name>
					</contributors>
					<titles><title>Predicting pairwise preferences between TTS audio stimuli using parallel ratings data and anti-symmetric twin neural networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>471</first_page>
						<last_page>475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10132</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/valentinibotinhao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ohsung</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chan-Ho</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Jae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suhyeon</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-Wook</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin-Seob</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>TTS-by-TTS 2: Data-Selective Augmentation for Neural Speech Synthesis Using Ranking Support Vector Machine with Variational Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1941</first_page>
						<last_page>1945</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10134</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/song22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felix</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wilfried</given_name>
<surname>Michel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Zeineldeen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Learning of Subword Dependent Model Scales</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4133</first_page>
						<last_page>4136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10136</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meyer22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Fong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Lyth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1213</first_page>
						<last_page>1217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10138</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chu-Xiao</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Yi</given_name>
<surname>Leng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu-Jun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Specific Utterance Ensemble based Transfer Attack on Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3203</first_page>
						<last_page>3207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10139</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zuo22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ling-Hui</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4282</first_page>
						<last_page>4286</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10140</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minyue</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Background Noise and Contribution of Visual Information in Emotion Identification by Native Mandarin Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1993</first_page>
						<last_page>1997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10142</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bastiaan</given_name>
<surname>Tamm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Balabin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rik</given_name>
<surname>Vandenberghe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Pre-trained Speech Representations as Feature Extractors for Speech Quality Assessment in Online Conferencing Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4083</first_page>
						<last_page>4087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10147</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tamm22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>SASV Based on Pre-trained ASV System and Integrated Scoring Module</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4376</first_page>
						<last_page>4380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10149</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Wen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>InQSS: a speech intelligibility and quality assessment model using a multi-task learning network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3088</first_page>
						<last_page>3092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10153</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuoya</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Huckvale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>McGlashan</surname>
</person_name>
					</contributors>
					<titles><title>Automated Voice Pathology Discrimination from Continuous Speech Benefits from Analysis by Phonetic Context</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2158</first_page>
						<last_page>2162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10154</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nils L.</given_name>
<surname>Westhausen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>tPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using a Short Temporal Context</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2903</first_page>
						<last_page>2907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10157</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/westhausen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeonjong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>An Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4910</first_page>
						<last_page>4914</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10158</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/choi22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Illa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Streaming model for Acoustic to Articulatory Inversion with transformer networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>625</first_page>
						<last_page>629</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10159</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/udupa22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mai Hoang</given_name>
<surname>Dao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thinh</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dat Quoc</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>From Disfluency Detection to Intent Detection and Slot Filling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1106</first_page>
						<last_page>1110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10161</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuende</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge of accent differences can be used to predict speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1372</first_page>
						<last_page>1376</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10162</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/szalay22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mosner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldrich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Training speaker embedding extractors using multi-speaker audio with unknown speaker boundaries</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>605</first_page>
						<last_page>609</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10165</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/stafylakis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johannah</given_name>
<surname>O'Mahony</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3388</first_page>
						<last_page>3392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10167</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/omahony22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Rybicka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Neural Speaker Diarization with an Iterative Refinement of Non-Autoregressive Attention-based Attractors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5090</first_page>
						<last_page>5094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10169</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rybicka22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ondrej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Electra</given_name>
<surname>Wallington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2288</first_page>
						<last_page>2292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10170</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/klejch22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nguyen Luong</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duong</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dat Quoc</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1751</first_page>
						<last_page>1755</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10177</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tran22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>George</given_name>
<surname>Close</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Hollands</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Goetze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Non-intrusive Speech Intelligibility Metric Prediction for Hearing Impaired Individuals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3483</first_page>
						<last_page>3487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10182</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/close22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Borsdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Scheck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>256</first_page>
						<last_page>260</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10187</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/borsdorf22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andre</given_name>
<surname>Bittar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johnny</given_name>
<surname>Downs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen L.</given_name>
<surname>Fisher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2458</first_page>
						<last_page>2462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10188</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mirheidari22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinzheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peipei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shidrokh</given_name>
<surname>Goudarzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>YONG</given_name>
<surname>XU</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Audio Visual Multi-Speaker Tracking with Improved GCF and PMBM Filter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3704</first_page>
						<last_page>3708</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10190</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salah</given_name>
<surname>Zaiem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Essid</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>669</first_page>
						<last_page>673</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10191</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zaiem22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaeuk</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Advanced Speaker Embedding with Predictive Variance of Gaussian Distribution for Speaker Adaptation in TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2988</first_page>
						<last_page>2992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10193</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Bilinski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Merritt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelhamid</given_name>
<surname>Ezzerg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamil</given_name>
<surname>Pokora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Cygert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kayoko</given_name>
<surname>Yanagisawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name>
					</contributors>
					<titles><title>Creating New Voices using Normalizing Flows</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2958</first_page>
						<last_page>2962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10195</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bilinski22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name>
					</contributors>
					<titles><title>Complex sounds and cross-language influence: The case of ejectives in Omani Mehri</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3433</first_page>
						<last_page>3437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10199</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ridouane22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngsik</given_name>
<surname>Eom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeonghyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji Sub</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoi Rin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Anti-Spoofing Using Transfer Learning with Variational Information Bottleneck</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3568</first_page>
						<last_page>3572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10200</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/eom22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic cognitive assessment: Combining sparse datasets with disparate cognitive scores</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2463</first_page>
						<last_page>2467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10205</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mirheidari22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominika</given_name>
<surname>Woszczyk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Hedlikova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alican</given_name>
<surname>Akman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soteris</given_name>
<surname>Demetriou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation for Dementia Detection in Spoken Language.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2858</first_page>
						<last_page>2862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10210</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/woszczyk22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>SiD-WaveFlow: A Low-Resource Vocoder Independent of Prior Knowledge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1616</first_page>
						<last_page>1620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10222</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shimin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziteng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukai</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihui</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueyue</given_name>
<surname>Na</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Acoustic Echo Cancellation for Full-duplex Communications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2518</first_page>
						<last_page>2522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10225</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoming</given_name>
<surname>Xhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Shinozaki</surname>
</person_name>
					</contributors>
					<titles><title>Censer: Curriculum Semi-supervised Learning for Speech Recognition Based on Self-supervised Pre-training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2653</first_page>
						<last_page>2657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10226</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jovan M.</given_name>
<surname>Dalhouse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katunobu</given_name>
<surname>Itou</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Transfer Learning Approach to Phoneme Error Detection via Latent Phonetic Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3133</first_page>
						<last_page>3137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10228</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dalhouse22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiulian</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Modal Multi-Correlation Learning for Audio-Visual Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>886</first_page>
						<last_page>890</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10229</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanfeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4725</first_page>
						<last_page>4729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10230</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarod</given_name>
<surname>Duret</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end model for named entity recognition from speech without paired training data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4068</first_page>
						<last_page>4072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10231</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mdhaffar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maximilian Karl</given_name>
<surname>Scharf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabine</given_name>
<surname>Hochmuth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena L.N.</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Birger</given_name>
<surname>Kollmeier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Warzybok</surname>
</person_name>
					</contributors>
					<titles><title>Lombard Effect for Bilingual Speakers in Cantonese and English: importance of spectro-temporal features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1377</first_page>
						<last_page>1381</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10235</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/scharf22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zikai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yin</surname>
</person_name>
					</contributors>
					<titles><title>An Automatic Soundtracking System for Text-to-Speech Audiobooks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>476</first_page>
						<last_page>480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10236</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Markitantov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Ryumina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Ryumin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1756</first_page>
						<last_page>1760</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10240</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eesung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Jin</given_name>
<surname>Jeon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeji</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1411</first_page>
						<last_page>1415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10245</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zongyang</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Disentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2603</first_page>
						<last_page>2607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10249</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/du22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wooseok</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Joon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin Sob</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byung Hoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Won</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Multi-View Attention Transfer for Efficient Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1198</first_page>
						<last_page>1202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10251</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shin22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenggang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>JinJiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>LCSM: A Lightweight Complex Spectral Mapping Framework for Stereophonic Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2523</first_page>
						<last_page>2527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10252</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zilin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shidong</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Fullband-Subband Cross-Attention Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>976</first_page>
						<last_page>980</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10257</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhanheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoming</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1681</first_page>
						<last_page>1685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10258</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Backend Ensemble for Speaker Verification and Spoofing Countermeasure</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4381</first_page>
						<last_page>4385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10259</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengdong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangjin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenhui</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raj</given_name>
<surname>Dabre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Rubino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Fusion of Self-supervised Learned Models for MOS Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5443</first_page>
						<last_page>5447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10262</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qibing</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Ao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhixiang</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1686</first_page>
						<last_page>1690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10269</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kavan</given_name>
<surname>Fatehi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mercedes</given_name>
<surname>Torres Torres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayse</given_name>
<surname>Kucukyilmaz</surname>
</person_name>
					</contributors>
					<titles><title>ScoutWav: Two-Step Fine-Tuning on Self-Supervised Automatic Speech Recognition for Low-Resource Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3523</first_page>
						<last_page>3527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10270</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fatehi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zohreh</given_name>
<surname>Mostaani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>On Breathing Pattern Information in Synthetic Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2768</first_page>
						<last_page>2772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10271</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mostaani22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinicius</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yves</given_name>
<surname>Laprie</surname>
</person_name>
					</contributors>
					<titles><title>Autoencoder-Based Tongue Shape Estimation During Continuous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>86</first_page>
						<last_page>90</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10272</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ribeiro22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Quinnell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cecilia</given_name>
<surname>Mascolo</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2468</first_page>
						<last_page>2472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10274</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoquan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liqun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu Ting</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nianzu</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Streamable Speech Representation Disentanglement and Multi-Level Prosody Modeling for Live One-Shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2578</first_page>
						<last_page>2582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10277</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Chu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong Li</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Feature Shuffling Network for Text-independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4790</first_page>
						<last_page>4794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10278</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsun-An</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>OSSEM: one-shot speaker adaptive speech enhancement using meta learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>981</first_page>
						<last_page>985</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10283</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoaki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Representing 'how you say' with 'what you say': English corpus of focused speech and text reflecting corresponding implications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4980</first_page>
						<last_page>4984</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10284</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/suzuki22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu Ting</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liqun</given_name>
<surname>Deng</surname>
</person_name>
					</contributors>
					<titles><title>reducing multilingual context confusion for end-to-end code-switching automatic speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3894</first_page>
						<last_page>3898</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10286</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuheng</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junzhao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CTFALite: Lightweight Channel-specific Temporal and Frequency Attention Mechanism for Enhancing the Speaker Embedding Extractor</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>341</first_page>
						<last_page>345</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10288</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wei22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takeru</given_name>
<surname>Gorai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>Text-to-speech synthesis using spectral modeling based on non-negative autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1621</first_page>
						<last_page>1625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10290</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gorai22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adria</given_name>
<surname>Mallol-Ragolta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Cuesta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Type Outer Product-Based Fusion of Respiratory Sounds for Detecting COVID-19</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2163</first_page>
						<last_page>2167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10291</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mallolragolta22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunghee</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eesung</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>21</first_page>
						<last_page>25</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10294</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lim22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Magistro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claudia</given_name>
<surname>Crocco</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic erosion and information structure in function words: the case of mia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>91</first_page>
						<last_page>95</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10305</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/magistro22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>BingHuai</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>YanLu</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>An Alignment Method Leveraging Articulatory Features for Mispronunciation Detection and Diagnosis in L2 English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4342</first_page>
						<last_page>4346</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10309</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenchun</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minglei</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Path GMM-MobileNet Based on Attack Algorithms and Codecs for Synthetic Speech and Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4795</first_page>
						<last_page>4799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10312</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Shu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanjie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuxiang</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshuai</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yehang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Non-intrusive Speech Quality Assessment with a Multi-Task Learning based Subband Adaptive Attention Temporal Convolutional Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3298</first_page>
						<last_page>3302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10315</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhonghao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benlai</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yibiao</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zejun</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4287</first_page>
						<last_page>4291</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10316</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1016</first_page>
						<last_page>1020</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10326</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wei22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong-Thanh</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama</given_name>
<surname>Doddipatla</surname>
</person_name>
					</contributors>
					<titles><title>Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4446</first_page>
						<last_page>4450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10330</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/do22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Ludusan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marin</given_name>
<surname>Schröer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name>
					</contributors>
					<titles><title>Investigating phonetic convergence of laughter in conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1332</first_page>
						<last_page>1336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10332</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ludusan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Fendler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Gerczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Berghaus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Distinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3623</first_page>
						<last_page>3627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10333</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/triantafyllopoulos22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chitralekha</given_name>
<surname>Bhat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Panda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Improved ASR Performance for Dysarthric Speech Using Two-stage DataAugmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>46</first_page>
						<last_page>50</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10335</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bhat22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tarun Sai</given_name>
<surname>Bandarupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakti</given_name>
<surname>Rath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nirmesh</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Onoe</given_name>
<surname>Naoyuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Semi-supervised Acoustic and Language Modeling for Hindi ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3528</first_page>
						<last_page>3532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10336</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bandarupalli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giulia</given_name>
<surname>Comini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Goeric</given_name>
<surname>Huybrechts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Gabryś</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name>
					</contributors>
					<titles><title>Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1946</first_page>
						<last_page>1950</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10338</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/comini22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dong-Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Hong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hwan</given_name>
<surname>Mo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>W2V2-Light: A Lightweight Version of Wav2vec 2.0 for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3038</first_page>
						<last_page>3042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10339</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fangjun</given_name>
<surname>Kuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingshuang</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengwei</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name>
					</contributors>
					<titles><title>Pruned RNN-T for fast, memory-eﬀicient ASR training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2068</first_page>
						<last_page>2072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10340</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kuang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runqiu</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenduo</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>The HCCL System for the NIST SRE21</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3709</first_page>
						<last_page>3713</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10342</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsuneo</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiro</given_name>
<surname>Tamura</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4466</first_page>
						<last_page>4470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10344</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/suzuki22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Environment Aware Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>481</first_page>
						<last_page>485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10348</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenke</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manwai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiwei</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>UNet-DenseNet for Robust Far-Field Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3714</first_page>
						<last_page>3718</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10350</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gao22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoru</given_name>
<surname>Fukayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panikos</given_name>
<surname>Heracleous</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ogata</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1998</first_page>
						<last_page>2002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10354</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Kothapally</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>YONG</given_name>
<surname>XU</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>ZHANG</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Joint Neural AEC and Beamforming with Double-Talk Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2528</first_page>
						<last_page>2532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10358</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kothapally22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sefik</given_name>
<surname>Emre Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Furu</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Separating Long-Form Speech with Group-wise Permutation Invariant Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5383</first_page>
						<last_page>5387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10362</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanyan</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mao-Kui</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>YuTing</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renyu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Online Speaker Diarization with Core Samples Selection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1466</first_page>
						<last_page>1470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10363</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yue22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Ao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziqiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Furu</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2658</first_page>
						<last_page>2662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10368</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Verdiana</given_name>
<surname>De Fino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lionel</given_name>
<surname>Fontan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Pinquier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabelle</given_name>
<surname>Ferrané</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Detey</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of L2 speech proficiency based on multi-level linguistic features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4043</first_page>
						<last_page>4047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10369</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/defino22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hagen</given_name>
<surname>Wierstorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uwe</given_name>
<surname>Reichel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Eyben</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Burkhardt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Probing speech emotion recognition transformers for linguistic knowledge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>146</first_page>
						<last_page>150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10371</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/triantafyllopoulos22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debottam</given_name>
<surname>Dutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debarpan</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir Hossein</given_name>
<surname>Poorjam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maneesh</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Representation Learning on Breathing and Speech Signals for COVID-19 Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2863</first_page>
						<last_page>2867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10376</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dutta22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shahaf</given_name>
<surname>Bassan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeffrey</given_name>
<surname>Rosenschein</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2423</first_page>
						<last_page>2427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10379</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bassan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianchao</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Deep Sparse Conformer for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2073</first_page>
						<last_page>2077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10384</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deebha</given_name>
<surname>Mumtaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ajit</given_name>
<surname>Jena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinit</given_name>
<surname>Jakhetiya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Nathwani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharath Chandra</given_name>
<surname>Guntuku</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-based quality assessment model for generalized user-generated multimedia audio content</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>674</first_page>
						<last_page>678</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10386</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mumtaz22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seonwoo</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>A Study on the Phonetic Inventory Development of Children with Cochlear Implants for 5 Years after Implantation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3628</first_page>
						<last_page>3632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10387</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Colotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Jouvet</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of expressivity transfer in non-autoregressive end-to-end multispeaker TTS systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4581</first_page>
						<last_page>4585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10388</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kulkarni22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debarpan</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debottam</given_name>
<surname>Dutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth Raj</given_name>
<surname>Chetupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pravin</given_name>
<surname>Mote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandrakiran</given_name>
<surname>C</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahiti</given_name>
<surname>Nori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suhail</given_name>
<surname>K K</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sadhana</given_name>
<surname>Gonuguntla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murali</given_name>
<surname>Alagesan</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2473</first_page>
						<last_page>2477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10389</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bhattacharya22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Fara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Goria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Molimpakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1911</first_page>
						<last_page>1915</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10393</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fara22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junpeng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanyan</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mian</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuoye</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Long</surname>
</person_name>
					</contributors>
					<titles><title>Negative Guided Abstractive Dialogue Summarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3253</first_page>
						<last_page>3257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10395</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carolina Lins</given_name>
<surname>Machado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Idiosyncratic lingual articulation of American English /æ/ and /ɑ/ using network analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>754</first_page>
						<last_page>758</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10397</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/machado22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehai</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Hidden Representations from a DNN-based Speech Recogniser for Speech Intelligibility Prediction in Hearing-impaired Listeners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3488</first_page>
						<last_page>3492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10399</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyong</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunzhou</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Pi</surname>
</person_name>
					</contributors>
					<titles><title>Soft-label Learn for No-Intrusive Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3303</first_page>
						<last_page>3307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10400</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xueshuai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiakun</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanfen</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fujie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoxing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aijun</given_name>
<surname>Sun</surname>
</person_name>
					</contributors>
					<titles><title>Robust Cough Feature Extraction and Classification Method for COVID-19 Cough Detection Based on Vocalization Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2168</first_page>
						<last_page>2172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10401</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>CTA-RNN: Channel and Temporal-wise Attention RNN leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4730</first_page>
						<last_page>4734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10403</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Robust End-to-end Speaker Diarization with Generic Neural Clustering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1471</first_page>
						<last_page>1475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10404</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Kocour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Zmolikova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ondel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Svec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Cernocky</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting joint decoding based multi-talker speech recognition with DNN acoustic model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4955</first_page>
						<last_page>4959</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10406</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kocour22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehai</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3493</first_page>
						<last_page>3497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10408</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Lehečka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luboš</given_name>
<surname>Šmídl</surname>
</person_name>
					</contributors>
					<titles><title>Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1886</first_page>
						<last_page>1890</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10409</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/svec22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenbin</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Speech Enhancement with Neural Homomorphic Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>986</first_page>
						<last_page>990</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10411</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jiang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Boris</given_name>
<surname>Bergsma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name>
					</contributors>
					<titles><title>PEAF: Learnable Power Efficient Analog Acoustic Features for Audio Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>381</first_page>
						<last_page>385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10412</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bergsma22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dehua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harold</given_name>
<surname>Chui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Luk</surname>
</person_name>
					</contributors>
					<titles><title>Characterizing Therapist's Speaking Style in Relation to Empathy in Psychotherapy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2003</first_page>
						<last_page>2007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10416</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yajian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Deep Segment Model for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4177</first_page>
						<last_page>4181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10418</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Lin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Y.-W. Peter</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>An Attention-Based Method for Guiding Attribute-Aligned Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5030</first_page>
						<last_page>5034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10419</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Farhad</given_name>
<surname>Javanmardi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manila</given_name>
<surname>Kodali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Comparing 1-dimensional and 2-dimensional spectral feature representations in voice pathology detection using machine learning and deep learning classifiers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2173</first_page>
						<last_page>2177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10420</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/javanmardi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guodong</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nurmemet</given_name>
<surname>Yolwas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1021</first_page>
						<last_page>1025</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10422</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ma22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mamady</given_name>
<surname>NABE</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Diard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Schwartz</surname>
</person_name>
					</contributors>
					<titles><title>Isochronous is beautiful? Syllabic event detection in a neuro-inspired oscillatory model is facilitated by isochrony in speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4671</first_page>
						<last_page>4675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10426</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nabe22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Baiyun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qi</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxue</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wuwen</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianbao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>PLCNet: Real-time Packet Loss Concealment with Semi-supervised Generative Adversarial Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>575</first_page>
						<last_page>579</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10428</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joao</given_name>
<surname>Vitor Menezes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pouriya</given_name>
<surname>Amini Digehsara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Mütze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Bärhold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Schaffer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Plettemeier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>Evaluation of different antenna types and positions in a stepped frequency continuous-wave radar-based silent speech interface</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3633</first_page>
						<last_page>3637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10431</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vitormenezes22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Hollands</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the Performance of State-of-the-Art ASR Systems on Non-Native English using Corpora with Extensive Language Background Variation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3958</first_page>
						<last_page>3962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10433</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hollands22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Förstel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bastian</given_name>
<surname>Oppermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Erzigkeit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hartmut</given_name>
<surname>Lehfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hillemacher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Automated Evaluation of Standardized Dementia Screening Tests</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2478</first_page>
						<last_page>2482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10436</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/braun22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chun-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Emotion-Shift Aware CRF for Decoding Emotion Sequence in Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1148</first_page>
						<last_page>1152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10438</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Lehečka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ales</given_name>
<surname>Prazak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Josef</given_name>
<surname>Psutka</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1831</first_page>
						<last_page>1835</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10439</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lehecka22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qijie</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinghao</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xian</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Linguistic-Acoustic Similarity Based Accent Shift for Accent Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3719</first_page>
						<last_page>3723</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10444</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shao22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Panagiotis</given_name>
<surname>Kakoulidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Vamvoukakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Markopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June Sig</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunu</given_name>
<surname>Jho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pirros</given_name>
<surname>Tsiakoulis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aimilios</given_name>
<surname>Chalamandaris</surname>
</person_name>
					</contributors>
					<titles><title>Karaoker: Alignment-free singing voice synthesis with speech training data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2993</first_page>
						<last_page>2997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10446</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kakoulidis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kirandevraj</given_name>
<surname>R</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinod Kumar</given_name>
<surname>Kurmi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Namboodiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C V</given_name>
<surname>Jawahar</surname>
</person_name>
					</contributors>
					<titles><title>Generalized Keyword Spotting using ASR embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>126</first_page>
						<last_page>130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10450</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/r22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Federico</given_name>
<surname>Landini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alicia</given_name>
<surname>Lozano-Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5095</first_page>
						<last_page>5099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10451</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/landini22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Vaccinating SER to Neutralize Adversarial Attacks with Self-Supervised Augmentation Strategy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1153</first_page>
						<last_page>1157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10453</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/su22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kunnar</given_name>
<surname>Kukk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name>
					</contributors>
					<titles><title>Improving Language Identification of Accented Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1288</first_page>
						<last_page>1292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10455</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kukk22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Han</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian Transformer Using Disentangled Mask Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1761</first_page>
						<last_page>1765</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10457</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chien22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jana</given_name>
<surname>Roßbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saskia</given_name>
<surname>Röttges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher F.</given_name>
<surname>Hauth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Biberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Brand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Rennies</surname>
</person_name>
					</contributors>
					<titles><title>Speech Intelligibility Prediction for Hearing-Impaired Listeners with the LEAP Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3498</first_page>
						<last_page>3502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10460</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/robach22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashtosh</given_name>
<surname>Sapru</surname>
</person_name>
					</contributors>
					<titles><title>Using Data Augmentation and Consistency Regularization to Improve Semi-supervised Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5115</first_page>
						<last_page>5119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10462</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sapru22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Teruki</given_name>
<surname>Toya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenyu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maori</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masashi</given_name>
<surname>Unoki</surname>
</person_name>
					</contributors>
					<titles><title>Method for improving the word intelligibility of presented speech using bone-conduction headphones</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>759</first_page>
						<last_page>763</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10463</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/toya22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan J.</given_name>
<surname>Macoskey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Radfar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-Ju</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant</given_name>
<surname>Strimel</surname>
</person_name>
					</contributors>
					<titles><title>Compute Cost Amortized Transformer for Streaming ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3043</first_page>
						<last_page>3047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10465</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xie22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoxiong</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyuan</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binwei</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>MSDWild: Multi-modal Speaker Diarization Dataset in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1476</first_page>
						<last_page>1480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10466</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sukanya</given_name>
<surname>Sonowal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anish</given_name>
<surname>Tamse</surname>
</person_name>
					</contributors>
					<titles><title>Novel Augmentation Schemes for Device Robust Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4182</first_page>
						<last_page>4186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10468</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sonowal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Weise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rivka</given_name>
<surname>Levitan</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the influence of personality on acoustic-prosodic entrainment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3093</first_page>
						<last_page>3097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10470</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/weise22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji Sub</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeunju</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoi Rin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>ACNN-VC: Utilizing Adaptive Convolution Neural Network for One-Shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2998</first_page>
						<last_page>3002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10473</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/um22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pin-Tuan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao-Fei</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Chain-based Discriminative Autoencoders for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2078</first_page>
						<last_page>2082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10474</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingwen</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoli</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A study of production error analysis for Mandarin-speaking Children with Hearing Impairment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4840</first_page>
						<last_page>4844</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10477</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cheng22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Contrast Stretching on Target Feature for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5448</first_page>
						<last_page>5452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10478</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zvi</given_name>
<surname>Kons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hagai</given_name>
<surname>Aronowitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edmilson</given_name>
<surname>Morais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matheus</given_name>
<surname>Damasceno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Kwang</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name>
					</contributors>
					<titles><title>Extending RNN-T-based speech recognition systems with emotion and language classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>546</first_page>
						<last_page>549</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10480</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kons22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusheng</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingdong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baocai</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Pan</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1766</first_page>
						<last_page>1770</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10483</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Flechl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shou-Chun</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junho</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Skala</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end speech recognition modeling from de-identified data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1382</first_page>
						<last_page>1386</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10484</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/flechl22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Franklin Alvarez</given_name>
<surname>Cardinale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Waldo</given_name>
<surname>Nogueira</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Speech Intelligibility using the Spike Acativity Mutual Information Index</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3503</first_page>
						<last_page>3507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10488</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cardinale22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Navin Raj</given_name>
<surname>Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Carbajal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nale</given_name>
<surname>Lehmann-Willenbrock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>End-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>151</first_page>
						<last_page>155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10490</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/prabhu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Attentive Training: A New Training Framework for Talker-independent Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>201</first_page>
						<last_page>205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10491</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pandey22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanjie</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>891</first_page>
						<last_page>895</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10493</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yin22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2883</first_page>
						<last_page>2887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10495</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zeng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gasser</given_name>
<surname>Elbanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Biryukov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neil</given_name>
<surname>Scheidwasser-Clow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lara</given_name>
<surname>Orlandic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Mainar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikolaj</given_name>
<surname>Kegler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Beckmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>386</first_page>
						<last_page>390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10498</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/elbanna22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjian</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Le</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaguan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoquan</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>NRI-FGSM: An Efficient Transferable Adversarial Attack for Speaker Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4386</first_page>
						<last_page>4390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10499</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nana</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3764</first_page>
						<last_page>3768</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10500</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xiao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>yangyang</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xing</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Dual-Aware with Hierarchical Interactive Memory Networks for Task-Oriented Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2713</first_page>
						<last_page>2717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10501</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ou22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yicheng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya Arie</given_name>
<surname>Nugraha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouhei</given_name>
<surname>Sekiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Fontaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuyoshi</given_name>
<surname>Yoshii</surname>
</person_name>
					</contributors>
					<titles><title>Direction-Aware Joint Adaptation of Neural Speech Enhancement and Recognition in Real Multiparty Conversational Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2918</first_page>
						<last_page>2922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10508</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/du22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nana</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heqing</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Interactive Auido-text Representation for Automated Audio Captioning with Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2773</first_page>
						<last_page>2777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10510</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Farhad</given_name>
<surname>Javanmardi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Convolutional Neural Networks for Classification of Voice Qualities from Speech and Neck Surface Accelerometer Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5253</first_page>
						<last_page>5257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10513</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kadiri22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Huckvale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaston</given_name>
<surname>Hilkhuysen</surname>
</person_name>
					</contributors>
					<titles><title>ELO-SPHERES intelligibility prediction model for the Clarity Prediction Challenge 2022</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3934</first_page>
						<last_page>3938</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10521</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huckvale22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>YANZHANG</given_name>
<surname>GENG</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>A speech enhancement method for long-range speech acquisition task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5453</first_page>
						<last_page>5457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10523</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/geng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanjie</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Iterative Sound Source Localization for Unknown Number of Sources</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>896</first_page>
						<last_page>900</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10525</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Guzik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>NTF of Spectral and Spatial Features for Tracking and Separation of Moving Sound Sources in Spherical Harmonic Domain</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>261</first_page>
						<last_page>265</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10526</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guzik22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoming</given_name>
<surname>Jiang</surname>
</person_name>
					</contributors>
					<titles><title>Common and differential acoustic representation of interpersonal and tactile iconic perception of Mandarin vowels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3098</first_page>
						<last_page>3102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10531</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eklavya</given_name>
<surname>Sarkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>RaviShankar</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Voice Activity Detection by Modeling Source and System Information using Zero Frequency Filtering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4626</first_page>
						<last_page>4630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10535</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sarkar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kei</given_name>
<surname>Furukawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeshi</given_name>
<surname>Kishiyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Applying Syntax–Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5258</first_page>
						<last_page>5262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10541</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/furukawa22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jovan</given_name>
<surname>Eranovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Pape</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magda</given_name>
<surname>Stroińska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabet</given_name>
<surname>Service</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marijana</given_name>
<surname>Matkovski</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Noise on Speech Perception and Spoken Word Comprehension</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3103</first_page>
						<last_page>3107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10543</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/eranovic22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dehua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harold</given_name>
<surname>Chui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Luk</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Attention Network for Evaluating Therapist Empathy in Counseling Session</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2008</first_page>
						<last_page>2012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10550</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tao22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiedan</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikas</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Streaming parallel transducer beam search with fast slow cascaded encoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2083</first_page>
						<last_page>2087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10551</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mahadeokar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Veronique</given_name>
<surname>Delvaux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Audrey</given_name>
<surname>Lavallée</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanny</given_name>
<surname>Degouis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Saloppe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Louis</given_name>
<surname>Nandrino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thierry</given_name>
<surname>Pham</surname>
</person_name>
					</contributors>
					<titles><title>Telling self-defining memories: An acoustic study of natural emotional speech productions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1337</first_page>
						<last_page>1341</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10554</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/delvaux22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Kanagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroyuki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Joint Modeling of Multi-Sample and Subband Signals for Fast Neural Vocoding on CPU</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1626</first_page>
						<last_page>1630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10556</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kanagawa22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitanya Prasad</given_name>
<surname>Narisetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Residual Language Model for End-to-end Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3899</first_page>
						<last_page>3903</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10557</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tsunoo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quandong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Exploring representation learning for small-footprint keyword spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3258</first_page>
						<last_page>3262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10558</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cui22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinghui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1026</first_page>
						<last_page>1030</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10560</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/audhkhasi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sichen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aijun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Acquisition of Two Consecutive Neutral Tones in Mandarin-Speaking Preschoolers: Phonological Representation and Phonetic Realization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3108</first_page>
						<last_page>3112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10561</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long</given_name>
<surname>Mai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised domain adaptation for speech recognition with unsupervised error correction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5120</first_page>
						<last_page>5124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10565</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mai22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danni</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xutai</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name>
					</contributors>
					<titles><title>From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1771</first_page>
						<last_page>1775</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10568</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dino</given_name>
<surname>Rattcliffe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>You</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Mansbridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Penny</given_name>
<surname>Karanasou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marius</given_name>
<surname>Cotescu</surname>
</person_name>
					</contributors>
					<titles><title>Cross-lingual Style Transfer with Conditional Prior VAE and Style Loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4586</first_page>
						<last_page>4590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10572</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rattcliffe22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huahuan</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>keyu</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglu</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>An Empirical Study of Language Model Integration for Transducer based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3904</first_page>
						<last_page>3908</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10576</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zheng22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anwesha</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Varun</given_name>
<surname>Belagali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Air tissue boundary segmentation using regional loss in real-time Magnetic Resonance Imaging video for speech production</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3113</first_page>
						<last_page>3117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10579</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/roy22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuta</given_name>
<surname>Ide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susumu</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teppei</given_name>
<surname>Nakano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Can Humans Correct Errors From System? Investigating Error Tendencies in Speaker Identification Using Crowdsourcing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5100</first_page>
						<last_page>5104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10580</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ide22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jack</given_name>
<surname>Parry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>DeMattos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Klementiev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Ind</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniela</given_name>
<surname>Morse-Kopp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Clarke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitri</given_name>
<surname>Palaz</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition in the Wild using Multi-task and Adversarial Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1158</first_page>
						<last_page>1162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10581</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/parry22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jash</given_name>
<surname>Rathod</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nauman</given_name>
<surname>Dawalatabad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>SHATRUGHAN</given_name>
<surname>SINGH</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhananjaya</given_name>
<surname>Gowda</surname>
</person_name>
					</contributors>
					<titles><title>Multi-stage Progressive Compression of Conformer Transducer for On-device Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1691</first_page>
						<last_page>1695</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10582</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rathod22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Apoorv</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name>
					</contributors>
					<titles><title>On-demand compute reduction with stochastic wav2vec 2.0</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3048</first_page>
						<last_page>3052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10584</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vyas22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaohuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiya</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deyi</given_name>
<surname>Tuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuren</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4292</first_page>
						<last_page>4296</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10585</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Champion</given_name>
<surname>Pierre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Jouvet</surname>
</person_name>
					</contributors>
					<titles><title>Are disentangled representations all you need to build speaker anonymization systems?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2793</first_page>
						<last_page>2797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10586</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pierre22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingbo</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gerstenberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jintao</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Self-Normalized Importance Sampling for Neural Language Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3909</first_page>
						<last_page>3913</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10588</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Fougeron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ina</given_name>
<surname>Kodrasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parvaneh</given_name>
<surname>Janbakhshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michaela</given_name>
<surname>Pernon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Leveque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Borel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marina</given_name>
<surname>Laganaro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herve</given_name>
<surname>Bourlard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederic</given_name>
<surname>Assal</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of 5 methods for the evaluation of intelligibility in mild to moderate French dysarthric speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2188</first_page>
						<last_page>2192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10590</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fougeron22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinya</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiazheng</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Language Contact on Vowel Nasalization in Wenzhou and Rugao Dialects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5263</first_page>
						<last_page>5267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10591</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Context-aware Multimodal Fusion for Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2013</first_page>
						<last_page>2017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10592</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Donghyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowon</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Phase Vocoder For Time Stretch Based On Center Frequency Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4416</first_page>
						<last_page>4420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10593</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuntao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanchu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sirui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Pay More Attention to History: A Context Modeling Strategy for Conversational Text-to-SQL</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2718</first_page>
						<last_page>2722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10596</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaoxiong</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Babak</given_name>
<surname>Naderi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wafaa</given_name>
<surname>Wardah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Mittag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Culter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuohuang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donald S.</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuzheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shidong</given_name>
<surname>Shang</surname>
</person_name>
					</contributors>
					<titles><title>ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3308</first_page>
						<last_page>3312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10597</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yi22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shichao</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhong</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiliang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wucheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingcheng</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weifeng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Jiang</surname>
</person_name>
					</contributors>
					<titles><title>WideResNet with Joint Representation Learning and Data Augmentation for Cover Song Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4187</first_page>
						<last_page>4191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10600</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hu22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejan</given_name>
<surname>Markovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Richard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Israel Dejene</given_name>
<surname>Gebru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjali</given_name>
<surname>Menon</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Binaural Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1218</first_page>
						<last_page>1222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10603</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsiky</given_name>
<surname>Rakotomalala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Baraduc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Perrier</surname>
</person_name>
					</contributors>
					<titles><title>Trajectories predicted by optimal speech motor control using LSTM networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>630</first_page>
						<last_page>634</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10604</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rakotomalala22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md Iftekhar</given_name>
<surname>Tanveer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Casabuena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jussi</given_name>
<surname>Karlgren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosie</given_name>
<surname>Jones</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1481</first_page>
						<last_page>1485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10605</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tanveer22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ayushi</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sébastien</given_name>
<surname>Le Maguer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Production characteristics of obstruents in WaveNet and older TTS systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2373</first_page>
						<last_page>2377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10606</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pandey22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fareeha S.</given_name>
<surname>Rana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Pape</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabet</given_name>
<surname>Service</surname>
</person_name>
					</contributors>
					<titles><title>The effect of increasing acoustic and linguistic complexity on auditory processing: an EEG study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4048</first_page>
						<last_page>4052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10607</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rana22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christin</given_name>
<surname>Jose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joe</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad Omar</given_name>
<surname>Khursheed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuriy</given_name>
<surname>Mishchenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kulis</surname>
</person_name>
					</contributors>
					<titles><title>Latency Control for Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1891</first_page>
						<last_page>1895</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10608</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jose22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai-Wei</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shang-Wen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5005</first_page>
						<last_page>5009</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10610</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youzheng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xia</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>A Scalable Model Specialization Framework for Training and Inference using Submodels and its Application to Speech Model Personalization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5125</first_page>
						<last_page>5129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10613</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/biadsy22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohd Abbas</given_name>
<surname>Zaidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beomseok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangha</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Decision Regularization for Simultaneous Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>116</first_page>
						<last_page>120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10617</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zaidi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanick</given_name>
<surname>Schraner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Scheller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michel</given_name>
<surname>Plüss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Neukom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manfred</given_name>
<surname>Vogel</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Unsupervised Learning and Supervised Learning with Noisy Labels for Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4875</first_page>
						<last_page>4879</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10620</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/schraner22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Perry</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huayun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name>
					</contributors>
					<titles><title>EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>823</first_page>
						<last_page>827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10626</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lam22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chih-Chiang</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5175</first_page>
						<last_page>5179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10627</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rini</given_name>
<surname>Sharon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heet</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debdoot</given_name>
<surname>Mukherjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Gupta</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual and Multimodal Abuse Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4631</first_page>
						<last_page>4635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10629</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sharon22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sondes</given_name>
<surname>Abderrazek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Fredouille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alain</given_name>
<surname>Ghio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muriel</given_name>
<surname>Lalain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Meunier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name>
					</contributors>
					<titles><title>Validation of the Neuro-Concept Detector framework for the characterization of speech disorders: A comparative study including Dysarthria and Dysphonia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3638</first_page>
						<last_page>3642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10631</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/abderrazek22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julitta</given_name>
<surname>Bartolewska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanisław</given_name>
<surname>Kacprzak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2923</first_page>
						<last_page>2927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10632</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bartolewska22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sébastien</given_name>
<surname>Le Maguer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Back to the Future: Extending the Blizzard Challenge 2013</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2378</first_page>
						<last_page>2382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10633</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lemaguer22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijiang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meishu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilhan</given_name>
<surname>Aslan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>An Overview &#38; Analysis of Sequence-to-Sequence Emotional Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4915</first_page>
						<last_page>4919</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10636</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Using cross-model learnings for the Gram Vaani ASR Challenge 2022</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4880</first_page>
						<last_page>4884</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10639</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/patel22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chau</given_name>
<surname>Luu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the contribution of speaker attributes to speaker separability using disentangled speaker representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>610</first_page>
						<last_page>614</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10643</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/luu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Fernandez-Marques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Mehrotra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Lane</surname>
</person_name>
					</contributors>
					<titles><title>Federated Self-supervised Speech Representations: Are We There Yet?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3809</first_page>
						<last_page>3813</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10644</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gao22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thi Thu Trang</given_name>
<surname>NGUYEN</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Duc Anh</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quoc Viet</given_name>
<surname>Vu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woomyoung</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Building Vietnamese Conversational Smart Home Dataset and Natural Language Understanding Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5180</first_page>
						<last_page>5184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10645</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nguyen22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debasish</given_name>
<surname>Mohapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mario</given_name>
<surname>Fleischer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Zappi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>Three-dimensional finite-difference time-domain acoustic analysis of simplified vocal tract shapes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>764</first_page>
						<last_page>768</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10649</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mohapatra22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hengshun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gongzhen</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoxu</given_name>
<surname>Nian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingdong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shifu</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian-Qing</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Wake Word Spotting in MISP2021 Challenge: Dataset Release and Deep Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1111</first_page>
						<last_page>1115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10650</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Word Discovery in Visually Grounded, Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2823</first_page>
						<last_page>2827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10652</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peng22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Simon</given_name>
<surname>Welker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julius</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2928</first_page>
						<last_page>2932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10653</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/welker22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuntao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Can</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Sha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daxin</given_name>
<surname>Jiang</surname>
</person_name>
					</contributors>
					<titles><title>Small Changes Make Big Differences: Improving Multi-turn Response Selection in Dialogue Systems via Fine-Grained Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2723</first_page>
						<last_page>2727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10656</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wang</given_name>
<surname>Weiran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tongzhou</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Variani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sepand</given_name>
<surname>Mavandadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name>
					</contributors>
					<titles><title>Improving Rare Word Recognition with LM-aware MWER Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1031</first_page>
						<last_page>1035</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10660</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/weiran22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miran</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonjeong</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Vertical Larynx Actions Under Prosodic Focus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>96</first_page>
						<last_page>100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10661</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/oh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Waris</given_name>
<surname>Quamer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Levis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Chukharev-Hudilainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Foreign Accent Conversion without a Native Reference</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4920</first_page>
						<last_page>4924</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10664</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/quamer22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shijun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamed</given_name>
<surname>Hemati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damian</given_name>
<surname>Borth</surname>
</person_name>
					</contributors>
					<titles><title>Generative Data Augmentation Guided by Triplet Loss for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>391</first_page>
						<last_page>395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10667</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leah</given_name>
<surname>Bradshaw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena</given_name>
<surname>Jäger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name>
					</contributors>
					<titles><title>Fundamental Frequency Variability over Time in Telephone Interactions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>101</first_page>
						<last_page>105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10669</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bradshaw22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>AMIR</given_name>
<surname>SHIRIAN</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Somandepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Sanchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanaya</given_name>
<surname>Guha</surname>
</person_name>
					</contributors>
					<titles><title>Visually-aware Acoustic Event Detection using Heterogeneous Graphs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2428</first_page>
						<last_page>2432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10670</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shirian22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Towards End-to-End Private Automatic Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2798</first_page>
						<last_page>2802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10672</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/teixeira22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcel</given_name>
<surname>Schlechtweg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beatriz</given_name>
<surname>Blecua Falgueras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judit</given_name>
<surname>Ayala Alcalde</surname>
</person_name>
					</contributors>
					<titles><title>Language-specific interactions of vowel discrimination in noise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3118</first_page>
						<last_page>3122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10673</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gibson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abner</given_name>
<surname>Hernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Cross-lingual Self-Supervised Speech Representations for Improved Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>51</first_page>
						<last_page>55</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10674</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hernandez22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luc</given_name>
<surname>Ardaillon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Henrich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Perrotin</surname>
</person_name>
					</contributors>
					<titles><title>Voicing decision based on phonemes classification and spectral moments for whisper-to-speech conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2253</first_page>
						<last_page>2257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10675</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ardaillon22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Chatzoudis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manos</given_name>
<surname>Plitsis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyridoula</given_name>
<surname>Stamouli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasia–Lida</given_name>
<surname>Dimou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nassos</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Katsouros</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2178</first_page>
						<last_page>2182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10681</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chatzoudis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama Sanand</given_name>
<surname>Doddipatla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catalin</given_name>
<surname>Zorila</surname>
</person_name>
					</contributors>
					<titles><title>Self-regularised Minimum Latency Training for Streaming Transformer-based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2088</first_page>
						<last_page>2092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10682</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dorina</given_name>
<surname>de Jong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aldo</given_name>
<surname>Pastore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noël</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>D'Ausilio</surname>
</person_name>
					</contributors>
					<titles><title>Speech imitation skills predict automatic phonetic convergence: a GMM-UBM study on L2</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>769</first_page>
						<last_page>773</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10684</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dejong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>K V Vijay</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Konjeti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendra</given_name>
<surname>Vepa</surname>
</person_name>
					</contributors>
					<titles><title>Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised Speech and Text Pre-Trained Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4496</first_page>
						<last_page>4500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10685</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/girish22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul Konstantin</given_name>
<surname>Krug</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Exploration strategies for articulatory synthesis of complex syllable onsets</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>635</first_page>
						<last_page>639</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10689</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vanniekerk22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Bannò</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhanu</given_name>
<surname>Balusu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Kyriakopoulos</surname>
</person_name>
					</contributors>
					<titles><title>View-Specific Assessment of L2 Spoken English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4471</first_page>
						<last_page>4475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10691</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/banno22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katrina Kechun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Schwarz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasper Hong</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Buchanan-Worster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brechtje</given_name>
<surname>Post</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirsty</given_name>
<surname>McDougall</surname>
</person_name>
					</contributors>
					<titles><title>Recording and timing vocal responses in online experimentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4053</first_page>
						<last_page>4057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10697</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karl</given_name>
<surname>El Hajal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Mainar</surname>
</person_name>
					</contributors>
					<titles><title>MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3313</first_page>
						<last_page>3317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10698</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/elhajal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marco</given_name>
<surname>Dinarelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Naguib</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Portet</surname>
</person_name>
					</contributors>
					<titles><title>Toward Low-Cost End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2728</first_page>
						<last_page>2732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10702</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dinarelli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarina</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Denisov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Koch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Tilli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Anonymization with Phonetic Intermediate Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4925</first_page>
						<last_page>4929</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10703</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meyer22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mudit D.</given_name>
<surname>Batra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>JAYESH</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C.S.</given_name>
<surname>Ramalingam</surname>
</person_name>
					</contributors>
					<titles><title>Robust Pitch Estimation Using Multi-Branch CNN-LSTM and 1-Norm LP Residual</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3573</first_page>
						<last_page>3577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10704</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/batra22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongmei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sefik</given_name>
<surname>Emre Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Real Conversational Data for Multi-Channel Continuous Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3814</first_page>
						<last_page>3818</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10706</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohamed Nabih</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Falavigna</given_name>
<surname>Daniele</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Embeddings for Speech Classification in Noisy Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2933</first_page>
						<last_page>2937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10707</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ali22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinjian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>ASR2K: Speech Recognition for Around 2000 Languages without Audio</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4885</first_page>
						<last_page>4889</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10712</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arshdeep</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>A Passive Similarity based CNN Filter Pruning for Efficient Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2433</first_page>
						<last_page>2437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10714</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/singh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wang</given_name>
<surname>Weiran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Align-Refine for Non-autoregressive Deliberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1696</first_page>
						<last_page>1700</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10715</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/weiran22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laura</given_name>
<surname>Spinu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Lilley</surname>
</person_name>
					</contributors>
					<titles><title>Voicing neutralization in Romanian fricatives across different speech styles</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1342</first_page>
						<last_page>1346</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10716</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/spinu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Tzinis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aswin Shanmugam</given_name>
<surname>Subramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paris</given_name>
<surname>Smaragdis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Heterogeneous Target Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1796</first_page>
						<last_page>1800</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10717</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tzinis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreeram</given_name>
<surname>Manghat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sreeja</given_name>
<surname>Manghat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Normalization of code-switched text for speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4297</first_page>
						<last_page>4301</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10719</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/manghat22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Zeineldeen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingjing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Lüscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Improving the Training Recipe for a Robust Conformer-based Hybrid Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1036</first_page>
						<last_page>1040</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10723</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zeineldeen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mathilde</given_name>
<surname>Hutin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Adda-Decker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name>
					</contributors>
					<titles><title>When Phonetics Meets Morphology: Intervocalic Voicing Within and Across Words in Romance Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3438</first_page>
						<last_page>3442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10725</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hutin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianjing</given_name>
<surname>Kuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>May Pik Yu</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nari</given_name>
<surname>Rhee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>The mapping between syntactic and prosodic phrasing in English and Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3443</first_page>
						<last_page>3447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10726</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kuang22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yen-Ju</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuele</given_name>
<surname>Cornell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoheng</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5458</first_page>
						<last_page>5462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10727</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lu22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan Nam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2583</first_page>
						<last_page>2587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10729</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nguyen22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arnon</given_name>
<surname>Turetzky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tzvi</given_name>
<surname>Michelson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shmuel</given_name>
<surname>Peleg</surname>
</person_name>
					</contributors>
					<titles><title>Deep Audio Waveform Prior</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2938</first_page>
						<last_page>2942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10735</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/turetzky22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Sklyar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Piunova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Osendorfer</surname>
</person_name>
					</contributors>
					<titles><title>Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4451</first_page>
						<last_page>4455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10738</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sklyar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Yadavalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Mirishkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1387</first_page>
						<last_page>1391</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10739</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yadavalli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Kuhlmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fritz</given_name>
<surname>Seebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janek</given_name>
<surname>Ebbers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Investigation into Target Speaking Rate Adaptation for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4930</first_page>
						<last_page>4934</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10740</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kuhlmann22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Geoffroy</given_name>
<surname>Vanderreydt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>REMY</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning from Multi-Lingual Speech Translation Benefits Low-Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3053</first_page>
						<last_page>3057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10744</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vanderreydt22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreyan</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samden</given_name>
<surname>Lepcha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Sakshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5185</first_page>
						<last_page>5189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10752</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ghosh22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bronya Roni</given_name>
<surname>Chernyak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Talia</given_name>
<surname>Ben Simon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yael</given_name>
<surname>Segal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeremy</given_name>
<surname>Steffman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Cole</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>DeepFry: Identifying Vocal Fry Using Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3578</first_page>
						<last_page>3582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10756</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chernyak22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theo</given_name>
<surname>Mariotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvio</given_name>
<surname>Montrésor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Hugh</given_name>
<surname>Thomas</surname>
</person_name>
					</contributors>
					<titles><title>Microphone Array Channel Combination Algorithms for Overlapped Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4636</first_page>
						<last_page>4640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10758</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mariotte22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leon</given_name>
<surname>Liebig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Mainka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>An investigation of regression-based prediction of the femininity or masculinity in speech of transgender people</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4676</first_page>
						<last_page>4680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10759</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liebig22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julian</given_name>
<surname>Zaïdi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Seuté</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc-André</given_name>
<surname>Carbonneau</surname>
</person_name>
					</contributors>
					<titles><title>Daft-Exprt: Cross-Speaker Prosody Transfer on Any Text for Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4591</first_page>
						<last_page>4595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10761</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zaidi22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ansen</given_name>
<surname>Antony</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sumanth Reddy</given_name>
<surname>Kota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akhilesh</given_name>
<surname>Lade</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spoorthy</given_name>
<surname>V</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashidhar G.</given_name>
<surname>Koolagudi</surname>
</person_name>
					</contributors>
					<titles><title>An Improved Transformer Transducer Architecture for Hindi-English Code Switched Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3123</first_page>
						<last_page>3127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10763</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/antony22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karolos</given_name>
<surname>Nikitaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Vamvoukakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Klapsas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Markopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyros</given_name>
<surname>Raptis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June Sig</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunu</given_name>
<surname>Jho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aimilios</given_name>
<surname>Chalamandaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pirros</given_name>
<surname>Tsiakoulis</surname>
</person_name>
					</contributors>
					<titles><title>Fine-grained Noise Control for Multispeaker Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>828</first_page>
						<last_page>832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10765</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nikitaras22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helard</given_name>
<surname>Becerra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Ragano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Hines</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4088</first_page>
						<last_page>4092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10766</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/becerra22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Strategies to Improve Robustness of Target Speech Extraction to Enrollment Variations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>996</first_page>
						<last_page>1000</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10767</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sato22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashishkumar</given_name>
<surname>Gudmalwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biplove</given_name>
<surname>Basel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anirban</given_name>
<surname>Dutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ch V</given_name>
<surname>Rama Rao</surname>
</person_name>
					</contributors>
					<titles><title>The Magnitude and Phase based Speech Representation Learning using Autoencoder for Classifying Speech Emotions using Deep Canonical Correlation Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1163</first_page>
						<last_page>1167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10769</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gudmalwar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Schoburg Carrillo de Mira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Haliassos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>SVTS: Scalable Video-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1836</first_page>
						<last_page>1840</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10770</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/schoburgcarrillodemira22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanya</given_name>
<surname>Talkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christina</given_name>
<surname>Manxhari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kara M.</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Quatieri</surname>
</person_name>
					</contributors>
					<titles><title>Speech Acoustics in Mild Cognitive Impairment and Parkinson's Disease With and Without Concurrent Drawing Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2258</first_page>
						<last_page>2262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10772</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/talkar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sishi</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phil</given_name>
<surname>Hoole</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Conceição</given_name>
<surname>Cunha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Kunay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aletheia</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lia Saki Bučar</given_name>
<surname>Shigemori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicitas</given_name>
<surname>Kleber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Voit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Frahm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Harrington</surname>
</person_name>
					</contributors>
					<titles><title>Nasal Coda Loss in the Chengdu Dialect of Mandarin: Evidence from RT-MRI</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1347</first_page>
						<last_page>1351</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10775</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Nonwords Pronunciation Classification in Language Development Tests for Preschool Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3643</first_page>
						<last_page>3647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10777</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baumann22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moakala</given_name>
<surname>Tzudir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Information in Dialect Identification of a Tonal Language: The case of Ao</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2238</first_page>
						<last_page>2242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10779</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tzudir22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mieszko</given_name>
<surname>Fras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Witkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Convolutive Weighted Multichannel Wiener Filter Front-end for Distant Automatic Speech Recognition in Reverberant Multispeaker Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2943</first_page>
						<last_page>2947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10780</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fras22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danilo</given_name>
<surname>de Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tal</given_name>
<surname>Peer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Transformer-based Speech Enhancement Using Long Frames and STFT Magnitudes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2948</first_page>
						<last_page>2952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10781</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/deoliveira22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nathan Joel</given_name>
<surname>Young</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Britain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Leemann</surname>
</person_name>
					</contributors>
					<titles><title>A blueprint for using deepfakes in sociolinguistic matched-guise experiments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5268</first_page>
						<last_page>5272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10782</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/young22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Scott</given_name>
<surname>Wisdom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chet</given_name>
<surname>Gnegy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard F.</given_name>
<surname>Lyon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sagar</given_name>
<surname>Savla</surname>
</person_name>
					</contributors>
					<titles><title>Listening with Googlears: Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3939</first_page>
						<last_page>3943</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10783</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan L.</given_name>
<surname>Preston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elaine</given_name>
<surname>Hitchcock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asif</given_name>
<surname>Salekin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harshit</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>McAllister</surname>
</person_name>
					</contributors>
					<titles><title>PERCEPT-R: An Open-Access American English Child/Clinical Speech Corpus Specialized for the Audio Classification of /ɹ/</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3648</first_page>
						<last_page>3652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10785</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/benway22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongmei</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tien-Ju</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ding</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Motta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>Federated Pruning: Improving Neural Network Efficiency with Federated Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1701</first_page>
						<last_page>1705</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10787</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lin22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wang</given_name>
<surname>Weiran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ding</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>David</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rami</given_name>
<surname>Botros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rina</given_name>
<surname>Panigrahy‎</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McGraw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1706</first_page>
						<last_page>1710</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10791</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ding22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ayush</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijit</given_name>
<surname>Malik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendra</given_name>
<surname>Vepa</surname>
</person_name>
					</contributors>
					<titles><title>Does Utterance entails Intent?: Evaluating Natural Language Inference Based Setup for Few-Shot Intent Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4501</first_page>
						<last_page>4505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10794</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kumar22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Baby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pasquale</given_name>
<surname>D'Alterio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valentin</given_name>
<surname>Mendelev</surname>
</person_name>
					</contributors>
					<titles><title>Incremental learning for RNN-Transducer based speech recognition models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>71</first_page>
						<last_page>75</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10795</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baby22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Berrebbi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Osbel</given_name>
<surname>López-Francisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Amith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3533</first_page>
						<last_page>3537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10796</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/berrebbi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hubert</given_name>
<surname>Siuzdak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Dura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pol</given_name>
<surname>van Rijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nori</given_name>
<surname>Jacoby</surname>
</person_name>
					</contributors>
					<titles><title>WavThruVec: Latent speech representation as intermediate features for neural speech synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>833</first_page>
						<last_page>837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10797</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/siuzdak22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Flint</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>A Step Towards Preserving Speakers’ Identity While Detecting Depression Via Speaker Disentanglement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3338</first_page>
						<last_page>3342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10798</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ravi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wiebke</given_name>
<surname>Toussaint</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauriane</given_name>
<surname>Gorce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron Yi</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>Design Guidelines for Inclusive Speaker Verification Evaluation Datasets</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1293</first_page>
						<last_page>1297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10799</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/toussaint22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshat</given_name>
<surname>Shrivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paden D.</given_name>
<surname>Tomasello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Livshits</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Deliberation Model for On-Device Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3468</first_page>
						<last_page>3472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10800</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/le22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1248</first_page>
						<last_page>1252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10801</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adaeze O.</given_name>
<surname>Adigwe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Klabbers</surname>
</person_name>
					</contributors>
					<titles><title>Strategies for developing a Conversational Speech Dataset for Text-To-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2318</first_page>
						<last_page>2322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10802</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/adigwe22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarenne Carrol</given_name>
<surname>Wallbridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Investigating perception of spoken dialogue acceptability through surprisal</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4506</first_page>
						<last_page>4510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10808</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wallbridge22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phoenix</given_name>
<surname>Meadowlark‎</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukasz</given_name>
<surname>Lew</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shivani</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov‎</surname>
</person_name>
					</contributors>
					<titles><title>4-bit Conformer with Native Quantization Aware Training for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1711</first_page>
						<last_page>1715</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10809</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ding22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ferdy</given_name>
<surname>Hubers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland</given_name>
<surname>van Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>The Effects of Implicit and Explicit Feedback in an ASR-based Reading Tutor for Dutch First-graders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4476</first_page>
						<last_page>4480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10810</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bai22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Roessig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena</given_name>
<surname>Pagel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doris</given_name>
<surname>Muecke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name>
					</contributors>
					<titles><title>ema2wav: doing articulation by Praat</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1352</first_page>
						<last_page>1356</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10813</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/buech22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Flint</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Instance Discriminative Learning for Depression Detection from Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2018</first_page>
						<last_page>2022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10814</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jody</given_name>
<surname>Kreiman</surname>
</person_name>
					</contributors>
					<titles><title>Effects of laryngeal manipulations on voice gender perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1856</first_page>
						<last_page>1860</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10815</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>PRANAV</given_name>
<surname>DHERAM</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murugesan</given_name>
<surname>Ramakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I-Fan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Powell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melissa</given_name>
<surname>Saboowala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Shetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1268</first_page>
						<last_page>1272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10816</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dheram22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kelvin</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriela</given_name>
<surname>Stegmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rene</given_name>
<surname>Utianski</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Impact of Speech Compression on the Acoustics of Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2263</first_page>
						<last_page>2267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10817</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tran22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor D.</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Kumar</surname>
</person_name>
					</contributors>
					<titles><title>Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>689</first_page>
						<last_page>693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10820</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Akeroyd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor J.</given_name>
<surname>Cox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John F.</given_name>
<surname>Culling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Firth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Graetzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Holly</given_name>
<surname>Griffiths</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lara</given_name>
<surname>Harris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graham</given_name>
<surname>Naylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuzanna</given_name>
<surname>Podwinska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eszter</given_name>
<surname>Porter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rhoddy Viveros</given_name>
<surname>Munoz</surname>
</person_name>
					</contributors>
					<titles><title>The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3508</first_page>
						<last_page>3512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10821</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/barker22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Lebourdais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Tahon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>LAURENT</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Meignier</surname>
</person_name>
					</contributors>
					<titles><title>Overlapped speech and gender detection with WavLM pre-trained features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5010</first_page>
						<last_page>5014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10825</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lebourdais22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryandhimas Edo</given_name>
<surname>Zezario</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiou-Shann</given_name>
<surname>Fuh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>MTI-Net: A Multi-Target Speech Intelligibility Prediction Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5463</first_page>
						<last_page>5467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10828</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zezario22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenz</given_name>
<surname>Diener</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sten</given_name>
<surname>Sootla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solomiya</given_name>
<surname>Branets</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ando</given_name>
<surname>Saabas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Aichner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name>
					</contributors>
					<titles><title>INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>580</first_page>
						<last_page>584</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10829</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/diener22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name>
					</contributors>
					<titles><title>Pharyngealization in Amazigh: Acoustic and articulatory marking over time</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3448</first_page>
						<last_page>3452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10831</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/buech22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karim</given_name>
<surname>Helwani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Soltanmohammadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael Mark</given_name>
<surname>Goodwin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arvindh</given_name>
<surname>Krishnaswamy</surname>
</person_name>
					</contributors>
					<titles><title>Clock Skew Robust Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2533</first_page>
						<last_page>2537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10833</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/helwani22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarthak</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neil</given_name>
<surname>Zeghidour</surname>
</person_name>
					</contributors>
					<titles><title>Learning neural audio features without supervision</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>396</first_page>
						<last_page>400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10834</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yadav22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ronit</given_name>
<surname>Damania</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Homan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Prud'hommeaux</surname>
</person_name>
					</contributors>
					<titles><title>Combining Simple but Novel Data Augmentation Methods for Improving Conformer ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4890</first_page>
						<last_page>4894</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10835</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/damania22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryandhimas</given_name>
<surname>Edo Zezario</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiou-Shann</given_name>
<surname>Fuh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3944</first_page>
						<last_page>3948</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10838</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/edozezario22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Maekaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3819</first_page>
						<last_page>3823</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10839</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julia</given_name>
<surname>Koch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadja</given_name>
<surname>Schauffler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toni</given_name>
<surname>Bernhart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Dieterle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Kuhn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Viehhauser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc</given_name>
<surname>Thang Vu</surname>
</person_name>
					</contributors>
					<titles><title>PoeticTTS - Controllable Poetry Reading for Literary Studies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1223</first_page>
						<last_page>1227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10841</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/koch22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jack</given_name>
<surname>Deadman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>266</first_page>
						<last_page>270</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10842</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/deadman22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Paturi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name>
					</contributors>
					<titles><title>Directed speech separation for automatic speech recognition of long form conversational speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5388</first_page>
						<last_page>5392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10843</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/paturi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Radfar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Barnwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupak Vignesh</given_name>
<surname>Swaminathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-Ju</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Susanj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Mouchtaris</surname>
</person_name>
					</contributors>
					<titles><title>ConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers for Streaming Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4431</first_page>
						<last_page>4435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10844</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/radfar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoonjeong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jody</given_name>
<surname>Kreiman</surname>
</person_name>
					</contributors>
					<titles><title>Linguistic versus biological factors governing acoustic voice variation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>640</first_page>
						<last_page>643</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10847</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srikanth Raj</given_name>
<surname>Chetupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuël</given_name>
<surname>Habets</surname>
</person_name>
					</contributors>
					<titles><title>Speech Separation for an Unknown Number of Speakers Using Transformers With Encoder-Decoder Attractors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5393</first_page>
						<last_page>5397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10849</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chetupalli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Josh</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Adelani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alp</given_name>
<surname>Öktem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Whitenack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salomon</given_name>
<surname>KABONGO KABENAMUALU</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Salesky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iroro</given_name>
<surname>Orife</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Leong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Perez</given_name>
<surname>Ogayo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Chinenye Emezue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Mukiibi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salomey</given_name>
<surname>Osei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apelete</given_name>
<surname>AGBOLO</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Akinode</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernard</given_name>
<surname>Opoku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olanrewaju</given_name>
<surname>Samuel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesujoba</given_name>
<surname>Alabi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shamsuddeen Hassan</given_name>
<surname>Muhammad</surname>
</person_name>
					</contributors>
					<titles><title>BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2383</first_page>
						<last_page>2387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10850</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meyer22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eleftherios</given_name>
<surname>Kapelonis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2733</first_page>
						<last_page>2737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10852</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kapelonis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Laptev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somshubra</given_name>
<surname>Majumdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>CTC Variations Through New WFST Topologies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1041</first_page>
						<last_page>1045</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10854</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/laptev22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pol</given_name>
<surname>van Rijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvan</given_name>
<surname>Mertes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Schiller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Dura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hubert</given_name>
<surname>Siuzdak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter M. C.</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeth</given_name>
<surname>André</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nori</given_name>
<surname>Jacoby</surname>
</person_name>
					</contributors>
					<titles><title>VoiceMe: Personalized voice generation in TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2588</first_page>
						<last_page>2592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10855</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vanrijn22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Klapsas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karolos</given_name>
<surname>Nikitaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Vamvoukakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panagiotis</given_name>
<surname>Kakoulidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Markopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyros</given_name>
<surname>Raptis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June Sig</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunu</given_name>
<surname>Jho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aimilios</given_name>
<surname>Chalamandaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pirros</given_name>
<surname>Tsiakoulis</surname>
</person_name>
					</contributors>
					<titles><title>Self supervised learning for robust voice cloning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4935</first_page>
						<last_page>4939</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10856</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/klapsas22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Chia</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Pawlicki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marko</given_name>
<surname>Stamenovic</surname>
</person_name>
					</contributors>
					<titles><title>CCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3318</first_page>
						<last_page>3322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10857</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christina</given_name>
<surname>Sartzetaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Extending Compositional Attention Networks for Social Reasoning in Videos</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1116</first_page>
						<last_page>1120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10858</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sartzetaki22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Venkatesh Shenoy</given_name>
<surname>Kadandale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan F.</given_name>
<surname>Montesinos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gloria</given_name>
<surname>Haro</surname>
</person_name>
					</contributors>
					<titles><title>VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3128</first_page>
						<last_page>3132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10861</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kadandale22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youxiang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohui</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John A.</given_name>
<surname>Batsis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert M.</given_name>
<surname>Roth</surname>
</person_name>
					</contributors>
					<titles><title>Domain-aware Intermediate Pretraining for Dementia Detection with Limited Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2183</first_page>
						<last_page>2187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10862</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhu22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Antonova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Bakhturina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Thutmose Tagger: Single-pass neural model for Inverse Text Normalization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>550</first_page>
						<last_page>554</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10864</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/antonova22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Teena tom</given_name>
<surname>Dieck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomas</given_name>
<surname>Arias</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name>
					</contributors>
					<titles><title>Wav2vec behind the Scenes: How end2end Models learn Phonetics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5130</first_page>
						<last_page>5134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10865</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dieck22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Richard</given_name>
<surname>Rose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Siohan</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End multi-talker audio-visual ASR using an active speaker attention module</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2828</first_page>
						<last_page>2832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10866</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rose22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nasim Mahdinazhad</given_name>
<surname>Sardhaei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marzena</given_name>
<surname>Zygis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamid</given_name>
<surname>Sharifzadeh</surname>
</person_name>
					</contributors>
					<titles><title>How do our eyebrows respond to masks and whispering? The case of Persians</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2023</first_page>
						<last_page>2027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10867</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sardhaei22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beiming</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Teplansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nordine</given_name>
<surname>Sebkhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arpan</given_name>
<surname>Bhavsar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Omer</given_name>
<surname>Inan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Samlan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ted</given_name>
<surname>Mau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation for End-to-end Silent Speech Recognition for Laryngectomees</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3653</first_page>
						<last_page>3657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10868</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cao22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>May Pik Yu</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June</given_name>
<surname>Choe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aini</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiran</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Holliday</surname>
</person_name>
					</contributors>
					<titles><title>Training and typological bias in ASR performance for world Englishes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1273</first_page>
						<last_page>1277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10869</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katariina</given_name>
<surname>Martikainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jussi</given_name>
<surname>Karlgren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khiet</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>Exploring audio-based stylistic variation in podcasts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2343</first_page>
						<last_page>2347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10871</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/martikainen22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshat</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai Krishna</given_name>
<surname>Rallabandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name>
					</contributors>
					<titles><title>Intent classification using pre-trained language agnostic embeddings for low resource languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3473</first_page>
						<last_page>3477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10873</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yadav22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul Konstantin</given_name>
<surname>Krug</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel Rudolph</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Synthesis for Data Augmentation in Phoneme Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1228</first_page>
						<last_page>1232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10874</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/krug22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc-Antoine</given_name>
<surname>Georges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Schwartz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>774</first_page>
						<last_page>778</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10876</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/georges22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tasnima</given_name>
<surname>Sadekova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Gogoryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Vovk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vadim</given_name>
<surname>Popov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikhail</given_name>
<surname>Kudinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiansheng</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>A Unified System for Voice Cloning and Voice Conversion through Diffusion Probabilistic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3003</first_page>
						<last_page>3007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10879</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sadekova22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abner</given_name>
<surname>Hernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomas</given_name>
<surname>Arias</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patricia</given_name>
<surname>Lillo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Slachevsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adolfo Martín</given_name>
<surname>García</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas K.</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2483</first_page>
						<last_page>2487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10883</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pereztoro22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Analysis of Self-supervised Representations of English Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3583</first_page>
						<last_page>3587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10884</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wells22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karan</given_name>
<surname>Singla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahab</given_name>
<surname>Jalalvand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeon-Jun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Price</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Pressel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivas</given_name>
<surname>Bangalore</surname>
</person_name>
					</contributors>
					<titles><title>Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3963</first_page>
						<last_page>3967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10885</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/singla22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomasz</given_name>
<surname>Rutowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Harati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Shriberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Chlebek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Oliveira</surname>
</person_name>
					</contributors>
					<titles><title>Toward Corpus Size Requirements for Training and Evaluating Depression Risk Models Using Spoken Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3343</first_page>
						<last_page>3347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10888</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rutowski22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Vovk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tasnima</given_name>
<surname>Sadekova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Gogoryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vadim</given_name>
<surname>Popov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikhail</given_name>
<surname>Kudinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiansheng</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Fast Grad-TTS: Towards Efficient Diffusion-Based Speech Generation on CPU</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>838</first_page>
						<last_page>842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10889</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vovk22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Dalmia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Two-Pass Low Latency End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3478</first_page>
						<last_page>3482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10890</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/arora22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Low-Latency Online Streaming VideoQA Using Audio-Visual Transformers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4511</first_page>
						<last_page>4515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10891</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hori22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala Krishna</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Deep Speech Synthesis from Articulatory Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>779</first_page>
						<last_page>783</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10892</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinzheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiushi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D.</given_name>
<surname>Plumbley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Separate What You Describe: Language-Queried Audio Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1801</first_page>
						<last_page>1805</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10894</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Amid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Om Dipakbhai</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Mathews</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>Extracting Targeted Training Data from ASR Models, and How to Mitigate It</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2803</first_page>
						<last_page>2807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10895</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/amid22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Herzig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diana</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Speaker adaptation for Wav2vec2 based dysarthric ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3403</first_page>
						<last_page>3407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10896</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baskar22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuoyu</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feier</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haining</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Mandarin Tone Sandhi Realization: Evidence from Large Speech Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5273</first_page>
						<last_page>5277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10897</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tian22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velázquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Joint domain adaptation and speech bandwidth extension using time-domain GANs for speaker verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>615</first_page>
						<last_page>619</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10900</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kataria22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhouyuan</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shefali</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ananya</given_name>
<surname>Misra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Siddhartha‎</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>Incremental Layer-Wise Self-Supervised Learning for Efficient Unsupervised Speech Domain Adaptation On Device</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4845</first_page>
						<last_page>4849</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10904</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rahil</given_name>
<surname>Parikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harshavardhan</given_name>
<surname>Sundar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyros</given_name>
<surname>Matsoukas</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Acoustic Event Tagging on Scene Classification in a Multi-Task Learning Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4192</first_page>
						<last_page>4196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10905</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/parikh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Cartwright</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Pablo</given_name>
<surname>Bello</surname>
</person_name>
					</contributors>
					<titles><title>Active Few-Shot Learning for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1551</first_page>
						<last_page>1555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10907</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sebastian Peter</given_name>
<surname>Bayerl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Noeth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2868</first_page>
						<last_page>2872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10908</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bayerl22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>W. Ronny</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Om Dipakbhai</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Mathews</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Unintended Memorization in Language-Model-Fused ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2808</first_page>
						<last_page>2812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10909</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Boram</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Yamaguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Fougeron</surname>
</person_name>
					</contributors>
					<titles><title>Why is Korean lenis stop difficult to perceive for L2 Korean learners?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1861</first_page>
						<last_page>1865</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10912</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theresa</given_name>
<surname>Breiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swaroop</given_name>
<surname>Ramaswamy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Variani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shefali</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Mathews</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kilol</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingqing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lara</given_name>
<surname>McConnaughey</surname>
</person_name>
					</contributors>
					<titles><title>UserLibri: A Dataset for ASR Personalization Using Only Text</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>694</first_page>
						<last_page>698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10915</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/breiner22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Umar</given_name>
<surname>Farooq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Impact of Crosslingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3849</first_page>
						<last_page>3853</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10916</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/farooq22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Szu-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiamin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3058</first_page>
						<last_page>3062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10917</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dmitriy</given_name>
<surname>Serdyuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Otavio</given_name>
<surname>Braga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Siohan</surname>
</person_name>
					</contributors>
					<titles><title>Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Muti-Person Video</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2833</first_page>
						<last_page>2837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10920</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/serdyuk22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Alenin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Torgashov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Okhotnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rostislav</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Yakovlev</surname>
</person_name>
					</contributors>
					<titles><title>A Subnetwork Approach for Spoofing Aware Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2888</first_page>
						<last_page>2892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10921</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/alenin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Maniati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Vioni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karolos</given_name>
<surname>Nikitaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Klapsas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June Sig</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunu</given_name>
<surname>Jho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aimilios</given_name>
<surname>Chalamandaris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pirros</given_name>
<surname>Tsiakoulis</surname>
</person_name>
					</contributors>
					<titles><title>SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2388</first_page>
						<last_page>2392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10922</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/maniati22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Nespoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Barreda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Naylor</surname>
</person_name>
					</contributors>
					<titles><title>Relative Acoustic Features for Distance Estimation in Smart-Homes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>724</first_page>
						<last_page>728</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10925</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nespoli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rahil</given_name>
<surname>Parikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shihab</given_name>
<surname>Shamma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic To Articulatory Speech Inversion Using Multi-Resolution Spectro-Temporal Representations Of Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4681</first_page>
						<last_page>4685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10926</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/parikh22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panagiotis</given_name>
<surname>Tzirakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeff</given_name>
<surname>Brooks</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Opara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Gregory</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Metrick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Garrett</given_name>
<surname>Boseck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dacher</given_name>
<surname>Keltner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>Cowen</surname>
</person_name>
					</contributors>
					<titles><title>State &#38; Trait Measurement from Nonverbal Vocalizations: A Multi-Task Joint Learning Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2028</first_page>
						<last_page>2032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10927</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baird22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Cord-Landwehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thilo</given_name>
<surname>von Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>An Initialization Scheme for Meeting Separation with Spatial Mixture Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>271</first_page>
						<last_page>275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10929</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/boeddeker22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thibault Bañeras</given_name>
<surname>Roux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mickael</given_name>
<surname>Rouvier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jane</given_name>
<surname>Wottawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Dufour</surname>
</person_name>
					</contributors>
					<titles><title>Qualitative Evaluation of Language Model Rescoring in Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3968</first_page>
						<last_page>3972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10931</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/roux22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>MISEUL</given_name>
<surname>KIM</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ZHENYU</given_name>
<surname>PIAO</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ran</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaemin</given_name>
<surname>Joh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungshin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Light-Weight Speaker Verification with Global Context Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5105</first_page>
						<last_page>5109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10932</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longxiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamid Reza</given_name>
<surname>Hassanzadeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schaaf</surname>
</person_name>
					</contributors>
					<titles><title>Extract and Abstract with BART for Clinical Notes from Doctor-Patient Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2488</first_page>
						<last_page>2492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10935</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/su22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name>
					</contributors>
					<titles><title>MAESTRO: Matched Speech Text Representations through Modality Matching</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4093</first_page>
						<last_page>4097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10937</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Cherry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Conneau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobu</given_name>
<surname>Morioka</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1721</first_page>
						<last_page>1725</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10938</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jia22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yann</given_name>
<surname>TEYTAUT</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baptiste</given_name>
<surname>Bouvier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Roebel</surname>
</person_name>
					</contributors>
					<titles><title>A study on constraining Connectionist Temporal Classification for temporal audio alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5015</first_page>
						<last_page>5019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10940</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/teytaut22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ayimnisagul</given_name>
<surname>Ablimit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Scholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning Approaches for Detecting Alzheimer’s Dementia from Conversational Speech of ILSE Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3348</first_page>
						<last_page>3352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10942</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ablimit22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mimansa</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minxue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristina</given_name>
<surname>Gorrostieta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Roddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kye</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reza</given_name>
<surname>Lotfian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Kane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Mind the gap: On the value of silence representations to lexical-based speech emotion recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>156</first_page>
						<last_page>160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10943</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/perez22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mirek</given_name>
<surname>Novak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavlos</given_name>
<surname>Papadopoulos</surname>
</person_name>
					</contributors>
					<titles><title>RNN-T lattice enhancement by grafting of pruned paths</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4960</first_page>
						<last_page>4964</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10945</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/novak22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Léane</given_name>
<surname>Salais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Arias</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clément</given_name>
<surname>Le Moine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Rosi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yann</given_name>
<surname>TEYTAUT</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Obin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Roebel</surname>
</person_name>
					</contributors>
					<titles><title>Production Strategies of Vocal Attitudes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4985</first_page>
						<last_page>4989</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10947</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/salais22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minho</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chelsea</given_name>
<surname>Ju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeya</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi Chieh</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Reweighting for Speaker Verification Fairness</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4800</first_page>
						<last_page>4804</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10948</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jin22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avamarie</given_name>
<surname>Brueggeman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Trait Enhancement for Cochlear Implant Users: A Case Study for Speaker Emotion Perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2268</first_page>
						<last_page>2272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10951</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/brueggeman22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Post</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Streaming End-to-End Speech Translation with Neural Transducers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3263</first_page>
						<last_page>3267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10953</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xue22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dario</given_name>
<surname>Albesano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Andrés-Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicola</given_name>
<surname>Ferri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puming</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>On the Prediction Network Architecture in RNN-T for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2093</first_page>
						<last_page>2097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10954</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/albesano22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erik</given_name>
<surname>Ekstedt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name>
					</contributors>
					<titles><title>Voice Activity Projection: Self-supervised Learning of Turn-taking Events</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5190</first_page>
						<last_page>5194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10955</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ekstedt22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaozhou</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruying</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William M.</given_name>
<surname>Campbell</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Embedding for ASR Robustness in Entity Resolution</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3268</first_page>
						<last_page>3272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10956</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhou22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arlo</given_name>
<surname>Faria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Janin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidhi</given_name>
<surname>Adkoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Toward Zero Oracle Word Error Rate on the Switchboard Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3973</first_page>
						<last_page>3977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10959</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/faria22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alan</given_name>
<surname>Baade</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>MAE-AST: Masked Autoencoding Audio Spectrogram Transformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2438</first_page>
						<last_page>2442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10961</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/baade22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manthan</given_name>
<surname>Thakker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sefik Emre</given_name>
<surname>Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huaming</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Fast Real-time Personalized Speech Enhancement: End-to-End Enhancement Network (E3Net) and Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>991</first_page>
						<last_page>995</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10962</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/thakker22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenthe</given_name>
<surname>Thienpondt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2213</first_page>
						<last_page>2217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10964</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/thienpondt22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Fougeron</surname>
</person_name>
					</contributors>
					<titles><title>Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4755</first_page>
						<last_page>4759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10965</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/audibert22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Few-Shot Fine-Tuning Strategies for Models of Visually Grounded Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1416</first_page>
						<last_page>1420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10966</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/miller22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giang</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chilin</given_name>
<surname>Shih</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>A Laryngographic Study on the Voice Quality of Northern Vietnamese Tones under the Lombard Effect</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5278</first_page>
						<last_page>5282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10970</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/le22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amrit</given_name>
<surname>Romana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minxue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Roberts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1916</first_page>
						<last_page>1920</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10971</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/romana22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kay</given_name>
<surname>Peterson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Audrey</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>OpenASR21: The Second Open Challenge for Automatic Speech Recognition of Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4895</first_page>
						<last_page>4899</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10972</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peterson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ambika</given_name>
<surname>Kirkland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harm</given_name>
<surname>Lameris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva</given_name>
<surname>Szekely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name>
					</contributors>
					<titles><title>Where's the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4990</first_page>
						<last_page>4994</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10973</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kirkland22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sankaran</given_name>
<surname>Panchapagesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Turaj Zakizadeh</given_name>
<surname>Shabestary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Howard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Walker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gruenstein</surname>
</person_name>
					</contributors>
					<titles><title>A Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2538</first_page>
						<last_page>2542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10974</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/panchapagesan22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Defense against Adversarial Attacks on Hybrid Speech Recognition System using Adversarial Fine-tuning with Denoiser</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5035</first_page>
						<last_page>5039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10977</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/joshi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helen</given_name>
<surname>Gent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chase</given_name>
<surname>Adams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chilin</given_name>
<surname>Shih</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning for Prosody-Based Irony Classification in Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3993</first_page>
						<last_page>3997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10978</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gent22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marzena</given_name>
<surname>Zygis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Wesolek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nina</given_name>
<surname>Hosseini-Kivanani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manfred</given_name>
<surname>Krifka</surname>
</person_name>
					</contributors>
					<titles><title>The Prosody of Cheering in Sport Events</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5283</first_page>
						<last_page>5287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10982</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zygis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>AdvEst: Adversarial Perturbation Estimation to Classify and Detect Adversarial Attacks against Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5060</first_page>
						<last_page>5064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10985</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/joshi22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maria del Mar</given_name>
<surname>Cordero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ambre</given_name>
<surname>Denis-Noël</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elsa</given_name>
<surname>Spinelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanny</given_name>
<surname>Meunier</surname>
</person_name>
					</contributors>
					<titles><title>Neural correlates of acoustic and semantic cues during speech segmentation in French</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4058</first_page>
						<last_page>4062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10986</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cordero22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ali</given_name>
<surname>Siahkoohi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Chinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Denton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>W. Bastiaan</given_name>
<surname>Kleijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Skoglund</surname>
</person_name>
					</contributors>
					<titles><title>Ultra-Low-Bitrate Speech Coding with Pretrained Transformers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4421</first_page>
						<last_page>4425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10988</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/siahkoohi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Minimum latency training of sequence transducers for streaming end-to-end speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2098</first_page>
						<last_page>2102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10989</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shinohara22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gary</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesse</given_name>
<surname>Emond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinghui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Non-Parallel Voice Conversion for ASR Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3408</first_page>
						<last_page>3412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10990</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Fox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Delworth</surname>
</person_name>
					</contributors>
					<titles><title>Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3914</first_page>
						<last_page>3918</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10991</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fox22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ge</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan-Pablo</given_name>
<surname>Caceres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Salamon</surname>
</person_name>
					</contributors>
					<titles><title>Filler Word Detection and Classification: A Dataset and Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3769</first_page>
						<last_page>3773</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10992</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhu22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Monica</given_name>
<surname>Ashokumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Schwartz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Ito</surname>
</person_name>
					</contributors>
					<titles><title>Orofacial somatosensory inputs in speech perceptual training modulate speech production</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>784</first_page>
						<last_page>787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10993</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ashokumar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tae-Woo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Su</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gyeong-Hoon</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3008</first_page>
						<last_page>3012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10994</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Challenges of using longitudinal and cross-domain corpora on studies of pathological speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1921</first_page>
						<last_page>1925</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-10995</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/botelho22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ramit</given_name>
<surname>Sawhney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megh</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwa</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puneet</given_name>
<surname>Mathur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasu</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh</given_name>
<surname>Manocha</surname>
</person_name>
					</contributors>
					<titles><title>PISA: PoIncaré Saliency-Aware Interpolative Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2663</first_page>
						<last_page>2667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11001</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sawhney22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wo Jae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuele</given_name>
<surname>Coviello</surname>
</person_name>
					</contributors>
					<titles><title>A Multimodal Strategy for Singing Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2243</first_page>
						<last_page>2247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11007</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Goncalves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1168</first_page>
						<last_page>1172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11012</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/goncalves22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woosung</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Neural Vocoder is All You Need for Speech Super-resolution</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4227</first_page>
						<last_page>4231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11017</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Nagamine</surname>
</person_name>
					</contributors>
					<titles><title>Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>644</first_page>
						<last_page>648</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11020</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nagamine22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiyi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gil</given_name>
<surname>Keren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yatharth</given_name>
<surname>Saraf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelrahman</given_name>
<surname>Mohamed</surname>
</person_name>
					</contributors>
					<titles><title>Scaling ASR Improves Zero and Few Shot Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5135</first_page>
						<last_page>5139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11023</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zheng22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Corey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manan</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanad</given_name>
<surname>Sarkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew C.</given_name>
<surname>Singer</surname>
</person_name>
					</contributors>
					<titles><title>Cooperative Speech Separation With a Microphone Array and Asynchronous Wearable Devices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5398</first_page>
						<last_page>5402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11025</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/corey22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4232</first_page>
						<last_page>4236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11026</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhongwei</given_name>
<surname>Teng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quchen</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jules</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Powell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>Schmidt</surname>
</person_name>
					</contributors>
					<titles><title>SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4391</first_page>
						<last_page>4395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11029</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/teng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heting</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junrui</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaizhi</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>WavPrompt: Towards Few-Shot Spoken Language Understanding with Frozen Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2738</first_page>
						<last_page>2742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11031</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gao22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sravya</given_name>
<surname>Popuri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng-Jen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Pino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatao</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ann</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5195</first_page>
						<last_page>5199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11032</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/popuri22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongseong</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhouyuan</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name>
					</contributors>
					<titles><title>Pseudo Label Is Better Than Human Label</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1421</first_page>
						<last_page>1425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11034</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hwang22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Bittar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian Recurrent Units and the Forward-Backward Algorithm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4137</first_page>
						<last_page>4141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11035</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bittar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruibin</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaxter</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2593</first_page>
						<last_page>2597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11036</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yuan22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name>
					</contributors>
					<titles><title>Contribution of the glottal flow residual in affect-related voice transformation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5288</first_page>
						<last_page>5292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11038</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Hirschi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen Daniel</given_name>
<surname>Looney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okim</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4481</first_page>
						<last_page>4485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11039</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huang-Cheng</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>161</first_page>
						<last_page>165</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11041</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chou22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Burke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Exner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandy</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Cornish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doug</given_name>
<surname>Habberstad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Pautler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Suendermann-Oeft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Statistical and clinical utility of multimodal dialogue-based speech and facial metrics for Parkinson's disease assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3658</first_page>
						<last_page>3662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11048</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kothare22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Hard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kurt</given_name>
<surname>Partridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sean</given_name>
<surname>Augenstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aishanee</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ignacio</given_name>
<surname>Lopez-Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Mathews</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>Production federated keyword spotting via distillation, filtering, and joint federated-centralized training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>76</first_page>
						<last_page>80</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11050</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hard22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Kilgour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beat</given_name>
<surname>Gfeller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingqing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aren</given_name>
<surname>Jansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Scott</given_name>
<surname>Wisdom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Tagliasacchi</surname>
</person_name>
					</contributors>
					<titles><title>Text-Driven Separation of Arbitrary Sounds</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5403</first_page>
						<last_page>5407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11052</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kilgour22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixiong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Ravichandran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4805</first_page>
						<last_page>4809</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11053</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeonjin</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trang</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Prosody for Punctuation Prediction of Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>555</first_page>
						<last_page>559</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11061</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cho22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viet Anh</given_name>
<surname>Trinh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pegah</given_name>
<surname>Ghahremani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasha</given_name>
<surname>Droppo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1298</first_page>
						<last_page>1302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11063</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/trinh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4426</first_page>
						<last_page>4430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11065</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/miao22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Artem</given_name>
<surname>Ploujnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>486</first_page>
						<last_page>490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11066</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ploujnikov22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milind</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Tiwari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>PRANAV</given_name>
<surname>DHERAM</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bryan</given_name>
<surname>Anderson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chul</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bach</given_name>
<surname>Bui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>On joint training with interfaces for spoken language understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1253</first_page>
						<last_page>1257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11067</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/raju22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander H.</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-I</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Simple and Effective Unsupervised Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>843</first_page>
						<last_page>847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11071</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Bakhturina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>491</first_page>
						<last_page>495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11074</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bakhturina22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Aizawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsutoshi</given_name>
<surname>Itoyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Nakadai</surname>
</person_name>
					</contributors>
					<titles><title>Weakly-Supervised Neural Full-Rank Spatial Covariance Analysis for a Front-End System of Distant Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3824</first_page>
						<last_page>3828</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11077</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bando22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phani Sankar</given_name>
<surname>Nidadavolu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Na</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nick</given_name>
<surname>Jutila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi Teja</given_name>
<surname>Gadde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aswarth Abhilash</given_name>
<surname>Dara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Savold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sapan</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Hoff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veerdhawal</given_name>
<surname>Pande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Crews</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name>
					</contributors>
					<titles><title>RefTextLAS: Reference Text Biased Listen, Attend, and Spell Model For Accurate Reading Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4347</first_page>
						<last_page>4351</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11078</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nidadavolu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinmiao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Waseem</given_name>
<surname>Gharbieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhui</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han Suk</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Chul</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5200</first_page>
						<last_page>5204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11080</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/huang22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Vuong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Stern</surname>
</person_name>
					</contributors>
					<titles><title>Improved Modulation-Domain Loss for Neural-Network-based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>206</first_page>
						<last_page>210</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11082</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vuong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suliang</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunxin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuo</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Steering vector correction in MVDR beamformer for speech enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5468</first_page>
						<last_page>5472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11084</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hira</given_name>
<surname>Dhamyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>166</first_page>
						<last_page>170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11085</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dhamyal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yogesh</given_name>
<surname>Virkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcello</given_name>
<surname>Federico</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Enyedi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic alignment for off-screen automatic dubbing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>496</first_page>
						<last_page>500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11089</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/virkar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Waseem</given_name>
<surname>Gharbieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinmiao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhui</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han Suk</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Chul</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>DyConvMixer: Dynamic Convolution Mixer Architecture for Open-Vocabulary Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5205</first_page>
						<last_page>5209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11090</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gharbieh22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muqiao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Lane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Online Continual Learning of End-to-End Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2668</first_page>
						<last_page>2672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11093</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samik</given_name>
<surname>Sadhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Complex Frequency Domain Linear Prediction: A Tool to Compute Modulation Spectrum of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3208</first_page>
						<last_page>3212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11095</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sadhu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kataria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Chunking Defense for Adversarial Attacks on ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5045</first_page>
						<last_page>5049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11096</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shao22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Depression Severity Score Prediction Using Articulatory Coordination Features and Hierarchical Attention Based Text Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3353</first_page>
						<last_page>3357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11099</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/seneviratne22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katharine</given_name>
<surname>Patterson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Scott</given_name>
<surname>Wisdom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name>
					</contributors>
					<titles><title>Distance-Based Sound Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>901</first_page>
						<last_page>905</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11100</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/patterson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeonghyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kangwook</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahyun</given_name>
<surname>Goo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngmoon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoi Rin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3588</first_page>
						<last_page>3592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11112</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianyuan</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Plumbley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>On Metric Learning for Audio-Text Cross-Modal Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4142</first_page>
						<last_page>4146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11115</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mei22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuma</given_name>
<surname>Udagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuyasu</given_name>
<surname>Itoh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name>
					</contributors>
					<titles><title>Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3919</first_page>
						<last_page>3923</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11123</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/udagawa22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raymond</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Synthesizing Near Native-accented Speech for a Non-native Speaker by Imitating the Pronunciation and Prosody of a Native Speaker</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4302</first_page>
						<last_page>4306</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11124</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chung22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Carne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuko</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shunichi</given_name>
<surname>Ishihara</surname>
</person_name>
					</contributors>
					<titles><title>High level feature fusion in forensic voice comparison</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5293</first_page>
						<last_page>5297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11127</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/carne22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruchao</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children’s ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4900</first_page>
						<last_page>4904</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11128</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fan22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bishal</given_name>
<surname>Lamichhane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nidal</given_name>
<surname>Moukaddam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankit B.</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Sabharwal</surname>
</person_name>
					</contributors>
					<titles><title>Dyadic Interaction Assessment from Free-living Audio for Depression Severity Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2493</first_page>
						<last_page>2497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11129</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lamichhane22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Reo</given_name>
<surname>Yoneyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>848</first_page>
						<last_page>852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11130</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yoneyama22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juliana N.</given_name>
<surname>Saba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Speech Modification for Intelligibility in Cochlear Implant Listeners: Individual Effects of Vowel- and Consonant-Boosting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5473</first_page>
						<last_page>5477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11131</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saba22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun-Wook</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ohsung</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Jae</given_name>
<surname>Hwang</surname>
</person_name>
					</contributors>
					<titles><title>Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4596</first_page>
						<last_page>4600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11133</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yoon22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amruta</given_name>
<surname>Saraf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elie</given_name>
<surname>Khoury</surname>
</person_name>
					</contributors>
					<titles><title>Confidence Measure for Automatic Age Estimation From Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2033</first_page>
						<last_page>2037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11135</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/saraf22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Derek</given_name>
<surname>Tam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Surafel M.</given_name>
<surname>Lakew</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yogesh</given_name>
<surname>Virkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Mathur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcello</given_name>
<surname>Federico</surname>
</person_name>
					</contributors>
					<titles><title>Isochrony-Aware Neural Machine Translation for Automatic Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1776</first_page>
						<last_page>1780</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11136</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tam22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juhan</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroko</given_name>
<surname>Terasawa</surname>
</person_name>
					</contributors>
					<titles><title>Deformable CNN and Imbalance-Aware Feature Learning for Singing Technique Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2778</first_page>
						<last_page>2782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11137</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yamamoto22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Sustek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samik</given_name>
<surname>Sadhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Dealing with Unknowns in Continual Learning for End-to-end Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1046</first_page>
						<last_page>1050</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11139</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sustek22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas R.</given_name>
<surname>O'Malley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A universally-deployable ASR frontend for joint acoustic echo cancellation, speech enhancement, and voice separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3829</first_page>
						<last_page>3833</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11140</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/omalley22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaejin</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghavendra</given_name>
<surname>Pappagari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano Moro</given_name>
<surname>Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Non-contrastive self-supervised learning of utterance-level speech representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4028</first_page>
						<last_page>4032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11141</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/cho22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiyi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tarun</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyu</given_name>
<surname>Zhai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Seltzer</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3978</first_page>
						<last_page>3982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11144</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kim22p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Seki</surname>
</person_name>
					</contributors>
					<titles><title>MISRNet: Lightweight Neural Vocoder Using Multi-Input Single Shared Residual Blocks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1631</first_page>
						<last_page>1635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11152</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kaneko22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dejan</given_name>
<surname>Markovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Defossez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Richard</surname>
</person_name>
					</contributors>
					<titles><title>Implicit Neural Spatial Filtering for Multichannel Source Separation in the Waveform Domain</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1806</first_page>
						<last_page>1810</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11153</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/markovic22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nianzu</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liqun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenyong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu Ting</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baohua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanyuan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qun</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation Detection and Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4352</first_page>
						<last_page>4356</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11155</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zheng22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Densely-connected Convolutional Recurrent Network for Fundamental Frequency Estimation in Noisy Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>401</first_page>
						<last_page>405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11156</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neha</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonjeong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh K.</given_name>
<surname>Chhetri</surname>
</person_name>
					</contributors>
					<titles><title>Optimal thyroplasty implant shape and stiffness for treatment of acute unilateral vocal fold paralysis: Evidence from a canine in vivo phonation model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2273</first_page>
						<last_page>2277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11158</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/reddy22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prateeth</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anmol</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shivesh</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Shum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Sigtia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Marchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Varun</given_name>
<surname>Lakshminarasimhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsik</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Adya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra</given_name>
<surname>Dhir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Tewfik</surname>
</person_name>
					</contributors>
					<titles><title>Improving Voice Trigger Detection with Metric Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1896</first_page>
						<last_page>1900</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11160</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nayak22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muqiao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Konan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Bick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Enhancement through Fine-Grained Speech Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2953</first_page>
						<last_page>2957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11161</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yang22x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yashish M.</given_name>
<surname>Siriwardena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-to-articulatory Speech Inversion with Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5020</first_page>
						<last_page>5024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11164</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/siriwardena22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heli</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sashi</given_name>
<surname>Novitasari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3413</first_page>
						<last_page>3417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11169</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/qi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shun</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiankun</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5523</first_page>
						<last_page>5527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11171</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lei22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiamin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1392</first_page>
						<last_page>1396</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11172</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xie22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qibing</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>501</first_page>
						<last_page>505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11173</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bai22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandramouli Shama</given_name>
<surname>Sastry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudolf</given_name>
<surname>Uher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sageev</given_name>
<surname>Oore</surname>
</person_name>
					</contributors>
					<titles><title>On Combining Global and Localized Self-Supervised Models of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3593</first_page>
						<last_page>3597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11174</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dumpala22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nimshi Venkat</given_name>
<surname>Meripo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep</given_name>
<surname>Konam</surname>
</person_name>
					</contributors>
					<titles><title>ASR Error Detection via Audio-Transcript entailment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3358</first_page>
						<last_page>3362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11177</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meripo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>VCSE: Time-Domain Visual-Contextual Speaker Extraction Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>906</first_page>
						<last_page>910</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11183</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alvaro Martin Iturralde</given_name>
<surname>Zurita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meghan</given_name>
<surname>Clayards</surname>
</person_name>
					</contributors>
					<titles><title>Lexical stress in Spanish word segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1866</first_page>
						<last_page>1870</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11185</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zurita22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abu Zaher Md</given_name>
<surname>Faridee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannes</given_name>
<surname>Gamper</surname>
</person_name>
					</contributors>
					<titles><title>Predicting label distribution improves non-intrusive speech quality estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>406</first_page>
						<last_page>410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11186</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/faridee22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiquan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Tagger with Multi-task Learning for Cross-domain Slot Filling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3273</first_page>
						<last_page>3277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11187</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wei22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiquan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaowang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>TopicKS: Topic-driven Knowledge Selection for Knowledge-grounded Dialogue Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1121</first_page>
						<last_page>1125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11188</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devaraja</given_name>
<surname>Adiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rishav</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amrith</given_name>
<surname>Krishna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Ramakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pawan</given_name>
<surname>Goyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>Linguistically Informed Post-processing for ASR Error correction in Sanskrit</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2293</first_page>
						<last_page>2297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11189</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kumar22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yikang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunfei</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4396</first_page>
						<last_page>4400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11190</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiang-Jen</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Ju</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yih-Liang</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tai-Shih</given_name>
<surname>Chi</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Characteristics Based Multi-objective Model for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>211</first_page>
						<last_page>215</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11197</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peng22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Asahi</given_name>
<surname>Ogushi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshiki</given_name>
<surname>Onishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohei</given_name>
<surname>Tahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Ishii</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Fukayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takao</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiro</given_name>
<surname>Miyata</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of praising skills focusing on utterance contents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2743</first_page>
						<last_page>2747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11200</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ogushi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahir</given_name>
<surname>Morshed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Cross-lingual articulatory feature information transfer for speech recognition using recurrent progressive neural networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2298</first_page>
						<last_page>2302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11202</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/morshed22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nathaniel Romney</given_name>
<surname>Robinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Perez</given_name>
<surname>Ogayo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swetha R.</given_name>
<surname>Gangu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>When Is TTS Augmentation Through a Pivot Language Useful?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3538</first_page>
						<last_page>3542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11203</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/robinson22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Venkata Srikanth</given_name>
<surname>Nallanthighal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aki</given_name>
<surname>Harma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>COVID-19 detection based on respiratory sensing from speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2498</first_page>
						<last_page>2502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11209</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nallanthighal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ShiLiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>560</first_page>
						<last_page>564</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11210</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/yu22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keyu</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huahuan</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglu</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2103</first_page>
						<last_page>2107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11214</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/an22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Donley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Calamia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Time-domain Ad-hoc Array Speech Enhancement Using a Triple-path Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>729</first_page>
						<last_page>733</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11215</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/pandey22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakeel</given_name>
<surname>Muhammad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Nakadai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Automatic Speech Recognition with Re-blocking Processing Based on Integrated Voice Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4641</first_page>
						<last_page>4645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11216</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/sudo22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gasper</given_name>
<surname>Begus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5298</first_page>
						<last_page>5302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11219</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/begus22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rahil</given_name>
<surname>Parikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaspar</given_name>
<surname>Rochette</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shihab</given_name>
<surname>Shamma</surname>
</person_name>
					</contributors>
					<titles><title>An Empirical Analysis on the Vulnerabilities of End-to-End Speech Segregation Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5408</first_page>
						<last_page>5412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11222</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/parikh22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhe</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianhao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Towards Cross-speaker Reading Style Transfer on Audiobook Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5528</first_page>
						<last_page>5532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11223</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunlei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala Krishna</given_name>
<surname>Anumanchipalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Towards Improved Zero-shot Voice Conversion with Conditional DSVAE</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2598</first_page>
						<last_page>2602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11225</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lian22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ognjen</given_name>
<surname>Rudovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pranay</given_name>
<surname>Dighe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed Hussen</given_name>
<surname>Abdelaziz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Marchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Adya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra</given_name>
<surname>Dhir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Tewfik</surname>
</person_name>
					</contributors>
					<titles><title>Device-Directed Speech Detection: Regularization via Distillation for Weakly-Supervised Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1258</first_page>
						<last_page>1262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11228</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/garg22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name>
					</contributors>
					<titles><title>CAUSE: Crossmodal Action Unit Sequence Estimation from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>506</first_page>
						<last_page>510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11232</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kameoka22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala Krishna</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4686</first_page>
						<last_page>4690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11233</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lian22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seongkyu</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhananjaya</given_name>
<surname>Gowda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihwan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changwoo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dokyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Prototypical speaker-interference loss for target voice separation using non-parallel audio samples</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>276</first_page>
						<last_page>280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11236</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mun22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreyan</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaman</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv</given_name>
<surname>Ratn Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>Span Classification with Structured Information for Disfluency Detection in Spoken Utterances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3998</first_page>
						<last_page>4002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11242</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ghosh22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chelzy</given_name>
<surname>Belitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Challenges in Metadata Creation for Massive Naturalistic Team-Based Audio Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5210</first_page>
						<last_page>5214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11243</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/belitz22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Tsung</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Membership Inference Attacks Against Self-supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5040</first_page>
						<last_page>5044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11245</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tseng22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Tsung</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4541</first_page>
						<last_page>4545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11247</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tseng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sepand</given_name>
<surname>Mavandadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo-Yiin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name>
					</contributors>
					<titles><title>Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3223</first_page>
						<last_page>3227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11249</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Zmolikova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Listen only to me! How well can target speech extraction handle false alarms?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>216</first_page>
						<last_page>220</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11252</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/delcroix22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenfeng</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>A compact transformer-based GAN vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1636</first_page>
						<last_page>1640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11254</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/miao22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shrutina</given_name>
<surname>Agarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3013</first_page>
						<last_page>3017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11256</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/agarwal22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sun-Ah</given_name>
<surname>Jun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Luisa</given_name>
<surname>Zubizarreta</surname>
</person_name>
					</contributors>
					<titles><title>Paraguayan Guarani: Tritonal pitch accent and Accentual Phrase</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5303</first_page>
						<last_page>5307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11257</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jun22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vipula</given_name>
<surname>Dissanayake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sachith</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hussel</given_name>
<surname>Suriyaarachchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elliott</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suranga</given_name>
<surname>Nanayakkara</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3598</first_page>
						<last_page>3602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11258</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/dissanayake22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenfeng</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Towards Efficiently Learning Monotonic Alignments for Attention-based End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1051</first_page>
						<last_page>1055</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11259</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/miao22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin-Yueh</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>A BERT-based Language Modeling Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>699</first_page>
						<last_page>703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11266</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chien22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Raj Chetupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Speaker conditioned acoustic modeling for multi-speaker conversational ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3834</first_page>
						<last_page>3838</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11267</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rajchetupalli22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Monaural Speech Enhancement Based on Spectrogram Decomposition for Convolutional Neural Network-sensitive Feature Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>221</first_page>
						<last_page>225</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11268</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shi22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ren</given_name>
<surname>Jigang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mao</given_name>
<surname>Qirong</surname>
</person_name>
					</contributors>
					<titles><title>DCTCN:Deep Complex Temporal Convolutional Network for Long Time Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5478</first_page>
						<last_page>5482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11269</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jigang22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>SASV 2022: The First Spoofing-Aware Speaker Verification Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2893</first_page>
						<last_page>2897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11270</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/jung22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingtian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixun</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>CALM: Constrastive Cross-modal Speaking Style Modeling for Expressive Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5533</first_page>
						<last_page>5537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11275</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meng22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaesong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Kida</surname>
</person_name>
					</contributors>
					<titles><title>Better Intermediates Improve CTC Inference</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4965</first_page>
						<last_page>4969</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11276</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/komatsu22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Kothapally</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2543</first_page>
						<last_page>2547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11277</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kothapally22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Terashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-Wook</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Tachibana</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3018</first_page>
						<last_page>3022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11278</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/terashima22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Transducer-based language embedding for spoken language identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3724</first_page>
						<last_page>3728</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11281</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/shen22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola Garcia</given_name>
<surname>Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohei</given_name>
<surname>Kawaguchi</surname>
</person_name>
					</contributors>
					<titles><title>Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2218</first_page>
						<last_page>2222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11282</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/takashima22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Nakagome</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuta</given_name>
<surname>Ichimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Kida</surname>
</person_name>
					</contributors>
					<titles><title>InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5140</first_page>
						<last_page>5144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11284</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/nakagome22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Liesenfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Dingemanse</surname>
</person_name>
					</contributors>
					<titles><title>Bottom-up discovery of structure and variation in response tokens (‘backchannels’) across diverse languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1126</first_page>
						<last_page>1130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11288</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liesenfeld22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arijit</given_name>
<surname>Mukherjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeepkumar</given_name>
<surname>Satpal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupesh</given_name>
<surname>Mehta</surname>
</person_name>
					</contributors>
					<titles><title>Text aware Emotional Text-to-speech with BERT</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4601</first_page>
						<last_page>4605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11293</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mukherjee22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alena</given_name>
<surname>Velichko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Markitantov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heysem</given_name>
<surname>Kaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>Complex Paralinguistic Analysis of Speech: Predicting Gender, Emotions and Deception in a Hierarchical Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4735</first_page>
						<last_page>4739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11294</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/velichko22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinlei</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiguang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Impairment Representation Learning for Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3323</first_page>
						<last_page>3327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11295</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chen22t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Mohammadamini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Driss</given_name>
<surname>Matrouf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Francois</given_name>
<surname>Bonastre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandipana</given_name>
<surname>Dowerah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Romain</given_name>
<surname>Serizel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Jouvet</surname>
</person_name>
					</contributors>
					<titles><title>Barlow Twins self-supervised learning for robust speaker recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4033</first_page>
						<last_page>4037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11301</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/mohammadamini22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Shan</surname>
</person_name>
					</contributors>
					<titles><title>A Hierarchical Speaker Representation Framework for One-shot Singing Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4307</first_page>
						<last_page>4311</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11305</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiro</given_name>
<surname>Yamazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mihiro</given_name>
<surname>Uchida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name>
					</contributors>
					<titles><title>Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4740</first_page>
						<last_page>4744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11307</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/takashima22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldrich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Learnable Sparse Filterbank for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5110</first_page>
						<last_page>5114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11309</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/peng22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daehun</given_name>
<surname>Yoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong Man</given_name>
<surname>Ro</surname>
</person_name>
					</contributors>
					<titles><title>Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2838</first_page>
						<last_page>2842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11311</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hong22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name>
					</contributors>
					<titles><title>Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>411</first_page>
						<last_page>415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11313</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ashihara22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Séverine</given_name>
<surname>Guillaume</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Wisniewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Galliot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh-Châu</given_name>
<surname>Nguyên</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxime</given_name>
<surname>Fily</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Jacques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Michaud</surname>
</person_name>
					</contributors>
					<titles><title>Plugging a neural phoneme recognizer into a simple language model: a workflow for low-resource setting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4905</first_page>
						<last_page>4909</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11314</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/guillaume22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aku</given_name>
<surname>Rouhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anja</given_name>
<surname>Virkkunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juho</given_name>
<surname>Leinonen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Low Resource Comparison of Attention-based and Hybrid ASR Exploiting wav2vec 2.0</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3543</first_page>
						<last_page>3547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11318</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rouhe22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kitamura</surname>
</person_name>
					</contributors>
					<titles><title>Vocal-Tract Area Functions with Articulatory Reality for Tract Opening</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4691</first_page>
						<last_page>4694</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11320</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Stoidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Cavallaro</surname>
</person_name>
					</contributors>
					<titles><title>Generating gender-ambiguous voices for privacy-preserving speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4237</first_page>
						<last_page>4241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11322</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/stoidis22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiyuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shekhar</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name>
					</contributors>
					<titles><title>Deep CNN-based Inductive Transfer Learning for Sarcasm Detection in Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2323</first_page>
						<last_page>2327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11323</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/gao22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxu</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhai</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5538</first_page>
						<last_page>5542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11336</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/he22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean-Marie</given_name>
<surname>Lemercier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joachim</given_name>
<surname>Thiemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Koning</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Neural Network-augmented Kalman Filtering for Robust Online Speech Dereverberation in Noisy Reverberant Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>226</first_page>
						<last_page>230</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11337</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lemercier22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>A</given_name>
<surname>Arunkumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>Joint Encoder-Decoder Self-Supervised Pre-training for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3418</first_page>
						<last_page>3422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11338</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/arunkumar22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwanath Pratap</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Supratik</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Pandey</surname>
</person_name>
					</contributors>
					<titles><title>Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children’s Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3213</first_page>
						<last_page>3217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11343</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/singh22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seo-Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Farhad</given_name>
<surname>Goodarzy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Vogrin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.</given_name>
<surname>Cook</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David B.</given_name>
<surname>Grayden</surname>
</person_name>
					</contributors>
					<titles><title>Evidence of Onset and Sustained Neural Responses to Isolated Phonemes from Intracranial Recordings in a Voice-based Cursor Control Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4063</first_page>
						<last_page>4067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11344</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meng22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengwei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinpei</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liangchen</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feijun</given_name>
<surname>Jiang</surname>
</person_name>
					</contributors>
					<titles><title>Speech2Slot: A Limited Generation Framework with Boundary Detection for Slot Filling from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2748</first_page>
						<last_page>2752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11347</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Binling</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxuan</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiulin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Oriental Language Recognition (OLR) 2021: Summary and Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3729</first_page>
						<last_page>3733</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11348</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22ga_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanh Tam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Nejdl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4003</first_page>
						<last_page>4007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11355</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/chang22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>MengLong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengqiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengdong</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-class AUC Optimization for Robust Small-footprint Keyword Spotting with Limited Training Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3278</first_page>
						<last_page>3282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11356</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiro</given_name>
<surname>Yamazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saki</given_name>
<surname>Mizuno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mihiro</given_name>
<surname>Uchida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Joint Modeling of Conversation History-Dependent and Independent ASR Systems with Multi-History Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3218</first_page>
						<last_page>3222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11357</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/masumura22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jisi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catalin</given_name>
<surname>Zorila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rama</given_name>
<surname>Doddipatla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1056</first_page>
						<last_page>1060</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11359</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhang22fa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nilaksh</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Polo</given_name>
<surname>Chau</surname>
</person_name>
					</contributors>
					<titles><title>Hear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3839</first_page>
						<last_page>3843</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11361</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/das22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hideyuki</given_name>
<surname>Tachibana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muneyoshi</given_name>
<surname>Inahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mocho</given_name>
<surname>Go</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yotaro</given_name>
<surname>Katayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yotaro</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Diffusion Generative Vocoder for Fullband Speech Synthesis Based on Weak Third-order SDE Solver</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1641</first_page>
						<last_page>1645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11366</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tachibana22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianrui</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phil</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Tandem Multitask Training of Speaker Diarisation and Speech Recognition for Meeting Transcription</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3844</first_page>
						<last_page>3848</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11368</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zheng22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Werner</given_name>
<surname>van der Merwe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Adam du Preez</surname>
</person_name>
					</contributors>
					<titles><title>A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1426</first_page>
						<last_page>1430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11369</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/vandermerwe22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anish</given_name>
<surname>Bhanushali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant</given_name>
<surname>Bridgman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deekshitha</given_name>
<surname>G</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pratik</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adithya</given_name>
<surname>Raj Kolladath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithya</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaditeshwar</given_name>
<surname>Seth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Seth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhayjeet</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vrunda</given_name>
<surname>Sukhadia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umesh</given_name>
<surname>S</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lodagala V. S. V. Durga</given_name>
<surname>Prasad</surname>
</person_name>
					</contributors>
					<titles><title>Gram Vaani ASR Challenge on spontaneous telephone speech recordings in regional variations of Hindi</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3548</first_page>
						<last_page>3552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11371</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/bhanushali22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qingcheng</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dading</given_name>
<surname>Chong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peilin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5308</first_page>
						<last_page>5312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11372</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zeng22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>A</given_name>
<surname>Arunkumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vrunda</given_name>
<surname>Nileshkumar Sukhadia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5145</first_page>
						<last_page>5149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11376</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/arunkumar22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peiying</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingchao</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Cross-modal Transfer Learning via Multi-grained Alignment for End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1131</first_page>
						<last_page>1135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11378</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhu22f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Rennie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olga</given_name>
<surname>Perepelkina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Vinciarelli</surname>
</person_name>
					</contributors>
					<titles><title>Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4008</first_page>
						<last_page>4012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11379</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/rennie22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuhito</given_name>
<surname>Sudoh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>121</first_page>
						<last_page>125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11382</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fukuda22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tong</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shijing</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Uncertainty Calibration for Deep Audio Classifiers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1556</first_page>
						<last_page>1560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11384</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ye22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ali</given_name>
<surname>Aroudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Uhlich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Ferras Font</surname>
</person_name>
					</contributors>
					<titles><title>TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>911</first_page>
						<last_page>915</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11386</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/aroudi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keiko</given_name>
<surname>Ochi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobutaka</given_name>
<surname>Ono</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keiho</given_name>
<surname>Owada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuroda</given_name>
<surname>Miho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Sagayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hidenori</given_name>
<surname>Yamasue</surname>
</person_name>
					</contributors>
					<titles><title>Use of Nods Less Synchronized with Turn-Taking and Prosody During Conversations in Adults with Autism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1136</first_page>
						<last_page>1140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11388</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/ochi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Selen</given_name>
<surname>Hande Kabil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herve</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>From Undercomplete to Sparse Overcomplete Autoencoders to Improve LF-MMI based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1061</first_page>
						<last_page>1065</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11390</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/handekabil22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianchao</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Attention Enhanced Citrinet for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2108</first_page>
						<last_page>2112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11394</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wu22j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quentin</given_name>
<surname>Meeus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Francine Moens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Multitask Learning for Low Resource Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4073</first_page>
						<last_page>4077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11401</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/meeus22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thilo von</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Utterance-by-utterance overlap-aware neural diarization with Graph-PIT</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1486</first_page>
						<last_page>1490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11408</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kinoshita22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuji</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binling</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Zhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shipeng</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Spatial-aware Speaker Diarizaiton for Multi-channel Multi-party Meeting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1491</first_page>
						<last_page>1495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11412</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/wang22ha_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name>
					</contributors>
					<titles><title>Domain Adversarial Self-Supervised Speech Representation Learning for Improving Unknown Domain Downstream Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1066</first_page>
						<last_page>1070</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11414</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/tanaka22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tongtong</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjie</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Self-Distillation Based on High-level Information Supervision for Compressing End-to-End ASR Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1716</first_page>
						<last_page>1720</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11423</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/xu22i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takahiro</given_name>
<surname>Shinozaki</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Target-Speaker ASR with Neural Transducer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2673</first_page>
						<last_page>2677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11425</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/moriya22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tongtong</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjie</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Language-specific Characteristic Assistance for Code-switching Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>3924</first_page>
						<last_page>3928</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11426</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/song22e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasunori</given_name>
<surname>Ohishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Niizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noboru</given_name>
<surname>Harada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunio</given_name>
<surname>Kashino</surname>
</person_name>
					</contributors>
					<titles><title>Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4197</first_page>
						<last_page>4201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11428</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/takeuchi22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiguang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Multi-Loss Training for Low Delay Packet Loss Concealment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>585</first_page>
						<last_page>589</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11439</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/li22ea_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Maekaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1071</first_page>
						<last_page>1075</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11441</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/maekaku22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Umar</given_name>
<surname>Farooq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Darshan Adiga Haniya</given_name>
<surname>Narayana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4850</first_page>
						<last_page>4854</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11449</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/farooq22b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuheng</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junqing</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianzong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4745</first_page>
						<last_page>4749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11456</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/kang22d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin Woo</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eungbeom</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junghyun</given_name>
<surname>Koo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Representation Selective Self-distillation and wav2vec 2.0 Feature Exploration for Spoof-aware Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>2898</first_page>
						<last_page>2902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11460</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/lee22q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ding</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuehai</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Improve Speech Enhancement using Perception-High-Related Time-Frequency Loss</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>5483</first_page>
						<last_page>5487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11471</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/zhao22l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tzeviya</given_name>
<surname>Fuchs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yedid</given_name>
<surname>Hoshen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Word Segmentation using K Nearest Neighbors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4646</first_page>
						<last_page>4650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11474</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/fuchs22_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqin</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenbo</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqi</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>4750</first_page>
						<last_page>4754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11480</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/liu22aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dick</given_name>
<surname>Botteldooren</surname>
</person_name>
					</contributors>
					<titles><title>Event-related data conditioning for acoustic event classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>18</day>
						<year>2022</year>
					</publication_date>
					<pages>
						<first_page>1561</first_page>
						<last_page>1565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2022-11481</doi>
						<resource>https://www.isca-archive.org/interspeech_2022/hou22c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>