<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2019</doi_batch_id>
		<timestamp>1705398653317398</timestamp>
		<depositor>
			<depositor_name>Martin Cooke</depositor_name> 
			<email_address>m.cooke@ikerbasque.org</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant> 
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>Interspeech 2019</conference_name>
				<conference_acronym>interspeech_2019</conference_acronym>
				<conference_date>15-19 September 2019</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>Interspeech 2019</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2019</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2019</doi>
					<timestamp>1705398653317398</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2019/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>García-Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Advances in Automatic Speech Recognition for Child Speech Using Factored Time Delay Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1</first_page>
						<last_page>5</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2980</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gary</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>A Frequency Normalization Technique for Kindergarten Speech Recognition Inspired by the Role of fo in Vowel Perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>6</first_page>
						<last_page>10</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1847</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yeung19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Gale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jill</given_name>
<surname>Dolata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan van</given_name>
<surname>Santen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meysam</given_name>
<surname>Asgari</surname>
</person_name>
					</contributors>
					<titles><title>Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>11</first_page>
						<last_page>15</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3161</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gale19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manuel Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aciel</given_name>
<surname>Eshky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Ultrasound Tongue Imaging for Diarization and Alignment of Child Speech Therapy Sessions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>16</first_page>
						<last_page>20</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2612</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ribeiro19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anastassia</given_name>
<surname>Loukina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beata Beigman</given_name>
<surname>Klebanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Lange</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binod</given_name>
<surname>Gyawali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nitin</given_name>
<surname>Madnani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Misra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Klaus</given_name>
<surname>Zechner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuowei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Sabatini</surname>
</person_name>
					</contributors>
					<titles><title>Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>21</first_page>
						<last_page>25</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2889</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/loukina19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vanessa</given_name>
<surname>Lopes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João</given_name>
<surname>Magalhães</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>Cavaco</surname>
</person_name>
					</contributors>
					<titles><title>Sustained Vowel Game: A Computer Therapy Game for Children with Dysphonia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>26</first_page>
						<last_page>30</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3017</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lopes19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Esposito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Terry</given_name>
<surname>Amorese</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marialucia</given_name>
<surname>Cuciniello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Teresa</given_name>
<surname>Riviello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonietta M.</given_name>
<surname>Esposito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alda</given_name>
<surname>Troncone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gennaro</given_name>
<surname>Cordasco</surname>
</person_name>
					</contributors>
					<titles><title>The Dependability of Voice on Elders&#8217; Acceptance of Humanoid Agents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>31</first_page>
						<last_page>35</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1734</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/esposito19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uffe</given_name>
<surname>Schjoedt</surname>
</person_name>
					</contributors>
					<titles><title>God as Interlocutor &#8212; Real or Imaginary? Prosodic Markers of Dialogue Speech and Expected Efficacy in Spoken Prayer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>36</first_page>
						<last_page>40</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1193</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/niebuhr19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name>
					</contributors>
					<titles><title>Expressiveness Influences Human Vocal Alignment Toward voice-AI</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>41</first_page>
						<last_page>45</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1368</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cohn19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beatrice</given_name>
<surname>Alex</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johanna D.</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leimin</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuro</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gianpiero</given_name>
<surname>Francesca</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Topic-Oriented Speaker Stance in Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>46</first_page>
						<last_page>50</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2632</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jilt</given_name>
<surname>Sebastian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piero</given_name>
<surname>Pierucci</surname>
</person_name>
					</contributors>
					<titles><title>Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>51</first_page>
						<last_page>55</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3201</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sebastian19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Rajwadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cornelius</given_name>
<surname>Glackin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Wall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gérard</given_name>
<surname>Chollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nigel</given_name>
<surname>Cannings</surname>
</person_name>
					</contributors>
					<titles><title>Explaining Sentiment Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>56</first_page>
						<last_page>60</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2743</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rajwadi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Kleinlein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristina Luna</given_name>
<surname>Jiménez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Manuel</given_name>
<surname>Montero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoraida</given_name>
<surname>Callejas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fernando</given_name>
<surname>Fernández-Martínez</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Group-Level Skin Attention to Short Movies from Audio-Based LSTM-Mixture of Experts Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>61</first_page>
						<last_page>65</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2799</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kleinlein19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thai-Son</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Very Deep Self-Attention Networks for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>66</first_page>
						<last_page>70</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2702</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pham19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Leary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleksii</given_name>
<surname>Kuchaiev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan M.</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huyen</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi Teja</given_name>
<surname>Gadde</surname>
</person_name>
					</contributors>
					<titles><title>Jasper: An End-to-End Convolutional Neural Acoustic Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>71</first_page>
						<last_page>75</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1819</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name>
					</contributors>
					<titles><title>Unidirectional Neural Network Architectures for End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>76</first_page>
						<last_page>80</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2837</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/moritz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yonatan</given_name>
<surname>Belinkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>81</first_page>
						<last_page>85</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2599</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/belinkov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Speech Enhancement Using Time-Domain Convolutional Denoising Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>86</first_page>
						<last_page>90</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3197</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tawara19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kristina</given_name>
<surname>Tesch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Rehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>On Nonlinear Spatial Filtering in Multichannel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>91</first_page>
						<last_page>95</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2751</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tesch19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan M.</given_name>
<surname>Martín-Doñas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Heitkaemper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel M.</given_name>
<surname>Gomez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio M.</given_name>
<surname>Peinado</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Block-Online Source Extraction Based on Utterance Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>96</first_page>
						<last_page>100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2244</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/martindonas19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saeed</given_name>
<surname>Bagheri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Giacobello</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Multi-Channel Speech Presence Probability in Parametric Multi-Channel Wiener Filter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>101</first_page>
						<last_page>105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2665</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bagheri19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masahito</given_name>
<surname>Togami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name>
					</contributors>
					<titles><title>Variational Bayesian Multi-Channel Speech Dereverberation Under Noisy Environments with Probabilistic Convolutive Transfer Function</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>106</first_page>
						<last_page>110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1220</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/togami19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name>
					</contributors>
					<titles><title>Simultaneous Denoising and Dereverberation for Low-Latency Applications Using Frame-by-Frame Online Unified Convolutional Beamformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>111</first_page>
						<last_page>115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1286</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nakatani19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cathryn</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name>
					</contributors>
					<titles><title>Individual Variation in Cognitive Processing Style Predicts Differences in Phonetic Imitation of Device and Human Voices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>116</first_page>
						<last_page>120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2669</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/snyder19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Illa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation on Speaker Specific Articulatory Synthesis with Speaker Independent Articulatory Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>121</first_page>
						<last_page>125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2664</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/illa19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongke</given_name>
<surname>Bi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhuan</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Individual Difference of Relative Tongue Size and its Acoustic Effects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>126</first_page>
						<last_page>130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2452</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsukasa</given_name>
<surname>Yoshinaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Nozaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeo</given_name>
<surname>Wada</surname>
</person_name>
					</contributors>
					<titles><title>Individual Differences of Airflow and Sound Generation in the Vocal Tract of Sibilant /s/</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>131</first_page>
						<last_page>135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1376</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yoshinaga19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shashwat</given_name>
<surname>Uttam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaman</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhruva</given_name>
<surname>Sahrawat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mansi</given_name>
<surname>Aggarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debanjan</given_name>
<surname>Mahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Stent</surname>
</person_name>
					</contributors>
					<titles><title>Hush-Hush Speak: Speech Reconstruction Using Silent Videos</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>136</first_page>
						<last_page>140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3269</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/uttam19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pramit</given_name>
<surname>Saha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Abdul-Mageed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>SPEAK YOUR MIND! Towards Imagined Speech Recognition with Hierarchical Deep Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>141</first_page>
						<last_page>145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3041</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/saha19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-An</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>An Unsupervised Autoregressive Model for Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>146</first_page>
						<last_page>150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1473</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chung19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Balazs</surname>
</person_name>
					</contributors>
					<titles><title>Harmonic-Aligned Frame Mask Based on Non-Stationary Gabor Transform with Application to Content-Dependent Speaker Comparison</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>151</first_page>
						<last_page>155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1327</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gurunath Reddy</given_name>
<surname>M.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sreenivasa</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Partha Pratim</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Glottal Closure Instants Detection from Speech Signal by Deep Features Extracted from Raw Speech and Linear Prediction Residual</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>156</first_page>
						<last_page>160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1981</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/m19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Santiago</given_name>
<surname>Pascual</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan</given_name>
<surname>Serrà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Bonafonte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshua</given_name>
<surname>Bengio</surname>
</person_name>
					</contributors>
					<titles><title>Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>161</first_page>
						<last_page>165</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2605</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pascual19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhanu Teja</given_name>
<surname>Nellore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Nathwani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suryakanth V.</given_name>
<surname>Gangashetty</surname>
</person_name>
					</contributors>
					<titles><title>Excitation Source and Vocal Tract System Based Acoustic Features for Detection of Nasals in Continuous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>166</first_page>
						<last_page>170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2785</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nellore19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aggelina</given_name>
<surname>Chatziagapi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitris</given_name>
<surname>Sgouropoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Pantazopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malvina</given_name>
<surname>Nikandrou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theodoros</given_name>
<surname>Giannakopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation Using GANs for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>171</first_page>
						<last_page>175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2561</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chatziagapi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zvi</given_name>
<surname>Kons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slava</given_name>
<surname>Shechtman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Sorin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carmel</given_name>
<surname>Rabinovitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron</given_name>
<surname>Hoory</surname>
</person_name>
					</contributors>
					<titles><title>High Quality, Lightweight and Adaptable TTS Using LPCNet</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>176</first_page>
						<last_page>180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1705</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kons19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Latorre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Merritt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bartosz</given_name>
<surname>Putrycz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexis</given_name>
<surname>Moinet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vatsal</given_name>
<surname>Aggarwal</surname>
</person_name>
					</contributors>
					<titles><title>Towards Achieving Robust Universal Neural Vocoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>181</first_page>
						<last_page>185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1424</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lorenzotrueba19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paarth</given_name>
<surname>Neekhara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Donahue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miller</given_name>
<surname>Puckette</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shlomo</given_name>
<surname>Dubnov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>McAuley</surname>
</person_name>
					</contributors>
					<titles><title>Expediting TTS Synthesis with Adversarial Vocoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>186</first_page>
						<last_page>190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3099</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/neekhara19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Mustafa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arijit</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Schottenhamml</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name>
					</contributors>
					<titles><title>Analysis by Adversarial Synthesis &#8212; A Novel Approach for Speech Vocoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>191</first_page>
						<last_page>195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1195</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mustafa19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick Lumban</given_name>
<surname>Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution Model for Parametric Speech Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>196</first_page>
						<last_page>200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1232</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohai</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>201</first_page>
						<last_page>205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1514</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tian19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziping</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongtian</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haishuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>206</first_page>
						<last_page>210</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1649</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeng-Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>211</first_page>
						<last_page>215</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2044</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ascensión</given_name>
<surname>Gallardo-Antolín</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Manuel</given_name>
<surname>Montero</surname>
</person_name>
					</contributors>
					<titles><title>A Saliency-Based Attention LSTM Model for Cognitive Load Classification from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>216</first_page>
						<last_page>220</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1603</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gallardoantolin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adria</given_name>
<surname>Mallol-Ragolta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziping</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Stappen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>221</first_page>
						<last_page>225</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2036</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mallolragolta19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Carmantini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Untranscribed Web Audio for Low Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>226</first_page>
						<last_page>230</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2623</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/carmantini19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Lüscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Beck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuki</given_name>
<surname>Irie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Kitza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wilfried</given_name>
<surname>Michel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>RWTH ASR Systems for LibriSpeech: Hybrid vs Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>231</first_page>
						<last_page>235</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1780</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/luscher19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryoichi</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>236</first_page>
						<last_page>240</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1126</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kanda19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Adaptation for Attention-Based End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>241</first_page>
						<last_page>245</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3135</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/meng19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Large Margin Training for Attention Based End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>246</first_page>
						<last_page>250</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1680</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khoi-Nguyen C.</given_name>
<surname>Mac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>251</first_page>
						<last_page>255</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2641</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mac19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Milde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Biemann</surname>
</person_name>
					</contributors>
					<titles><title>SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>256</first_page>
						<last_page>260</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2938</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/milde19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ondel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hari Krishna</given_name>
<surname>Vydana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>261</first_page>
						<last_page>265</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2224</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ondel19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>266</first_page>
						<last_page>270</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2052</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/higuchi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manasa</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daan van</given_name>
<surname>Esch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandy</given_name>
<surname>Ritchie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas Fromseier</given_name>
<surname>Mortensen</surname>
</person_name>
					</contributors>
					<titles><title>Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>271</first_page>
						<last_page>275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1775</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/prasad19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Azuh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>276</first_page>
						<last_page>280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1718</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/azuh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>281</first_page>
						<last_page>285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1338</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/feng19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shawn</given_name>
<surname>Nissen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharalee</given_name>
<surname>Blunck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Dromey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Dromey</surname>
</person_name>
					</contributors>
					<titles><title>Listeners&#8217; Ability to Identify the Gender of Preadolescent Children in Different Linguistic Contexts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>286</first_page>
						<last_page>290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1865</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nissen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wiebke</given_name>
<surname>Ahlers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Meer</surname>
</person_name>
					</contributors>
					<titles><title>Sibilant Variation in New Englishes: A Comparative Sociophonetic Study of Trinidadian and American English /s(tr)/-Retraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>291</first_page>
						<last_page>295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1821</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ahlers19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michele</given_name>
<surname>Gubian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Harrington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mary</given_name>
<surname>Stevens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Schiel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Warren</surname>
</person_name>
					</contributors>
					<titles><title>Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>296</first_page>
						<last_page>300</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2115</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gubian19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iona</given_name>
<surname>Gessinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bistra</given_name>
<surname>Andreeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eran</given_name>
<surname>Raveh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingmar</given_name>
<surname>Steiner</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Accommodation in a Wizard-of-Oz Experiment: Intonation and Segments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>301</first_page>
						<last_page>305</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2445</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gessinger19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Michalsky</surname>
</person_name>
					</contributors>
					<titles><title>PASCAL and DPA: A Pilot Study on Using Prosodic Competence Scores to Predict Communicative Skills for Team Working and Public Speaking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>306</first_page>
						<last_page>310</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3034</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/niebuhr19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Michalsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heike</given_name>
<surname>Schoormann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schultze</surname>
</person_name>
					</contributors>
					<titles><title>Towards the Prosody of Persuasion in Competitive Negotiation. The Relationship Between f0 and Negotiation Success in Same Sex Sales Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>311</first_page>
						<last_page>315</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3031</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/michalsky19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Sager</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Reinhold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archana</given_name>
<surname>Venkataraman</surname>
</person_name>
					</contributors>
					<titles><title>VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>316</first_page>
						<last_page>320</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1413</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sager19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia Xin</given_name>
<surname>Koh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aqilah</given_name>
<surname>Mislan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Khoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Ang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wilson</given_name>
<surname>Ang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charmaine</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying-Ying</given_name>
<surname>Tan</surname>
</person_name>
					</contributors>
					<titles><title>Building the Singapore English National Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>321</first_page>
						<last_page>325</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1525</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/koh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name>
					</contributors>
					<titles><title>Challenging the Boundaries of Speech Recognition: The MALACH Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>326</first_page>
						<last_page>330</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1907</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/picheny19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pravin Bhaskar</given_name>
<surname>Ramteke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sujata</given_name>
<surname>Supanekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradyoth</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Nelson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkataraja</given_name>
<surname>Aithal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashidhar G.</given_name>
<surname>Koolagudi</surname>
</person_name>
					</contributors>
					<titles><title>NITK Kids&#8217; Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>331</first_page>
						<last_page>335</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2061</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ramteke19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salam</given_name>
<surname>Khalifa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nizar</given_name>
<surname>Habash</surname>
</person_name>
					</contributors>
					<titles><title>Towards Variability Resistant Dialectal Speech Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>336</first_page>
						<last_page>340</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2692</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ali19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Per</given_name>
<surname>Fallgren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zofia</given_name>
<surname>Malisz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name>
					</contributors>
					<titles><title>How to Annotate 100 Hours in 45 Minutes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>341</first_page>
						<last_page>345</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1648</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fallgren19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian HMM Based x-Vector Clustering for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>346</first_page>
						<last_page>350</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2813</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/diez19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ville</given_name>
<surname>Vestman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi H.</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Koshinaka</surname>
</person_name>
					</contributors>
					<titles><title>Unleashing the Unused Potential of i-Vectors Enabled by GPU Acceleration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>351</first_page>
						<last_page>355</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1955</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vestman19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>Reynolds</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>MCE 2018: The 1st Multi-Target Speaker Detection and Identification Challenge Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>356</first_page>
						<last_page>360</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1572</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shon19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>361</first_page>
						<last_page>365</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1489</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qingjian</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruiqing</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claude</given_name>
<surname>Barras</surname>
</person_name>
					</contributors>
					<titles><title>LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>366</first_page>
						<last_page>370</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1388</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bong-Jin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Icksang</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Who Said That?: Audio-Visual Speaker Diarisation of Real-World Meetings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>371</first_page>
						<last_page>375</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3116</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chung19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiamin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>García-Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Multi-PLDA Diarization on Children&#8217;s Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>376</first_page>
						<last_page>380</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2961</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xie19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alan</given_name>
<surname>McCree</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization Using Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>381</first_page>
						<last_page>385</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2912</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mccree19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Omid</given_name>
<surname>Ghahabi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Fischer</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Corrupted Embeddings for Online Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>386</first_page>
						<last_page>390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2756</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ghahabi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tae Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization with Lexical Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>391</first_page>
						<last_page>395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1947</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/park19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laurent El</given_name>
<surname>Shafey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hagen</given_name>
<surname>Soltau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Izhak</given_name>
<surname>Shafran</surname>
</person_name>
					</contributors>
					<titles><title>Joint Speech Recognition and Speaker Diarization via Sequence Transduction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>396</first_page>
						<last_page>400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1943</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shafey19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name>
					</contributors>
					<titles><title>Normal Variance-Mean Mixtures for Unsupervised Score Calibration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>401</first_page>
						<last_page>405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1609</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cumani19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Okabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Koshinaka</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Augmentation and Bandwidth Extension for Deep Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>406</first_page>
						<last_page>410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1508</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yamamoto19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adem</given_name>
<surname>Derinel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henk van den</given_name>
<surname>Heuvel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Brummer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David A. van</given_name>
<surname>Leeuwen</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Speaker Diarization of Radio Broadcast Archives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>411</first_page>
						<last_page>415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1399</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ylmaz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harishchandra</given_name>
<surname>Dubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhijeet</given_name>
<surname>Sangwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Toeplitz Inverse Covariance Based Robust Speaker Clustering for Naturalistic Audio Streams</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>416</first_page>
						<last_page>420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1102</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dubey19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>György</given_name>
<surname>Kovács</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk Van</given_name>
<surname>Compernolle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcus</given_name>
<surname>Liwicki</surname>
</person_name>
					</contributors>
					<titles><title>Examining the Combination of Multi-Band Processing and Channel Dropout for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>421</first_page>
						<last_page>425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3215</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kovacs19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meet</given_name>
<surname>Soni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Panda</surname>
</person_name>
					</contributors>
					<titles><title>Label Driven Time-Frequency Masking for Robust Continuous Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>426</first_page>
						<last_page>430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2172</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/soni19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Invariant Feature-Mapping for Distant Speech Recognition via Adversarial Teacher-Student Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>431</first_page>
						<last_page>435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2136</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ji</given_name>
<surname>Ming</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danny</given_name>
<surname>Crookes</surname>
</person_name>
					</contributors>
					<titles><title>Full-Sentence Correlation: A Method to Handle Unpredictable Noise for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>436</first_page>
						<last_page>440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2127</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ming19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meet</given_name>
<surname>Soni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Panda</surname>
</person_name>
					</contributors>
					<titles><title>Generative Noise Modeling and Channel Simulation for Robust Speech Recognition in Unseen Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>441</first_page>
						<last_page>445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2090</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/soni19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shashi</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakti P.</given_name>
<surname>Rath</surname>
</person_name>
					</contributors>
					<titles><title>Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>446</first_page>
						<last_page>450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2032</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kumar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End SpeakerBeam for Single Channel Target Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>451</first_page>
						<last_page>455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1856</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/delcroix19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>I-Hung</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayush</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Premkumar</given_name>
<surname>Natarajan</surname>
</person_name>
					</contributors>
					<titles><title>NIESR: Nuisance Invariant End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>456</first_page>
						<last_page>460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1836</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hsu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takahito</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ogata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Tsunakawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masafumi</given_name>
<surname>Nishida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masafumi</given_name>
<surname>Nishimura</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for Throat Microphone Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>461</first_page>
						<last_page>465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1597</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/suzuki19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Improved Speaker-Dependent Separation for CHiME-5 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>466</first_page>
						<last_page>470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1569</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>471</first_page>
						<last_page>475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1495</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced Spectral Features for Distortion-Independent Acoustic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>476</first_page>
						<last_page>480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1493</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paarth</given_name>
<surname>Neekhara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shehzeen</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prakhar</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shlomo</given_name>
<surname>Dubnov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>McAuley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Farinaz</given_name>
<surname>Koushanfar</surname>
</person_name>
					</contributors>
					<titles><title>Universal Adversarial Perturbations for Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>481</first_page>
						<last_page>485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1353</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/neekhara19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masakiyo</given_name>
<surname>Fujimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>One-Pass Single-Channel Noisy Speech Recognition Using a Combination of Noisy and Enhanced Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>486</first_page>
						<last_page>490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1270</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fujimoto19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenju</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shouye</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changliang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>491</first_page>
						<last_page>495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1242</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixiaofan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingyan</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Humor by Learning from Time-Aligned Comments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>496</first_page>
						<last_page>500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3113</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoan</given_name>
<surname>Dinkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Koychev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preslav</given_name>
<surname>Nakov</surname>
</person_name>
					</contributors>
					<titles><title>Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>501</first_page>
						<last_page>505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2965</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dinkov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guozhen</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rivka</given_name>
<surname>Levitan</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Gender and L1 Differences to Improve State and Trait Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>506</first_page>
						<last_page>509</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2868</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/an19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felix</given_name>
<surname>Weninger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junho</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Willett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puming</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning Based Mandarin Accent Identification for Accent Robust ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>510</first_page>
						<last_page>514</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2737</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/weninger19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name>
					</contributors>
					<titles><title>Calibrating DNN Posterior Probability Estimates of HMM/DNN Models to Improve Social Signal Detection from Audio Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>515</first_page>
						<last_page>519</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2552</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gosztolya19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nagata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiko</given_name>
<surname>Arimoto</surname>
</person_name>
					</contributors>
					<titles><title>Conversational and Social Laughter Synthesis with WaveNet</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>520</first_page>
						<last_page>523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2131</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mori19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Ludusan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name>
					</contributors>
					<titles><title>Laughter Dynamics in Dyadic Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>524</first_page>
						<last_page>528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1733</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ludusan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khiet P.</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Trouvain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michel-Pierre</given_name>
<surname>Jansen</surname>
</person_name>
					</contributors>
					<titles><title>Towards an Annotation Scheme for Complex Laughter in Speech Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>529</first_page>
						<last_page>533</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1557</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/truong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Sturmbauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johanna</given_name>
<surname>Janson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva-Maria</given_name>
<surname>Messner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harald</given_name>
<surname>Baumeister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Rohleder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Using Speech to Predict Sequentially Measured Cortisol Levels During a Trier Social Stress Test</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>534</first_page>
						<last_page>538</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1352</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/baird19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Coutinho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Sincerity in Acted Speech: Presenting the Sincere Apology Corpus and Results</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>539</first_page>
						<last_page>543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1349</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/baird19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kerstin</given_name>
<surname>Fischer</surname>
</person_name>
					</contributors>
					<titles><title>Do not Hesitate! &#8212; Unless You Do it Shortly or Nasally: How the Phonetics of Filled Pauses Determine Their Subjective Frequency and Perceived Speaker Performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>544</first_page>
						<last_page>548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1194</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/niebuhr19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>J.C.</given_name>
<surname>Vásquez-Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Phonet: A Tool Based on Gated Recurrent Neural Networks to Extract Phonological Posteriors from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>549</first_page>
						<last_page>553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1405</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vasquezcorrea19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ching-Ting</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun-Po</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Code-Switching Sentence Generation by Generative Adversarial Networks and its Application to Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>554</first_page>
						<last_page>558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3214</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moritz</given_name>
<surname>Meier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Celeste</given_name>
<surname>Mason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Putze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Comparative Analysis of Think-Aloud Methods for Everyday Activities in the Context of Cognitive Robotics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>559</first_page>
						<last_page>563</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3072</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/meier19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Doug</given_name>
<surname>Beeferman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Brannon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deb</given_name>
<surname>Roy</surname>
</person_name>
					</contributors>
					<titles><title>RadioTalk: A Large-Scale Corpus of Talk Radio Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>564</first_page>
						<last_page>568</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2714</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/beeferman19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Hernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Dufour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solen</given_name>
<surname>Quiniou</surname>
</person_name>
					</contributors>
					<titles><title>Qualitative Evaluation of ASR Adaptation in a Lecture Context: Application to the PASTEL Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>569</first_page>
						<last_page>573</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2661</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mdhaffar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Federico</given_name>
<surname>Marinelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandra</given_name>
<surname>Cervone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuliano</given_name>
<surname>Tortoreto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny A.</given_name>
<surname>Stepanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe Di</given_name>
<surname>Fabbrizio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Riccardi</surname>
</person_name>
					</contributors>
					<titles><title>Active Annotation: Bootstrapping Annotation Lexicon and Guidelines for Supervised NLU Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>574</first_page>
						<last_page>578</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2537</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/marinelli19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gerardo Roa</given_name>
<surname>Dabike</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Lyric Transcription from Karaoke Vocal Tracks: Resources and a Baseline System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>579</first_page>
						<last_page>583</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2378</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dabike19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>584</first_page>
						<last_page>588</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2125</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jazmín</given_name>
<surname>Vidal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Brambilla</surname>
</person_name>
					</contributors>
					<titles><title>EpaDB: A Database for Development of Pronunciation Assessment Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>589</first_page>
						<last_page>593</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1839</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vidal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Angerbauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heike</given_name>
<surname>Adel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>594</first_page>
						<last_page>598</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1750</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/angerbauer19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongyin</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitra</given_name>
<surname>Mohtarami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Krishnamurthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brigitte</given_name>
<surname>Richardson</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>599</first_page>
						<last_page>603</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1736</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/luo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarah E.</given_name>
<surname>Gutz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yana</given_name>
<surname>Yunusova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>Early Identification of Speech Changes Due to Amyotrophic Lateral Sclerosis Using Machine Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>604</first_page>
						<last_page>608</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2967</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gutz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohamed Ismail Yasar Arafath</given_name>
<surname>K.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurobinda</given_name>
<surname>Routray</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Breath Using Voice Activity Detection and SVM Classifier with Application on News Reports</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>609</first_page>
						<last_page>613</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2434</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/k19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Scene Classification Using Teacher-Student Learning with Soft-Labels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>614</first_page>
						<last_page>618</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1989</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/heo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanping</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Rare Sound Event Detection Using Deep Learning and Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>619</first_page>
						<last_page>623</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1985</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Combination of Model-Based and Feature-Based Strategy for Speech-to-Singing Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>624</first_page>
						<last_page>628</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1942</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sharma19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosi</given_name>
<surname>Shrem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Goldrick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>Dr.VOT: Measuring Positive and Negative Voice Onset Time in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>629</first_page>
						<last_page>633</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1735</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shrem19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>J.</given_name>
<surname>Hui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Y.</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.T.</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>R.H.Y.</given_name>
<surname>So</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Base-Frequency and Spectral Envelope on Deep-Learning Speech Separation and Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>634</first_page>
						<last_page>638</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1715</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hui19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nirmesh J.</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant A.</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>639</first_page>
						<last_page>643</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1504</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shah19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archana</given_name>
<surname>Venkataraman</surname>
</person_name>
					</contributors>
					<titles><title>Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>644</first_page>
						<last_page>648</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1450</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shankar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Mateju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Cerva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindrich</given_name>
<surname>Zdansky</surname>
</person_name>
					</contributors>
					<titles><title>An Approach to Online Speaker Change Point Detection Using DNNs and WFSTs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>649</first_page>
						<last_page>653</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1407</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mateju19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenyu</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John D.</given_name>
<surname>Kanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Hogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh</given_name>
<surname>Manocha</surname>
</person_name>
					</contributors>
					<titles><title>Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>654</first_page>
						<last_page>658</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1111</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dipjyoti</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Pantazis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>659</first_page>
						<last_page>663</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2869</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/paul19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju-chieh</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>664</first_page>
						<last_page>668</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2663</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chou19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongyang</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runnan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>One-Shot Voice Conversion with Global Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>669</first_page>
						<last_page>673</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2365</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrick Lumban</given_name>
<surname>Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Non-Parallel Voice Conversion with Cyclic Variational Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>674</first_page>
						<last_page>678</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2307</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tobing19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name>
					</contributors>
					<titles><title>StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>679</first_page>
						<last_page>683</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2236</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kaneko19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Kurita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>684</first_page>
						<last_page>688</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2206</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kurita19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Hieu</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>689</first_page>
						<last_page>693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2067</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bajibabu</given_name>
<surname>Bollepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>694</first_page>
						<last_page>698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2008</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/juvela19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Min</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>699</first_page>
						<last_page>703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1965</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yamamoto19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyed Hamidreza</given_name>
<surname>Mohammadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taehwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>704</first_page>
						<last_page>708</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1798</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mohammadi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen-Chou</given_name>
<surname>Lo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick Lumban</given_name>
<surname>Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>709</first_page>
						<last_page>713</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1774</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuewen</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lifa</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>714</first_page>
						<last_page>718</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1316</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li-Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>719</first_page>
						<last_page>723</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1265</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>724</first_page>
						<last_page>728</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1198</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ding19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cory</given_name>
<surname>Stephenson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gokce</given_name>
<surname>Keskin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oguz H.</given_name>
<surname>Elibol</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Voice Conversion with Amortized Variational Inference</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>729</first_page>
						<last_page>733</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1840</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/stephenson19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Subhadeep</given_name>
<surname>Dey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung</given_name>
<surname>Bui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franck</given_name>
<surname>Dernoncourt</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Semi-Supervised Training Through a Dropout Regularization in End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>734</first_page>
						<last_page>738</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3246</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dey19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minkyu</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhananjaya</given_name>
<surname>Gowda</surname>
</person_name>
					</contributors>
					<titles><title>Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>739</first_page>
						<last_page>743</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3227</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kim19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Accent Adaptation Based on Gate Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>744</first_page>
						<last_page>748</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3155</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Adaptation with Adversarial Dropout Regularization for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>749</first_page>
						<last_page>753</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2544</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/guo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markus</given_name>
<surname>Kitza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Golik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Cumulative Adaptation for BLSTM Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>754</first_page>
						<last_page>758</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2162</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kitza19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Fast DNN Acoustic Model Speaker Adaptation by Learning Hidden Unit Contribution Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>759</first_page>
						<last_page>763</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2050</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xie19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Asakawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshiyuki</given_name>
<surname>Kumakura</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Adaptation with Backpropagation Through WFST for On-Device Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>764</first_page>
						<last_page>768</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1880</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tsunoo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leda</given_name>
<surname>Sarı</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark A.</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Learning Speaker Aware Offsets for Speaker Adaptation of Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>769</first_page>
						<last_page>773</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1788</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Zadrazil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Françoise</given_name>
<surname>Beaufays</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation into On-Device Personalization of End-to-End Automatic Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>774</first_page>
						<last_page>778</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1752</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sim19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwanath P.</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakti P.</given_name>
<surname>Rath</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Accent Acoustic Model Using Mixture of Experts for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>779</first_page>
						<last_page>783</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1667</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jain19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joel</given_name>
<surname>Shor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dotan</given_name>
<surname>Emanuel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oran</given_name>
<surname>Lang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Omry</given_name>
<surname>Tuval</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Brenner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Cattiau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fernando</given_name>
<surname>Vieira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maeve</given_name>
<surname>McNally</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taylor</given_name>
<surname>Charbonneau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melissa</given_name>
<surname>Nollstadt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avinatan</given_name>
<surname>Hassidim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Matias</surname>
</person_name>
					</contributors>
					<titles><title>Personalizing ASR for Dysarthric and Accented Speech with Limited Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>784</first_page>
						<last_page>788</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1427</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shor19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Denis</given_name>
<surname>Peskov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joe</given_name>
<surname>Barrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Rodriguez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graham</given_name>
<surname>Neubig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Boyd-Graber</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Noisy Inputs for Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>789</first_page>
						<last_page>793</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3154</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/peskov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aman</given_name>
<surname>Alok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Ananthakrishnan</surname>
</person_name>
					</contributors>
					<titles><title>One-vs-All Models for Asynchronous Training: An Empirical Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>794</first_page>
						<last_page>798</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2760</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gupta19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Marzinotto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Géraldine</given_name>
<surname>Damnati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frédéric</given_name>
<surname>Béchet</surname>
</person_name>
					</contributors>
					<titles><title>Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>799</first_page>
						<last_page>803</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2732</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/marzinotto19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Morchid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Bost</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georges</given_name>
<surname>Linarès</surname>
</person_name>
					</contributors>
					<titles><title>M2H-GAN: A GAN-Based Mapping from Machine to Human Transcripts for Speech Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>804</first_page>
						<last_page>808</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2662</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/parcollet19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Munir</given_name>
<surname>Georges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krzysztof</given_name>
<surname>Czarnowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Ultra-Compact NLU: Neuronal Network Binarization as Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>809</first_page>
						<last_page>813</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2591</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/georges19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Loren</given_name>
<surname>Lugosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Ignoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikrant Singh</given_name>
<surname>Tomar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshua</given_name>
<surname>Bengio</surname>
</person_name>
					</contributors>
					<titles><title>Speech Model Pre-Training for End-to-End Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>814</first_page>
						<last_page>818</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2396</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lugosch19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prashanth Gurunath</given_name>
<surname>Shivakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Language Intent Detection Using Confusion2Vec</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>819</first_page>
						<last_page>823</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2226</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shivakumar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Caubrière</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>824</first_page>
						<last_page>828</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2158</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tomashenko19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanfeng</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raymond Chi-Wing</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Topic-Aware Dialogue Speech Recognition with Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>829</first_page>
						<last_page>833</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1694</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/song19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosana</given_name>
<surname>Kamiyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanobu</given_name>
<surname>Oba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Kobashikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>834</first_page>
						<last_page>838</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1534</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/masumura19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei Xiang</given_name>
<surname>Lieow</surname>
</person_name>
					</contributors>
					<titles><title>Meta Learning for Hyperparameter Optimization in Dialogue System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>839</first_page>
						<last_page>843</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1383</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chien19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyle</given_name>
<surname>Williams</surname>
</person_name>
					</contributors>
					<titles><title>Zero Shot Intent Classification Using Long-Short Term Memory Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>844</first_page>
						<last_page>848</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1274</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/williams19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mandy</given_name>
<surname>Korpusik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>A Comparison of Deep Learning Methods for Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>849</first_page>
						<last_page>853</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1262</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/korpusik19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuka</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takami</given_name>
<surname>Yoshida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Iwata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Fujimura</surname>
</person_name>
					</contributors>
					<titles><title>Slot Filling with Weighted Multi-Encoders for Out-of-Domain Values</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>854</first_page>
						<last_page>858</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1226</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kobayashi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Corpus Acoustic-to-Articulatory Speech Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>859</first_page>
						<last_page>863</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3168</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/seneviratne19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debadatta</given_name>
<surname>Dash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>Wisler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Ferrari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Speaker Independent Speech-BCI Using Speaker Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>864</first_page>
						<last_page>868</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3109</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dash19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Janaki</given_name>
<surname>Sheth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariel</given_name>
<surname>Tankus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lindy</given_name>
<surname>Comstock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itzhak</given_name>
<surname>Fried</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Speier</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Input Features for Development of Real-Time Translation of Neural Signals to Text</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>869</first_page>
						<last_page>873</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3092</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sheth19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>António</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Conceição</given_name>
<surname>Cunha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nuno</given_name>
<surname>Almeida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun A.</given_name>
<surname>Joseph</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Frahm</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Critical Articulator Identification from 50Hz RT-MRI Data of the Vocal Tract</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>874</first_page>
						<last_page>878</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2897</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/silva19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ioannis K.</given_name>
<surname>Douros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasiia</given_name>
<surname>Tsukanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karyna</given_name>
<surname>Isaieva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-André</given_name>
<surname>Vuissoz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yves</given_name>
<surname>Laprie</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Method of Dynamic Vocal Tract Shapes Generation by Combining Static 3D and Dynamic 2D MRI Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>879</first_page>
						<last_page>883</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2880</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/douros19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oksana</given_name>
<surname>Rasskazova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Mooshammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susanne</given_name>
<surname>Fuchs</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Coordination of Articulatory and Respiratory Events Prior to Speech Initiation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>884</first_page>
						<last_page>888</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2876</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rasskazova19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michele</given_name>
<surname>Gubian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manfred</given_name>
<surname>Pastätter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marianne</given_name>
<surname>Pouplier</surname>
</person_name>
					</contributors>
					<titles><title>Zooming in on Spatiotemporal V-to-C Coarticulation with Functional PCA</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>889</first_page>
						<last_page>893</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2143</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gubian19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammed Salah</given_name>
<surname>Al-Radhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Géza</given_name>
<surname>Németh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Markó</surname>
</person_name>
					</contributors>
					<titles><title>Ultrasound-Based Silent Speech Interface Built on a Continuous Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>894</first_page>
						<last_page>898</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2046</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/csapo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Klein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jana</given_name>
<surname>Brunner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phil</given_name>
<surname>Hoole</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Acoustic and Articulatory Dimensions of Speech Motor Adaptation with Random Forests</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>899</first_page>
						<last_page>903</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1812</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/klein19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hironori</given_name>
<surname>Takemoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Goto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Hagihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sayaka</given_name>
<surname>Hamanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kitamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukiko</given_name>
<surname>Nota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kikuo</given_name>
<surname>Maekawa</surname>
</person_name>
					</contributors>
					<titles><title>Speech Organ Contour Extraction Using Real-Time MRI and Machine Learning Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>904</first_page>
						<last_page>908</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1593</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/takemoto19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>K.G. van</given_name>
<surname>Leeuwen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>P.</given_name>
<surname>Bos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Trebeschi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M.J.A. van</given_name>
<surname>Alphen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L.</given_name>
<surname>Voskuilen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L.E.</given_name>
<surname>Smeele</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>F. van der</given_name>
<surname>Heijden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>R.J.J.H. van</given_name>
<surname>Son</surname>
</person_name>
					</contributors>
					<titles><title>CNN-Based Phoneme Classifier from Vocal Tract MRI Learns Embedding Consistent with Articulatory Topology</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>909</first_page>
						<last_page>913</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1173</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/leeuwen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Doris</given_name>
<surname>Mücke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Tilsen</surname>
</person_name>
					</contributors>
					<titles><title>Strength and Structure: Coupling Tones with Oral Constriction Gestures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>914</first_page>
						<last_page>918</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2650</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mucke19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>W. Bastiaan</given_name>
<surname>Kleijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicia S.C.</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Chinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Skoglund</surname>
</person_name>
					</contributors>
					<titles><title>Salient Speech Representations Based on Cloned Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>919</first_page>
						<last_page>923</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1861</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kleijn19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manoj Kumar</given_name>
<surname>Ramanathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>ASR Inspired Syllable Stress Detection for Pronunciation Evaluation Without Using a Supervised Classifier and Syllable Level Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>924</first_page>
						<last_page>928</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2091</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ramanathi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Renuka</given_name>
<surname>Mannem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jhansi</given_name>
<surname>Mallela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Illa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic and Articulatory Feature Based Speech Rate Estimation Using a Convolutional Dense Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>929</first_page>
						<last_page>933</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2295</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mannem19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Springenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Egor</given_name>
<surname>Lakomkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cornelius</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Wermter</surname>
</person_name>
					</contributors>
					<titles><title>Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>934</first_page>
						<last_page>938</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2845</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/springenberg19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Tzinis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Ellinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theodoros</given_name>
<surname>Giannakopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Low-Rank Representations for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>939</first_page>
						<last_page>943</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2769</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/paraskevopoulos19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jitendra Kumar</given_name>
<surname>Dhiman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nagaraj</given_name>
<surname>Adiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra Sekhar</given_name>
<surname>Seelamantula</surname>
</person_name>
					</contributors>
					<titles><title>On the Suitability of the Riesz Spectro-Temporal Envelope for WaveNet Based Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>944</first_page>
						<last_page>948</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2626</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dhiman19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinzhou</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>949</first_page>
						<last_page>953</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2406</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sweekar</given_name>
<surname>Sudhakara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manoj Kumar</given_name>
<surname>Ramanathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System Considering HMM Transition Probabilities</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>954</first_page>
						<last_page>958</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2363</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sudhakara19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atreyee</given_name>
<surname>Saha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Low Resource Automatic Intonation Classification Using Gated Recurrent Unit (GRU) Networks Pre-Trained with Synthesized Pitch Patterns</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>959</first_page>
						<last_page>963</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2351</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/saha19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neville</given_name>
<surname>Ryant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenneth</given_name>
<surname>Church</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Cieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name>
					</contributors>
					<titles><title>The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>978</first_page>
						<last_page>982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1268</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ryant19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prachi</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harsha Vardhan</given_name>
<surname>M.A.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A.</given_name>
<surname>Kanagasundaram</surname>
</person_name>
					</contributors>
					<titles><title>LEAP Diarization System for the Second DIHARD Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>983</first_page>
						<last_page>987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2716</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/singh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ignacio</given_name>
<surname>Viñals</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Gimeno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>ViVoLAB Speaker Diarization System for the DIHARD 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>988</first_page>
						<last_page>992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2462</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vinals19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zbyněk</given_name>
<surname>Zajíc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Kunešová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marek</given_name>
<surname>Hrúz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Vaněk</surname>
</person_name>
					</contributors>
					<titles><title>UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>993</first_page>
						<last_page>997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1385</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zajic19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tae Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manoj</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Flemotomos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monisankha</given_name>
<surname>Pal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghuveer</given_name>
<surname>Peri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rimita</given_name>
<surname>Lahiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>The Second DIHARD Challenge: System Description for USC-SAIL Team</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>998</first_page>
						<last_page>1002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1903</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/park19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Gusev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Ivanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timur</given_name>
<surname>Pekhovsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Shulipa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Avdeeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Gorlanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandr</given_name>
<surname>Kozlov</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization with Deep Speaker Embeddings for DIHARD Challenge II</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1003</first_page>
						<last_page>1007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2757</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/novoselov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ville</given_name>
<surname>Vestman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md.</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Delgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Nautsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi H.</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1008</first_page>
						<last_page>1012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2249</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/todisco19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng-I</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1013</first_page>
						<last_page>1017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1794</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lai19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhusan</given_name>
<surname>Chettri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Stoller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronica</given_name>
<surname>Morfi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco A. Martínez</given_name>
<surname>Ramírez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanouil</given_name>
<surname>Benetos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bob L.</given_name>
<surname>Sturm</surname>
</person_name>
					</contributors>
					<titles><title>Ensemble Models for Spoofing Detection in Automatic Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1018</first_page>
						<last_page>1022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2505</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chettri19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weicheng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haiwei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danwei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1023</first_page>
						<last_page>1027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1230</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Radosław</given_name>
<surname>Białobrzeski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Kośmider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Matuszewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Plata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Rakowski</surname>
</person_name>
					</contributors>
					<titles><title>Robust Bayesian and Light Neural Networks for Voice Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1028</first_page>
						<last_page>1032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2676</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/biaobrzeski19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Galina</given_name>
<surname>Lavrentyeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andzhukaev</given_name>
<surname>Tseren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marina</given_name>
<surname>Volkova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Gorlanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandr</given_name>
<surname>Kozlov</surname>
</person_name>
					</contributors>
					<titles><title>STC Antispoofing Systems for the ASVspoof2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1033</first_page>
						<last_page>1037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1768</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lavrentyeva19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yexin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongji</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1038</first_page>
						<last_page>1042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2170</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>K.N.R.K. Raju</given_name>
<surname>Alluri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>IIIT-H Spoofing Countermeasures for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1043</first_page>
						<last_page>1047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1623</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/alluri19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongjin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1048</first_page>
						<last_page>1052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1698</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Rownicka</surname>
</person_name>
					</contributors>
					<titles><title>Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1053</first_page>
						<last_page>1057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1760</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/williams19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jichen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Long Range Acoustic Features for Spoofed Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1058</first_page>
						<last_page>1062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1887</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/das19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Su-Yu</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai-Cheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chia-Ping</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Transfer-Representation Learning for Detecting Spoofing Attacks with Converted and Synthesized Speech in Automatic Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1063</first_page>
						<last_page>1067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2014</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chang19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Gomez-Alanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio M.</given_name>
<surname>Peinado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose A.</given_name>
<surname>Gonzalez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel M.</given_name>
<surname>Gomez</surname>
</person_name>
					</contributors>
					<titles><title>A Light Convolutional GRU-RNN Deep Feature Extractor for ASV Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1068</first_page>
						<last_page>1072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2212</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gomezalanis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hossein</given_name>
<surname>Zeinali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Athanasopoulou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioannis</given_name>
<surname>Gkinis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1073</first_page>
						<last_page>1077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2892</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zeinali19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moustafa</given_name>
<surname>Alzantot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziqi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mani B.</given_name>
<surname>Srivastava</surname>
</person_name>
					</contributors>
					<titles><title>Deep Residual Neural Networks for Audio Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1078</first_page>
						<last_page>1082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3174</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/alzantot19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Replay Attack Detection with Complementary High-Resolution Information Using End-to-End DNN for the ASVspoof 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1083</first_page>
						<last_page>1087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1991</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jung19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Dunbar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Algayres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Karadayi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Bernard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Benjumea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan-Nga</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucie</given_name>
<surname>Miskic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charlotte</given_name>
<surname>Dugrain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ondel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>The Zero Resource Speech Challenge 2019: TTS Without T</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1088</first_page>
						<last_page>1092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2904</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dunbar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1093</first_page>
						<last_page>1097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1337</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/feng19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alican</given_name>
<surname>Gök</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Batuhan</given_name>
<surname>Gundogdu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oyku Deniz</given_name>
<surname>Kose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murat</given_name>
<surname>Saraclar</surname>
</person_name>
					</contributors>
					<titles><title>Temporally-Aware Acoustic Unit Discovery for Zerospeech 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1098</first_page>
						<last_page>1102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1430</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yusuf19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Eloff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>André</given_name>
<surname>Nortje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin van</given_name>
<surname>Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avashna</given_name>
<surname>Govender</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leanne</given_name>
<surname>Nortje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnu</given_name>
<surname>Pretorius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elan van</given_name>
<surname>Biljon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewald van der</given_name>
<surname>Westhuizen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa van</given_name>
<surname>Staden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1103</first_page>
						<last_page>1107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1518</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/eloff19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andy T.</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-chun</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1108</first_page>
						<last_page>1112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2048</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karthik Pandia D.</given_name>
<surname>S.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema A.</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1113</first_page>
						<last_page>1117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2336</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/s19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1118</first_page>
						<last_page>1122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3232</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tjandra19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Macherey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1123</first_page>
						<last_page>1127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1951</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jia19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongjun</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haifeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengqing</given_name>
<surname>Zong</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Translation with Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1128</first_page>
						<last_page>1132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2582</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mattia A. Di</given_name>
<surname>Gangi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Turchi</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Transformer to End-to-End Spoken Language Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1133</first_page>
						<last_page>1137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3045</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gangi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steven</given_name>
<surname>Hillis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anushree Prasanna</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Phonetic and Word Level Discovery for Speech to Speech Translation for Unwritten Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1138</first_page>
						<last_page>1142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3026</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hillis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Kenny</surname>
</person_name>
					</contributors>
					<titles><title>Deep Speaker Recognition: Modular or Monolithic?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1143</first_page>
						<last_page>1147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3146</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bhattacharya19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>On the Usage of Phonetic Information for Text-Independent Speaker Embedding Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1148</first_page>
						<last_page>1152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3036</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshua</given_name>
<surname>Bengio</surname>
</person_name>
					</contributors>
					<titles><title>Learning Speaker Representations with Mutual Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1153</first_page>
						<last_page>1157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2380</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ravanelli19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lanhua</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Learning with High-Order Statistics for x-Vector Based Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1158</first_page>
						<last_page>1162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2264</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/you19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhanghao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1163</first_page>
						<last_page>1167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2248</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lanhua</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1168</first_page>
						<last_page>1172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1746</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/you19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Riyaz</given_name>
<surname>Bhat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rashmi</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivas</given_name>
<surname>Bangalore</surname>
</person_name>
					</contributors>
					<titles><title>Neural Transition Systems for Modeling Hierarchical Semantic Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1173</first_page>
						<last_page>1177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3075</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bhat19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vedran</given_name>
<surname>Vukotić</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Raymond</surname>
</person_name>
					</contributors>
					<titles><title>Mining Polysemous Triplets with Recurrent Neural Networks for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1178</first_page>
						<last_page>1182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2977</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vukotic19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avik</given_name>
<surname>Ray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Iterative Delexicalization for Improved Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1183</first_page>
						<last_page>1187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2955</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ray19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Swapnil</given_name>
<surname>Bhosale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Imran</given_name>
<surname>Sheikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Spoken Language Understanding: Bootstrapping in Low Resource Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1188</first_page>
						<last_page>1192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2366</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bhosale19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Takatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuya</given_name>
<surname>Yokoyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoichi</given_name>
<surname>Matsuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Fujie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Recognition of Intentions of Users&#8217; Short Responses for Conversational News Delivery System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1193</first_page>
						<last_page>1197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2121</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/takatsu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Caubrière</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Morin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Camelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Curriculum-Based Transfer Learning for an Effective End-to-End Spoken Language Understanding and Domain Portability</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1198</first_page>
						<last_page>1202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1832</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/caubriere19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debadatta</given_name>
<surname>Dash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Ferrari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Spatial and Spectral Fingerprint in the Brain: Speaker Identification from Single Trial MEG Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1203</first_page>
						<last_page>1207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3105</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dash19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Annika</given_name>
<surname>Nijveld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L. ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name>
					</contributors>
					<titles><title>ERP Signal Analysis with Temporal Resolution Using a Time Window Bank</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1208</first_page>
						<last_page>1212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2729</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nijveld19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>L. ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K.</given_name>
<surname>Mulder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L.</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Phase Synchronization Between EEG Signals as a Function of Differences Between Stimuli Characteristics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1213</first_page>
						<last_page>1217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2443</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bosch19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mariya</given_name>
<surname>Kharaman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manluolan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carsten</given_name>
<surname>Eulitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bettina</given_name>
<surname>Braun</surname>
</person_name>
					</contributors>
					<titles><title>The Processing of Prosodic Cues to Rhetorical Question Interpretation: Psycholinguistic and Neurolinguistics Evidence</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1218</first_page>
						<last_page>1222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2528</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kharaman19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiska</given_name>
<surname>Koemans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cybelle</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark A.</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kara D.</given_name>
<surname>Federmeier</surname>
</person_name>
					</contributors>
					<titles><title>The Neural Correlates Underlying Lexically-Guided Perceptual Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1223</first_page>
						<last_page>1227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2328</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/scharenborg19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan Halim</given_name>
<surname>Parmonangan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Speech Quality Evaluation of Synthesized Japanese Speech Using EEG</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1228</first_page>
						<last_page>1232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2059</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/parmonangan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Turaj Z.</given_name>
<surname>Shabestary</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gruenstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Microphone Adaptive Noise Cancellation for Robust Hotword Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1233</first_page>
						<last_page>1237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3006</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1238</first_page>
						<last_page>1242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2078</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Khokhlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Zatvornitskiy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Medennikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Sorokin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Prisyach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Romanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Mitrofanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariya</given_name>
<surname>Korenevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Petrov</surname>
</person_name>
					</contributors>
					<titles><title>R-Vectors: New Technique for Adaptation to Room Acoustics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1243</first_page>
						<last_page>1247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2645</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/khokhlov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Heitkaemper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Guided Source Separation Meets a Strong ASR Backend: Hitachi/Paderborn University Joint Investigation for Dinner Party ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1248</first_page>
						<last_page>1252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1167</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kanda19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Drude</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahn</given_name>
<surname>Heymann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Training of Neural Mask-Based Beamforming</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1253</first_page>
						<last_page>1257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2549</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/drude19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diyuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongfu</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Model Ensembling Using Effective Data Augmentation for CHiME-5 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1258</first_page>
						<last_page>1262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2601</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ma19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bharat</given_name>
<surname>Padi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anand</given_name>
<surname>Mohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Attention Based Hybrid i-Vector BLSTM Model for Language Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1263</first_page>
						<last_page>1267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2371</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/padi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju-ho</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>RawNet: Advanced End-to-End Deep Neural Network Using Raw Waveforms for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1268</first_page>
						<last_page>1272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1982</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jung19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Target Speaker Extraction for Multi-Talker Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1273</first_page>
						<last_page>1277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1410</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Mazzawi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavi</given_name>
<surname>Gonzalvo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleks</given_name>
<surname>Kracun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niranjan</given_name>
<surname>Subrahmanya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ignacio Lopez</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Violette</surname>
</person_name>
					</contributors>
					<titles><title>Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1278</first_page>
						<last_page>1282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1916</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mazzawi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shifeng</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank K.</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>Forward-Backward Decoding for Regularizing End-to-End TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1283</first_page>
						<last_page>1287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2325</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zheng19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank K.</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>A New GAN-Based End-to-End TTS Training Algorithm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1288</first_page>
						<last_page>1292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2176</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/guo19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mutian</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1293</first_page>
						<last_page>1297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1972</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/he19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuming</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1298</first_page>
						<last_page>1302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1357</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hieu-Thi</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuyuki</given_name>
<surname>Nishizawa</surname>
</person_name>
					</contributors>
					<titles><title>Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1303</first_page>
						<last_page>1307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1311</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/luong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuma</given_name>
<surname>Okamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Shiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Neural Text-to-Speech with Sequence-to-Sequence Acoustic Model and WaveGlow or Single Gaussian WaveRNN Vocoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1308</first_page>
						<last_page>1312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1288</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/okamoto19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sushant</given_name>
<surname>Kafle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cecilia Ovesdotter</given_name>
<surname>Alm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Huenerfauth</surname>
</person_name>
					</contributors>
					<titles><title>Fusion Strategy for Prosodic and Lexical Representations of Word Importance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1313</first_page>
						<last_page>1317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1898</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kafle19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun-Wei</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Self Attention in Variational Sequential Learning for Summarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1318</first_page>
						<last_page>1322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1548</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chien19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhongkai</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prathusha K.</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Sethares</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik P.</given_name>
<surname>Bucy</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1323</first_page>
						<last_page>1327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2482</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sun19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1328</first_page>
						<last_page>1332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3184</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Máté Ákos</given_name>
<surname>Tündik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valér</given_name>
<surname>Kaszás</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>György</given_name>
<surname>Szaszák</surname>
</person_name>
					</contributors>
					<titles><title>Assessing the Semantic Space Bias Caused by ASR Error Propagation and its Effect on Spoken Document Summarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1333</first_page>
						<last_page>1337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2154</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tundik19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peisong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peijie</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wencheng</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiande</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Latent Topic Attention for Domain Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1338</first_page>
						<last_page>1342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2228</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chaitanya</given_name>
<surname>Narisetty</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Bayesian Source Modelling for Determined Blind Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1343</first_page>
						<last_page>1347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1272</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/narisetty19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsanam</given_name>
<surname>Parthasaarathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nabarun</given_name>
<surname>Goswami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Mitsufuji</surname>
</person_name>
					</contributors>
					<titles><title>Recursive Speech Separation for Unknown Number of Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1348</first_page>
						<last_page>1352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1550</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/takahashi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pieter</given_name>
<surname>Appeltans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeroen</given_name>
<surname>Zegers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Practical Applicability of Deep Neural Networks for Overlapping Speaker Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1353</first_page>
						<last_page>1357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1807</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/appeltans19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoyi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Speech Separation Using Independent Vector Analysis with an Amplitude Variable Gaussian Mixture Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1358</first_page>
						<last_page>1362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2076</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gene-Ping</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-I</given_name>
<surname>Tuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin-shan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Improved Speech Separation with Time-and-Frequency Cross-Domain Joint Embedding and Clustering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1363</first_page>
						<last_page>1367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2181</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joe</given_name>
<surname>Antognini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Flynn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Licheng Richard</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmett</given_name>
<surname>McQuinn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dwight</given_name>
<surname>Crow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ethan</given_name>
<surname>Manilow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name>
					</contributors>
					<titles><title>WHAM!: Extending Speech Separation to Noisy Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1368</first_page>
						<last_page>1372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2821</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wichern19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carol</given_name>
<surname>Chermaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cassia</given_name>
<surname>Valentini-Botinhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henning</given_name>
<surname>Schepker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Near End Listening Enhancement Algorithms in Realistic Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1373</first_page>
						<last_page>1377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1800</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chermaz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amin</given_name>
<surname>Edraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wai-Yip</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Fogerty</surname>
</person_name>
					</contributors>
					<titles><title>Improvement and Assessment of Spectro-Temporal Modulation Analysis for Speech Intelligibility Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1378</first_page>
						<last_page>1382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2898</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/edraki19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuohuang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Shen</surname>
</person_name>
					</contributors>
					<titles><title>Listener Preference on the Local Criterion for Ideal Binary-Masked Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1383</first_page>
						<last_page>1387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1369</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan</given_name>
<surname>Dinh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Tjaden</surname>
</person_name>
					</contributors>
					<titles><title>Using a Manifold Vocoder for Spectral Voice and Style Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1388</first_page>
						<last_page>1392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1176</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dinh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>P. von</given_name>
<surname>Platen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>P.C.</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Span Acoustic Modelling Using Raw Waveform Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1393</first_page>
						<last_page>1397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2454</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/platen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>André</given_name>
<surname>Merboldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>An Analysis of Local Monotonic Attention Variants</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1398</first_page>
						<last_page>1402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2879</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/merboldt19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eric</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Layer Trajectory BLSTM</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1403</first_page>
						<last_page>1407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2971</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sun19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nelson Enrique Yalta</given_name>
<surname>Soplin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1408</first_page>
						<last_page>1412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1938</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/karita19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yumo</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Trainable Dynamic Subsampling for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1413</first_page>
						<last_page>1417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2778</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ding</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pat</given_name>
<surname>Rondon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepti</given_name>
<surname>Bhatia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name>
					</contributors>
					<titles><title>Shallow-Fusion End-to-End Contextual Biasing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1418</first_page>
						<last_page>1422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1209</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md.</given_name>
<surname>Nasir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep Nallan</given_name>
<surname>Chakravarthula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian R.W.</given_name>
<surname>Baucom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David C.</given_name>
<surname>Atkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Interpersonal Linguistic Coordination in Conversations Using Word Mover&#8217;s Distance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1423</first_page>
						<last_page>1427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1900</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nasir19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenchao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis-Philippe</given_name>
<surname>Morency</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeffrey</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1428</first_page>
						<last_page>1432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3059</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/du19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Voleti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Woolridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie M.</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melissa</given_name>
<surname>Milanovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher R.</given_name>
<surname>Bowie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>Objective Assessment of Social Skills Using Automated Language Analysis for Identification of Schizophrenia and Bipolar Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1433</first_page>
						<last_page>1437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2960</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/voleti19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katie</given_name>
<surname>Matton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin G.</given_name>
<surname>McInnis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Into the Wild: Transitioning from Recognizing Mood in Clinical Interactions to Personal Conversations for Individuals with Bipolar Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1438</first_page>
						<last_page>1442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2698</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/matton19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Morteza</given_name>
<surname>Rohanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Hough</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Purver</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Depression with Word-Level Multimodal Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1443</first_page>
						<last_page>1447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2283</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rohanian19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam C.</given_name>
<surname>Lammert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F.</given_name>
<surname>Quatieri</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1448</first_page>
						<last_page>1452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1815</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/espywilson19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shachi</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilek</given_name>
<surname>Hakkani-Tür</surname>
</person_name>
					</contributors>
					<titles><title>Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1453</first_page>
						<last_page>1457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1866</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/paul19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shachi</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilek</given_name>
<surname>Hakkani-Tür</surname>
</person_name>
					</contributors>
					<titles><title>HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1458</first_page>
						<last_page>1462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1863</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/goel19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiří</given_name>
<surname>Martínek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Král</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Lenc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Cerisara</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Lingual Dialogue Act Recognition with Deep Learning Methods</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1463</first_page>
						<last_page>1467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1691</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/martinek19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guan-Lin</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Lane</surname>
</person_name>
					</contributors>
					<titles><title>BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1468</first_page>
						<last_page>1472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1355</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Griol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoraida</given_name>
<surname>Callejas</surname>
</person_name>
					</contributors>
					<titles><title>Discovering Dialog Rules by Means of an Evolutionary Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1473</first_page>
						<last_page>1477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2230</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/griol19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xi C.</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adithya</given_name>
<surname>Sagar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justine T.</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tony Y.</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Klein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Pulman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason D.</given_name>
<surname>Williams</surname>
</person_name>
					</contributors>
					<titles><title>Active Learning for Domain Classification in a Commercial Spoken Personal Assistant</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1478</first_page>
						<last_page>1482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1315</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyed Omid</given_name>
<surname>Sadjadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Greenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elliot</given_name>
<surname>Singer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>Reynolds</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Mason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Hernandez-Cordero</surname>
</person_name>
					</contributors>
					<titles><title>The 2018 NIST Speaker Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1483</first_page>
						<last_page>1487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1351</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sadjadi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>McCree</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Borgstrom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fred</given_name>
<surname>Richardson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Grondin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Réda</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>García-Perera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro A.</given_name>
<surname>Torres-Carrasquillo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>State-of-the-Art Speaker Recognition for Telephone and Video Speech: The JHU-MIT Submission for NIST SRE18</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1488</first_page>
						<last_page>1492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2713</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/villalba19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>McCree</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1493</first_page>
						<last_page>1496</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2205</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/garciaromero19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ville</given_name>
<surname>Hautamäki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi H.</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Okabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ville</given_name>
<surname>Vestman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guohong</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanwu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mickael</given_name>
<surname>Rouvier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-Michel</given_name>
<surname>Bousquet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunlei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fahimeh</given_name>
<surname>Bahmaninezhad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Delgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name>
					</contributors>
					<titles><title>I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1497</first_page>
						<last_page>1501</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1533</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lee19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elie</given_name>
<surname>Khoury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khaled</given_name>
<surname>Lakhdhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Vaughan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parav</given_name>
<surname>Nagarsheth</surname>
</person_name>
					</contributors>
					<titles><title>Pindrop Labs&#8217; Submission to the First Multi-Target Speaker Detection and Identification Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1502</first_page>
						<last_page>1505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3179</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/khoury19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>McCree</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Recognition Benchmark Using the CHiME-5 Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1506</first_page>
						<last_page>1510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2174</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/garciaromero19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Ayllón</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor A.</given_name>
<surname>Sánchez-Hevia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Figueroa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Lanchantin</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Effects of Noisy and Reverberant Speech in Text-to-Speech Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1511</first_page>
						<last_page>1515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3104</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ayllon19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>F.-Y.</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I.C.</given_name>
<surname>Ouyang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Aryal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Lanchantin</surname>
</person_name>
					</contributors>
					<titles><title>Selection and Training Schemes for Improving TTS Voice Built on Found Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1516</first_page>
						<last_page>1520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2816</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kuo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David A.</given_name>
<surname>Braude</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew P.</given_name>
<surname>Aylett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caoimhín</given_name>
<surname>Laoide-Kemp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Ashby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristen M.</given_name>
<surname>Scott</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian Ó</given_name>
<surname>Raghallaigh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Braudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Brouwer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Stan</surname>
</person_name>
					</contributors>
					<titles><title>All Together Now: The Living Audio Dataset</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1521</first_page>
						<last_page>1525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2448</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/braude19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rob</given_name>
<surname>Clark</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1526</first_page>
						<last_page>1530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2441</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meysam</given_name>
<surname>Shamsi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Lolive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nelly</given_name>
<surname>Barbot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Chevelu</surname>
</person_name>
					</contributors>
					<titles><title>Corpus Design Using Convolutional Auto-Encoder Embeddings for Audio-Book Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1531</first_page>
						<last_page>1535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2190</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shamsi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nobukatsu</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noboru</given_name>
<surname>Miyazaki</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Intention Communication by TTS Using Explicit Definitions of Illocutionary Act Performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1536</first_page>
						<last_page>1540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2188</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hojo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chen-Chou</given_name>
<surname>Lo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1541</first_page>
						<last_page>1545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2003</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Fong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pilar Oplustil</given_name>
<surname>Gallegos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zack</given_name>
<surname>Hodari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1546</first_page>
						<last_page>1550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1824</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avashna</given_name>
<surname>Govender</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita E.</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Using Pupil Dilation to Measure Cognitive Load When Listening to Text-to-Speech in Quiet and in Noise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1551</first_page>
						<last_page>1555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1783</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/govender19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ioannis K.</given_name>
<surname>Douros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacques</given_name>
<surname>Felblinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Frahm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karyna</given_name>
<surname>Isaieva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun A.</given_name>
<surname>Joseph</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yves</given_name>
<surname>Laprie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Freddy</given_name>
<surname>Odille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasiia</given_name>
<surname>Tsukanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Voit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-André</given_name>
<surname>Vuissoz</surname>
</person_name>
					</contributors>
					<titles><title>A Multimodal Real-Time MRI Articulatory Corpus of French for Speech Research</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1556</first_page>
						<last_page>1560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1700</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/douros19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia-Xiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>A Chinese Dataset for Identifying Speakers in Novels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1561</first_page>
						<last_page>1565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1614</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyubyong</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Mulc</surname>
</person_name>
					</contributors>
					<titles><title>CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1566</first_page>
						<last_page>1570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1500</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/park19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ievgen</given_name>
<surname>Karaulov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmytro</given_name>
<surname>Tkanov</surname>
</person_name>
					</contributors>
					<titles><title>Attention Model for Articulatory Features Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1571</first_page>
						<last_page>1575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3020</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/karaulov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sibo</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apoorv</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Unbiased Semi-Supervised LF-MMI Training Using Dropout</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1576</first_page>
						<last_page>1580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2678</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1581</first_page>
						<last_page>1585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2620</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cui19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nirmesh J.</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik B.</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant A.</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Whether to Pretrain DNN or not?: An Empirical Analysis for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1586</first_page>
						<last_page>1590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2608</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shah19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohit</given_name>
<surname>Goyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Varun</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prathosh</given_name>
<surname>A.P.</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Glottal Closure Instants from Raw Speech Using Convolutional Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1591</first_page>
						<last_page>1595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2587</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/goyal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joachim</given_name>
<surname>Fainberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Lattice-Based Lightly-Supervised Acoustic Model Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1596</first_page>
						<last_page>1600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2533</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fainberg19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wilfried</given_name>
<surname>Michel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1601</first_page>
						<last_page>1605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2254</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/michel19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanobu</given_name>
<surname>Oba</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Automatic Speech Recognition with a Reconstruction Criterion Using Speech-to-Text and Text-to-Speech Encoder-Decoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1606</first_page>
						<last_page>1610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2111</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/masumura19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdelwahab</given_name>
<surname>Heba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Pierre</given_name>
<surname>Lorré</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Régine</given_name>
<surname>Andre-Obrecht</surname>
</person_name>
					</contributors>
					<titles><title>Char+CV-CTC: Combining Graphemes and Consonant/Vowel Units for CTC-Based ASR Using Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1611</first_page>
						<last_page>1615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1975</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/heba19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name>
					</contributors>
					<titles><title>Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1616</first_page>
						<last_page>1620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1952</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kurata19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name>
					</contributors>
					<titles><title>Direct Neuron-Wise Fusion of Cognate Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1621</first_page>
						<last_page>1625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1930</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fukuda19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pranav</given_name>
<surname>Ladkat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rybakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radhika</given_name>
<surname>Arava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sree Hari Krishnan</given_name>
<surname>Parthasarathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I-Fan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikko</given_name>
<surname>Strom</surname>
</person_name>
					</contributors>
					<titles><title>Two Tiered Distributed Training Algorithm for Acoustic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1626</first_page>
						<last_page>1630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1859</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ladkat19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pin-Tuan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Shin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syu-Siang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Encoder Layers of Discriminative Autoencoders for LVCSR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1631</first_page>
						<last_page>1635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1717</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task CTC Training with Auxiliary Feature Reconstruction for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1636</first_page>
						<last_page>1640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1710</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kurata19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanjiang</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weicong</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Framewise Supervised Training Towards End-to-End Speech Recognition Models: First Results</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1641</first_page>
						<last_page>1645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1117</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charilaos</given_name>
<surname>Papaioannou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Deep Hierarchical Fusion with Application in Sentiment Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1646</first_page>
						<last_page>1650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3243</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/georgiou19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sue</given_name>
<surname>Booker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Marchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David Scott</given_name>
<surname>Farrar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ute Dorothea</given_name>
<surname>Peitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bridget</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ermine</given_name>
<surname>Teves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anuj</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devang</given_name>
<surname>Naik</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect Expression from Voice</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1651</first_page>
						<last_page>1655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2998</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mitra19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jack</given_name>
<surname>Parry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitri</given_name>
<surname>Palaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Clarke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Lecomte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rebecca</given_name>
<surname>Mead</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregor</given_name>
<surname>Hofer</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1656</first_page>
						<last_page>1660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2753</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/parry19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Liakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Terry</given_name>
<surname>Lyons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejo J.</given_name>
<surname>Nevado-Holgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate</given_name>
<surname>Saunders</surname>
</person_name>
					</contributors>
					<titles><title>A Path Signature Approach for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1661</first_page>
						<last_page>1665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2624</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olga</given_name>
<surname>Egorow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tarik</given_name>
<surname>Mrech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norman</given_name>
<surname>Weißkirchen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Wendemuth</surname>
</person_name>
					</contributors>
					<titles><title>Employing Bottleneck and Convolutional Features for Speech-Based Physical Load Detection on Limited Data Amounts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1666</first_page>
						<last_page>1670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2502</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/egorow19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shizhe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingjun</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qin</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition in Dyadic Dialogues with Attentive Interaction Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1671</first_page>
						<last_page>1675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2103</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shun-Chang</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun-Min</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ching</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1676</first_page>
						<last_page>1680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2087</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gao-Yi</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun-Min</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Enforcing Semantic Consistency for Cross Corpus Valence Regression from Speech Using Adversarial Discrepancy Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1681</first_page>
						<last_page>1685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2037</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chao19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuiyang</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>P.C.</given_name>
<surname>Ching</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1686</first_page>
						<last_page>1690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1968</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gil</given_name>
<surname>Keren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingmar</given_name>
<surname>Steiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1691</first_page>
						<last_page>1695</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1811</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/triantafyllopoulos19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhixuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1696</first_page>
						<last_page>1700</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1683</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md. Asif</given_name>
<surname>Jalal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roger K.</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Learning Temporal Clusters Using Capsule Routing for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1701</first_page>
						<last_page>1705</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3068</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jalal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sonia</given_name>
<surname>d’Apolito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara Gili</given_name>
<surname>Fivela</surname>
</person_name>
					</contributors>
					<titles><title>L2 Pronunciation Accuracy and Context: A Pilot Study on the Realization of Geminates in Italian as L2 by French Learners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1706</first_page>
						<last_page>1710</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2934</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dapolito19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nisad</given_name>
<surname>Jamakovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Fuchs</surname>
</person_name>
					</contributors>
					<titles><title>The Monophthongs of Formal Nigerian English: An Acoustic Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1711</first_page>
						<last_page>1715</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2866</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jamakovic19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Arantes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anders</given_name>
<surname>Eriksson</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying Fundamental Frequency Modulation as a Function of Language, Speaking Style and Speaker</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1716</first_page>
						<last_page>1720</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2857</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/arantes19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niamh E.</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lara</given_name>
<surname>Keshishian</surname>
</person_name>
					</contributors>
					<titles><title>The Voicing Contrast in Stops and Affricates in the Western Armenian of Lebanon</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1721</first_page>
						<last_page>1725</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2529</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kelly19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adèle</given_name>
<surname>Jatteau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Adda-Decker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name>
					</contributors>
					<titles><title>&#8220; Gra[f] e!&#8221; Word-Final Devoicing of Obstruents in Standard French: An Acoustic Study Based on Large Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1726</first_page>
						<last_page>1730</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2329</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jatteau19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chih-Hsiang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huang-Cheng</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Tong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Wen</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Indicators of Deception in Mandarin Daily Conversations Recorded from an Interactive Game</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1731</first_page>
						<last_page>1735</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2216</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Effects on Plosive Duration in German and Austrian German</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1736</first_page>
						<last_page>1740</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2197</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schuppler19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cibu</given_name>
<surname>Johny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gutkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Jansche</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Consistency of Phonological Features: An Empirical Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1741</first_page>
						<last_page>1745</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2184</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/johny19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fanny</given_name>
<surname>Guitard-Ivent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriele</given_name>
<surname>Chignoli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cécile</given_name>
<surname>Fougeron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurianne</given_name>
<surname>Georgeton</surname>
</person_name>
					</contributors>
					<titles><title>Are IP Initial Vowels Acoustically More Distinct? Results from LDA and CNN Classifications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1746</first_page>
						<last_page>1750</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2153</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/guitardivent19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xizi</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvyn</given_name>
<surname>Hunt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Skilling</surname>
</person_name>
					</contributors>
					<titles><title>Neural Network-Based Modeling of Phonetic Durations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1751</first_page>
						<last_page>1755</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2102</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wei19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Janina</given_name>
<surname>Mołczanow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beata</given_name>
<surname>Łukaszewicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Łukaszewicz</surname>
</person_name>
					</contributors>
					<titles><title>An Acoustic Study of Vowel Undershoot in a System with Several Degrees of Prominence</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1756</first_page>
						<last_page>1760</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1806</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/moczanow19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name>
					</contributors>
					<titles><title>A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1761</first_page>
						<last_page>1765</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1664</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/berger19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shan</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Detail Encoding in Explaining the Size of Speech Planning Window</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1766</first_page>
						<last_page>1770</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1412</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/luo19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dina El</given_name>
<surname>Zarka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Cangemi</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Cues to Topic and Narrow Focus in Egyptian Arabic</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1771</first_page>
						<last_page>1775</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1189</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zarka19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kowovi Comivi</given_name>
<surname>Alowonou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhuan</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhicheng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Honda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic and Articulatory Study of Ewe Vowels: A Comparative Study of Male and Female</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1776</first_page>
						<last_page>1780</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2196</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/alowonou19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ya’nan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziping</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yide</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Speech Augmentation via Speaker-Specific Noise in Unseen Environment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1781</first_page>
						<last_page>1785</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2712</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/guo19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangdong</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Batushiren</surname>
</person_name>
					</contributors>
					<titles><title>UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1786</first_page>
						<last_page>1790</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1567</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Santiago</given_name>
<surname>Pascual</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan</given_name>
<surname>Serrà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Bonafonte</surname>
</person_name>
					</contributors>
					<titles><title>Towards Generalized Speech Enhancement with Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1791</first_page>
						<last_page>1795</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2688</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pascual19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanjie</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinrong</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengwu</given_name>
<surname>Xiong</surname>
</person_name>
					</contributors>
					<titles><title>A Convolutional Neural Network with Non-Local Module for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1796</first_page>
						<last_page>1800</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2472</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Chen</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Te</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tei-Wei</given_name>
<surname>Kuo</surname>
</person_name>
					</contributors>
					<titles><title>IA-NET: Acceleration and Compression of Speech Enhancement Using Integer-Adder Deep Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1801</first_page>
						<last_page>1805</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1207</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lin19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>KL-Divergence Regularized Deep Neural Network Adaptation for Low-Resource Speaker-Dependent Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1806</first_page>
						<last_page>1810</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2426</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Llombart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dayana</given_name>
<surname>Ribas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis</given_name>
<surname>Vicente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Wide Residual Networks in Reverberant Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1811</first_page>
						<last_page>1815</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1745</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/llombart19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandan K.A.</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ebrahim</given_name>
<surname>Beyrami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jamie</given_name>
<surname>Pool</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Gehrke</surname>
</person_name>
					</contributors>
					<titles><title>A Scalable Noisy Speech Dataset and Online Subjective Test Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1816</first_page>
						<last_page>1820</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3087</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/reddy19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nagaraj</given_name>
<surname>Adiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Pantazis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Tsiaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1821</first_page>
						<last_page>1825</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2648</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/adiga19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammed Shifas</given_name>
<surname>P.V.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nagaraj</given_name>
<surname>Adiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Tsiaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>A Non-Causal FFTNet Architecture for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1826</first_page>
						<last_page>1830</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2622</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pv19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>D.T.</given_name>
<surname>Braithwaite</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>W. Bastiaan</given_name>
<surname>Kleijn</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Variance Constrained Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1831</first_page>
						<last_page>1835</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1809</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/braithwaite19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Kyriakopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Learning Approach to Automatic Characterisation of Rhythm in Non-Native English Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1836</first_page>
						<last_page>1840</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3186</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kyriakopoulos19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danny</given_name>
<surname>Merkx</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan L.</given_name>
<surname>Frank</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name>
					</contributors>
					<titles><title>Language Learning Using Speech to Image Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1841</first_page>
						<last_page>1845</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3067</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/merkx19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucy</given_name>
<surname>Skidmore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roger K.</given_name>
<surname>Moore</surname>
</person_name>
					</contributors>
					<titles><title>Using Alexa for Flashcard-Based Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1846</first_page>
						<last_page>1850</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2893</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/skidmore19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Joglekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meena Chandra</given_name>
<surname>Shekhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Kothapally</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengzhu</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lakshmish</given_name>
<surname>Kaushik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhijeet</given_name>
<surname>Sangwan</surname>
</person_name>
					</contributors>
					<titles><title>The 2019 Inaugural Fearless Steps Challenge: A Giant Leap for Naturalistic Audio</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1851</first_page>
						<last_page>1855</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2301</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hansen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Che-Ping</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da-Rong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin-shan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1856</first_page>
						<last_page>1860</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2068</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tasavat</given_name>
<surname>Trisitichoke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shintaro</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Native Listeners&#8217; Facial Microexpressions While Shadowing Non-Native Speech &#8212; Potential of Shadowers&#8217; Facial Expressions for Comprehensibility Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1861</first_page>
						<last_page>1865</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1953</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/trisitichoke19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Reima</given_name>
<surname>Karhila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna-Riikka</given_name>
<surname>Smolander</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sari</given_name>
<surname>Ylinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Transparent Pronunciation Scoring Using Articulatorily Weighted Phoneme Edit Distance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1866</first_page>
						<last_page>1870</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1785</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/karhila19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Su-Youn</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong Min</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Klaus</given_name>
<surname>Zechner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keelan</given_name>
<surname>Evanini</surname>
</person_name>
					</contributors>
					<titles><title>Development of Robust Automated Scoring Models Using Adversarial Input for Oral Proficiency Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1871</first_page>
						<last_page>1875</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1711</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yoon19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Y.</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>P.</given_name>
<surname>Manakul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L.</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Y.</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Impact of ASR Performance on Spoken Grammatical Error Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1876</first_page>
						<last_page>1880</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1706</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seung Hee</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1881</first_page>
						<last_page>1885</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1478</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anoop</given_name>
<surname>Cherian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim K.</given_name>
<surname>Marks</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name>
					</contributors>
					<titles><title>Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1886</first_page>
						<last_page>1890</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3143</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hori19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Gopalakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Behnam</given_name>
<surname>Hedayatnia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinlang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Gottardi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Kwatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anu</given_name>
<surname>Venkatesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raefer</given_name>
<surname>Gabriel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilek</given_name>
<surname>Hakkani-Tür</surname>
</person_name>
					</contributors>
					<titles><title>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1891</first_page>
						<last_page>1895</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3079</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gopalakrishnan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Uliyana</given_name>
<surname>Kubasova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Murray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>McKenzie</given_name>
<surname>Braley</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Verbal and Nonverbal Features for Predicting Group Performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1896</first_page>
						<last_page>1900</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3062</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kubasova19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victor R.</given_name>
<surname>Martinez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Flemotomos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Ardulov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Somandepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon B.</given_name>
<surname>Goldberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zac E.</given_name>
<surname>Imel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David C.</given_name>
<surname>Atkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Therapist and Client Personae for Therapeutic Alliance Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1901</first_page>
						<last_page>1905</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2829</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/martinez19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Haake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Schimke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Betz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sina</given_name>
<surname>Zarrieß</surname>
</person_name>
					</contributors>
					<titles><title>Do Hesitations Facilitate Processing of Partially Defective System Utterances? An Exploratory Eye Tracking Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1906</first_page>
						<last_page>1910</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2820</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/haake19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Jia</surname>
</person_name>
					</contributors>
					<titles><title>Influence of Contextuality on Prosodic Realization of Information Structure in Chinese Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1911</first_page>
						<last_page>1915</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2291</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kristijan</given_name>
<surname>Gjoreski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandar</given_name>
<surname>Gjoreski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Kraljevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diane</given_name>
<surname>Hirschfeld</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Transfer Learning for Affective Spoken Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1916</first_page>
						<last_page>1920</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2163</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gjoreski19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingzhi</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emer</given_name>
<surname>Gilmartin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diane</given_name>
<surname>Litman</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Personality Traits Using Overlap Dynamics in Multiparty Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1921</first_page>
						<last_page>1925</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1886</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zakaria</given_name>
<surname>Aldeneh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mimansa</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin G.</given_name>
<surname>McInnis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Mood Episodes Using Dialogue Features from Clinical Interviews</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1926</first_page>
						<last_page>1930</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1878</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/aldeneh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nichola</given_name>
<surname>Lubold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie A.</given_name>
<surname>Borrie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyson S.</given_name>
<surname>Barrett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megan</given_name>
<surname>Willi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>Do Conversational Partners Entrain on Articulatory Precision?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1931</first_page>
						<last_page>1935</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1786</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lubold19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Conversational Emotion Analysis via Attention Mechanisms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1936</first_page>
						<last_page>1940</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1577</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lian19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emma</given_name>
<surname>O’Neill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name>
					</contributors>
					<titles><title>The Effect of Phoneme Distribution on Perceptual Similarity in English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1941</first_page>
						<last_page>1945</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3042</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/oneill19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sofoklis</given_name>
<surname>Kakouros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antti</given_name>
<surname>Suni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martti</given_name>
<surname>Vainio</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Representations of Prominence Classification Neural Networks and Autoencoders Using Bottleneck Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1946</first_page>
						<last_page>1950</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2984</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kakouros19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Peperkamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alvaro Martin Iturralde</given_name>
<surname>Zurita</surname>
</person_name>
					</contributors>
					<titles><title>Compensation for French Liquid Deletion During Auditory Sentence Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1951</first_page>
						<last_page>1955</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2950</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/peperkamp19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniil</given_name>
<surname>Kocharov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Kachkovskaia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Skrelin</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Factors Influencing Vowel Reduction in Russian</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1956</first_page>
						<last_page>1960</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2918</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kocharov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailbhe Ní</given_name>
<surname>Chasaide</surname>
</person_name>
					</contributors>
					<titles><title>Time to Frequency Domain Mapping of the Voice Source: The Influence of Open Quotient and Glottal Skew on the Low End of the Source Spectrum</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1961</first_page>
						<last_page>1965</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2888</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gobl19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer S.</given_name>
<surname>Cole</surname>
</person_name>
					</contributors>
					<titles><title>Testing the Distinctiveness of Intonational Tunes: Evidence from Imitative Productions in American English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1966</first_page>
						<last_page>1970</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2684</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chodroff19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sangwook</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David K.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mounya</given_name>
<surname>Elhilali</surname>
</person_name>
					</contributors>
					<titles><title>A Study of a Cross-Language Perception Based on Cortical Analysis Using Biomimetic STRFs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1971</first_page>
						<last_page>1975</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2507</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/park19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Šturm</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Volín</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Evaluation of Early versus Late F0 Peaks in the Intonation Structure of Czech Question-Word Questions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1976</first_page>
						<last_page>1980</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2082</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sturm19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anneliese</given_name>
<surname>Kelterer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Correlates of Phonation Type in Chichimec</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1981</first_page>
						<last_page>1985</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2066</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kelterer19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Ren</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Borský</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name>
					</contributors>
					<titles><title>F0 Variability Measures Based on Glottal Closure Instants</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1986</first_page>
						<last_page>1989</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1326</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chien19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Tavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Werner</surname>
</person_name>
					</contributors>
					<titles><title>Recognition of Creaky Voice from Emergency Calls</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1990</first_page>
						<last_page>1994</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1253</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tavi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuzhuang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Shimodaira</surname>
</person_name>
					</contributors>
					<titles><title>Direct F0 Estimation with Neural-Network-Based Regression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>1995</first_page>
						<last_page>1999</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3267</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanay</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohith Chandrashekar</given_name>
<surname>Aralikatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilip Kumar</given_name>
<surname>Margam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Thanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharad</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pujitha Appan</given_name>
<surname>Kandala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar M.</given_name>
<surname>Venkatesan</surname>
</person_name>
					</contributors>
					<titles><title>Real Time Online Visual End Point Detection Using Unidirectional LSTM</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2000</first_page>
						<last_page>2004</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3253</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sharma19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luc</given_name>
<surname>Ardaillon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Axel</given_name>
<surname>Roebel</surname>
</person_name>
					</contributors>
					<titles><title>Fully-Convolutional Network for Pitch Estimation of Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2005</first_page>
						<last_page>2009</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2815</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ardaillon19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingye</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name>
					</contributors>
					<titles><title>Vocal Pitch Extraction in Polyphonic Music Using Convolutional Residual Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2010</first_page>
						<last_page>2014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2286</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Level Adaptive Speech Activity Detector for Speech in Naturalistic Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2015</first_page>
						<last_page>2019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1928</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sharma19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2020</first_page>
						<last_page>2024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1925</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sharma19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroko</given_name>
<surname>Terasawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenta</given_name>
<surname>Wakasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ken-Ichi</given_name>
<surname>Sakakibara</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Physiological and Acoustic Contrasts Between Choral and Operatic Singing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2025</first_page>
						<last_page>2029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1864</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/terasawa19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruixi</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Costello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Jankowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwas</given_name>
<surname>Mruthyunjaya</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing Voice Activity Detection for Noisy Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2030</first_page>
						<last_page>2034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1776</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lin19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taiki</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryota</given_name>
<surname>Nishimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Misaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>Small-Footprint Magic Word Detection Method Using Convolutional LSTM Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2035</first_page>
						<last_page>2039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1662</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yamamoto19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chitralekha</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Modeling for Automatic Lyrics-to-Audio Alignment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2040</first_page>
						<last_page>2044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1520</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gupta19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anastasios</given_name>
<surname>Vafeiadis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleftherios</given_name>
<surname>Fanioudakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilyas</given_name>
<surname>Potamitis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Votis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Giakoumis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Tzovaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liming</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raouf</given_name>
<surname>Hamzaoui</surname>
</person_name>
					</contributors>
					<titles><title>Two-Dimensional Convolutional Recurrent Neural Networks for Speech Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2045</first_page>
						<last_page>2049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1354</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vafeiadis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tokihiko</given_name>
<surname>Kaburagi</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Soprano Singing in Light of the Source-Filter Interaction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2050</first_page>
						<last_page>2054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1153</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kaburagi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linhao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Boosting Character-Based Chinese Speech Synthesis via Multi-Task Learning and Dictionary Tutoring</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2055</first_page>
						<last_page>2059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3233</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zou19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liumeng</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanghui</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizheng</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Building a Mixed-Lingual Neural TTS System with Only Monolingual Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2060</first_page>
						<last_page>2064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3191</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xue19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alex</given_name>
<surname>Sokolov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tracy</given_name>
<surname>Rohlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2065</first_page>
						<last_page>2069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3176</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sokolov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Pronunciation Learning in End-to-End Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2070</first_page>
						<last_page>2074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2830</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/taylor19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-chieh</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2075</first_page>
						<last_page>2079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2730</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>R.J.</given_name>
<surname>Skerry-Ryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name>
					</contributors>
					<titles><title>Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2080</first_page>
						<last_page>2084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2668</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markéta</given_name>
<surname>Jůzová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Tihelka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Vít</surname>
</person_name>
					</contributors>
					<titles><title>Unified Language-Independent DNN-Based G2P Converter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2085</first_page>
						<last_page>2089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2335</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/juzova19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongyang</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-Trained BERT</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2090</first_page>
						<last_page>2094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2292</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sevinj</given_name>
<surname>Yolchuyeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Géza</given_name>
<surname>Németh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bálint</given_name>
<surname>Gyires-Tóth</surname>
</person_name>
					</contributors>
					<titles><title>Transformer Based Grapheme-to-Phoneme Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2095</first_page>
						<last_page>2099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1954</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yolchuyeva19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harry</given_name>
<surname>Bleyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandy</given_name>
<surname>Ritchie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas Fromseier</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daan van</given_name>
<surname>Esch</surname>
</person_name>
					</contributors>
					<titles><title>Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2100</first_page>
						<last_page>2104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1781</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bleyan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengnan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuang</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2105</first_page>
						<last_page>2109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1632</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zexin</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaogen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuxiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural Network with Multi-Level Embedding Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2110</first_page>
						<last_page>2114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1235</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cai19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun-Wei</given_name>
<surname>Gan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongzhi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tie-Yan</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2115</first_page>
						<last_page>2119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1208</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sun19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinjian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Dalmia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Speech Recognition with Corpus Relatedness Sampling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2120</first_page>
						<last_page>2124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3052</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harish</given_name>
<surname>Arsikere</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashtosh</given_name>
<surname>Sapru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Garimella</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2125</first_page>
						<last_page>2129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2881</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/arsikere19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anjuli</given_name>
<surname>Kannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arindrima</given_name>
<surname>Datta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugene</given_name>
<surname>Weinstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Bapna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungji</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2130</first_page>
						<last_page>2134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2858</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kannan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Mendes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João Paulo</given_name>
<surname>Neto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Recognition of Latin American Spanish Using Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2135</first_page>
						<last_page>2139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2772</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mendes19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thibault</given_name>
<surname>Viglino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Accented Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2140</first_page>
						<last_page>2144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2122</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/viglino19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenchen</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Articulatory Attribute Modeling for Low-Resource Multilingual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2145</first_page>
						<last_page>2149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2092</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karan</given_name>
<surname>Taneja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satarupa</given_name>
<surname>Guha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Basil</given_name>
<surname>Abraham</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Monolingual Speech Corpora for Code-Mixed Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2150</first_page>
						<last_page>2154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1959</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/taneja19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Bruguier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Golan</given_name>
<surname>Pundak</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2155</first_page>
						<last_page>2159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1868</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Tung</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiping</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Constrained Output Embeddings for End-to-End Code-Switching Speech Recognition with Only Monolingual Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2160</first_page>
						<last_page>2164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1867</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/khassanov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiping</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Tung</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>On the End-to-End Solution to Mandarin-English Code-Switching Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2165</first_page>
						<last_page>2169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1429</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zeng19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Towards Language-Universal Mandarin-English Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2170</first_page>
						<last_page>2174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1365</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prakhar</given_name>
<surname>Swarup</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Garimella</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri Harish</given_name>
<surname>Mallidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Hoffmeister</surname>
</person_name>
					</contributors>
					<titles><title>Improving ASR Confidence Scores for Alexa Using Acoustic and Hypothesis Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2175</first_page>
						<last_page>2179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1241</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/swarup19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2180</first_page>
						<last_page>2184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1290</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cal</given_name>
<surname>Peyser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Performance of End-to-End ASR on Numeric Sequences</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2185</first_page>
						<last_page>2189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1345</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/peyser19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenghao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name>
					</contributors>
					<titles><title>A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2190</first_page>
						<last_page>2194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1676</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chieh-Chi</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiv</given_name>
<surname>Vitaladevuni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Sub-Band Convolutional Neural Networks for Small-Footprint Spoken Term Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2195</first_page>
						<last_page>2199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1766</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenchen</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Radical-Based End-to-End Speech Recognition Systems for Chinese Dialects and Japanese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2200</first_page>
						<last_page>2204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2104</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongbin</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Joint Decoding of CTC Based Systems for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2205</first_page>
						<last_page>2209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2026</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/guo19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanobu</given_name>
<surname>Oba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>A Joint End-to-End and DNN-HMM Hybrid Automatic Speech Recognition System with Transferring Sharable Knowledge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2210</first_page>
						<last_page>2214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2263</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tanaka19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karan</given_name>
<surname>Malhotra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Active Learning Methods for Low Resource End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2215</first_page>
						<last_page>2219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2316</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/malhotra19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Karafiát</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Multilingual Sequence-to-Sequence Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2220</first_page>
						<last_page>2224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2355</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/karafiat19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michał</given_name>
<surname>Zapotoczny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Pietrzak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Łańcucki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Chorowski</surname>
</person_name>
					</contributors>
					<titles><title>Lattice Generation in Attention-Based Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2225</first_page>
						<last_page>2229</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2667</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zapotoczny19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Jansche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Gutkin</surname>
</person_name>
					</contributors>
					<titles><title>Sampling from Stochastic Finite Automata with Applications to CTC Decoding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2230</first_page>
						<last_page>2234</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2740</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jansche19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Łukasz</given_name>
<surname>Dudziak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed S.</given_name>
<surname>Abdelfattah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravichander</given_name>
<surname>Vipperla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefanos</given_name>
<surname>Laskaridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas D.</given_name>
<surname>Lane</surname>
</person_name>
					</contributors>
					<titles><title>ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2235</first_page>
						<last_page>2239</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2811</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dudziak19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-to-Phrase Models for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2240</first_page>
						<last_page>2244</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3056</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gaur19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruizhi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Performance Monitoring for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2245</first_page>
						<last_page>2249</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3137</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santiago</given_name>
<surname>Barreda</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Musical Experience in the Perceptual Weighting of Acoustic Cues for the Obstruent Coda Voicing Contrast in American English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2250</first_page>
						<last_page>2254</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3103</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cohn19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Lewandowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Duran</surname>
</person_name>
					</contributors>
					<titles><title>Individual Differences in Implicit Attention to Phonetic Detail in Speech Perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2255</first_page>
						<last_page>2259</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2989</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lewandowski19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaylah</given_name>
<surname>Lalonde</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Natural Variability in Cross-Modal Temporal Correlations on Audiovisual Speech Recognition Benefit</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2260</first_page>
						<last_page>2264</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2931</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lalonde19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>M.</given_name>
<surname>Bentum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L. ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A. van den</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name>
					</contributors>
					<titles><title>Listening with Great Expectations: An Investigation of Word Form Anticipations in Naturalistic Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2265</first_page>
						<last_page>2269</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2741</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bentum19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>M.</given_name>
<surname>Bentum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L. ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>A. van den</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying Expectation Modulation in Human Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2270</first_page>
						<last_page>2274</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2685</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bentum19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel R.</given_name>
<surname>Turner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ann R.</given_name>
<surname>Bradlow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer S.</given_name>
<surname>Cole</surname>
</person_name>
					</contributors>
					<titles><title>Perception of Pitch Contours in Speech and Nonspeech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2275</first_page>
						<last_page>2279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2619</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/turner19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>L. ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>L.</given_name>
<surname>Boves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K.</given_name>
<surname>Mulder</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Reaction Time and Error Sequences in Lexical Decision Experiments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2280</first_page>
						<last_page>2284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2611</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bosch19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianze</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Ping</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2285</first_page>
						<last_page>2289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2353</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuriko</given_name>
<surname>Yokoe</surname>
</person_name>
					</contributors>
					<titles><title>Place Shift as an Autonomous Process: Evidence from Japanese Listeners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2290</first_page>
						<last_page>2294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2302</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yokoe19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julien</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laure</given_name>
<surname>Dentel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silvain</given_name>
<surname>Gerber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name>
					</contributors>
					<titles><title>A Perceptual Study of CV Syllables in Both Spoken and Whistled Speech: A Tashlhiyt Berber Perspective</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2295</first_page>
						<last_page>2299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2251</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/meyer19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han-Chi</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Zhong</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ko-Chiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying-Hui</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>Consonant Classification in Mandarin Based on the Depth Image Feature: A Pilot Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2300</first_page>
						<last_page>2304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1893</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hsieh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiri</given_name>
<surname>Lev-Ari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Dodsworth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeff</given_name>
<surname>Mielke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Peperkamp</surname>
</person_name>
					</contributors>
					<titles><title>The Different Roles of Expectations in Phonetic and Lexical Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2305</first_page>
						<last_page>2309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1795</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/levari19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bruno Ferenc</given_name>
<surname>Segedin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Cohn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgia</given_name>
<surname>Zellou</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Adaptation to Device and Human Voices: Learning and Generalization of a Phonetic Shift Across Real and Voice-AI Talkers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2310</first_page>
						<last_page>2314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1433</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/segedin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Papadimitriou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2315</first_page>
						<last_page>2319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2422</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/papadimitriou19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Somandepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naveen</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arindam</given_name>
<surname>Jati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Multiview Shared Subspace Learning Across Speakers and Speech Commands</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2320</first_page>
						<last_page>2324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3130</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/somandepalli19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chelzy</given_name>
<surname>Belitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hussnain</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2325</first_page>
						<last_page>2329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3091</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/belitz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Truc</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Franz</given_name>
<surname>Pernkopf</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2330</first_page>
						<last_page>2334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3002</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nguyen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohsin Y.</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Mahbubur</given_name>
<surname>Rahman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jilong</given_name>
<surname>Kuang</surname>
</person_name>
					</contributors>
					<titles><title>DeepLung: Smartphone Convolutional Neural Network-Based Inference of Lung Anomalies for Pulmonary Patients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2335</first_page>
						<last_page>2339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2953</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ahmed19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roger K.</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucy</given_name>
<surname>Skidmore</surname>
</person_name>
					</contributors>
					<titles><title>On the Use/Misuse of the Term &#8216;Phoneme&#8217;</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2340</first_page>
						<last_page>2344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2711</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/moore19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Muckenhirn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai-Doss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sébastien</given_name>
<surname>Marcel</surname>
</person_name>
					</contributors>
					<titles><title>Understanding and Visualizing Raw Waveform-Based CNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2345</first_page>
						<last_page>2349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2341</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/muckenhirn19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Kilgour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mauricio</given_name>
<surname>Zuluaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Roblek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Sharifi</surname>
</person_name>
					</contributors>
					<titles><title>Fr&#233;chet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2350</first_page>
						<last_page>2354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2219</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kilgour19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>MacKnight</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Poellabauer</surname>
</person_name>
					</contributors>
					<titles><title>ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2355</first_page>
						<last_page>2359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1541</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Balamurali</given_name>
<surname>B.T.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jer-Ming</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Intra-Speaker and Inter-Speaker Vocal Tract Impedance Characteristics in a Low-Dimensional Feature Space Using t-SNE</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2360</first_page>
						<last_page>2363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1492</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bt19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian B.</given_name>
<surname>Pokorny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarek</given_name>
<surname>Krajewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Cychosz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Vollmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonja-Dana</given_name>
<surname>Roelen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Schnieder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elika</given_name>
<surname>Bergelson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Seidl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne S.</given_name>
<surname>Warlaumont</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Yankowitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Hantke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Schmitt</surname>
</person_name>
					</contributors>
					<titles><title>The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds &amp; Orca Activity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2378</first_page>
						<last_page>2382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1122</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schuller19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>S. Pavankumar</given_name>
<surname>Dubagunta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2383</first_page>
						<last_page>2387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2398</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dubagunta19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Elsner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Langer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabian</given_name>
<surname>Ritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Mueller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steffen</given_name>
<surname>Illium</surname>
</person_name>
					</contributors>
					<titles><title>Deep Neural Baselines for Computational Paralinguistics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2388</first_page>
						<last_page>2392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2478</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/elsner19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Kisler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Winkelmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Schiel</surname>
</person_name>
					</contributors>
					<titles><title>Styrian Dialect Classification: Comparing and Fusing Classifiers Based on a Feature Selection Using a Genetic Algorithm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2393</first_page>
						<last_page>2397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2540</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kisler19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sung-Lin</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gao-Yi</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Lin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng-Han</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yin-Chun</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Wen</given_name>
<surname>Tai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Chi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chieh-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsung-Ming</given_name>
<surname>Tai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiu-Wang</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-Kuang</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2398</first_page>
						<last_page>2402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2110</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yeh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>SaiKrishna</given_name>
<surname>Rallabandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Nyberg</surname>
</person_name>
					</contributors>
					<titles><title>Ordinal Triplet Loss: Investigating Sleepiness Detection from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2403</first_page>
						<last_page>2407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2278</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amber</given_name>
<surname>Afshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Voice Quality and Between-Frame Entropy for Sleepiness Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2408</first_page>
						<last_page>2412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2988</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ravi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name>
					</contributors>
					<titles><title>Using Fisher Vector and Bag-of-Audio-Words Representations to Identify Styrian Dialects, Sleepiness, Baby &amp; Orca Sounds</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2413</first_page>
						<last_page>2417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1726</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gosztolya19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Instantaneous Phase and Long-Term Acoustic Cues for Orca Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2418</first_page>
						<last_page>2422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1894</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/das19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Schiller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lingenfelser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Dietz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Seiderer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeth</given_name>
<surname>André</surname>
</person_name>
					</contributors>
					<titles><title>Relevance-Based Feature Masking: Improving Neural Network Based Whale Classification Through Explainable Artificial Intelligence</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2423</first_page>
						<last_page>2427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2707</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schiller19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marie-José</given_name>
<surname>Caraty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claude</given_name>
<surname>Montacié</surname>
</person_name>
					</contributors>
					<titles><title>Spatial, Temporal and Spectral Multiresolution Analysis for the INTERSPEECH 2019 ComParE Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2428</first_page>
						<last_page>2432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1693</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/caraty19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haiwei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The DKU-LENOVO Systems for the INTERSPEECH 2019 Computational Paralinguistic Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2433</first_page>
						<last_page>2437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1386</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wu19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien van</given_name>
<surname>Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colleen</given_name>
<surname>Richey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria A.</given_name>
<surname>Barrios</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Lawson</surname>
</person_name>
					</contributors>
					<titles><title>The VOiCES from a Distance Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2438</first_page>
						<last_page>2442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1837</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nandwana19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Gusev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Ivanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timur</given_name>
<surname>Pekhovsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Shulipa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Galina</given_name>
<surname>Lavrentyeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Volokhov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandr</given_name>
<surname>Kozlov</surname>
</person_name>
					</contributors>
					<titles><title>STC Speaker Recognition Systems for the VOiCES from a Distance Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2443</first_page>
						<last_page>2447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2783</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/novoselov19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Matějka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hossein</given_name>
<surname>Zeinali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Glembek</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2448</first_page>
						<last_page>2452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2471</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/matejka19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Medennikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Khokhlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Romanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Sorokin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Mitrofanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Prisyach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariya</given_name>
<surname>Korenevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Petrov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Zatvornitskiy</surname>
</person_name>
					</contributors>
					<titles><title>The STC ASR System for the VOiCES from a Distance Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2453</first_page>
						<last_page>2457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1574</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/medennikov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tze Yuang</given_name>
<surname>Chong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kye Min</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kah Kuan</given_name>
<surname>Teh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang Huai</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanwu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>The I2R&#8217;s ASR System for the VOiCES from a Distance Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2458</first_page>
						<last_page>2462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2130</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arindam</given_name>
<surname>Jati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghuveer</given_name>
<surname>Peri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monisankha</given_name>
<surname>Pal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naveen</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruchir</given_name>
<surname>Travadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2463</first_page>
						<last_page>2467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3010</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jati19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>The JHU Speaker Recognition System for the VOiCES 2019 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2468</first_page>
						<last_page>2472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2979</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/snyder19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Intel Far-Field Speaker Recognition System for VOiCES Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2473</first_page>
						<last_page>2477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2894</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanwu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kah Kuan</given_name>
<surname>Teh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Kukanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>The I2R&#8217;s Submission to VOiCES Distance Speaker Recognition Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2478</first_page>
						<last_page>2482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1997</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sun19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yulong</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>The LeVoice Far-Field Speech Recognition System for VOiCES from a Distance Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2483</first_page>
						<last_page>2487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1944</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vimal</given_name>
<surname>Manohar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phani Sankar</given_name>
<surname>Nidadavolu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>The JHU ASR System for VOiCES from a Distance Challenge 2019</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2488</first_page>
						<last_page>2492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1948</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danwei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weicheng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2493</first_page>
						<last_page>2497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1435</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cai19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yermiyahu</given_name>
<surname>Hauptman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruth</given_name>
<surname>Aloni-Lavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itshak</given_name>
<surname>Lapidot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanya</given_name>
<surname>Gurevich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yael</given_name>
<surname>Manor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stav</given_name>
<surname>Naor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noa</given_name>
<surname>Diamant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irit</given_name>
<surname>Opher</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Distinctive Acoustic and Spectral Features in Parkinson&#8217;s Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2498</first_page>
						<last_page>2502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2465</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hauptman19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlo</given_name>
<surname>Drioli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Aichinger</surname>
</person_name>
					</contributors>
					<titles><title>Aerodynamics and Lumped-Masses Combined with Delay Lines for Modeling Vertical and Anterior-Posterior Phase Differences in Pathological Vocal Fold Vibration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2503</first_page>
						<last_page>2507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2338</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/drioli19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2508</first_page>
						<last_page>2512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2863</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kadiri19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sunghye</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neville</given_name>
<surname>Ryant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meredith</given_name>
<surname>Cola</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert T.</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Parish-Morris</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Autism Spectrum Disorder in Children Using Acoustic and Text Features from Brief Natural Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2513</first_page>
						<last_page>2517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1452</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cho19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean</given_name>
<surname>Schoentgen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Aichinger</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and Synthesis of Vocal Flutter and Vocal Jitter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2518</first_page>
						<last_page>2522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1998</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schoentgen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felix</given_name>
<surname>Schaeffler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Jannetts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janet</given_name>
<surname>Beck</surname>
</person_name>
					</contributors>
					<titles><title>Reliability of Clinical Voice Parameters Captured with Smartphones &#8212; Measurements of Added Noise and Spectral Tilt</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2523</first_page>
						<last_page>2527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2910</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schaeffler19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meredith</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Saxon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemanth</given_name>
<surname>Venkateswara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sethuraman</given_name>
<surname>Panchanathan</surname>
</person_name>
					</contributors>
					<titles><title>Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2528</first_page>
						<last_page>2532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3096</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/moore19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Simon</given_name>
<surname>Roessig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doris</given_name>
<surname>Mücke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena</given_name>
<surname>Pagel</surname>
</person_name>
					</contributors>
					<titles><title>Dimensions of Prosodic Prominence in an Attractor Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2533</first_page>
						<last_page>2537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2227</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/roessig19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antti</given_name>
<surname>Suni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Włodarczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martti</given_name>
<surname>Vainio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name>
					</contributors>
					<titles><title>Comparative Analysis of Prosodic Characteristics Using WaveNet Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2538</first_page>
						<last_page>2542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2373</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/suni19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andy</given_name>
<surname>Murphy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irena</given_name>
<surname>Yanushevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailbhe Ní</given_name>
<surname>Chasaide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Voice Quality in the Perception of Prominence in Synthetic Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2543</first_page>
						<last_page>2547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2761</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/murphy19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachel</given_name>
<surname>Albar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiyon</given_name>
<surname>Yoo</surname>
</person_name>
					</contributors>
					<titles><title>Phonological Awareness of French Rising Contours in Japanese Learners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2548</first_page>
						<last_page>2552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2856</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/albar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masaki</given_name>
<surname>Okawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Sawada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiromitsu</given_name>
<surname>Nishizaki</surname>
</person_name>
					</contributors>
					<titles><title>Audio Classification of Bit-Representation Waveform</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2553</first_page>
						<last_page>2557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1855</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/okawa19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manjunath</given_name>
<surname>Mulimani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashidhar G.</given_name>
<surname>Koolagudi</surname>
</person_name>
					</contributors>
					<titles><title>Locality-Constrained Linear Coding Based Fused Visual Features for Robust Acoustic Event Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2558</first_page>
						<last_page>2562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1421</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mulimani19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Han</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke-Xin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2563</first_page>
						<last_page>2567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2045</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shen19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Logan</given_name>
<surname>Ford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Grondin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Residual Network for Large-Scale Acoustic Scene Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2568</first_page>
						<last_page>2572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2731</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ford19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandan K.A.</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ross</given_name>
<surname>Cutler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Gehrke</surname>
</person_name>
					</contributors>
					<titles><title>Supervised Classifiers for Audio Impairments with Noisy Labels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2573</first_page>
						<last_page>2577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3074</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/reddy19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Tarantino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Lazaridis</surname>
</person_name>
					</contributors>
					<titles><title>Self-Attention for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2578</first_page>
						<last_page>2582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2822</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tarantino19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eliya</given_name>
<surname>Nachmani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Wolf</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Singing Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2583</first_page>
						<last_page>2587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1761</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nachmani19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juheon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeong-Seok</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang-Bin</given_name>
<surname>Jeon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junghyun</given_name>
<surname>Koo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Adversarially Trained End-to-End Korean Singing Voice Synthesis System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2588</first_page>
						<last_page>2592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1722</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lee19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan-Hao</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2593</first_page>
						<last_page>2597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1563</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sara</given_name>
<surname>Dahmani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Colotte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valérian</given_name>
<surname>Girard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Ouni</surname>
</person_name>
					</contributors>
					<titles><title>Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2598</first_page>
						<last_page>2602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2848</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dahmani19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Ayllón</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fernando</given_name>
<surname>Villavicencio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Lanchantin</surname>
</person_name>
					</contributors>
					<titles><title>A Strategy for Improved Phone-Level Lyrics-to-Audio Alignment for Speech-to-Singing Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2603</first_page>
						<last_page>2607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3049</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ayllon19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Théo</given_name>
<surname>Biasutto--Lervat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Dahmani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Ouni</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Labial Coarticulation with Bidirectional Gated Recurrent Networks and Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2608</first_page>
						<last_page>2612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2097</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/biasuttolervat19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel S.</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barret</given_name>
<surname>Zoph</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekin D.</given_name>
<surname>Cubuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quoc V.</given_name>
<surname>Le</surname>
</person_name>
					</contributors>
					<titles><title>SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2613</first_page>
						<last_page>2617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2680</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/park19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name>
					</contributors>
					<titles><title>Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2618</first_page>
						<last_page>2622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2841</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/audhkhasi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ta</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2623</first_page>
						<last_page>2627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2018</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/miao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ulrich</given_name>
<surname>Finkler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdullah</given_name>
<surname>Kayi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alper</given_name>
<surname>Buyuktosunoglu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Kung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name>
					</contributors>
					<titles><title>A Highly Efficient Distributed Deep Learning System for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2628</first_page>
						<last_page>2632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2700</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2633</first_page>
						<last_page>2637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3192</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Menne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Sklyar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2638</first_page>
						<last_page>2642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1728</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/menne19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John S.</given_name>
<surname>Novak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Bunn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert V.</given_name>
<surname>Kenyon</surname>
</person_name>
					</contributors>
					<titles><title>The Effects of Time Expansion on English as a Second Language Individuals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2643</first_page>
						<last_page>2647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2763</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/novak19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuju</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chilin</given_name>
<surname>Shih</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Capturing L1 Influence on L2 Pronunciation by Simulating Perceptual Space Using Acoustic Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2648</first_page>
						<last_page>2652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3183</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juqiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine T.</given_name>
<surname>Best</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Antoniou</surname>
</person_name>
					</contributors>
					<titles><title>Cognitive Factors in Thai-Na&#239;ve Mandarin Speakers&#8217; Imitation of Thai Lexical Tones</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2653</first_page>
						<last_page>2657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1403</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Annie</given_name>
<surname>Tremblay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Broersma</surname>
</person_name>
					</contributors>
					<titles><title>Foreign-Language Knowledge Enhances Artificial-Language Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2658</first_page>
						<last_page>2662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2446</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tremblay19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdalghani</given_name>
<surname>Abujabal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judith</given_name>
<surname>Gaspers</surname>
</person_name>
					</contributors>
					<titles><title>Neural Named Entity Recognition from Subword Units</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2663</first_page>
						<last_page>2667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1305</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/abujabal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabhchand</given_name>
<surname>Bhati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shekhar</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2668</first_page>
						<last_page>2672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2981</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bhati19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murat</given_name>
<surname>Saraclar</surname>
</person_name>
					</contributors>
					<titles><title>An Empirical Evaluation of DTW Subsampling Methods for Keyword Search</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2673</first_page>
						<last_page>2677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2413</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yusuf19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixiaofan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2678</first_page>
						<last_page>2682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3119</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark A.</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Word Discovery and Retrieval with Phone Sequence and Image Concepts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2683</first_page>
						<last_page>2687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1487</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marcely Zanon</given_name>
<surname>Boito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aline</given_name>
<surname>Villavicencio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-Resource Settings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2688</first_page>
						<last_page>2692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2029</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/boito19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guohong</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2693</first_page>
						<last_page>2697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1488</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xue19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>François</given_name>
<surname>Grondin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Sound Source Localization with SVD-PHAT</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2698</first_page>
						<last_page>2702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2653</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/grondin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2703</first_page>
						<last_page>2707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3158</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masahito</given_name>
<surname>Togami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name>
					</contributors>
					<titles><title>Multichannel Loss Function for Supervised Speech Source Separation by Mask-Based Beamforming</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2708</first_page>
						<last_page>2712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1289</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/masuyama19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guanjun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenju</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shouye</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changliang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Direction-Aware Speaker Beam for Multi-Channel Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2713</first_page>
						<last_page>2717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1474</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2718</first_page>
						<last_page>2722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1513</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ochiai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qifeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladlen</given_name>
<surname>Koltun</surname>
</person_name>
					</contributors>
					<titles><title>Speech Denoising with Deep Feature Losses</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2723</first_page>
						<last_page>2727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1924</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/germain19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Muckenhirn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zelin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rif A.</given_name>
<surname>Saurous</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ignacio Lopez</given_name>
<surname>Moreno</surname>
</person_name>
					</contributors>
					<titles><title>VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2728</first_page>
						<last_page>2732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1101</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chien-Feng</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Symbolic Sequential Modeling for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2733</first_page>
						<last_page>2737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1777</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pejman</given_name>
<surname>Mowlaee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Scheran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Stahl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sean U.N.</given_name>
<surname>Wood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>W. Bastiaan</given_name>
<surname>Kleijn</surname>
</person_name>
					</contributors>
					<titles><title>Maximum a posteriori Speech Enhancement Based on Double Spectrum</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2738</first_page>
						<last_page>2742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1197</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mowlaee19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Al-Dahle</surname>
</person_name>
					</contributors>
					<titles><title>Coarse-to-Fine Optimization for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2743</first_page>
						<last_page>2747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2792</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Like</given_name>
<surname>Hui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikhail</given_name>
<surname>Belkin</surname>
</person_name>
					</contributors>
					<titles><title>Kernel Machines Beat Deep Neural Networks on Mask-Based Single-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2748</first_page>
						<last_page>2752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1344</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hui19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nilay</given_name>
<surname>Shrivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Astitwa</given_name>
<surname>Saxena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaman</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv Ratn</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Stent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debanjan</given_name>
<surname>Mahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Kaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roger</given_name>
<surname>Zimmermann</surname>
</person_name>
					</contributors>
					<titles><title>MobiVSR : Efficient and Light-Weight Neural Network for Visual Speech Recognition on Mobile Devices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2753</first_page>
						<last_page>2757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3273</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shrivastava19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pujitha Appan</given_name>
<surname>Kandala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Thanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilip Kumar</given_name>
<surname>Margam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohith Chandrashekar</given_name>
<surname>Aralikatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanay</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharad</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar M.</given_name>
<surname>Venkatesan</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Adaptation for Lip-Reading Using Visual Identity Vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2758</first_page>
						<last_page>2762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3237</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kandala19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Koumparoulis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>MobiLipNet: Resource-Efficient Deep Learning Based Lipreading</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2763</first_page>
						<last_page>2767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2618</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/koumparoulis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leyuan</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cornelius</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Wermter</surname>
</person_name>
					</contributors>
					<titles><title>LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2768</first_page>
						<last_page>2772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1393</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/qu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tara N.</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanzhang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirkó</given_name>
<surname>Visontai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiao</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Strohman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McGraw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name>
					</contributors>
					<titles><title>Two-Pass End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2773</first_page>
						<last_page>2777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1341</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sainath19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Max W.Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Extract, Adapt and Recognize: An End-to-End Neural Network for Corrupted Monaural Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2778</first_page>
						<last_page>2782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1626</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lam19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dhananjaya</given_name>
<surname>Gowda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwangyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehul</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Multi-Resolution Char-to-BPE Cross-Attention Decoder for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2783</first_page>
						<last_page>2787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3216</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gowda19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Stride Self-Attention for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2788</first_page>
						<last_page>2792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1973</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/han19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Max W.Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>LF-MMI Training of Bayesian and Gaussian Process Time Delay Neural Networks for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2793</first_page>
						<last_page>2797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2379</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Self-Teaching Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2798</first_page>
						<last_page>2802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1467</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lu19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyu</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2803</first_page>
						<last_page>2807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2594</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Continuous Emotion Recognition in Speech &#8212; Do We Need Recurrence?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2808</first_page>
						<last_page>2812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2710</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schmitt19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anda</given_name>
<surname>Ouyang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Speech Based Emotion Prediction: Can a Linear Model Work?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2813</first_page>
						<last_page>2817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3149</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ouyang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosana</given_name>
<surname>Kamiyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Kobashikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition Based on Multi-Label Emotion Existence Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2818</first_page>
						<last_page>2822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2524</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ando19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cristina</given_name>
<surname>Gorrostieta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reza</given_name>
<surname>Lotfian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kye</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Kane</surname>
</person_name>
					</contributors>
					<titles><title>Gender De-Biasing in Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2823</first_page>
						<last_page>2827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1708</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gorrostieta19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fang</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2828</first_page>
						<last_page>2832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2293</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bajibabu</given_name>
<surname>Bollepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Lombard Speech Synthesis Using Transfer Learning in a Tacotron Text-to-Speech System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2833</first_page>
						<last_page>2837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1333</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bollepalli19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreyas</given_name>
<surname>Seshadri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name>
					</contributors>
					<titles><title>Augmented CycleGANs for Continuous Scale Normal-to-Lombard Speaking Style Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2838</first_page>
						<last_page>2842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1681</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/seshadri19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2843</first_page>
						<last_page>2847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1778</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Sager</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archana</given_name>
<surname>Venkataraman</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2848</first_page>
						<last_page>2852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2512</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shankar19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Itshak</given_name>
<surname>Lapidot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Waveform PMF on Anti-Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2853</first_page>
						<last_page>2857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2607</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lapidot19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deep</given_name>
<surname>Chakraborty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamidou</given_name>
<surname>Tembine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olaitan</given_name>
<surname>Olaleye</surname>
</person_name>
					</contributors>
					<titles><title>Nonparallel Emotional Speech Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2858</first_page>
						<last_page>2862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2878</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gao19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Mizera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2863</first_page>
						<last_page>2867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2842</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/stafylakis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Nautsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amos</given_name>
<surname>Treiber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Mizera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schneider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Speaker Recognition with Cohort Score Normalisation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2868</first_page>
						<last_page>2872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2638</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nautsch19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Large Margin Softmax Loss for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2873</first_page>
						<last_page>2877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2357</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amirhossein</given_name>
<surname>Hajavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali</given_name>
<surname>Etemad</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Neural Network for Short-Segment Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2878</first_page>
						<last_page>2882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2240</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hajavi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianfeng</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Deep Speaker Embedding Extraction with Channel-Wise Feature Responses and Additive Supervision Softmax Loss Function</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2883</first_page>
						<last_page>2887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1704</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhou19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>VoiceID Loss: Speech Enhancement for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2888</first_page>
						<last_page>2892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1496</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shon19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anderson R.</given_name>
<surname>Avila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>O’Shaughnessy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiago H.</given_name>
<surname>Falk</surname>
</person_name>
					</contributors>
					<titles><title>Blind Channel Response Estimation for Replay Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2893</first_page>
						<last_page>2897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2956</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/avila19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankur T.</given_name>
<surname>Patil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajul</given_name>
<surname>Acharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pulikonda Aditya</given_name>
<surname>Sai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant A.</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Feature for Replay Spoof Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2898</first_page>
						<last_page>2902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2742</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/patil19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victoria</given_name>
<surname>Mingote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dayana</given_name>
<surname>Ribas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Optimization of False Acceptance/Rejection Rates and Decision Threshold for End-to-End Text-Dependent Speaker Verification Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2903</first_page>
						<last_page>2907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2550</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mingote19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lei</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing-Yuan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Qi</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu-Jun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Deep Hashing for Speaker Identification and Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2908</first_page>
						<last_page>2912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2457</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mirko</given_name>
<surname>Marras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paweł</given_name>
<surname>Korus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nasir</given_name>
<surname>Memon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gianni</given_name>
<surname>Fenu</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Optimization for Dictionary Attacks on Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2913</first_page>
						<last_page>2917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2430</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/marras19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tharshini</given_name>
<surname>Gunendradasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>An Adaptive-Q Cochlear Model for Replay Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2918</first_page>
						<last_page>2922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2361</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gunendradasan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sungrack</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janghoon</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungyun</given_name>
<surname>Eum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonil</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyuwoong</given_name>
<surname>Hwang</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Text-Independent Speaker Verification Framework with a Keyword Adversarial Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2923</first_page>
						<last_page>2927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2208</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yun19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soonshin</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel Jun</given_name>
<surname>Rim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minkyu</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosung</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junseok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changmin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Shortcut Connections Based Deep Speaker Embeddings for End-to-End Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2928</first_page>
						<last_page>2932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2195</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/seo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang Huai</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jichen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>Device Feature Extractor for Replay Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2933</first_page>
						<last_page>2937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2137</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/you19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongji</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2938</first_page>
						<last_page>2942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2120</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>A.</given_name>
<surname>Kanagasundaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Sridharan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>G.</given_name>
<surname>Sriram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Prachi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C.</given_name>
<surname>Fookes</surname>
</person_name>
					</contributors>
					<titles><title>A Study of x-Vector Based Speaker Recognition on Short Utterances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2943</first_page>
						<last_page>2947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1891</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kanagasundaram19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Tied Mixture of Factor Analyzers Layer to Combine Frame Level Representations in Neural Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2948</first_page>
						<last_page>2952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1782</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Buddhi</given_name>
<surname>Wickramasinghe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Biologically Inspired Adaptive-Q Filterbanks for Replay Spoofing Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2953</first_page>
						<last_page>2957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1535</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wickramasinghe19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre-Michel</given_name>
<surname>Bousquet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mickael</given_name>
<surname>Rouvier</surname>
</person_name>
					</contributors>
					<titles><title>On Robustness of Unsupervised Domain Adaptation for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2958</first_page>
						<last_page>2962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1524</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bousquet19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suwon</given_name>
<surname>Shon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Younggun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taesu</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Speaker Retrieval on Random Speaker Variability Subspace</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2963</first_page>
						<last_page>2967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1498</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shon19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Dimitriadis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Hinthorn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuedong</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Meeting Transcription Using Asynchronous Distant Microphones</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2968</first_page>
						<last_page>2972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3088</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yoshioka19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinghui</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Picheny</surname>
</person_name>
					</contributors>
					<titles><title>Detection and Recovery of OOVs for Improved English Broadcast News Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2973</first_page>
						<last_page>2977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2793</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/thomas19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Umar</given_name>
<surname>Farooq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Farah</given_name>
<surname>Adeeba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Rauf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarmad</given_name>
<surname>Hussain</surname>
</person_name>
					</contributors>
					<titles><title>Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2978</first_page>
						<last_page>2982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2629</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/farooq19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid Arbitration Using Raw ASR String and NLU Information &#8212; Taking the Best of Both Embedded World and Cloud World</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2983</first_page>
						<last_page>2987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2586</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tang19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>György</given_name>
<surname>Szaszák</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Máté Ákos</given_name>
<surname>Tündik</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging a Character, Word and Prosody Triplet for an ASR Error Robust and Agglutination Friendly Punctuation Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2988</first_page>
						<last_page>2992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2132</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/szaszak19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Pellegrini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jérôme</given_name>
<surname>Farinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Estelle</given_name>
<surname>Delpech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Lancelot</surname>
</person_name>
					</contributors>
					<titles><title>The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2993</first_page>
						<last_page>2997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1962</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pellegrini19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneață</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title> Kite: Automatic Speech Recognition for Unmanned Aerial Vehicles</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>2998</first_page>
						<last_page>3002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1390</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/oneata19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruizhi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samik</given_name>
<surname>Sadhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Methods for the Automatic Detection of Errors in Manual Transcription</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3003</first_page>
						<last_page>3007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1343</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Astik</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghav</given_name>
<surname>Menon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewald van der</given_name>
<surname>Westhuizen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Improved Low-Resource Somali Speech Recognition by Semi-Supervised Acoustic and Language Model Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3008</first_page>
						<last_page>3012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1328</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/biswas19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Inga R.</given_name>
<surname>Helgadóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna Björk</given_name>
<surname>Nikulásdóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Borský</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judy Y.</given_name>
<surname>Fong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Róbert</given_name>
<surname>Kjaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name>
					</contributors>
					<titles><title>The Althingi ASR System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3013</first_page>
						<last_page>3017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1248</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/helgadottir19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwa</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lise</given_name>
<surname>Rebout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilles</given_name>
<surname>Boulianne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-André</given_name>
<surname>Ménard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name>
					</contributors>
					<titles><title>CRIM&#8217;s Speech Transcription and Call Sign Detection System for the ATC Airbus Challenge Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3018</first_page>
						<last_page>3022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1131</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gupta19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomasz</given_name>
<surname>Rutowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Harati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Shriberg</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing Speech-Input Length for Speaker-Independent Depression Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3023</first_page>
						<last_page>3027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3095</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rutowski19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mary</given_name>
<surname>Pietrowicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carla</given_name>
<surname>Agurto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raquel</given_name>
<surname>Norel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elif</given_name>
<surname>Eyigoz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillermo</given_name>
<surname>Cecchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zarina R.</given_name>
<surname>Bilgrami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheryl</given_name>
<surname>Corcoran</surname>
</person_name>
					</contributors>
					<titles><title>A New Approach for Automating Analysis of Responses on Verbal Fluency Tests from Subjects At-Risk for Schizophrenia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3028</first_page>
						<last_page>3032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2987</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pietrowicz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laetitia</given_name>
<surname>Jeancolas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graziella</given_name>
<surname>Mangone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Christophe</given_name>
<surname>Corvol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Vidailhet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stéphane</given_name>
<surname>Lehéricy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Badr-Eddine</given_name>
<surname>Benkelfat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Habib</given_name>
<surname>Benali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dijana</given_name>
<surname>Petrovska-Delacrétaz</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Telephone Recordings and Professional Microphone Recordings for Early Detection of Parkinson&#8217;s Disease, Using Mel-Frequency Cepstral Coefficients with Gaussian Mixture Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3033</first_page>
						<last_page>3037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2825</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jeancolas19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Parvaneh</given_name>
<surname>Janbakhshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ina</given_name>
<surname>Kodrasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Spectral Subspace Analysis for Automatic Assessment of Pathological Speech Intelligibility</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3038</first_page>
						<last_page>3042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2791</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/janbakhshi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carolina De</given_name>
<surname>Pasquale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charlie</given_name>
<surname>Cullen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Vaughan</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Therapeutic Rapport Through Prosody in Brief Psychodynamic Psychotherapy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3043</first_page>
						<last_page>3047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2551</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pasquale19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Rueda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.C.</given_name>
<surname>Vásquez-Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian David</given_name>
<surname>Rios-Urrego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sridhar</given_name>
<surname>Krishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Feature Representation of Pathophysiology of Parkinsonian Dysarthria</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3048</first_page>
						<last_page>3052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2490</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rueda19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Charles C.</given_name>
<surname>Onu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Lebensold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William L.</given_name>
<surname>Hamilton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doina</given_name>
<surname>Precup</surname>
</person_name>
					</contributors>
					<titles><title>Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3053</first_page>
						<last_page>3057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2340</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/onu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui-Ting</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeng-Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ming</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chip-Jin</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Variability of Voice Quality and Pain Levels as a Function of Multiple Clinical Parameters</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3058</first_page>
						<last_page>3062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2247</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>José Vicente Egas</given_name>
<surname>López</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Parkinson&#8217;s Disease from Speech Using Fisher Vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3063</first_page>
						<last_page>3067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2217</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lopez19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Klumpp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.C.</given_name>
<surname>Vásquez-Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tino</given_name>
<surname>Haderlein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Feature Space Visualization with Spatial Similarity Maps for Pathological Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3068</first_page>
						<last_page>3072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2080</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/klumpp19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandeep Nallan</given_name>
<surname>Chakravarthula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shao-Yen</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maija</given_name>
<surname>Reblin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3073</first_page>
						<last_page>3077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1888</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chakravarthula19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony Pak Hin</given_name>
<surname>Kong</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Language Impairment Based on Raw ASR Output</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3078</first_page>
						<last_page>3082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1688</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/qin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Spectral and Temporal Cues to Mandarin Concurrent-Vowels Identification for Normal-Hearing and Hearing-Impaired Listeners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3083</first_page>
						<last_page>3087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3209</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vicky</given_name>
<surname>Zayats</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trang</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Wright</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Courtney</given_name>
<surname>Mansfield</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name>
					</contributors>
					<titles><title>Disfluencies and Human Speech Transcription Errors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3088</first_page>
						<last_page>3092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3134</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zayats19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandra I.</given_name>
<surname>Parhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miriam</given_name>
<surname>Ebersberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jenny</given_name>
<surname>Tippmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katja</given_name>
<surname>Stärk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Opitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Hinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonja</given_name>
<surname>Rossi</surname>
</person_name>
					</contributors>
					<titles><title>The Influence of Distraction on Speech Processing: How Selective is Selective Attention?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3093</first_page>
						<last_page>3097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2699</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/parhammer19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valerie</given_name>
<surname>Hazan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Outi</given_name>
<surname>Tuomainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linda</given_name>
<surname>Taschenberger</surname>
</person_name>
					</contributors>
					<titles><title>Subjective Evaluation of Communicative Effort for Younger and Older Adults in Interactive Tasks with Energetic and Informational Masking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3098</first_page>
						<last_page>3102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2215</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hazan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chris</given_name>
<surname>Davis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeesun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Perceiving Older Adults Producing Clear and Lombard Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3103</first_page>
						<last_page>3107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2210</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/davis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>T.</given_name>
<surname>Arias-Vergara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Gollwitzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>M.</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Phone-Attribute Posteriors to Evaluate the Speech of Cochlear Implant Users</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3108</first_page>
						<last_page>3112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2144</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ariasvergara19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nao</given_name>
<surname>Hodoshima</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Urgent Speech and Congruent/Incongruent Text on Speech Intelligibility in Noise and Reverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3113</first_page>
						<last_page>3117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1902</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hodoshima19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nursadul</given_name>
<surname>Mamun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ria</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying Cochlear Implant Users&#8217; Ability for Speaker Identification Using CI Auditory Stimuli</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3118</first_page>
						<last_page>3122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1852</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mamun19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>E.</given_name>
<surname>Felker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Broersma</surname>
</person_name>
					</contributors>
					<titles><title>Lexically Guided Perceptual Learning of a Vowel Shift in an Interactive L2 Listening Context</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3123</first_page>
						<last_page>3127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1414</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/felker19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maximillian</given_name>
<surname>Paulus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valerie</given_name>
<surname>Hazan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patti</given_name>
<surname>Adank</surname>
</person_name>
					</contributors>
					<titles><title>Talker Intelligibility and Listening Effort with Temporally Modified Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3128</first_page>
						<last_page>3132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1402</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/paulus19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Ward</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Robinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Paradis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine M.</given_name>
<surname>Tucker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ben</given_name>
<surname>Shirley</surname>
</person_name>
					</contributors>
					<titles><title>R2SPIN: Re-Recording the Revised Speech Perception in Noise Test</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3133</first_page>
						<last_page>3137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1281</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ward19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Contributions of Consonant-Vowel Transitions to Mandarin Tone Identification in Simulated Electric-Acoustic Hearing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3138</first_page>
						<last_page>3142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1124</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shadi</given_name>
<surname>Pirhosseinloo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan S.</given_name>
<surname>Brumberg</surname>
</person_name>
					</contributors>
					<titles><title>Monaural Speech Enhancement with Dilated Convolutions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3143</first_page>
						<last_page>3147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2782</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pirhosseinloo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chien-Feng</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Noise Adaptive Speech Enhancement Using Domain Adversarial Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3148</first_page>
						<last_page>3152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1519</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liao19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3153</first_page>
						<last_page>3157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1477</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ge19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Pariente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Deleforge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3158</first_page>
						<last_page>3162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1398</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pariente19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sufeng</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zice</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Lan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adriaan J. van</given_name>
<surname>Wijngaarden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melissa C.</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuang-Ching</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement Using Forked Generative Adversarial Networks with Spectral Subtraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3163</first_page>
						<last_page>3167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2954</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lin19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryandhimas E.</given_name>
<surname>Zezario</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Specialized Speech Enhancement Model Selection Based on Learned Non-Intrusive Quality Assessment Metric</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3168</first_page>
						<last_page>3172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2425</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zezario19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fu-Kai</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syu-Siang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeih-weih</given_name>
<surname>Hung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Hau</given_name>
<surname>Fang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Aware Deep Denoising Autoencoder with Embedded Speaker Identity for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3173</first_page>
						<last_page>3177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2108</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chuang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhang</given_name>
<surname>Cao</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Cost Function for Supervised Monaural Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3178</first_page>
						<last_page>3182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1897</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqiang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huibin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anyan</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3183</first_page>
						<last_page>3187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1373</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shi19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xianyun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changchun</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Masking Estimation with Phase Restoration of Clean Speech for Monaural Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3188</first_page>
						<last_page>3192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1141</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Llombart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dayana</given_name>
<surname>Ribas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis</given_name>
<surname>Vicente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Progressive Speech Enhancement with Residual Connections</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3193</first_page>
						<last_page>3197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1748</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/llombart19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Langzhou</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Leutnant</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Model Bootstrapping Using Semi-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3198</first_page>
						<last_page>3202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2818</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Mantena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ossama</given_name>
<surname>Abdel-Hamid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Don</given_name>
<surname>McAllaster</surname>
</person_name>
					</contributors>
					<titles><title>Bandwidth Embeddings for Mixed-Bandwidth Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3203</first_page>
						<last_page>3207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2589</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mantena19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreya</given_name>
<surname>Khare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Aralikatte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Senthil</given_name>
<surname>Mani</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3208</first_page>
						<last_page>3212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2420</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/khare19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bilal</given_name>
<surname>Soomro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anssi</given_name>
<surname>Kanervisto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Ngo</given_name>
<surname>Trong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ville</given_name>
<surname>Hautamäki</surname>
</person_name>
					</contributors>
					<titles><title>Towards Debugging Deep Neural Networks by Generating Speech Utterances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3213</first_page>
						<last_page>3217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2339</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/soomro19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haisong</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Huo</surname>
</person_name>
					</contributors>
					<titles><title>Compression of CTC-Trained Acoustic Models by Dynamic Frame-Wise Distillation or Segment-Wise N-Best Hypotheses Imitation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3218</first_page>
						<last_page>3222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2182</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ding19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iván</given_name>
<surname>López-Espejo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name>
					</contributors>
					<titles><title>Keyword Spotting for Hearing Assistive Devices Robust to External Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3223</first_page>
						<last_page>3227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2010</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lopezespejo19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mortaza</given_name>
<surname>Doulaty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3228</first_page>
						<last_page>3232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1797</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/doulaty19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3233</first_page>
						<last_page>3237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1692</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Motoyuki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sho</given_name>
<surname>Tomita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Morita</surname>
</person_name>
					</contributors>
					<titles><title>Lyrics Recognition from Singing Voice Focused on Correspondence Between Voice and Notes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3238</first_page>
						<last_page>3241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1318</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/suzuki19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning from Audio-Visual Grounding to Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3242</first_page>
						<last_page>3246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1227</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hsu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Corpus Speech Emotion Recognition Using Semi-Supervised Transfer Non-Negative Matrix Factorization with Adaptation Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3247</first_page>
						<last_page>3251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2041</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/luo19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aniruddha</given_name>
<surname>Tammewar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandra</given_name>
<surname>Cervone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva-Maria</given_name>
<surname>Messner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Riccardi</surname>
</person_name>
					</contributors>
					<titles><title>Modeling User Context for Valence Prediction from Narratives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3252</first_page>
						<last_page>3256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2489</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tammewar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rupayan</given_name>
<surname>Chakraborty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Panda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meghna</given_name>
<surname>Pandharipande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonal</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>Front-End Feature Compensation and Denoising for Noise Robust Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3257</first_page>
						<last_page>3261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2243</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chakraborty19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Akagi</surname>
</person_name>
					</contributors>
					<titles><title>The Contribution of Acoustic Features Analysis to Model Emotion Perceptual Process for Language Diversity</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3262</first_page>
						<last_page>3266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2229</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haritha</given_name>
<surname>U.G.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sujitha</given_name>
<surname>A.C.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rejisha T.</given_name>
<surname>M.</surname>
</person_name>
					</contributors>
					<titles><title>Design and Development of a Multi-Lingual Speech Corpora (TaMaR-EmoDB) for Emotion Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3267</first_page>
						<last_page>3271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2034</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rajan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kusha</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition with a Reject Option</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3272</first_page>
						<last_page>3276</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1842</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sridhar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenghao</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Houwei</given_name>
<surname>Cao</surname>
</person_name>
					</contributors>
					<titles><title>Development of Emotion Rankers Based on Intended and Perceived Emotion Labels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3277</first_page>
						<last_page>3281</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1831</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Gideon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heather T.</given_name>
<surname>Schatten</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin G.</given_name>
<surname>McInnis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily Mower</given_name>
<surname>Provost</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Recognition from Natural Phone Conversations in Individuals with and without Recent Suicidal Ideation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3282</first_page>
						<last_page>3286</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1830</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gideon19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deniece S.</given_name>
<surname>Nazareth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ellen</given_name>
<surname>Tournier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Leimkötter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Janse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Heylen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerben J.</given_name>
<surname>Westerhof</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khiet P.</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>An Acoustic and Lexical Analysis of Emotional Valence in Spontaneous Speech: Autobiographical Memory Recall in Older Adults</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3287</first_page>
						<last_page>3291</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1823</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nazareth19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Takaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Kobashikawa</surname>
</person_name>
					</contributors>
					<titles><title>Does the Lombard Effect Improve Emotional Communication in Noise? &#8212; Analysis of Emotional Speech Acted in Noise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3292</first_page>
						<last_page>3296</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1605</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhao19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soumaya</given_name>
<surname>Gharsellaoui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sid Ahmed</given_name>
<surname>Selouani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammed Sidi</given_name>
<surname>Yakoub</surname>
</person_name>
					</contributors>
					<titles><title>Linear Discriminant Differential Evolution for Feature Selection in Emotional Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3297</first_page>
						<last_page>3301</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1218</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gharsellaoui19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Sahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3302</first_page>
						<last_page>3306</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1149</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sahu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laura</given_name>
<surname>Spinu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maida</given_name>
<surname>Percival</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Kochetov</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3307</first_page>
						<last_page>3311</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3039</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/spinu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louise</given_name>
<surname>Ratko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proctor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name>
					</contributors>
					<titles><title>Articulation of Vowel Length Contrasts in Australian English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3312</first_page>
						<last_page>3316</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2995</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ratko19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Deme</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Márton</given_name>
<surname>Bartók</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tekla Etelka</given_name>
<surname>Gráczi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Markó</surname>
</person_name>
					</contributors>
					<titles><title>V-to-V Coarticulation Induced Acoustic and Articulatory Variability of Vowels: The Effect of Pitch-Accent</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3317</first_page>
						<last_page>3321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2890</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/deme19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>King</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Ferragne</surname>
</person_name>
					</contributors>
					<titles><title>The Contribution of Lip Protrusion to Anglo-English /r/: Evidence from Hyper- and Non-Hyperarticulated Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3322</first_page>
						<last_page>3326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2851</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/king19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Markó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Márton</given_name>
<surname>Bartók</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tekla Etelka</given_name>
<surname>Gráczi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Deme</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Analysis of Transparent Vowel /i&#720;/ in Harmonic and Antiharmonic Hungarian Stems: Is There a Difference?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3327</first_page>
						<last_page>3331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2352</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/marko19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Conceição</given_name>
<surname>Cunha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>António</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula</given_name>
<surname>Martins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun A.</given_name>
<surname>Joseph</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Frahm</surname>
</person_name>
					</contributors>
					<titles><title>On the Role of Oral Configurations in European Portuguese Nasal Vowels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3332</first_page>
						<last_page>3336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2232</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cunha19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitali</given_name>
<surname>Chakrabarti</surname>
</person_name>
					</contributors>
					<titles><title>Residual + Capsule Networks (ResCap) for Simultaneous Single-Channel Overlapped Keyword Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3337</first_page>
						<last_page>3341</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2913</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xiong19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Che-Wei</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri Harish</given_name>
<surname>Mallidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Hoffmeister</surname>
</person_name>
					</contributors>
					<titles><title>A Study for Improving Device-Directed Speech Detection Toward Frictionless Human-Machine Interaction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3342</first_page>
						<last_page>3346</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2840</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Borislav</given_name>
<surname>Dzodzo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Methods for Audio Classification from Lecture Discussion Recordings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3347</first_page>
						<last_page>3351</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2384</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/su19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoaki</given_name>
<surname>Matsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Fukutomi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshikazu</given_name>
<surname>Yamaguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Neural Whispered Speech Detection with Imbalanced Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3352</first_page>
						<last_page>3356</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2161</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ashihara19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachael Xi</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Maier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Barth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning for Orca Call Type Identification &#8212; A Fully Unsupervised Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3357</first_page>
						<last_page>3361</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1857</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bergler19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niccolò</given_name>
<surname>Sacchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Nanchen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Jaggi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name>
					</contributors>
					<titles><title>Open-Vocabulary Keyword Spotting with Audio and Text Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3362</first_page>
						<last_page>3366</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1846</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sacchi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shutao</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaping</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>ToneNet: A CNN Model of Tone Classification of Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3367</first_page>
						<last_page>3371</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1483</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gao19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seungwoo</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seokjun</given_name>
<surname>Seo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beomjun</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeongmin</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Kersner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beomsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongyoung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungjoo</given_name>
<surname>Ha</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3372</first_page>
						<last_page>3376</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1363</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/choi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiying</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name>
					</contributors>
					<titles><title>Audio Tagging with Compact Feedforward Sequential Memory Network and Audio-to-Audio Ratio Based Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3377</first_page>
						<last_page>3381</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1302</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/huang19j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hansi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Music Genre Classification Using Duplicated Convolutional Layers in Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3382</first_page>
						<last_page>3386</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1298</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nehory</given_name>
<surname>Carmi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Azaria</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireille</given_name>
<surname>Avigal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anat</given_name>
<surname>Lerner</surname>
</person_name>
					</contributors>
					<titles><title>A Storyteller&#8217;s Tale: Literature Audiobooks Genre Classification Using CNN and RNN Architectures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3387</first_page>
						<last_page>3390</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1154</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/carmi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min-Jae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3391</first_page>
						<last_page>3395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3249</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hwang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai</given_name>
<surname>Zhen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jongmo</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mi Suk</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungkwon</given_name>
<surname>Beack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minje</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3396</first_page>
						<last_page>3400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1816</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Optimization of Source Models for Speech and Audio Coding Using a Machine Learning Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3401</first_page>
						<last_page>3405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1284</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/backstrom19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Valin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Skoglund</surname>
</person_name>
					</contributors>
					<titles><title>A Real-Time Wideband Neural Vocoder at 1.6kb/s Using LPCNet</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3406</first_page>
						<last_page>3410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1255</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/valin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Fuchs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chamran</given_name>
<surname>Ashour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>Super-Wideband Spectral Envelope Modeling for Speech Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3411</first_page>
						<last_page>3415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1620</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fuchs19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkata</given_name>
<surname>Chebiyyam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Speech Audio Super-Resolution for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3416</first_page>
						<last_page>3420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3043</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deepika</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanumant Singh</given_name>
<surname>Shekhawat</surname>
</person_name>
					</contributors>
					<titles><title>Artificial Bandwidth Extension Using H&#x221E; Optimization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3421</first_page>
						<last_page>3425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1580</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gupta19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Mittag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Quality Degradation Diagnosis for Voice Networks &#8212; Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3426</first_page>
						<last_page>3430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2636</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mittag19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Cross-Entropy-Guided (CEG) Measure for Speech Enhancement Front-End Assessing Performances of Back-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3431</first_page>
						<last_page>3435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2511</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chai19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Mittag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thilo</given_name>
<surname>Michael</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Barriac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Aoki</surname>
</person_name>
					</contributors>
					<titles><title>Extending the E-Model Towards Super-Wideband and Fullband Speech Communication Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3436</first_page>
						<last_page>3440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1340</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/moller19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samik</given_name>
<surname>Sadhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3441</first_page>
						<last_page>3445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2723</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sadhu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Prosody Usage Optimization for Children Speech Recognition with Zero Resource Children Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3446</first_page>
						<last_page>3450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2659</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Purvi</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Raw Waveform Representation Learning for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3451</first_page>
						<last_page>3455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2652</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/agrawal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David B.</given_name>
<surname>Ramsay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Kilgour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Roblek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Sharifi</surname>
</person_name>
					</contributors>
					<titles><title>Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3456</first_page>
						<last_page>3459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2193</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ramsay19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Riviello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Pierre</given_name>
<surname>David</surname>
</person_name>
					</contributors>
					<titles><title>Binary Speech Features for Keyword Spotting Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3460</first_page>
						<last_page>3464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1877</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/riviello19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steffen</given_name>
<surname>Schneider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Baevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name>
					</contributors>
					<titles><title>wav2vec: Unsupervised Pre-Training for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3465</first_page>
						<last_page>3469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1873</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/schneider19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sunghye</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong-cheol</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Prosodic Focus in American English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3470</first_page>
						<last_page>3474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1668</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cho19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raghav</given_name>
<surname>Menon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewald van der</given_name>
<surname>Westhuizen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Quinn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Feature Exploration for Almost Zero-Resource ASR-Free Keyword Spotting Using a Multilingual Bottleneck Extractor and Correspondence Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3475</first_page>
						<last_page>3479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1665</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/menon19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3480</first_page>
						<last_page>3484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1257</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/loweimi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lyan</given_name>
<surname>Verwimp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jerome R.</given_name>
<surname>Bellegarda</surname>
</person_name>
					</contributors>
					<titles><title>Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3485</first_page>
						<last_page>3489</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1332</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/verwimp19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahaveer</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael L.</given_name>
<surname>Seltzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Fuegen</surname>
</person_name>
					</contributors>
					<titles><title>Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3490</first_page>
						<last_page>3494</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1434</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Character-Aware Sub-Word Level Language Modeling for Uyghur and Turkish ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3495</first_page>
						<last_page>3499</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1484</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ernest</given_name>
<surname>Pusateri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe Van</given_name>
<surname>Gysel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rami</given_name>
<surname>Botros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Badaskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirko</given_name>
<surname>Hannemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youssef</given_name>
<surname>Oualil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Oparin</surname>
</person_name>
					</contributors>
					<titles><title>Connecting and Comparing Language Model Interpolation Techniques</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3500</first_page>
						<last_page>3504</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1822</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pusateri19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiping</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van Tung</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3505</first_page>
						<last_page>3509</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1858</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/khassanov19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Max W.Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuewen</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Comparative Study of Parametric and Representation Uncertainty Modeling for Recurrent Neural Network Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3510</first_page>
						<last_page>3514</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1927</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wiehan</given_name>
<surname>Agenbag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Improving Automatically Induced Lexicons for Highly Agglutinating Languages Using Data-Driven Morphological Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3515</first_page>
						<last_page>3519</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2164</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/agenbag19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Coucheiro-Limeres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fernando</given_name>
<surname>Fernández-Martínez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rubén</given_name>
<surname>San-Segundo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Ferreiros-López</surname>
</person_name>
					</contributors>
					<titles><title>Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3520</first_page>
						<last_page>3524</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2347</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/coucheirolimeres19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leijing</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Code-Switching Sentence Generation by Bert and Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3525</first_page>
						<last_page>3529</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2501</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gao19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandy</given_name>
<surname>Ritchie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Sproat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyle</given_name>
<surname>Gorman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daan van</given_name>
<surname>Esch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Schallhart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikos</given_name>
<surname>Bampounis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoît</given_name>
<surname>Brard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas Fromseier</given_name>
<surname>Mortensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Millie</given_name>
<surname>Holt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eoin</given_name>
<surname>Mahon</surname>
</person_name>
					</contributors>
					<titles><title>Unified Verbalization for Speech Recognition &amp; Synthesis Across Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3530</first_page>
						<last_page>3534</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2807</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ritchie19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dravyansh</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melissa</given_name>
<surname>Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Bruguier</surname>
</person_name>
					</contributors>
					<titles><title>Better Morphology Prediction for Better Speech Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3535</first_page>
						<last_page>3539</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3207</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sharma19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anke</given_name>
<surname>Sennema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name>
					</contributors>
					<titles><title>Vietnamese Learners Tackling the German /&#643;t/ in Perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3540</first_page>
						<last_page>3543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2832</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sennema19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Scott</given_name>
<surname>Lewis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adib</given_name>
<surname>Mehrabi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther de</given_name>
<surname>Leeuw</surname>
</person_name>
					</contributors>
					<titles><title>An Articulatory-Acoustic Investigation into GOOSE-Fronting in German-English Bilinguals Residing in London, UK</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3544</first_page>
						<last_page>3548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2637</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lewis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sabrina</given_name>
<surname>Jenne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Articulation-Based Pronunciation Error Detection with Spectrogram and Acoustic Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3549</first_page>
						<last_page>3553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1677</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jenne19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anouschka</given_name>
<surname>Foltz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamsin M.</given_name>
<surname>McKelvey</surname>
</person_name>
					</contributors>
					<titles><title>Using Prosody to Discover Word Order Alternations in a Novel Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3554</first_page>
						<last_page>3558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1183</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/foltz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ann R.</given_name>
<surname>Bradlow</surname>
</person_name>
					</contributors>
					<titles><title>Speaking Rate, Information Density, and Information Rate in First-Language and Second-Language Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3559</first_page>
						<last_page>3563</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1150</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bradlow19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Calbert</given_name>
<surname>Graham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francis</given_name>
<surname>Nolan</surname>
</person_name>
					</contributors>
					<titles><title>Articulation Rate as a Metric in Spoken Language Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3564</first_page>
						<last_page>3568</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2098</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/graham19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haiyang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiping</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Learning Alignment for Multimodal Emotion Recognition from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3569</first_page>
						<last_page>3573</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3247</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xu19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Peperkamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Julia</given_name>
<surname>Carbajal</surname>
</person_name>
					</contributors>
					<titles><title>Liquid Deletion in French Child-Directed Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3574</first_page>
						<last_page>3578</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2838</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/peperkamp19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Seidl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne S.</given_name>
<surname>Warlaumont</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>Towards Detection of Canonical Babbling by Citizen Scientists: Performance as a Function of Clip Length</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3579</first_page>
						<last_page>3583</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1773</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/seidl19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Ludusan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Annett</given_name>
<surname>Jorschick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reiko</given_name>
<surname>Mazuka</surname>
</person_name>
					</contributors>
					<titles><title>Nasal Consonant Discrimination in Infant- and Adult-Directed Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3584</first_page>
						<last_page>3588</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1737</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ludusan19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ellen</given_name>
<surname>Marklund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Sjons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Gustavsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elísabet Eir</given_name>
<surname>Cortes</surname>
</person_name>
					</contributors>
					<titles><title>No Distributional Learning in Adults from Attended Listening to Non-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3589</first_page>
						<last_page>3593</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1674</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/marklund19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khazar</given_name>
<surname>Khorrami</surname>
</person_name>
					</contributors>
					<titles><title>A Computational Model of Early Language Acquisition from Audiovisual Experiences of Young Infants</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3594</first_page>
						<last_page>3598</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1523</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rasanen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The Production of Chinese Affricates /ts/ and /tsh/ by Native Urdu Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3599</first_page>
						<last_page>3603</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1638</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/du19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkata</given_name>
<surname>Chebiyyam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Kirchhoff</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Stream Network with Temporal Attention for Environmental Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3604</first_page>
						<last_page>3608</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3019</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gianmarco</given_name>
<surname>Cerutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabetta</given_name>
<surname>Farella</surname>
</person_name>
					</contributors>
					<titles><title>Neural Network Distillation on IoT Platforms for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3609</first_page>
						<last_page>3613</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2394</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cerutti19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Class-Wise Centroid Distance Metric Learning for Acoustic Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3614</first_page>
						<last_page>3618</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2271</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lu19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xue</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zi-Rui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>A Hybrid Approach to Acoustic Scene Classification Based on Universal Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3619</first_page>
						<last_page>3623</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2171</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bai19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke-Xin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Han</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3624</first_page>
						<last_page>3628</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2049</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/he19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhito</given_name>
<surname>Koishida</surname>
</person_name>
					</contributors>
					<titles><title>Sound Event Detection in Multichannel Audio Using Convolutional Time-Frequency-Channel Squeeze and Excitation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3629</first_page>
						<last_page>3633</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1860</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/xia19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lam</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramaswamy</given_name>
<surname>Palaniappan</surname>
</person_name>
					</contributors>
					<titles><title>A Robust Framework for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3634</first_page>
						<last_page>3638</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1841</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pham19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chieh-Chi</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viktor</given_name>
<surname>Rozgic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyros</given_name>
<surname>Matsoukas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Compression of Acoustic Event Detection Models with Quantized Distillation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3639</first_page>
						<last_page>3643</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1747</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shi19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shicai</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Pu</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Audio Classification System Based on Raw Waveforms and Mix-Training Strategy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3644</first_page>
						<last_page>3648</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1579</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kewei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghua</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Audio Classification with Attentional Graph Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3649</first_page>
						<last_page>3653</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1532</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kangkang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan-Sheng</given_name>
<surname>Foo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kah Kuan</given_name>
<surname>Teh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay Ramaseshan</given_name>
<surname>Chandrasekhar</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Audio Classification with Consistency-Based Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3654</first_page>
						<last_page>3658</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1231</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lu19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Nautsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Jasserand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Els</given_name>
<surname>Kindt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>The GDPR &amp; Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3695</first_page>
						<last_page>3699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2647</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nautsch19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brij Mohan Lal</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurélien</given_name>
<surname>Bellet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Tommasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3700</first_page>
						<last_page>3704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2415</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/srivastava19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandru</given_name>
<surname>Nelus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silas</given_name>
<surname>Rech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timm</given_name>
<surname>Koppelmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henrik</given_name>
<surname>Biermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Siamese Feature Extraction for Gender Recognition versus Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3705</first_page>
						<last_page>3709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1148</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nelus19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandru</given_name>
<surname>Nelus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janek</given_name>
<surname>Ebbers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Martin</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Variational Information Feature Extraction for Domestic Activity Monitoring versus Speaker Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3710</first_page>
						<last_page>3714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1703</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nelus19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patricia</given_name>
<surname>Thaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerald</given_name>
<surname>Penn</surname>
</person_name>
					</contributors>
					<titles><title>Extracting Mel-Frequency and Bark-Frequency Cepstral Coefficients from Encrypted Signals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3715</first_page>
						<last_page>3719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1136</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/thaine19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pablo Pérez</given_name>
<surname>Zarazaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>V. V. Vidyadhara</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>Sound Privacy: A Conversational Speech Corpus for Quantifying the Experience of Privacy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3720</first_page>
						<last_page>3724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1172</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zarazaga19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victor</given_name>
<surname>Soto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Improving Code-Switched Language Modeling Performance Using Cognate Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3725</first_page>
						<last_page>3729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2681</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/soto19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Grandee</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianghu</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3730</first_page>
						<last_page>3734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1382</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lee19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>SaiKrishna</given_name>
<surname>Rallabandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W.</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>Variational Attention Using Articulatory Priors for Generating Code Mixed Speech Using Monolingual Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3735</first_page>
						<last_page>3739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1103</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rallabandi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qinyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adem</given_name>
<surname>Derinel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Code-Switching Detection Using ASR-Generated Language Posteriors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3740</first_page>
						<last_page>3744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1161</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Astik</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Febe de</given_name>
<surname>Wet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewald van der</given_name>
<surname>Westhuizen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Acoustic Model Training for Five-Lingual Code-Switched ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3745</first_page>
						<last_page>3749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1325</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/biswas19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianghu</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David A. van</given_name>
<surname>Leeuwen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Graph Decoding for Code-Switching ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3750</first_page>
						<last_page>3754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1125</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ylmaz19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John R.</given_name>
<surname>Hershey</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Multilingual Multi-Speaker Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3755</first_page>
						<last_page>3759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3038</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/seki19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debasish Ray</given_name>
<surname>Mohapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Zappi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>An Extended Two-Dimensional Vocal Tract Model for Fast Acoustic Simulation of Single-Axis Symmetric Three-Dimensional Tubes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3760</first_page>
						<last_page>3764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1764</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mohapatra19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susanne</given_name>
<surname>Drechsel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Stone</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Optimization of an Enhanced Geometric Vocal Fold Model for Articulatory Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3765</first_page>
						<last_page>3769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2410</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/birkholz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Stone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Copy Synthesis Based on a Genetic Algorithm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3770</first_page>
						<last_page>3774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1334</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gao19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdolreza Sabzi</given_name>
<surname>Shahrebabaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Negar</given_name>
<surname>Olfati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali Shariq</given_name>
<surname>Imran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name>
					</contributors>
					<titles><title>A Phonetic-Level Analysis of Different Input Features for Articulatory Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3775</first_page>
						<last_page>3779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2526</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shahrebabaki19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Audhkhasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name>
					</contributors>
					<titles><title>Advancing Sequence-to-Sequence Based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3780</first_page>
						<last_page>3784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3018</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tuske19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Awni</given_name>
<surname>Hannun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ann</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiantong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3785</first_page>
						<last_page>3789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2460</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hannun19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramon</given_name>
<surname>Astudillo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Sequence-to-Sequence ASR Using Unpaired Speech and Text</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3790</first_page>
						<last_page>3794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3167</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/baskar19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3795</first_page>
						<last_page>3799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1554</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bai19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuki</given_name>
<surname>Irie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjuli</given_name>
<surname>Kannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Bruguier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3800</first_page>
						<last_page>3804</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2277</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/irie19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felix</given_name>
<surname>Weninger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Andrés-Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinwei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puming</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3805</first_page>
						<last_page>3809</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2719</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/weninger19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna V.</given_name>
<surname>Rúnarsdóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inga R.</given_name>
<surname>Helgadóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name>
					</contributors>
					<titles><title>Lattice Re-Scoring During Manual Editing for Automatic Error Correction of ASR Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3810</first_page>
						<last_page>3814</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1790</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/runarsdottir19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Fukunaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuichi</given_name>
<surname>Kageyama</surname>
</person_name>
					</contributors>
					<titles><title>GPU-Based WFST Decoding with Extra Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3815</first_page>
						<last_page>3819</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2101</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fukunaga19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Javier</given_name>
<surname>Jorge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrià</given_name>
<surname>Giménez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Iranzo-Sánchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Civera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Sanchis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfons</given_name>
<surname>Juan</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3820</first_page>
						<last_page>3824</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2798</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jorge19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Moritz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan Le</given_name>
<surname>Roux</surname>
</person_name>
					</contributors>
					<titles><title>Vectorized Beam Search for CTC-Attention-Based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3825</first_page>
						<last_page>3829</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2860</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/seki19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jack</given_name>
<surname>Serrino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonid</given_name>
<surname>Velikovich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petar</given_name>
<surname>Aleksic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cyril</given_name>
<surname>Allauzen</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Recovery of Out-of-Lattice Named Entities in Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3830</first_page>
						<last_page>3834</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2962</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/serrino19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sashi</given_name>
<surname>Novitasari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3835</first_page>
						<last_page>3839</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2985</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/novitasari19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3840</first_page>
						<last_page>3844</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1582</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lian19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huy</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver Y.</given_name>
<surname>Chén</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lam</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Koch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maarten De</given_name>
<surname>Vos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfred</given_name>
<surname>Mertins</surname>
</person_name>
					</contributors>
					<titles><title>Spatio-Temporal Attention Pooling for Audio Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3845</first_page>
						<last_page>3849</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3040</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/phan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiuying</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Subspace Pooling Based Temporal Features Extraction for Audio Event Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3850</first_page>
						<last_page>3854</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2047</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shi19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jintao</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Scale Time-Frequency Attention for Acoustic Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3855</first_page>
						<last_page>3859</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1587</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwen</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Scene Classification by Implicitly Identifying Distinct Sound Events</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3860</first_page>
						<last_page>3864</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2231</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/song19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoke</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Transfer Learning for Low-Resource Individualization of Head-Related Transfer Functions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3865</first_page>
						<last_page>3869</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2558</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/qi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Jian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wentao</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Characteristics of Mandarin Declarative and Interrogative Utterances in Parkinson&#8217;s Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3870</first_page>
						<last_page>3874</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3276</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laureano</given_name>
<surname>Moro-Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>JaeJin</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark A.</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heejin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson&#8217;s Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3875</first_page>
						<last_page>3879</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2993</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/morovelazquez19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianqi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongyuan</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingshen</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quanlei</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiqi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manwa L.</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3880</first_page>
						<last_page>3884</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2414</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Child Speech Disorder Detection with Siamese Recurrent Network Using Speech Attribute Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3885</first_page>
						<last_page>3889</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2320</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Korzekwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bozena</given_name>
<surname>Kostek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Lajszczak</surname>
</person_name>
					</contributors>
					<titles><title>Interpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3890</first_page>
						<last_page>3894</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1206</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/korzekwa19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Camille</given_name>
<surname>Noufi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam C.</given_name>
<surname>Lammert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daryush D.</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James R.</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Ciccarelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>Sturim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F.</given_name>
<surname>Campbell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F.</given_name>
<surname>Quatieri</surname>
</person_name>
					</contributors>
					<titles><title>Vocal Biomarker Assessment Following Pediatric Traumatic Brain Injury: A Retrospective Cohort Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3895</first_page>
						<last_page>3899</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1200</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/noufi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Improved Deep Duel Model for Rescoring N-Best Speech Recognition List Using Backward LSTMLM and Ensemble Encoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3900</first_page>
						<last_page>3904</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1949</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ogawa19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuki</given_name>
<surname>Irie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Language Modeling with Deep Transformers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3905</first_page>
						<last_page>3909</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2225</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/irie19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Filimonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Tiwari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guitang</given_name>
<surname>Lan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Scalable Multi Corpora Neural Language Models for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3910</first_page>
						<last_page>3914</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3060</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/raju19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Likhomanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Synnaeve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name>
					</contributors>
					<titles><title>Who Needs Words? Lexicon-Free Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3915</first_page>
						<last_page>3919</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3107</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/likhomanenko19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddique</given_name>
<surname>Latif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajib</given_name>
<surname>Rana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Khalifa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raja</given_name>
<surname>Jurdak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Direct Modelling of Speech Emotion from Raw Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3920</first_page>
						<last_page>3924</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3252</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/latif19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mousmita</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pegah</given_name>
<surname>Ghahremani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nagendra Kumar</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kandarpa Kumar</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Improving Emotion Identification Using Phone Posteriors in Raw Speech Waveform Based DNN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3925</first_page>
						<last_page>3929</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2093</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sarma19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miao</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu-cheng</given_name>
<surname>Yin</surname>
</person_name>
					</contributors>
					<titles><title>Pyramid Memory Block and Timestep Attention for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3930</first_page>
						<last_page>3934</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3140</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cao19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Oates</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Triantafyllopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingmar</given_name>
<surname>Steiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Robust Speech Emotion Recognition Under Different Encoding Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3935</first_page>
						<last_page>3939</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1658</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/oates19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name>
					</contributors>
					<titles><title>Using the Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for Paralinguistic Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3940</first_page>
						<last_page>3944</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1163</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gosztolya19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling Style Factors from Speaker Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3945</first_page>
						<last_page>3949</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1769</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/williams19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Yin</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Sentence Prosody and  Wh-Indeterminates in Taiwan Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3950</first_page>
						<last_page>3954</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2545</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hsu19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youjue</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Frication as a Vowel Feature? &#8212; Evidence from the Rui&#8217;an Wu Chinese Dialect</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3955</first_page>
						<last_page>3959</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1134</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hu19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenrui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Vowels and Diphthongs in the Xupu Xiang Chinese Dialect</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3960</first_page>
						<last_page>3964</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1174</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Albuquerque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>António</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Sa-Couto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniela</given_name>
<surname>Figueiredo</surname>
</person_name>
					</contributors>
					<titles><title>Age-Related Changes in European Portuguese Vowel Acoustics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3965</first_page>
						<last_page>3969</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1818</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/albuquerque19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wendy</given_name>
<surname>Lalhminghlui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viyazonuo</given_name>
<surname>Terhiija</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name>
					</contributors>
					<titles><title>Vowel-Tone Interaction in Two Tibeto-Burman Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3970</first_page>
						<last_page>3974</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2808</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lalhminghlui19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenifer Vega</given_name>
<surname>Rodríguez</surname>
</person_name>
					</contributors>
					<titles><title>The Vowel System of Korebaju</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3975</first_page>
						<last_page>3979</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3210</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rodriguez19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Omnia</given_name>
<surname>Ibrahim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabine</given_name>
<surname>Stoll</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name>
					</contributors>
					<titles><title>Fundamental Frequency Accommodation in Multi-Party Human-Robot Game Interactions: The Effect of Winning or Losing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3980</first_page>
						<last_page>3984</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2496</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ibrahim19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nataliya</given_name>
<surname>Bryhadyr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marin</given_name>
<surname>Schröer</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Accent Trajectories Across Different Conditions of Visibility and Information Structure &#8212; Evidence from Spontaneous Dyadic Interaction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3985</first_page>
						<last_page>3989</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1619</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wagner19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Simon</given_name>
<surname>Betz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sina</given_name>
<surname>Zarrieß</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name>
					</contributors>
					<titles><title>The Greennn Tree &#8212; Lengthening Position Influences Uncertainty Perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3990</first_page>
						<last_page>3994</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2572</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/betz19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengfei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aijun</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>CNN-BLSTM Based Question Detection from Dialogs Considering Phase and Context Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>3995</first_page>
						<last_page>3999</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1701</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/si19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Metcalf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barry-John</given_name>
<surname>Theobald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Garrett</given_name>
<surname>Weinberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ing-Marie</given_name>
<surname>Jonsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Russ</given_name>
<surname>Webb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Apostoloff</surname>
</person_name>
					</contributors>
					<titles><title>Mirroring to Build Trust in Digital Assistants</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4000</first_page>
						<last_page>4004</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1829</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/metcalf19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eran</given_name>
<surname>Raveh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Siegert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ingmar</given_name>
<surname>Steiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iona</given_name>
<surname>Gessinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>Three&#8217;s a Crowd? Effects of a Second Human on Vocal Accommodation with a Voice Assistant</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4005</first_page>
						<last_page>4009</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1825</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/raveh19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Regularization for End-to-End Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4010</first_page>
						<last_page>4014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2983</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>João</given_name>
<surname>Monteiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiago H.</given_name>
<surname>Falk</surname>
</person_name>
					</contributors>
					<titles><title>Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4015</first_page>
						<last_page>4019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2974</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/monteiro19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>VAE-Based Regularization for Deep Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4020</first_page>
						<last_page>4024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2486</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victoria</given_name>
<surname>Mingote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Castan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name>
					</contributors>
					<titles><title>Language Recognition Using Triplet Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4025</first_page>
						<last_page>4029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2437</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mingote19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngmoon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Younggwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungjun</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeunju</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Spatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4030</first_page>
						<last_page>4034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2177</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jung19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hee-Soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>IL-Ho</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung-Hyun</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Losses Based on Speaker Basis Vectors and All-Speaker Hard Negative Mining for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4035</first_page>
						<last_page>4039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1986</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/heo19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>An Effective Deep Embedding Learning Architecture for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4040</first_page>
						<last_page>4044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1606</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/jiang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danwei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Far-Field End-to-End Text-Dependent Speaker Verification Based on Mixed Training Data with Transfer Learning and Enrollment Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4045</first_page>
						<last_page>4049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1542</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/qin19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zongze</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guofu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shugong</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Two-Stage Training for Chinese Dialect Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4050</first_page>
						<last_page>4054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1522</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ren19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryota</given_name>
<surname>Kaminishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haruna</given_name>
<surname>Miyamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sayaka</given_name>
<surname>Shiota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Kiya</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on Blind Bandwidth Extension with a Non-Linear Function and its Evaluation of x-Vector-Based Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4055</first_page>
						<last_page>4059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1510</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kaminishi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Umair</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miquel</given_name>
<surname>India</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Hernando</surname>
</person_name>
					</contributors>
					<titles><title>Auto-Encoding Nearest Neighbor i-Vectors for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4060</first_page>
						<last_page>4064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1444</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/khan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Suo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Lei</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Fault-Tolerant Speaker Verification System: A Regularization Approach to Reduce the Condition Number</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4065</first_page>
						<last_page>4069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1442</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zheng19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Taherian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4070</first_page>
						<last_page>4074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1428</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/taherian19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon-Young</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Joint Optimization of Neural Acoustic Beamforming and Dereverberation with x-Vectors for Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4075</first_page>
						<last_page>4079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1356</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>A New Time-Frequency Attention Mechanism for TDNN and CNN-LSTM-TDNN, with Application to Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4080</first_page>
						<last_page>4084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1256</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/miao19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jieping</given_name>
<surname>Ye</surname>
</person_name>
					</contributors>
					<titles><title>An Attention-Based Hybrid Network for Automatic Detection of Alzheimer&#8217;s Disease from Narrative Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4085</first_page>
						<last_page>4089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2872</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4090</first_page>
						<last_page>4094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2726</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ma19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jasper</given_name>
<surname>Ooster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pia Nancy Porysek</given_name>
<surname>Moreta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörg-Hendrik</given_name>
<surname>Bach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inga</given_name>
<surname>Holube</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>&#8220;Computer, Test My Hearing&#8221;: Accurate Speech Audiometry with Smart Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4095</first_page>
						<last_page>4099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2118</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ooster19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aciel</given_name>
<surname>Eshky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steve</given_name>
<surname>Renals</surname>
</person_name>
					</contributors>
					<titles><title>Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4100</first_page>
						<last_page>4104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1804</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/eshky19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Reuber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Annalena</given_name>
<surname>Venneri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Hierarchical Attention Neural Network for Detecting AD</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4105</first_page>
						<last_page>4109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1799</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Venkata Srikanth</given_name>
<surname>Nallanthighal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aki</given_name>
<surname>Härmä</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Deep Sensing of Breathing Signal During Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4110</first_page>
						<last_page>4114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1796</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nallanthighal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fadi</given_name>
<surname>Biadsy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron J.</given_name>
<surname>Weiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro J.</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dimitri</given_name>
<surname>Kanvesky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Jia</surname>
</person_name>
					</contributors>
					<titles><title>Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4115</first_page>
						<last_page>4119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1789</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/biadsy19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4120</first_page>
						<last_page>4124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1536</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Vougioukas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>Video-Driven Speech Reconstruction Using Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4125</first_page>
						<last_page>4129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1445</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vougioukas19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>On the Use of Pitch Features for Disordered Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4130</first_page>
						<last_page>4134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2609</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brendan</given_name>
<surname>Shillingford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Assael</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew W.</given_name>
<surname>Hoffman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Paine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cían</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Utsav</given_name>
<surname>Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hank</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hasim</given_name>
<surname>Sak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanishka</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorrayne</given_name>
<surname>Bennett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Mulville</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Misha</given_name>
<surname>Denil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ben</given_name>
<surname>Coppin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ben</given_name>
<surname>Laurie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Senior</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nando de</given_name>
<surname>Freitas</surname>
</person_name>
					</contributors>
					<titles><title>Large-Scale Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4135</first_page>
						<last_page>4139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1669</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shillingford19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>S. Zahra</given_name>
<surname>Razavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Kane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lenhart K.</given_name>
<surname>Schubert</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Linguistic and Semantic Features for Turn-Taking Prediction in Open-Domain Human-Computer Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4140</first_page>
						<last_page>4144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3152</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/razavi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Frédéric</given_name>
<surname>Béchet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Raymond</surname>
</person_name>
					</contributors>
					<titles><title>Benchmarking Benchmarks: Introducing New Automatic Indicators for Benchmarking Spoken Language Understanding Corpora</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4145</first_page>
						<last_page>4149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3033</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bechet19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chaoran</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Ishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Ishiguro</surname>
</person_name>
					</contributors>
					<titles><title>A Neural Turn-Taking Model without RNN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4150</first_page>
						<last_page>4154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2270</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrei C.</given_name>
<surname>Coman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Yoshino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukitoshi</given_name>
<surname>Murase</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Riccardi</surname>
</person_name>
					</contributors>
					<titles><title>An Incremental Turn-Taking Model for Task-Oriented Dialog Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4155</first_page>
						<last_page>4159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1826</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/coman19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Feng-Guang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aliyah R.</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Lin</given_name>
<surname>Tuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-Yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Dialogue Response Generation Learned from Monologues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4160</first_page>
						<last_page>4164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1696</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/su19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mattias</given_name>
<surname>Heldner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Włodarczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Štefan</given_name>
<surname>Beňuš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agustín</given_name>
<surname>Gravano</surname>
</person_name>
					</contributors>
					<titles><title>Voice Quality as a Turn-Taking Cue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4165</first_page>
						<last_page>4169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1592</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/heldner19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Hara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuya</given_name>
<surname>Takanashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Turn-Taking Prediction Based on Detection of Transition Relevance Place</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4170</first_page>
						<last_page>4174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1537</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hara19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Divesh</given_name>
<surname>Lala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shizuka</given_name>
<surname>Nakamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Effect and Timing of Fillers in Natural Turn-Taking</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4175</first_page>
						<last_page>4179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1527</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lala19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Response Obligation Detection with Unsupervised Online Domain Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4180</first_page>
						<last_page>4184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1313</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/horiguchi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ming-Hsiang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Follow-Up Question Generation Using Neural Tensor Network-Based Domain Ontology Population in an Interview Coaching System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4185</first_page>
						<last_page>4189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1300</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/su19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Trang</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name>
					</contributors>
					<titles><title>On the Role of Style in Parsing Speech with Neural Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4190</first_page>
						<last_page>4194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3122</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tran19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankita</given_name>
<surname>Pasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name>
					</contributors>
					<titles><title>On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4195</first_page>
						<last_page>4199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3051</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pasad19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Su-Youn</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keelan</given_name>
<surname>Evanini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Klaus</given_name>
<surname>Zechner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Off-Topic Spoken Responses Using Very Deep Convolutional Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4200</first_page>
						<last_page>4204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1848</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Piunova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Beck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Rescoring Keyword Search Confidence Estimates with Graph-Based Re-Ranking Using Acoustic Word Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4205</first_page>
						<last_page>4209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1817</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/piunova19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yael</given_name>
<surname>Segal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tzeviya Sylvia</given_name>
<surname>Fuchs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>SpeechYOLO: Detection and Localization of Speech Objects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4210</first_page>
						<last_page>4214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1749</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/segal19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alp</given_name>
<surname>Öktem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Farrús</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Bonafonte</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Phrase Alignment for Machine Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4215</first_page>
						<last_page>4219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1621</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/oktem19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christina</given_name>
<surname>Tånnander</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Per</given_name>
<surname>Fallgren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gusafsson</surname>
</person_name>
					</contributors>
					<titles><title>Spot the Pleasant People! Navigating the Cocktail Party Buzz</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4220</first_page>
						<last_page>4224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1553</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tannander19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name>
					</contributors>
					<titles><title>Neural Text Clustering with Document-Level Attention Based on Dynamic Soft Labels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4225</first_page>
						<last_page>4229</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1417</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nguyen</given_name>
<surname>Bach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Noisy BiLSTM-Based Models for Disfluency Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4230</first_page>
						<last_page>4234</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1336</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bach19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mittul</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sami</given_name>
<surname>Virpioja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Smit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4235</first_page>
						<last_page>4239</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1329</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/singh19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Maekaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Kida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiko</given_name>
<surname>Sugiyama</surname>
</person_name>
					</contributors>
					<titles><title>Simultaneous Detection and Localization of a Wake-Up Word Using Multi-Task Learning of the Duration and Endpoint</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4240</first_page>
						<last_page>4244</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1180</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/maekaku19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ching-Hua</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Lin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fred</given_name>
<surname>Harris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhaskar D.</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harinath</given_name>
<surname>Garudadri</surname>
</person_name>
					</contributors>
					<titles><title>On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4245</first_page>
						<last_page>4249</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3195</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lee19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amin</given_name>
<surname>Fazel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>El-Khamy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwon</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Deep Multitask Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4250</first_page>
						<last_page>4254</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2908</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fazel19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4255</first_page>
						<last_page>4259</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2651</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhang19o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Charlotte</given_name>
<surname>Sørensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper B.</given_name>
<surname>Boldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mads G.</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Harmonic Beamformers for Non-Intrusive Speech Intelligibility Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4260</first_page>
						<last_page>4264</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2929</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/srensen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nursadul</given_name>
<surname>Mamun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soheil</given_name>
<surname>Khorram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4265</first_page>
						<last_page>4269</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1850</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mamun19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Charlotte</given_name>
<surname>Sørensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper B.</given_name>
<surname>Boldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mads G.</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Validation of the Non-Intrusive Codebook-Based Short Time Objective Intelligibility Metric for Processed Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4270</first_page>
						<last_page>4274</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1625</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/srensen19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Arai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuhiko</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Speech Intelligibility of Enhanced Speech Using Phone Accuracy of DNN-Based ASR System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4275</first_page>
						<last_page>4279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1381</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/arai19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suliang</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunxin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mei-Yuh</given_name>
<surname>Hwang</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Method to Correct Steering Vectors in MVDR Beamformer for Noise Robust ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4280</first_page>
						<last_page>4284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2944</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyeonseung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung Yong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woo Hyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeunghun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Multi-Channel Speech Enhancement Using Inter-Channel Time-Restricted Attention on Raw Waveform</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4285</first_page>
						<last_page>4289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2397</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lee19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimeng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Neural Spatial Filter: Target Speaker Speech Separation Assisted with Directional Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4290</first_page>
						<last_page>4294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2266</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Triantafyllos</given_name>
<surname>Afouras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Zisserman</surname>
</person_name>
					</contributors>
					<titles><title>My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4295</first_page>
						<last_page>4299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3114</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/afouras19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Neural Speaker Diarization with Permutation-Free Objectives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4300</first_page>
						<last_page>4304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2899</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fujita19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miquel</given_name>
<surname>India</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pooyan</given_name>
<surname>Safari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Hernando</surname>
</person_name>
					</contributors>
					<titles><title>Self Multi-Head Attention for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4305</first_page>
						<last_page>4309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2616</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/india19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ignacio</given_name>
<surname>Viñals</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dayana</given_name>
<surname>Ribas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victoria</given_name>
<surname>Mingote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Llombart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Gimeno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4310</first_page>
						<last_page>4314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2417</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/vinals19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youzhi</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name>
					</contributors>
					<titles><title>Variational Domain Adversarial Learning for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4315</first_page>
						<last_page>4319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2168</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maulik</given_name>
<surname>Madhavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Framework for Speaker and Utterance Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4320</first_page>
						<last_page>4324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1994</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/liu19m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Castan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Lawson</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Critical Metadata Factors for the Calibration of Speaker Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4325</first_page>
						<last_page>4329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1808</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nandwana19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Glembek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Factorization of Discriminatively Trained i-Vector Extractor for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4330</first_page>
						<last_page>4334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1757</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/novotny19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Salvati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlo</given_name>
<surname>Drioli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gian Luca</given_name>
<surname>Foresti</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speaker Identification in Noisy and Reverberant Environments Using Raw Waveform Convolutional Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4335</first_page>
						<last_page>4339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2403</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/salvati19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abinay Reddy</given_name>
<surname>Naini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Achuth Rao</given_name>
<surname>M.V.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Whisper to Neutral Mapping Using Cosine Similarity Maximization in i-Vector Space for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4340</first_page>
						<last_page>4344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2280</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/naini19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingke</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Mixup Learning Strategies for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4345</first_page>
						<last_page>4349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2250</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zhu19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing a Speaker Embedding Extractor Through Backend-Driven Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4350</first_page>
						<last_page>4354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1820</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ferrer19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Okabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiongqiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ling</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Koshinaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiacen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>The NEC-TT 2018 Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4355</first_page>
						<last_page>4359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1517</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lee19g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Suo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Lei</surname>
</person_name>
					</contributors>
					<titles><title>Autoencoder-Based Semi-Supervised Curriculum Learning for Out-of-Domain Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4360</first_page>
						<last_page>4364</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1440</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zheng19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danwei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoyi</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Training for End-to-End Speaker Recognition Under Reverberant and Noisy Environment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4365</first_page>
						<last_page>4369</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1437</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cai19d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danwei</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weicheng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4370</first_page>
						<last_page>4374</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1436</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/cai19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adithya</given_name>
<surname>Renduchintala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunxi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Pretraining by Backtranslation for End-to-End ASR in Low-Resource Settings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4375</first_page>
						<last_page>4379</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3254</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wiesner19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Dalmia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Attention End-to-End ASR for Two-Party Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4380</first_page>
						<last_page>4384</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3173</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kim19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Chorowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Łańcucki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bartosz</given_name>
<surname>Kostka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Zapotoczny</surname>
</person_name>
					</contributors>
					<titles><title>Towards Using Context-Dependent Symbols in CTC Without State-Tying Decision Trees</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4385</first_page>
						<last_page>4389</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2720</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chorowski19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruchao</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>An Online Attention-Based Model for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4390</first_page>
						<last_page>4394</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2218</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fan19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengkun</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>Self-Attention Transducers for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4395</first_page>
						<last_page>4399</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2203</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tian19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dabre</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Improving Transformer-Based Speech Recognition Systems with Compressed Structure and Speech Attributes Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4400</first_page>
						<last_page>4404</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2112</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeong-Uk</given_name>
<surname>Bang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mu-Yeol</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Hun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oh-Wook</given_name>
<surname>Kwon</surname>
</person_name>
					</contributors>
					<titles><title>Extending an Acoustic Data-Driven Phone Set for Spontaneous Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4405</first_page>
						<last_page>4409</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1979</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bang19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshikazu</given_name>
<surname>Yamaguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Joint Maximization Decoder with Neural Converters for Fully Neural Network-Based Japanese Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4410</first_page>
						<last_page>4414</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1558</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/moriya19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Morchid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georges</given_name>
<surname>Linarès</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renato De</given_name>
<surname>Mori</surname>
</person_name>
					</contributors>
					<titles><title>Real to H-Space Encoder for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4415</first_page>
						<last_page>4419</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1539</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/parcollet19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title> Ectc-Docd: An End-to-End Structure with CTC Encoder and OCD Decoder for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4420</first_page>
						<last_page>4424</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1212</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yi19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Denisov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4425</first_page>
						<last_page>4429</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1130</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/denisov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Takeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Toshniwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name>
					</contributors>
					<titles><title>Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4430</first_page>
						<last_page>4434</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3177</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hayashi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Beskow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafson</surname>
</person_name>
					</contributors>
					<titles><title>Spontaneous Conversational Speech Synthesis from Found Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4435</first_page>
						<last_page>4439</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2836</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/szekely19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viacheslav</given_name>
<surname>Klimkov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Rohnke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Drugman</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4440</first_page>
						<last_page>4444</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2571</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/klimkov19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nusrah</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Engin</given_name>
<surname>Erzin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>T. Metin</given_name>
<surname>Sezgin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yücel</given_name>
<surname>Yemez</surname>
</person_name>
					</contributors>
					<titles><title>Speech Driven Backchannel Generation Using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4445</first_page>
						<last_page>4449</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2521</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hussain19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takao</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Prosody Modeling Using Deep Gaussian Process Latent Variable Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4450</first_page>
						<last_page>4454</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2497</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/koriyama19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna Björk</given_name>
<surname>Nikulásdóttir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name>
					</contributors>
					<titles><title>Bootstrapping a Text Normalization System for an Inflected Language. Numbers as a Test Case</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4455</first_page>
						<last_page>4459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2367</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/nikulasdottir19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haohan</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank K.</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4460</first_page>
						<last_page>4464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2167</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/guo19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinfu</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Shiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Duration Modeling with Global Phoneme-Duration Vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4465</first_page>
						<last_page>4469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2126</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ni19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adèle</given_name>
<surname>Aubin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandra</given_name>
<surname>Cervone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Watts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Synthesis with Discourse Relations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4470</first_page>
						<last_page>4474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1945</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/aubin19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noé</given_name>
<surname>Tits</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengna</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin El</given_name>
<surname>Haddad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Pagel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thierry</given_name>
<surname>Dutoit</surname>
</person_name>
					</contributors>
					<titles><title>Visualization and Interpretation of Latent Spaces for Controlling Expressive Speech Synthesis Through Audio Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4475</first_page>
						<last_page>4479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1426</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/tits19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Pre-Trained Text Representations for Improving Front-End Text Processing in Mandarin Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4480</first_page>
						<last_page>4484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1418</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yang19h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huashan</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiulin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4485</first_page>
						<last_page>4488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1400</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/pan19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajda</given_name>
<surname>Gokcen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Sproat</surname>
</person_name>
					</contributors>
					<titles><title>Dual Encoder Classifier Models as Constraints in Neural Text Normalization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4489</first_page>
						<last_page>4493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1135</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/gokcen19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingbei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runnan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengpeng</given_name>
<surname>Zhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Song</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge-Based Linguistic Encoding for End-to-End Mandarin Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4494</first_page>
						<last_page>4498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1118</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/li19v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsi-Wei</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Charon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archana</given_name>
<surname>Venkataraman</surname>
</person_name>
					</contributors>
					<titles><title>Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4499</first_page>
						<last_page>4503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2386</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shankar19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kathryn P.</given_name>
<surname>Connaghan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabrina</given_name>
<surname>Paganoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harli</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ella</given_name>
<surname>Collins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Richburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marziye</given_name>
<surname>Eshghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>J.P.</given_name>
<surname>Onnela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James D.</given_name>
<surname>Berry</surname>
</person_name>
					</contributors>
					<titles><title>Use of Beiwe Smartphone App to Identify and Track Speech Decline in Amyotrophic Lateral Sclerosis (ALS)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4504</first_page>
						<last_page>4508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3126</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/connaghan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah P.</given_name>
<surname>Rowe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>Profiling Speech Motor Impairments in Persons with Amyotrophic Lateral Sclerosis: An Acoustic-Based Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4509</first_page>
						<last_page>4513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2911</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/rowe19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alex</given_name>
<surname>Mayle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Mou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Razvan</given_name>
<surname>Bunescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sadegh</given_name>
<surname>Mirshekarian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Diagnosing Dysarthria with Long Short-Term Memory Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4514</first_page>
						<last_page>4518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2903</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/mayle19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Protima Nomo</given_name>
<surname>Sudro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.R. Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Modification of Devoicing Error in Cleft Lip and Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4519</first_page>
						<last_page>4523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2604</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/sudro19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marziye</given_name>
<surname>Eshghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panying</given_name>
<surname>Rong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antje S.</given_name>
<surname>Mefferd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaila L.</given_name>
<surname>Stipancic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yana</given_name>
<surname>Yunusova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>Reduced Task Adaptation in Alternating Motion Rate Tasks as an Early Marker of Bulbar Involvement in Amyotrophic Lateral Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4524</first_page>
						<last_page>4528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2546</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/eshghi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianqi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quanlei</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingshen</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiqi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Towards the Speech Features of Early-Stage Dementia: Design and Application of the Mandarin Elderly Cognitive Speech Database</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4529</first_page>
						<last_page>4533</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2453</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeroen van de</given_name>
<surname>Weijer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuangshuang</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manna</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Characteristics of Lexical Tone Disruption in Mandarin Speakers After Brain Damage</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4534</first_page>
						<last_page>4538</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2432</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/chen19p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doris</given_name>
<surname>Mücke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tabea</given_name>
<surname>Thies</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael T.</given_name>
<surname>Barbe</surname>
</person_name>
					</contributors>
					<titles><title>Intragestural Variation in Natural Sentence Production: Essential Tremor Patients Treated with DBS</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4539</first_page>
						<last_page>4543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2389</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/hermes19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Protima Nomo</given_name>
<surname>Sudro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.R. Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Dandapat</surname>
</person_name>
					</contributors>
					<titles><title>Nasal Air Emission in Sibilant Fricatives of Cleft Lip and Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4544</first_page>
						<last_page>4548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2345</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/kalita19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luis</given_name>
<surname>Serrano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Raman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Tavarez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva</given_name>
<surname>Navas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inma</given_name>
<surname>Hernaez</surname>
</person_name>
					</contributors>
					<titles><title>Parallel vs. Non-Parallel Voice Conversion for Esophageal Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4549</first_page>
						<last_page>4553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2194</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/serrano19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akhilesh Kumar</given_name>
<surname>Dubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.R. Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Dandapat</surname>
</person_name>
					</contributors>
					<titles><title>Hypernasality Severity Detection Using Constant Q Cepstral Coefficients</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4554</first_page>
						<last_page>4558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2151</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/dubey19b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyue</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Depression Level Detection via ℓp-Norm Pooling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4559</first_page>
						<last_page>4563</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1617</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/niu19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhas</given_name>
<surname>B.N.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deep</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Belur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradeep</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nalini</given_name>
<surname>Atchayaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dipanjan</given_name>
<surname>Gope</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Speech Tasks and Recording Devices for Voice Based Automatic Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4564</first_page>
						<last_page>4568</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1285</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bn19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongxiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>A Modified Algorithm for Multiple Input Spectrogram Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4569</first_page>
						<last_page>4573</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3242</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/wang19r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fahimeh</given_name>
<surname>Bahmaninezhad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Xiong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4574</first_page>
						<last_page>4578</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-3181</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bahmaninezhad19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Berkay</given_name>
<surname>İnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmut</given_name>
<surname>Grabner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena Peic</given_name>
<surname>Tukuljac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo C.G.</given_name>
<surname>Pena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Ricaud</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Audiovisual Source Separation in the Context of Video Conferencing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4579</first_page>
						<last_page>4583</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2671</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/inan19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Ditter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Influence of Speaker-Specific Parameters on Speech Separation Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4584</first_page>
						<last_page>4588</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2459</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/ditter19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeroen</given_name>
<surname>Zegers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>CNN-LSTM Models for Multi-Speaker Source Separation Using Bayesian Hyper Parameter Optimization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4589</first_page>
						<last_page>4593</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2423</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/zegers19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Helen L.</given_name>
<surname>Bear</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inês</given_name>
<surname>Nolasco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanouil</given_name>
<surname>Benetos</surname>
</person_name>
					</contributors>
					<titles><title>Towards Joint Sound Scene and Polyphonic Sound Event Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4594</first_page>
						<last_page>4598</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-2169</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/bear19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyan</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>Discriminative Learning for Monaural Speech Separation Using Deep Embedding Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4599</first_page>
						<last_page>4603</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1940</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/fan19c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Midia</given_name>
<surname>Yousefi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soheil</given_name>
<surname>Khorram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Probabilistic Permutation Invariant Training for Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4604</first_page>
						<last_page>4608</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1827</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/yousefi19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4609</first_page>
						<last_page>4613</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1591</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shi19e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqiang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huibin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rujie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoji</given_name>
<surname>Hayakawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shouji</given_name>
<surname>Harada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4614</first_page>
						<last_page>4618</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1292</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/shi19f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesc</given_name>
<surname>Lluís</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordi</given_name>
<surname>Pons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Serra</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Music Source Separation: Is it Possible in the Waveform Domain?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>15</day>
						<year>2019</year>
					</publication_date>
					<pages>
						<first_page>4619</first_page>
						<last_page>4623</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2019-1177</doi>
						<resource>https://www.isca-archive.org/interspeech_2019/lluis19_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>