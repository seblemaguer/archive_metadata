<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2018</doi_batch_id>
		<timestamp>1705398646961720</timestamp>
		<depositor>
			<depositor_name>Martin Cooke</depositor_name> 
			<email_address>m.cooke@ikerbasque.org</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant> 
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>Interspeech 2018</conference_name>
				<conference_acronym>interspeech_2018</conference_acronym>
				<conference_date>2-6 September 2018</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>Interspeech 2018</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2018</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2018</doi>
					<timestamp>1705398646961720</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2018/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kondo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Taira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>Binaural Speech Intelligibility Estimation Using Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1858</first_page>
						<last_page>1862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-27</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kondo18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Cheng</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Scoring of an Oral Reading Assessment on Mobile Devices</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1621</first_page>
						<last_page>1625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-34</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cheng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Albert</given_name>
<surname>Haque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prateek</given_name>
<surname>Verma</surname>
</person_name>
					</contributors>
					<titles><title>Conditional End-to-End Audio Transforms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2295</first_page>
						<last_page>2299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-38</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/haque18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshuman</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Co</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Navdeep</given_name>
<surname>Jaitly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diana</given_name>
<surname>Jaunzeikare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjuli</given_name>
<surname>Kannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hasim</given_name>
<surname>Sak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ananth</given_name>
<surname>Sankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Tansuwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuedong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Speech Recognition for Medical Conversations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2972</first_page>
						<last_page>2976</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-40</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chiu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lanhua</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Improved Supervised Locality Preserving Projection for I-vector Based Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>62</first_page>
						<last_page>66</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-41</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/you18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iroro</given_name>
<surname>Orife</surname>
</person_name>
					</contributors>
					<titles><title>Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2848</first_page>
						<last_page>2852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-42</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/orife18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ken-Ichi</given_name>
<surname>Sakakibara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanori</given_name>
<surname>Morise</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hideki</given_name>
<surname>Banno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name>
					</contributors>
					<titles><title>Frequency Domain Variants of Velvet Noise and Their Application to Speech Processing and Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2027</first_page>
						<last_page>2031</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-43</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kawahara18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiguang</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Normalization Method for Autocorrelation Function for Pitch Detection and for Speech Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2097</first_page>
						<last_page>2101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-45</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lin18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Fischer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Dithered Quantization for Frequency-Domain Speech and Audio Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3533</first_page>
						<last_page>3537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-46</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/backstrom18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Parada-Cabaleiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Costantini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Categorical vs Dimensional Perception of Italian Emotional Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3638</first_page>
						<last_page>3642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-47</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/paradacabaleiro18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kimiko</given_name>
<surname>Tsukada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Rong</surname>
</person_name>
					</contributors>
					<titles><title>Cross-language Perception of Mandarin Lexical Tones by Mongolian-speaking Bilinguals in the Inner Mongolia Autonomous Region, China</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2539</first_page>
						<last_page>2543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-48</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tsukada18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harishchandra</given_name>
<surname>Dubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhijeet</given_name>
<surname>Sangwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Robust Speaker Clustering using Mixtures of von Mises-Fisher Distributions for Naturalistic Audio Streams</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3603</first_page>
						<last_page>3607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-50</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dubey18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Steidl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Batliner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter B.</given_name>
<surname>Marschik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harald</given_name>
<surname>Baumeister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengquan</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Hantke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian B.</given_name>
<surname>Pokorny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eva-Maria</given_name>
<surname>Rathner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin D.</given_name>
<surname>Bartl-Pokorny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christa</given_name>
<surname>Einspieler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dajie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panagiotis</given_name>
<surname>Tzirakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefanos</given_name>
<surname>Zafeiriou</surname>
</person_name>
					</contributors>
					<titles><title>The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical &#38; Self-Assessed Affect, Crying &#38; Heart Beats</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>122</first_page>
						<last_page>126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-51</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/schuller18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Henk</given_name>
<surname>van den Heuvel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>van Leeuwen</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic and Textual Data Augmentation for Improved ASR of Code-Switching Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1933</first_page>
						<last_page>1937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-52</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ylmaz18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jochen</given_name>
<surname>Weiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miguel</given_name>
<surname>Angrick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Umesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Effect of Audio Duration on Dementia Detection Using Acoustic Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2324</first_page>
						<last_page>2328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-57</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/weiner18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chen</given_name>
<surname>Xuanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Ziyu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hu</given_name>
<surname>Jian</surname>
</person_name>
					</contributors>
					<titles><title>The Trajectory of Voice Onset Time with Vocal Aging</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1556</first_page>
						<last_page>1560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-60</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xuanda18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moez</given_name>
<surname>Ajili</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solange</given_name>
<surname>Rossato</surname>
</person_name>
					</contributors>
					<titles><title>Voice Comparison and Rhythm: Behavioral Differences between Target and Non-target Comparisons</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1061</first_page>
						<last_page>1065</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-61</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ajili18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad Sadegh</given_name>
<surname>Rasooli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarangarajan</given_name>
<surname>Parthasarathy</surname>
</person_name>
					</contributors>
					<titles><title>Entity-Aware Language Model as an Unsupervised Reranker</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>406</first_page>
						<last_page>410</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-62</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rasooli18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John S.</given_name>
<surname>Novak III</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert V.</given_name>
<surname>Kenyon</surname>
</person_name>
					</contributors>
					<titles><title>Effects of User Controlled Speech Rate on Intelligibility in Noisy Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1853</first_page>
						<last_page>1857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-63</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/novakiii18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erica</given_name>
<surname>Gold</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sula</given_name>
<surname>Ross</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate</given_name>
<surname>Earnshaw</surname>
</person_name>
					</contributors>
					<titles><title>The ‘West Yorkshire Regional English Database’: Investigations into the Generalizability of Reference Populations for Forensic Speaker Comparison Casework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2748</first_page>
						<last_page>2752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-65</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gold18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Bartels</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horacio</given_name>
<surname>Franco</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Features for ASR of Pathological Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2958</first_page>
						<last_page>2962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-67</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ylmaz18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rob</given_name>
<surname>van Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Middag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name>
					</contributors>
					<titles><title>Vowel Space as a Tool to Evaluate Articulation Problems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>357</first_page>
						<last_page>361</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-68</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vanson18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyed Omid</given_name>
<surname>Sadjadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothee</given_name>
<surname>Kheyrkhah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Greenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elliot</given_name>
<surname>Singer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>Reynolds</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Mason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Hernandez-Cordero</surname>
</person_name>
					</contributors>
					<titles><title>Performance Analysis of the 2017 NIST Language Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1798</first_page>
						<last_page>1802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-69</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sadjadi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peixin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lanhua</given_name>
<surname>You</surname>
</person_name>
					</contributors>
					<titles><title>Gated Convolutional Neural Network for Sentence Matching</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2853</first_page>
						<last_page>2857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-70</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marie-Lou</given_name>
<surname>Barnaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juien</given_name>
<surname>Diard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Bessière</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Schwartz</surname>
</person_name>
					</contributors>
					<titles><title>COSMO SylPhon: A Bayesian Perceptuo-motor Model to Assess Phonological Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3786</first_page>
						<last_page>3790</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-73</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/barnaud18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oscar</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Active Memory Networks for Language Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3338</first_page>
						<last_page>3342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-78</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenji</given_name>
<surname>Nagamatsu</surname>
</person_name>
					</contributors>
					<titles><title>Lattice-free State-level Minimum Bayes Risk Training of Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2923</first_page>
						<last_page>2927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-79</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kanda18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeffrey</given_name>
<surname>Hetherly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Gamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria Alejandra</given_name>
<surname>Barrios</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cory</given_name>
<surname>Stephenson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karl</given_name>
<surname>Ni</surname>
</person_name>
					</contributors>
					<titles><title>Deep Speech Denoising with Vector Space Projections</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3474</first_page>
						<last_page>3478</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-83</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hetherly18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Levit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarangarajan</given_name>
<surname>Parthasarathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuangyu</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>What to Expect from Expected Kneser-Ney Smoothing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3378</first_page>
						<last_page>3382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-84</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/levit18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzhu</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Emotional Prosody Perception in Mandarin-speaking Congenital Amusics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2196</first_page>
						<last_page>2200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-91</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weicheng</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinkun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Length Normalization in End-to-End Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3618</first_page>
						<last_page>3622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-92</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Claudia</given_name>
<surname>Baur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Caines</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cathy</given_name>
<surname>Chua</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johanna</given_name>
<surname>Gerlach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manny</given_name>
<surname>Rayner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Russell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xizi</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>Overview of the 2018 Spoken CALL Shared Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2354</first_page>
						<last_page>2358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-97</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/baur18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juncheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>Comparing the Max and Noisy-Or Pooling Functions in Multiple Instance Learning for Weakly Supervised Sequence Learning Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1339</first_page>
						<last_page>1343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-990</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ragesh Rajan</given_name>
<surname>M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashwin</given_name>
<surname>Vijayakumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepu</given_name>
<surname>Vijayasenan</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of Aesthetic Elements in Karnatic Music: A Machine Learning Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2042</first_page>
						<last_page>2046</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-991</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/m18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koji</given_name>
<surname>Okabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Koshinaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>Attentive Statistics Pooling for Deep Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2252</first_page>
						<last_page>2256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-993</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/okabe18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenzo</given_name>
<surname>Spreafico</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Pucher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Matosova</surname>
</person_name>
					</contributors>
					<titles><title>UltraFit: A Speaker-friendly Headset for Ultrasound Recordings in Speech Science</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1517</first_page>
						<last_page>1520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-995</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/spreafico18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Schmitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabien</given_name>
<surname>Ringeval</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Bags in Bag: Generating Context-Aware Bags for Tracking Emotions from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3082</first_page>
						<last_page>3086</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-996</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/han18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fumiaki</given_name>
<surname>Taguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tokihiko</given_name>
<surname>Kaburagi</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory-to-speech Conversion Using Bi-directional Long Short-term Memory</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2499</first_page>
						<last_page>2503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-999</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/taguchi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Jülg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mario</given_name>
<surname>Kunstek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cem Philipp</given_name>
<surname>Freimoser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kay</given_name>
<surname>Berkling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>The CSU-K Rule-Based System for the 2nd Edition Spoken CALL Shared Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2359</first_page>
						<last_page>2363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1000</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/julg18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ming-Hsiang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun-Yi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian-Bei</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huai-Hung</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Follow-up Question Generation Using Pattern-based Seq2seq with a Small Corpus for Interview Coaching</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1006</first_page>
						<last_page>1010</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1007</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/su18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yougen</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheung-Chi</given_name>
<surname>Leung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongjie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>97</first_page>
						<last_page>101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1010</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yuan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Renkens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>van Hamme</surname>
</person_name>
					</contributors>
					<titles><title>Capsule Networks for Low Resource Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>601</first_page>
						<last_page>605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1013</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/renkens18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarthak</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atul</given_name>
<surname>Rai</surname>
</person_name>
					</contributors>
					<titles><title>Learning Discriminative Features for Speaker Identification and Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2237</first_page>
						<last_page>2241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1015</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yadav18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laxmi</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Nathwani</surname>
</person_name>
					</contributors>
					<titles><title>LSTM Based Attentive Fusion of Spectral and Prosodic Information for Keyword Spotting in Hindi Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>112</first_page>
						<last_page>116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1016</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pandey18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gunnam</given_name>
<surname>Aneeja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bayya</given_name>
<surname>Yegnanarayana</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Glottal Closure Instants in Degraded Speech Using Single Frequency Filtering Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2300</first_page>
						<last_page>2304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1018</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/aneeja18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Simone</given_name>
<surname>Hantke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Stemp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Annotator Trustability-based Cooperative Learning Solutions for Intelligent Audio Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3504</first_page>
						<last_page>3508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1019</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hantke18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaping</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenju</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>Deep Noise Tracking Network: A Hybrid Signal Processing/Deep Learning Approach to Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3219</first_page>
						<last_page>3223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1020</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nie18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yerbolat</given_name>
<surname>Khassanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised and Efficient Vocabulary Expansion for Recurrent Neural Network Language Models in ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3343</first_page>
						<last_page>3347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1021</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/khassanov18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3633</first_page>
						<last_page>3637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1023</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ding18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jessie S.</given_name>
<surname>Nixon</surname>
</person_name>
					</contributors>
					<titles><title>Effective Acoustic Cue Learning Is Not Just Statistical, It Is Discriminative</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1447</first_page>
						<last_page>1451</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1024</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nixon18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruoming</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Prabhavalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyog</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Cheng</given_name>
<surname>Chiu</surname>
</person_name>
					</contributors>
					<titles><title>Compression of End-to-End Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>27</first_page>
						<last_page>31</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1025</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>Postfiltering with Complex Spectral Correlations for Speech and Audio Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3538</first_page>
						<last_page>3542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1026</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/das18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Bäckström</surname>
</person_name>
					</contributors>
					<titles><title>Postfiltering Using Log-Magnitude Spectrum for Speech and Audio Coding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3543</first_page>
						<last_page>3547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1027</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/das18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangsen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengzhu</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Improving Attention Based Sequence-to-Sequence Models for End-to-End English Conversational Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>761</first_page>
						<last_page>765</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1030</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/weng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>RaviShankar</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suryakanth V</given_name>
<surname>Gangashetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bayya</given_name>
<surname>Yegnanarayana</surname>
</person_name>
					</contributors>
					<titles><title>Discriminating Nasals and Approximants in English Language Using Zero Time Windowing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>177</first_page>
						<last_page>181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1032</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/prasad18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Scalable Factorized Hierarchical Variational Autoencoder Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1462</first_page>
						<last_page>1466</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1034</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hsu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chetan</given_name>
<surname>Naik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arpit</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hancheng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathias</given_name>
<surname>Lambert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruhi</given_name>
<surname>Sarikaya</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Slot Carryover for Disparate Schemas</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>596</first_page>
						<last_page>600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1035</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/naik18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruizhi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Stream Attention for Distributed Multi-Microphone Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3033</first_page>
						<last_page>3037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1037</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elisabet Eir</given_name>
<surname>Cortes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Wlodarczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Consequences of Vocal Effort Elicitation Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1521</first_page>
						<last_page>1525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1038</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cortes18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuemin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiqun</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Lingual Multi-Task Neural Architecture for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>566</first_page>
						<last_page>570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1039</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanjun</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Togneri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Sreeram</surname>
</person_name>
					</contributors>
					<titles><title>Spoofing Detection Using Adaptive Weighting Framework and Clustering Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>626</first_page>
						<last_page>630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1042</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Farzaneh</given_name>
<surname>Ahmadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Designing a Pneumatic Bionic Voice Prosthesis - A Statistical Approach for Source Excitation Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3142</first_page>
						<last_page>3146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1043</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ahmadi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heewoong</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sukhyun</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyubyong</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namju</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonghun</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Training Utterance-level Embedding Networks for Speaker Identification and Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3563</first_page>
						<last_page>3567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1044</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/park18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ching-Hua</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhaskar D.</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harinath</given_name>
<surname>Garudadri</surname>
</person_name>
					</contributors>
					<titles><title>Bone-Conduction Sensor Assisted Noise Estimation for Improved Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1180</first_page>
						<last_page>1184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1046</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lee18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seshadri</given_name>
<surname>Shreyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marisa</given_name>
<surname>Casillas</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Syllabification Algorithms and Training Strategies for Robust Word Count Estimation across Different Languages and Recording Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1200</first_page>
						<last_page>1204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1047</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rasanen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>ShiLiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Modeling with DFSMN-CTC and Joint CTC-CE Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>771</first_page>
						<last_page>775</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1049</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Veronique</given_name>
<surname>Delvaux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kathy</given_name>
<surname>Huet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myriam</given_name>
<surname>Piccaluga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophie</given_name>
<surname>van Malderen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernard</given_name>
<surname>Harmegnies</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Better Characterization of Parkinsonian Speech: A Multidimensional Acoustic Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>362</first_page>
						<last_page>366</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1054</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/delvaux18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanh-Le</given_name>
<surname>Ha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Sperber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Low-Latency Neural Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1293</first_page>
						<last_page>1297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1055</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/niehues18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaiyu</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Structured Word Embedding for Low Memory Neural Network Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1254</first_page>
						<last_page>1258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1057</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruifang</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xu</given_name>
<surname>Bo</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Text-Independent Speaker Identification System on Short Utterances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3628</first_page>
						<last_page>3632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1058</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ji18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Narendra</given_name>
<surname>N P</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Classification Using Glottal Features Computed from Non-words, Words and Sentences</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3403</first_page>
						<last_page>3407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1059</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/np18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppina</given_name>
<surname>Turco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>Length Contrast and Covarying Features: Whistled Speech as a Case Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1843</first_page>
						<last_page>1847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1060</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ridouane18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>On the Usefulness of the Speech Phase Spectrum for Pitch Extraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>696</first_page>
						<last_page>700</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1062</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/loweimi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Semi-supervised Cross-domain Visual Feature Learning for Audio-Visual Broadcast Speech Transcription</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3509</first_page>
						<last_page>3513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1063</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/su18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Leemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephan</given_name>
<surname>Schmid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dieter</given_name>
<surname>Studer-Joho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie-José</given_name>
<surname>Kolly</surname>
</person_name>
					</contributors>
					<titles><title>Regional Variation of /r/ in Swiss German Dialects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2738</first_page>
						<last_page>2742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1065</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/leemann18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karel</given_name>
<surname>Beneš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>i-Vectors in Language Modeling: An Efficient Way of Domain Adaptation for Feed-Forward Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3383</first_page>
						<last_page>3387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1070</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/benes18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doris</given_name>
<surname>Mücke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bastian</given_name>
<surname>Auris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name>
					</contributors>
					<titles><title>Structural Effects on Properties of Consonantal Gestures in Tashlhiyt</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>197</first_page>
						<last_page>201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1074</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hermes18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name>
					</contributors>
					<titles><title>General Utterance-Level Feature Extraction for Classifying Crying Sounds, Atypical &#38; Self-Assessed Affect and Heart Beats</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>531</first_page>
						<last_page>535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1076</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gosztolya18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Markó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás Gábor</given_name>
<surname>Csapó</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Task Learning of Speech Recognition and Speech Synthesis Parameters for Ultrasound-based Silent Speech Interfaces</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3172</first_page>
						<last_page>3176</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1078</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/toth18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Bagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szilvia</given_name>
<surname>Szalóki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>István</given_name>
<surname>Szendi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ildikó</given_name>
<surname>Hoffmann</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Schizophrenia Based on Temporal Parameters in Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3408</first_page>
						<last_page>3412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1079</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gosztolya18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Tanihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Yonekura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasuo</given_name>
<surname>Kuniyoshi</surname>
</person_name>
					</contributors>
					<titles><title>Implementation of Respiration in Articulatory Synthesis Using a Pressure-Volume Lung Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2504</first_page>
						<last_page>2508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1080</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tanihara18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Speaker and Phonetic Diversity of Mismatched Language Resources for Unsupervised Subword Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2673</first_page>
						<last_page>2677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1081</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/feng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition System Development in the "Wild"</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2217</first_page>
						<last_page>2221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1085</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ragni18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linhao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Extending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>816</first_page>
						<last_page>820</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1086</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dong18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Kyriakopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Learning Approach to Assessing Non-native Pronunciation of English Using Phone Distances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1626</first_page>
						<last_page>1630</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1087</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kyriakopoulos18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martha</given_name>
<surname>Larson</surname>
</person_name>
					</contributors>
					<titles><title>The Conversation Continues: the Effect of Lyrics and Music Complexity of Background Music on Spoken-Word Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2280</first_page>
						<last_page>2284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1088</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/scharenborg18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Modeling with Densely Connected Residual Network for Multichannel Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1783</first_page>
						<last_page>1787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1089</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilia</given_name>
<surname>Parada-Cabaleiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Hantke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felix</given_name>
<surname>Burkhardt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>The Perception and Analysis of the Likeability and Human Likeness of Synthesized Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2863</first_page>
						<last_page>2867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1093</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/baird18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Szymański</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Mizgajski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Szymczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yishay</given_name>
<surname>Carmiel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Punctuation Prediction Model for Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2633</first_page>
						<last_page>2637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1096</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zelasko18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Adaptation with Interpretable Disentangled Representations for Distant Conversational Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1576</first_page>
						<last_page>1580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1097</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hsu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Mittag</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Packet-Loss Concealment Using Formant Features and Decision Tree Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1883</first_page>
						<last_page>1887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1098</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mittag18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Victor</given_name>
<surname>Soto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nishmar</given_name>
<surname>Cestero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Cognate Words, POS Tags and Entrainment in Code-Switching</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1938</first_page>
						<last_page>1942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1099</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/soto18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Monath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Ananthakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abishek</given_name>
<surname>Ravi</surname>
</person_name>
					</contributors>
					<titles><title>Play Duration Based User-Entity Affinity Modeling in Spoken Dialog System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2057</first_page>
						<last_page>2061</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1100</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xiao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego</given_name>
<surname>Castan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>van Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Lawson</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Complementary Information Sources in the Speaker Embeddings Framework</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3568</first_page>
						<last_page>3572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1102</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nandwana18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqiang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huibin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rujie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Double Joint Bayesian Modeling of DNN Local I-Vector for Text Dependent Speaker Verification with Random Digit Strings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>67</first_page>
						<last_page>71</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1103</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shi18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>TV</given_name>
<surname>Ananthapadmanabha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramakrishnan</given_name>
<surname>A G</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of the Vocal Tract Length of Vowel Sounds Based on the Frequency of the Significant Spectral Valley</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2102</first_page>
						<last_page>2106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1105</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ananthapadmanabha18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linhao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>791</first_page>
						<last_page>795</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1107</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhou18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianguo</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiang</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianrong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoshi</given_name>
<surname>Honda</surname>
</person_name>
					</contributors>
					<titles><title>Tongue Segmentation with Geometrically Constrained Snake Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3117</first_page>
						<last_page>3121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1108</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/su18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sinem</given_name>
<surname>Sonsaat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alif</given_name>
<surname>Silpachai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivana</given_name>
<surname>Lucic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Chukharev-Hudilainen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Levis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>L2-ARCTIC: A Non-native English Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2783</first_page>
						<last_page>2787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1110</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhao18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Improving Language Modeling with an Adversarial Critic for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3348</first_page>
						<last_page>3352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1111</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kei</given_name>
<surname>Akuzawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Iwasawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutaka</given_name>
<surname>Matsuo</surname>
</person_name>
					</contributors>
					<titles><title>Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3067</first_page>
						<last_page>3071</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1113</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/akuzawa18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiheng</given_name>
<surname>Ouyang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongjiang</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ping</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benoit</given_name>
<surname>Champagne</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Neural Network Based Harmonic Noise Model for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3224</first_page>
						<last_page>3228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1114</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ouyang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew C.</given_name>
<surname>Kelley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin V.</given_name>
<surname>Tucker</surname>
</person_name>
					</contributors>
					<titles><title>A Comparison of Input Types to a Deep Neural Network-based Forced Aligner</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1205</first_page>
						<last_page>1209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1115</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kelley18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shao-Yen</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juncheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Szurley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samarjit</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3279</first_page>
						<last_page>3283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1120</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tseng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Horgan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivek</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristina</given_name>
<surname>Vasco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Darcy</surname>
</person_name>
					</contributors>
					<titles><title>Voice Conversion with Conditional SampleRNN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1973</first_page>
						<last_page>1977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1121</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhou18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Behnam</given_name>
<surname>Hedayatnia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linda</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Gandhe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra</given_name>
<surname>Khatri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angeliki</given_name>
<surname>Metallinou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anu</given_name>
<surname>Venkatesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Language Model Adaptation for Conversational Agents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3333</first_page>
						<last_page>3337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1122</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/raju18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Di</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boon Pang</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuesong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deming</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Improved ASR for Under-resourced Languages through Multi-task Learning with Acoustic Landmarks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2618</first_page>
						<last_page>2622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1124</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/he18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samarendra</given_name>
<surname>Dandapat</surname>
</person_name>
					</contributors>
					<titles><title>Self-similarity Matrix Based Intelligibility Assessment of Cleft Lip and Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>367</first_page>
						<last_page>371</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1125</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kalita18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marija</given_name>
<surname>Tabain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Beare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Butcher</surname>
</person_name>
					</contributors>
					<titles><title>Formant Measures of Vowels Adjacent to Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Position</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2181</first_page>
						<last_page>2185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1126</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tabain18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Madhusudan</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debadatta</given_name>
<surname>Pati</surname>
</person_name>
					</contributors>
					<titles><title>Linear Prediction Residual based Short-term Cepstral Features for Replay Attacks Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>751</first_page>
						<last_page>755</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1128</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/singh18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phil</given_name>
<surname>Rose</surname>
</person_name>
					</contributors>
					<titles><title>Dialect-geographical Acoustic-Tonetics: Five Disyllabic Tone Sandhi Patterns in Cognate Words from the Wu Dialects of ZhèJiāNg Province</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2733</first_page>
						<last_page>2737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1130</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rose18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Voice Conversion Framework with Tandem Feature Sparse Representation and Speaker-Adapted WaveNet Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1978</first_page>
						<last_page>1982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1131</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sisman18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rif A.</given_name>
<surname>Saurous</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Recognition from Human Speech Using Temporal Information and Deep Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>937</first_page>
						<last_page>940</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1132</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Nicolson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuldip K.</given_name>
<surname>Paliwal</surname>
</person_name>
					</contributors>
					<titles><title>Bidirectional Long-Short Term Memory Network-based Estimation of Reliable Spectral Component Locations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1606</first_page>
						<last_page>1610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1134</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nicolson18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Disong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>Joint Noise and Reverberation Adaptive Learning for Robust Speaker DOA Estimation with an Acoustic Vector Sensor</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>821</first_page>
						<last_page>825</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1135</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Teng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kailai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-modal Attention Mechanisms in LSTM and Its Application to Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3328</first_page>
						<last_page>3332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1138</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Agha Ali</given_name>
<surname>Raza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Awais</given_name>
<surname>Athar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Randhawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zain</given_name>
<surname>Tariq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad Bilal</given_name>
<surname>Saleem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haris</given_name>
<surname>Bin Zia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umar</given_name>
<surname>Saif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roni</given_name>
<surname>Rosenfeld</surname>
</person_name>
					</contributors>
					<titles><title>Rapid Collection of Spontaneous Speech Corpora Using Telephonic Community Forums</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1021</first_page>
						<last_page>1025</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1139</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/raza18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laxmi</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurendra</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Namboodiri</surname>
</person_name>
					</contributors>
					<titles><title>Monoaural Audio Source Separation Using Variational Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3489</first_page>
						<last_page>3493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1140</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pandey18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Himawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Towsey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bradley</given_name>
<surname>Law</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Roe</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning Techniques for Koala Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2107</first_page>
						<last_page>2111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1143</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/himawan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jindřich</given_name>
<surname>Matoušek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Tihelka</surname>
</person_name>
					</contributors>
					<titles><title>Glottal Closure Instant Detection from Speech Signal Using Voting Classifier and Recursive Feature Elimination</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2112</first_page>
						<last_page>2116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1147</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/matousek18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>User Information Augmented Semantic Frame Parsing Using Progressive Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3464</first_page>
						<last_page>3468</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1149</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenglin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3479</first_page>
						<last_page>3483</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1150</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngmoon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Younggwan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeunju</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning Using Denoising Variational Autoencoders for Voice Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1210</first_page>
						<last_page>1214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1151</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jung18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Teng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kailai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Transformer Networks for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1349</first_page>
						<last_page>1353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1152</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lyan</given_name>
<surname>Verwimp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>van Hamme</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Renkens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Wambacq</surname>
</person_name>
					</contributors>
					<titles><title>State Gradients for RNN Memory Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1467</first_page>
						<last_page>1471</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1153</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/verwimp18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moquan</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilles</given_name>
<surname>Degottex</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Waveform-Based Speaker Representations for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>897</first_page>
						<last_page>901</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1154</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankit</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakti P</given_name>
<surname>Rath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendra</given_name>
<surname>Vepa</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Second-Order Log-Linear Model for Improved Deep Learning Based ASR Performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3738</first_page>
						<last_page>3742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1156</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/raj18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingke</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name>
					</contributors>
					<titles><title>Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3573</first_page>
						<last_page>3577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1158</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosi</given_name>
<surname>Mass</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slava</given_name>
<surname>Shechtman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moran</given_name>
<surname>Mordechay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron</given_name>
<surname>Hoory</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oren</given_name>
<surname>Sar Shalom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guy</given_name>
<surname>Lev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Konopnicki</surname>
</person_name>
					</contributors>
					<titles><title>Word Emphasis Prediction for Expressive Text to Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2868</first_page>
						<last_page>2872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1159</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mass18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinsuke</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Forward-Backward Attention Decoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2232</first_page>
						<last_page>2236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1160</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mimura18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanhua</given_name>
<surname>Long</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaen</given_name>
<surname>Liang</surname>
</person_name>
					</contributors>
					<titles><title>Active Learning for LF-MMI Trained Neural Networks in ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2898</first_page>
						<last_page>2902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1162</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/long18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Mateju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Cerva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jindrich</given_name>
<surname>Zdansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radek</given_name>
<surname>Safarik</surname>
</person_name>
					</contributors>
					<titles><title>Using Deep Neural Networks for Identification of Slavic Languages from Acoustic Signal</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1803</first_page>
						<last_page>1807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1165</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mateju18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brij Mohan Lal</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunayana</given_name>
<surname>Sitaram</surname>
</person_name>
					</contributors>
					<titles><title>Homophone Identification and Merging for Code-switched Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1943</first_page>
						<last_page>1947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1171</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/srivastava18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>B Ganga</given_name>
<surname>Gowri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K P</given_name>
<surname>Soman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>D</given_name>
<surname>Govind</surname>
</person_name>
					</contributors>
					<titles><title>Improved Epoch Extraction from Telephonic Speech Using Chebfun and Zero Frequency Filtering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2152</first_page>
						<last_page>2156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1173</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gowri18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avashna</given_name>
<surname>Govender</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Using Pupillometry to Measure the Cognitive Load of Synthetic Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2838</first_page>
						<last_page>2842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1174</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/govender18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahesh</given_name>
<surname>M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeena J</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Resyllabification in Indian Languages and Its Implications in Text-to-speech Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>212</first_page>
						<last_page>216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1176</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/m18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anju Leela</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anusha</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Baby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Code-switching in Indic Speech Synthesisers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1948</first_page>
						<last_page>1952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1178</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/thomas18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Improving Cross-Lingual Knowledge Transferability Using Multilingual TDNN-BLSTM with Language-Dependent Pre-Final Layer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2439</first_page>
						<last_page>2443</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1182</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/feng18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nattanun</given_name>
<surname>Chanchaochai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Cieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Japhet</given_name>
<surname>Debrah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishi</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Liberman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Wright</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juhong</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>GlobalTIMIT: Acoustic-Phonetic Datasets for the World’s Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>192</first_page>
						<last_page>196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1185</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chanchaochai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeena</given_name>
<surname>JPrakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Golda Brunet</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Transcription Correction for Indian Languages Using Acoustic Signatures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3177</first_page>
						<last_page>3181</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1188</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jprakash18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li-Juan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>WaveNet Vocoder with Limited Training Data for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1983</first_page>
						<last_page>1987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1190</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhi-Ping</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Rong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>Learning and Modeling Unit Embeddings for Improving HMM-based Unit Selection Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2509</first_page>
						<last_page>2513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1198</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhou18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avashna</given_name>
<surname>Govender</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Measuring the Cognitive Load of Synthetic Speech Using a Dual Task Paradigm</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2843</first_page>
						<last_page>2847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1199</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/govender18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Théo</given_name>
<surname>Biasutto-Lervat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Ouni</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme-to-Articulatory Mapping Using Bidirectional Gated RNN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3112</first_page>
						<last_page>3116</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1202</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/biasuttolervat18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nauman</given_name>
<surname>Dawalatabad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jom</given_name>
<surname>Kuriakose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra Sekhar</given_name>
<surname>Chellu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Information Bottleneck Based Percussion Instrument Diarization System for Taniavartanam Segments of Carnatic Music Concerts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1215</first_page>
						<last_page>1219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1203</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dawalatabad18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>ShiLiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haitao</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>Compact Feedforward Sequential Memory Networks for Small-footprint Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2663</first_page>
						<last_page>2667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1204</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Deep Extractor Network for Target Speaker Recovery from Single Channel Speech Mixtures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>307</first_page>
						<last_page>311</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1205</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vadim</given_name>
<surname>Shchemelinin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Shulipa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandr</given_name>
<surname>Kozlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Kremnev</surname>
</person_name>
					</contributors>
					<titles><title>Triplet Loss Based Cosine Similarity Metric Learning for Text-independent Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2242</first_page>
						<last_page>2246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1209</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/novoselov18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Chiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Kobayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick Lumban</given_name>
<surname>Tobing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Collapsed Speech Segment Detection and Suppression for WaveNet Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1988</first_page>
						<last_page>1992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1210</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raul</given_name>
<surname>Fernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Rosenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhuvana</given_name>
<surname>Ramabhadran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Sorin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation Improves Recognition of Foreign Accented Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2409</first_page>
						<last_page>2413</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1211</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fukuda18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Beck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirko</given_name>
<surname>Hannemann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Dötsch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Segmental Encoder-Decoder Models for Large Vocabulary Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>766</first_page>
						<last_page>770</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1212</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/beck18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guan-Ting</given_name>
<surname>Liou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen-Yu</given_name>
<surname>Chiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yih-Ru</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sin-Horng</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>An Exploration of Local Speaking Rate Variations in Mandarin Read Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>42</first_page>
						<last_page>46</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1214</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liou18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fasih</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fahim</given_name>
<surname>A. Salim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Owen</given_name>
<surname>Conlan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name>
					</contributors>
					<titles><title>An Active Feature Transformation Method for Attitude Recognition of Video Bloggers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>431</first_page>
						<last_page>435</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1222</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/haider18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A New Framework for Supervised Speech Enhancement in the Time Domain</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1136</first_page>
						<last_page>1140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1223</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pandey18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xavier</given_name>
<surname>Serra</surname>
</person_name>
					</contributors>
					<titles><title>Singing Voice Phoneme Segmentation by Hierarchically Inferring Syllable and Phoneme Onset Positions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>716</first_page>
						<last_page>720</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1224</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gong18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Vowels and Diphthongs in Hangzhou Wu Chinese Dialect</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>207</first_page>
						<last_page>211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1225</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yue18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael T.</given_name>
<surname>Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Embedding Extraction with Phonetic Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2247</first_page>
						<last_page>2251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1226</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hieu-Thi</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuyuki</given_name>
<surname>Nishizawa</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Accuracy of Pitch-accent Annotations in Neural Network-based Speech Synthesis and Denoising Effects</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>37</first_page>
						<last_page>41</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1227</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/luong18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manu</given_name>
<surname>Airaksinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Time-regularized Linear Prediction for Noise-robust Extraction of the Spectral Envelope of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>701</first_page>
						<last_page>705</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1230</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/airaksinen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre-Alexandre</given_name>
<surname>Broux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florent</given_name>
<surname>Desnous</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Petitrenaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean</given_name>
<surname>Carrive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Meignier</surname>
</person_name>
					</contributors>
					<titles><title>S4D: Speaker Diarization Toolkit in Python</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1368</first_page>
						<last_page>1372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1232</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/broux18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jane</given_name>
<surname>Mertens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doris</given_name>
<surname>Mücke</surname>
</person_name>
					</contributors>
					<titles><title>Age-related Effects on Sensorimotor Control of Speech Production</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1526</first_page>
						<last_page>1530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1233</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hermes18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tieqiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Single-channel Speech Dereverberation via Generative Adversarial Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1309</first_page>
						<last_page>1313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1234</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Baby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Verhulst</surname>
</person_name>
					</contributors>
					<titles><title>Biophysically-inspired Features Improve the Generalizability of Neural Network-based Speech Enhancement Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3264</first_page>
						<last_page>3268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1237</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/baby18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johannes</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Schiller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Seiderer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisabeth</given_name>
<surname>André</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning in Paralinguistic Recognition Tasks: Are Hand-crafted Features Still Relevant?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>147</first_page>
						<last_page>151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1238</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wagner18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Murakami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunao</given_name>
<surname>Hara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masanobu</given_name>
<surname>Abe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masaaki</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Minagi</surname>
</person_name>
					</contributors>
					<titles><title>Naturalness Improvement Algorithm for Reconstructed Glossectomy Patient's Speech Using Spectral Differential Modification in Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2464</first_page>
						<last_page>2468</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1239</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/murakami18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Selen Hande</given_name>
<surname>Kabil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Muckenhirn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>On Learning to Identify Genders from Raw Speech Signal Using CNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>287</first_page>
						<last_page>291</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1240</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kabil18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markus</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stüker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Neural Language Codes for Multilingual Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2419</first_page>
						<last_page>2423</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1241</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/muller18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3087</first_page>
						<last_page>3091</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1242</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liwen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwen</given_name>
<surname>Deng</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Temporal Feature Learning Based on Sparse Coding Embedded BoAW for Acoustic Event Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3284</first_page>
						<last_page>3288</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1243</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joachim</given_name>
<surname>Fainberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Learning to Adapt: A Meta-learning Approach for Speaker Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>867</first_page>
						<last_page>871</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1244</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/klejch18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qinglin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nengheng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ambika Prasad</given_name>
<surname>Mishra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacinta Dan</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan W. H.</given_name>
<surname>Schnupp</surname>
</person_name>
					</contributors>
					<titles><title>Weighting Pitch Contour and Loudness Contour in Mandarin Tone Perception in Cochlear Implant Listeners</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3768</first_page>
						<last_page>3771</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1245</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/meng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longting</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Co-whitening of I-vectors for Short and Long Duration Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1066</first_page>
						<last_page>1070</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1246</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Feng</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mari</given_name>
<surname>Ostendorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mei-Yuh</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Training Augmentation with Adversarial Examples for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2404</first_page>
						<last_page>2408</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1247</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sun18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haipeng</given_name>
<surname>Lan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Pang</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Concurrent Sound Source Tracking Based on Observation-Guided Adaptive Particle Filter</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>826</first_page>
						<last_page>830</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1248</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhang</given_name>
<surname>Teng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kailai</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Data Independent Sequence Augmentation Method for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3289</first_page>
						<last_page>3293</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1250</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/teng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akhilesh Kumar</given_name>
<surname>Dubey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Dandapat</surname>
</person_name>
					</contributors>
					<titles><title>Pitch-Adaptive Front-end Feature for Hypernasality Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>372</first_page>
						<last_page>376</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1251</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dubey18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zbyněk</given_name>
<surname>Zajíc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Kunešová</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Zelinka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marek</given_name>
<surname>Hrúz</surname>
</person_name>
					</contributors>
					<titles><title>ZCU-NTIS Speaker Diarization System for the DIHARD 2018 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2788</first_page>
						<last_page>2792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1252</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zajic18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuarik</given_name>
<surname>Buanzur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Zellers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saudah</given_name>
<surname>Namyalo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alena</given_name>
<surname>Witzlack-Makarevich</surname>
</person_name>
					</contributors>
					<titles><title>A First Investigation of the Timing of Turn-taking in Ruuli</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>621</first_page>
						<last_page>625</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1254</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/buanzur18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nidia</given_name>
<surname>Hernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bianca</given_name>
<surname>Vieru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Temporal Reduction in Dialectal Spanish: A Large-scale Study of Lenition of Voiced Stops and Coda-s</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2728</first_page>
						<last_page>2732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1256</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vasilescu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kanru</given_name>
<surname>Hua</surname>
</person_name>
					</contributors>
					<titles><title>Nebula: F0 Estimation and Voicing Detection by Modeling the Statistical Properties of Feature Extractors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>337</first_page>
						<last_page>341</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1258</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hua18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreeram</given_name>
<surname>Ganji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Sinha</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Approach for Effective Recognition of the Code-Switched Data on Monolingual Language Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1953</first_page>
						<last_page>1957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1259</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ganji18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Szu-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aswin Shanmugam</given_name>
<surname>Subramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Building State-of-the-art Distant Speech Recognition Using the CHiME-4 Challenge with a Setup of Speech Enhancement Baseline</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1571</first_page>
						<last_page>1575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1262</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Imed</given_name>
<surname>Laaridh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Tardieu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cynthia</given_name>
<surname>Magnen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Gaillard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jérôme</given_name>
<surname>Farinas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Pinquier</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual and Automatic Evaluations of the Intelligibility of Speech Degraded by Noise Induced Hearing Loss Simulation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2953</first_page>
						<last_page>2957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1264</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/laaridh18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning Based Progressive Neural Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>907</first_page>
						<last_page>911</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1265</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Imed</given_name>
<surname>Laaridh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Fredouille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alain</given_name>
<surname>Ghio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muriel</given_name>
<surname>Lalain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Virginie</given_name>
<surname>Woisard</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Evaluation of Speech Intelligibility Based on I-vectors in the Context of Head and Neck Cancers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2943</first_page>
						<last_page>2947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1266</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/laaridh18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chitralekha</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Pronunciation Evaluation of Singing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1507</first_page>
						<last_page>1511</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1267</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gupta18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weipeng</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Odobez</surname>
</person_name>
					</contributors>
					<titles><title>Joint Localization and Classification of Multiple Sound Sources Using a Multi-task Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>312</first_page>
						<last_page>316</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1269</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/he18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujia</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Soong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenping</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Paired Phone-Posteriors Approach to ESL Pronunciation Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1631</first_page>
						<last_page>1635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1270</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xiao18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noelia</given_name>
<surname>Do Carmo Blanco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michel</given_name>
<surname>Hoen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanny</given_name>
<surname>Meunier</surname>
</person_name>
					</contributors>
					<titles><title>Phoneme Resistance and Phoneme Confusion in Noise: Impact of Dyslexia</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2290</first_page>
						<last_page>2294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1271</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/docarmoblanco18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Liberatore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Improving Sparse Representations in Exemplar-Based Voice Conversion with a Phoneme-Selective Objective Function</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>476</first_page>
						<last_page>480</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1272</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ding18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name>
					</contributors>
					<titles><title>A Generalization of PLDA for Joint Modeling of Speaker Identity and Multiple Nuisance Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>82</first_page>
						<last_page>86</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1280</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ferrer18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Glottal Closure Instants from Speech Signals: A Convolutional Neural Network Based Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>317</first_page>
						<last_page>321</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1281</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenda</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy F.</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Topic and Keyword Identification for Low-resourced Speech Using Cross-Language Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2047</first_page>
						<last_page>2051</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1283</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Barbara E.</given_name>
<surname>Bullock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gualberto</given_name>
<surname>Guzmán</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacqueline</given_name>
<surname>Serigos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Almeida Jacqueline</given_name>
<surname>Toribio</surname>
</person_name>
					</contributors>
					<titles><title>Should Code-switching Models Be Asymmetric?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2534</first_page>
						<last_page>2538</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1284</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bullock18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hui</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biao</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Visual Timing Information in Audiovisual Speech Perception: Evidence from Lexical Tone Contour</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3781</first_page>
						<last_page>3785</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1285</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xie18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gérard</given_name>
<surname>Bailly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>A Weighted Superposition of Functional Contours Model for Modelling Contextual Prominence of Elementary Prosodic Contours</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2524</first_page>
						<last_page>2528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1286</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gerazov18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susan Shur-Fen</given_name>
<surname>Gau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>An Interlocutor-Modulated Attentional LSTM for Differentiating between Subgroups of Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2329</first_page>
						<last_page>2333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1288</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lin18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katsuhiko</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshio</given_name>
<surname>Irino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Narumi</given_name>
<surname>Ohashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Multi-resolution Gammachirp Envelope Distortion Index for Intelligibility Prediction of Noisy Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1863</first_page>
						<last_page>1867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1291</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yamamoto18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Margarita</given_name>
<surname>Kotti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilios</given_name>
<surname>Diakoloukas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Papangelis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michail</given_name>
<surname>Lagoudakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>A Case Study on the Importance of Belief State Representation for Dialogue Policy Management</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>986</first_page>
						<last_page>990</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1293</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kotti18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jishnu</given_name>
<surname>Sadasivan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhadip</given_name>
<surname>Mukherjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra Sekhar</given_name>
<surname>Seelamantula</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement Using the Minimum-probability-of-error Criterion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1141</first_page>
						<last_page>1145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1294</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sadasivan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Liberatore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricardo</given_name>
<surname>Gutierrez-Osuna</surname>
</person_name>
					</contributors>
					<titles><title>Learning Structured Dictionaries for Exemplar-based Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>481</first_page>
						<last_page>485</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1295</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ding18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Mack</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumitro</given_name>
<surname>Chakrabarty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabian-Robert</given_name>
<surname>Stöter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Edler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuël</given_name>
<surname>Habets</surname>
</person_name>
					</contributors>
					<titles><title>Single-Channel Dereverberation Using Direct MMSE Optimization and Bidirectional LSTM Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1314</first_page>
						<last_page>1318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1296</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mack18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarfaraz</given_name>
<surname>Jelil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Sinha</surname>
</person_name>
					</contributors>
					<titles><title>Exploration of Compressed ILPR Features for Replay Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>631</first_page>
						<last_page>635</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1297</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jelil18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeng-Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ming</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chip-Jin</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Learning Conditional Acoustic Latent Representation with Gender and Age Attributes for Automatic Pain Level Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3438</first_page>
						<last_page>3442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1298</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongwei</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwen</given_name>
<surname>Deng</surname>
</person_name>
					</contributors>
					<titles><title>A Compact and Discriminative Feature Based on Auditory Summary Statistics for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3294</first_page>
						<last_page>3298</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1299</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/song18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Neil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendar</given_name>
<surname>Anumula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enea</given_name>
<surname>Ceolini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Chii</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Multi-channel Attention for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>17</first_page>
						<last_page>21</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1301</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/braun18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhargav</given_name>
<surname>Pulugundla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Egorova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Karafiát</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>BUT System for Low Resource Indian Language ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3182</first_page>
						<last_page>3186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1302</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pulugundla18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>Deep Metric Learning for the Target Cost in Unit-Selection Speech Synthesizer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2514</first_page>
						<last_page>2518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1305</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kévin</given_name>
<surname>Vythelingum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Rosec</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-dependent Phonemic Transcription for Text-to-speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2489</first_page>
						<last_page>2493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1306</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vythelingum18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Godard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcely Zanon</given_name>
<surname>Boito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ondel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Berard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Yvon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aline</given_name>
<surname>Villavicencio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Besacier</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Word Segmentation from Speech with Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2678</first_page>
						<last_page>2682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1308</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/godard18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huy</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramon</given_name>
<surname>Prieto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Liulishuo's System for the Spoken CALL Shared Task 2018</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2364</first_page>
						<last_page>2368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1309</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nguyen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gurunath Reddy</given_name>
<surname>M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sreenivasa</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Partha Pratim</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Harmonic-Percussive Source Separation of Polyphonic Music by Suppressing Impulsive Noise Events</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>831</first_page>
						<last_page>835</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1310</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/m18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kate</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Kyriakopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Malinin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Caines</surname>
</person_name>
					</contributors>
					<titles><title>Impact of ASR Performance on Free Speaking Language Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1641</first_page>
						<last_page>1645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1312</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/knill18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kai-Zhan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>A Comparison of Speaker-based and Utterance-based Data Selection for Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2873</first_page>
						<last_page>2877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1313</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lee18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markus</given_name>
<surname>Toman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Geoffrey S.</given_name>
<surname>Meltzner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupal</given_name>
<surname>Patel</surname>
</person_name>
					</contributors>
					<titles><title>Data Requirements, Selection and Augmentation for DNN-based Speech Synthesis from Crowdsourced Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2878</first_page>
						<last_page>2882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1316</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/toman18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anjuli</given_name>
<surname>Kannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diana</given_name>
<surname>Jaunzeikare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alvin</given_name>
<surname>Rajkomar</surname>
</person_name>
					</contributors>
					<titles><title>Semi-supervised Learning for Information Extraction from Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2077</first_page>
						<last_page>2081</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1318</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kannan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jim X.</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name>
					</contributors>
					<titles><title>Anomaly Detection Approach for Pronunciation Verification of Disordered Speech Using Speech Attribute Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1671</first_page>
						<last_page>1675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1319</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shahin18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chadi</given_name>
<surname>Farah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephane</given_name>
<surname>Roman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariapaola</given_name>
<surname>D'Imperio</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Focus Acquisition in French Early Cochlear Implanted Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2977</first_page>
						<last_page>2981</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1320</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/farah18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Lopez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name>
					</contributors>
					<titles><title>Low-Resource Speech-to-Text Translation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1298</first_page>
						<last_page>1302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1326</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bansal18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Che-Wei</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Stochastic Shake-Shake Regularization for Affective Learning from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3658</first_page>
						<last_page>3662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1327</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Ateeq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abualsoud</given_name>
<surname>Hanani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aziz</given_name>
<surname>Qaroush</surname>
</person_name>
					</contributors>
					<titles><title>An Optimization Based Approach for Solving Spoken CALL Shared Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2369</first_page>
						<last_page>2373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1328</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ateeq18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Claude</given_name>
<surname>Montacié</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie-José</given_name>
<surname>Caraty</surname>
</person_name>
					</contributors>
					<titles><title>Vocalic, Lexical and Prosodic Cues for the INTERSPEECH 2018 Self-Assessed Affect Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>541</first_page>
						<last_page>545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1331</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/montacie18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Grant P.</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanthashree Mysore</given_name>
<surname>Sathyendra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanislav</given_name>
<surname>Peshterliev</surname>
</person_name>
					</contributors>
					<titles><title>Statistical Model Compression for Small-Footprint Natural Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>571</first_page>
						<last_page>575</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1333</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/strimel18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lionel</given_name>
<surname>Fontan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxime</given_name>
<surname>Le Coz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Detey</surname>
</person_name>
					</contributors>
					<titles><title>Automatically Measuring L2 Speech Fluency without the Need of ASR: A Proof-of-concept Study with Japanese Learners of French</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2544</first_page>
						<last_page>2548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1336</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fontan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Luitjens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>A GPU-based WFST Decoder with Exact Lattice Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2212</first_page>
						<last_page>2216</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1339</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hagai</given_name>
<surname>Taitelbaum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehud</given_name>
<surname>Ben-Reuven</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Goldberger</surname>
</person_name>
					</contributors>
					<titles><title>Adding New Classes without Access to the Original Training Data with Applications to Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1808</first_page>
						<last_page>1812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1342</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/taitelbaum18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanmay</given_name>
<surname>Parekh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>Dual Language Models for Code Switched Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2598</first_page>
						<last_page>2602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1343</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/garg18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Andrés-Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Bodenstab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Vozila</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Language Model Adaptation with Noise Contrastive Estimation and Kullback-Leibler Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3368</first_page>
						<last_page>3372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1345</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/andresferrer18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pei-Hung</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan</given_name>
<surname>Tung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Lun</given_name>
<surname>Tai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning of Interactive Spoken Content Retrieval and Trainable User Simulator</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2032</first_page>
						<last_page>2036</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1346</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chung18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Kraljevski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diane</given_name>
<surname>Hirschfeld</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Correction Turns in Multilingual Dialogue Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>591</first_page>
						<last_page>595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1348</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kraljevski18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesin</given_name>
<surname>James</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Inez Watson</surname>
</person_name>
					</contributors>
					<titles><title>An Open Source Emotional Speech Corpus for Human Robot Interaction Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2768</first_page>
						<last_page>2772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1349</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/james18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ming</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Grabek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Role of L1 in Automatic Pronunciation Evaluation of L2 Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1636</first_page>
						<last_page>1640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1350</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huiyi</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Soraghan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anja</given_name>
<surname>Lowit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaetano</given_name>
<surname>Di-Caterina</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Learning Method for Pathological Voice Detection Using Convolutional Deep Belief Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>446</first_page>
						<last_page>450</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1351</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Máté Ákos</given_name>
<surname>Tündik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>György</given_name>
<surname>Szaszák</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>András</given_name>
<surname>Beke</surname>
</person_name>
					</contributors>
					<titles><title>User-centric Evaluation of Automatic Punctuation in ASR Closed Captioning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2628</first_page>
						<last_page>2632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1352</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tundik18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mousmita</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pegah</given_name>
<surname>Ghahremani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nagendra Kumar</given_name>
<surname>Goel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kandarpa Kumar</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Identification from Raw Speech Signals Using DNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3097</first_page>
						<last_page>3101</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1353</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sarma18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olympia</given_name>
<surname>Simantiraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Different Speech Types on Listening Effort</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2267</first_page>
						<last_page>2271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1358</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/simantiraki18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arindam</given_name>
<surname>Jati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name>
					</contributors>
					<titles><title>An Unsupervised Neural Prediction Framework for Learning Speaker Embeddings Using Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1131</first_page>
						<last_page>1135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1363</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jati18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tae Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Speaker Segmentation and Diarization Using Lexical and Acoustic Cues via Sequence to Sequence Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1373</first_page>
						<last_page>1377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1364</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/park18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>KaWai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Training Recurrent Neural Network through Moment Matching for NLP Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3353</first_page>
						<last_page>3357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1369</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/deng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinxi</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiyuan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Filter Sampling and Combination CNN (FSC-CNN): A Compact CNN Model for Small-footprint ASR Acoustic Modeling Using Raw Waveforms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3713</first_page>
						<last_page>3717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1370</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/guo18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Poellabauer</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2698</first_page>
						<last_page>2702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1371</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gong18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xizi</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Jančovič</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Russell</surname>
</person_name>
					</contributors>
					<titles><title>The University of Birmingham 2018 Spoken CALL Shared Task Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2374</first_page>
						<last_page>2378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1372</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/qian18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hansjörg</given_name>
<surname>Mixdorff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew K. H.</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelika</given_name>
<surname>Hönemann</surname>
</person_name>
					</contributors>
					<titles><title>Cross-cultural (A)symmetries in Audio-visual Attitude Perception</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>426</first_page>
						<last_page>430</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1373</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mixdorff18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jasper</given_name>
<surname>Ooster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of Perceived Speech Quality Using Deep Machine Listening</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>976</first_page>
						<last_page>980</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1374</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ooster18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul</given_name>
<surname>Kranzusch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rainer</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Krüger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Birger</given_name>
<surname>Kollmeier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of Subjective Listening Effort from Acoustic Data with Non-Intrusive Deep Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>981</first_page>
						<last_page>985</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1375</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kranzusch18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linxue</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Jančovič</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Russell</surname>
</person_name>
					</contributors>
					<titles><title>Phone Recognition Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3753</first_page>
						<last_page>3757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1376</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/qian18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Tzinis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christos</given_name>
<surname>Baziotis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Recurrence Dynamics for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>927</first_page>
						<last_page>931</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1377</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tzinis18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shahram</given_name>
<surname>Ghorbani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Native Language Information for Improved Accented Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2449</first_page>
						<last_page>2453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1378</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ghorbani18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Reinforcement Learning Based Multimodal Coaching Model (DCM) for Slot Filling in Spoken Language Understanding(SLU)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3444</first_page>
						<last_page>3448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1379</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Bruguier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiga</given_name>
<surname>Zen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arkady</given_name>
<surname>Arkhangorodsky</surname>
</person_name>
					</contributors>
					<titles><title>Sequence-to-sequence Neural Network Model with 2D Attention for Learning Japanese Pitch Accents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1284</first_page>
						<last_page>1287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1381</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bruguier18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erica</given_name>
<surname>Gold</surname>
</person_name>
					</contributors>
					<titles><title>Articulation Rate as a Speaker Discriminant in British English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1828</first_page>
						<last_page>1832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1384</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gold18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quy-Thao</given_name>
<surname>Truong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsuneo</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seiichi</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of L2 English Word Prosody Using Weighted Distances of F0 and Intensity Contours</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2186</first_page>
						<last_page>2190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1386</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/truong18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavlos</given_name>
<surname>Papadopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Vaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Relationship between Conic Affinity of NMF Dictionaries and Speech Enhancement Metrics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1146</first_page>
						<last_page>1150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1387</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/papadopoulos18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srinivas</given_name>
<surname>Parthasarathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3698</first_page>
						<last_page>3702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1391</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/parthasarathy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anuroop</given_name>
<surname>Sriram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heewoo</given_name>
<surname>Jun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Satheesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Coates</surname>
</person_name>
					</contributors>
					<titles><title>Cold Fusion: Training Seq2Seq Models Together with Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>387</first_page>
						<last_page>391</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1392</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sriram18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md</given_name>
<surname>Nasir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Baucom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name>
					</contributors>
					<titles><title>Towards an Unsupervised Entrainment Distance in Conversational Speech Using Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3423</first_page>
						<last_page>3427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1395</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nasir18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amber</given_name>
<surname>Afshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinxi</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soo Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijay</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Flint</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Effectiveness of Voice Quality Features in Detecting Depression</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1676</first_page>
						<last_page>1680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1399</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/afshan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Triantafyllos</given_name>
<surname>Afouras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Zisserman</surname>
</person_name>
					</contributors>
					<titles><title>The Conversation: Deep Audio-Visual Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3244</first_page>
						<last_page>3248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1400</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/afouras18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soo Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amber</given_name>
<surname>Afshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhi Ming</given_name>
<surname>Chua</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>Using Voice Quality Supervectors for Affect Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>157</first_page>
						<last_page>161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1401</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/park18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Output-Gate Projected Gated Recurrent Unit for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1793</first_page>
						<last_page>1797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1403</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cheng18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phil</given_name>
<surname>Howson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Kochetov</surname>
</person_name>
					</contributors>
					<titles><title>Gestural Lenition of Rhotics Captures Variation in Brazilian Portuguese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>182</first_page>
						<last_page>186</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1404</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/howson18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3229</first_page>
						<last_page>3233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1405</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Two-Stage Approach to Noisy Cochannel Speech Separation with Gated Residual Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3484</first_page>
						<last_page>3488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1406</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tan18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitriy</given_name>
<surname>Serdyuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshua</given_name>
<surname>Bengio</surname>
</person_name>
					</contributors>
					<titles><title>Twin Regularization for Online Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3718</first_page>
						<last_page>3722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1407</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ravanelli18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Recurrent Neural Network Language Model Adaptation for Conversational Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3373</first_page>
						<last_page>3377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1413</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahsa</given_name>
<surname>Yarmohammadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3743</first_page>
						<last_page>3747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1417</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/povey18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Megan</given_name>
<surname>Willi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie A.</given_name>
<surname>Borrie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyson S.</given_name>
<surname>Barrett</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name>
					</contributors>
					<titles><title>A Discriminative Acoustic-Prosodic Approach for Measuring Local Entrainment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>581</first_page>
						<last_page>585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1419</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/willi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqiang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huibin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rujie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Latent Factor Analysis of Deep Bottleneck Features for Speaker Verification with Random Digit Strings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1081</first_page>
						<last_page>1085</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1422</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shi18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hossein</given_name>
<surname>Hadian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hossein</given_name>
<surname>Sameti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end Speech Recognition Using Lattice-free MMI</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>12</first_page>
						<last_page>16</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1423</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hadian18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sei</given_name>
<surname>Ueno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinsuke</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshikazu</given_name>
<surname>Yamaguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Encoder Transfer for Attention-based Acoustic-to-word Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2424</first_page>
						<last_page>2428</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1424</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ueno18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Chiba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Nose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akinori</given_name>
<surname>Ito</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Effect of Physical Expression on English Proficiency for Multimodal Computer-Assisted Language Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1746</first_page>
						<last_page>1750</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1425</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wu18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangjian</given_name>
<surname>Tian</surname>
</person_name>
					</contributors>
					<titles><title>Long Distance Voice Channel Diagnosis Using Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2968</first_page>
						<last_page>2971</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1428</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/qin18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Masataki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Neural Error Corrective Language Models for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>401</first_page>
						<last_page>405</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1430</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tanaka18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debayan</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>R</given_name>
<surname>Muralishankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Gurugopinath</surname>
</person_name>
					</contributors>
					<titles><title>Robust Voice Activity Detection Using Frequency Domain Long-Term Differential Entropy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1220</first_page>
						<last_page>1224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1431</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ghosh18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Lian</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3673</first_page>
						<last_page>3677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1432</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huang18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ravi</given_name>
<surname>Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C M</given_name>
<surname>Vikram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R M</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Keyword Detection Using Joint DTW-CNN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>117</first_page>
						<last_page>121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1436</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shankar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Auxiliary Feature Based Adaptation of End-to-end ASR Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2444</first_page>
						<last_page>2448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1438</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/delcroix18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Chai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Error Modeling via Asymmetric Laplace Distribution for Deep Neural Network Based Single-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3269</first_page>
						<last_page>3273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1439</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Hara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuya</given_name>
<surname>Takanashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of Turn-taking Using Multitask Learning with Prediction of Backchannels and Fillers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>991</first_page>
						<last_page>995</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1442</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hara18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fahimeh</given_name>
<surname>Bahmaninezhad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Compensation for Domain Mismatch in Text-independent Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1071</first_page>
						<last_page>1075</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1446</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bahmaninezhad18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajay</given_name>
<surname>Srinivasamurthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mittul</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youssef</given_name>
<surname>Oualil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Kleinert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heiko</given_name>
<surname>Ehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hartmut</given_name>
<surname>Helmke</surname>
</person_name>
					</contributors>
					<titles><title>Iterative Learning of Speech Recognition Models for Air Traffic Control</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3519</first_page>
						<last_page>3523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1447</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/srinivasamurthy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Improving DNNs Trained with Non-Native Transcriptions Using Knowledge Distillation and Target Interpolation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2434</first_page>
						<last_page>2438</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1450</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/das18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengzhu</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunlei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>A Multistage Training Framework for Acoustic-to-Word Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>786</first_page>
						<last_page>790</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1452</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pegah</given_name>
<surname>Ghahremani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hossein</given_name>
<surname>Hadian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Modeling from Frequency Domain Representations of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1596</first_page>
						<last_page>1600</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1453</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ghahremani18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Colleen</given_name>
<surname>Richey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria A.</given_name>
<surname>Barrios</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeb</given_name>
<surname>Armstrong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Bartels</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horacio</given_name>
<surname>Franco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Graciarena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Lawson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Allen</given_name>
<surname>Stauffer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>van Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Gamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeffrey</given_name>
<surname>Hetherly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cory</given_name>
<surname>Stephenson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karl</given_name>
<surname>Ni</surname>
</person_name>
					</contributors>
					<titles><title>Voices Obscured in Complex Environmental Settings (VOiCES) Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1566</first_page>
						<last_page>1570</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1454</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/richey18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeng-Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Encoding Individual Acoustic Features Using Dyad-Augmented Deep Variational Representations for Dialog-level Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3102</first_page>
						<last_page>3106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1455</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiro</given_name>
<surname>Nishitoba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Unno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nelson</given_name>
<surname>Enrique Yalta Soplin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahn</given_name>
<surname>Heymann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adithya</given_name>
<surname>Renduchintala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name>
					</contributors>
					<titles><title>ESPnet: End-to-End Speech Processing Toolkit</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2207</first_page>
						<last_page>2211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1456</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/watanabe18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Kochetov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Faytak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiranpreet</given_name>
<surname>Nara</surname>
</person_name>
					</contributors>
					<titles><title>The Retroflex-dental Contrast in Punjabi Stops and Nasals: A Principal Component Analysis of Ultrasound Images</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>202</first_page>
						<last_page>206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1457</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kochetov18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Fast Derivation of Cross-lingual Document Vectors from Self-attentive Neural Machine Translation Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>107</first_page>
						<last_page>111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1459</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Sone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toru</given_name>
<surname>Nakashika</surname>
</person_name>
					</contributors>
					<titles><title>DNN-based Speech Synthesis for Small Data Sets Considering Bidirectional Speech-Text Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2519</first_page>
						<last_page>2523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1460</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sone18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rajat</given_name>
<surname>Hebbar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Somandepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Improving Gender Identification in Movie Audio Using Cross-Domain Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>282</first_page>
						<last_page>286</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1462</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hebbar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Midia</given_name>
<surname>Yousefi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Navid</given_name>
<surname>Shokouhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Assessing Speaker Engagement in 2-Person Debates: Overlap Detection in United States Presidential Debates</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2117</first_page>
						<last_page>2121</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1463</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yousefi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prasanna</given_name>
<surname>Kothalkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johanna</given_name>
<surname>Rudolph</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Dollaghan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>McGlothlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Campbell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Fusing Text-dependent Word-level i-Vector Models to Screen ‘at Risk’ Child Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1681</first_page>
						<last_page>1685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1465</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kothalkar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ram Charan</given_name>
<surname>Chandra Shekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hussnain</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Testing Paradigms for Assistive Hearing Devices in Diverse Acoustic Environments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1686</first_page>
						<last_page>1690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1471</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chandrashekar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>BLSTM-CRF Based End-to-End Prosodic Boundary Prediction with Context Sensitive Embeddings in a Text-to-Speech Front-End</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>47</first_page>
						<last_page>51</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1472</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zheng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tharshini</given_name>
<surname>Gunendradasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buddhi</given_name>
<surname>Wickramasinghe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Phu</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Replay-Spoofing Attacks Using Frequency Modulation Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>636</first_page>
						<last_page>640</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1473</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gunendradasan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryoichi</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Improving CTC-based Acoustic Model with Very Deep Residual Time-delay Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3708</first_page>
						<last_page>3712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1475</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olga</given_name>
<surname>Maxwell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elinor</given_name>
<surname>Payne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosey</given_name>
<surname>Billington</surname>
</person_name>
					</contributors>
					<titles><title>Homogeneity vs Heterogeneity in Indian English: Investigating Influences of L1 on f0 Range</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2191</first_page>
						<last_page>2195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1476</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/maxwell18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziping</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haishuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiqin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>272</first_page>
						<last_page>276</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1477</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhao18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pulkit</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshul</given_name>
<surname>Thakur</surname>
</person_name>
					</contributors>
					<titles><title>ASe: Acoustic Scene Embedding Using Deep Archetypal Analysis and GMM</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3299</first_page>
						<last_page>3303</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1481</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sharma18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Deep Learning for Acoustic Echo Cancellation in Noisy and Double-Talk Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3239</first_page>
						<last_page>3243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1484</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changliang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Layer Trajectory LSTM</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1768</first_page>
						<last_page>1772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1485</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshay</given_name>
<surname>Chandrashekaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungsuk</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Lane</surname>
</person_name>
					</contributors>
					<titles><title>Densely Connected Networks for Conversational Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>796</first_page>
						<last_page>800</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1486</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/han18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>G. Nisha</given_name>
<surname>Meenakshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Whispered Speech to Neutral Speech Conversion Using Bidirectional LSTMs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>491</first_page>
						<last_page>495</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1487</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/meenakshi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saranya</given_name>
<surname>M S</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Decision-level Feature Switching as a Paradigm for Replay Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>686</first_page>
						<last_page>690</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1494</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ms18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Wavelet Analysis of Speaker Dependent and Independent Prosody for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>52</first_page>
						<last_page>56</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1499</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sisman18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqiang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huibin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rujie</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning of J-Vector Extractor and Joint Bayesian Model for Text Dependent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1076</first_page>
						<last_page>1080</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1500</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shi18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinghua</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lifa</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>496</first_page>
						<last_page>500</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1504</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongguo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-task WaveNet: A Multi-task Generative Model for Statistical Parametric Speech Synthesis without Fundamental Frequency Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2007</first_page>
						<last_page>2011</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1506</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nadee</given_name>
<surname>Seneviratne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Sivaraman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Noise Robust Acoustic to Articulatory Speech Inversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3137</first_page>
						<last_page>3141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1509</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/seneviratne18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongguo</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenyu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>EMPHASIS: An Emotional Phoneme-based Acoustic Model for Speech Synthesis System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3077</first_page>
						<last_page>3081</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1511</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsuyoki</given_name>
<surname>Ujiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroyoshi</given_name>
<surname>Adachi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Kazui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manabu</given_name>
<surname>Ikeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Kudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Dementia from Responses to Atypical Questions Asked by Embodied Conversational Agents</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1691</first_page>
						<last_page>1695</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1514</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ujiro18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhifu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name>
					</contributors>
					<titles><title>An Improved Deep Embedding Learning Method for Short Duration Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3578</first_page>
						<last_page>3582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1515</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manoj</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pooja</given_name>
<surname>Chebolu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>So Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kassandra</given_name>
<surname>Martinez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lord</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>A Knowledge Driven Structural Segmentation Approach for Play-Talk Classification During Autism Assessment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2763</first_page>
						<last_page>2767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1516</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kumar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Flemotomos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Martinez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Atkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torrey</given_name>
<surname>Creed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Language Features for Automated Evaluation of Cognitive Behavior Psychotherapy Sessions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1908</first_page>
						<last_page>1912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1518</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/flemotomos18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Feature Representation of Short Utterances Based on Knowledge Distillation for Spoken Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1813</first_page>
						<last_page>1817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1519</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shen18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangquan</given_name>
<surname>Gui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianqi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manwa</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Features Associated with Sustained Vowel and Continuous Speech Productions by Chinese Children with Functional Articulation Disorders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1696</first_page>
						<last_page>1700</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1521</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arjun</given_name>
<surname>Pankajakshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshul</given_name>
<surname>Thakur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daksh</given_name>
<surname>Thapar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Padmanabhan</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Nigam</surname>
</person_name>
					</contributors>
					<titles><title>All-Conv Net for Bird Activity Detection: Significance of Learned Pooling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2122</first_page>
						<last_page>2126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1522</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pankajakshan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fu-Sheng</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao-Chun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Wen</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Assessment of Individual Culture Attribute of Power Distance Using a Social Context-Enhanced Prosodic Network Representation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>436</first_page>
						<last_page>440</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1523</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tsai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haichuan</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingsheng</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiuguo</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Deep Convolutional Neural Network with Scalogram for Audio Scene Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3304</first_page>
						<last_page>3308</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1524</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sunit</given_name>
<surname>Sivasankaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominique</given_name>
<surname>Fohr</surname>
</person_name>
					</contributors>
					<titles><title>Keyword Based Speaker Localization: Localizing a Target Speaker in a Multi-speaker Environment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2703</first_page>
						<last_page>2707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1526</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sivasankaran18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahao</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>High-quality Voice Conversion Using Spectrogram-Based WaveNet Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1993</first_page>
						<last_page>1997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1528</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Cole</surname>
</person_name>
					</contributors>
					<titles><title>Information Structure, Affect and Prenuclear Prominence in American English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1848</first_page>
						<last_page>1852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1529</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chodroff18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri Harish</given_name>
<surname>Mallidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Maas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyle</given_name>
<surname>Goehner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyros</given_name>
<surname>Matsoukas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Hoffmeister</surname>
</person_name>
					</contributors>
					<titles><title>Device-directed Utterance Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1225</first_page>
						<last_page>1228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1531</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mallidi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Hosseini-Asl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingbo</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caiming</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Socher</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech Domain Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3758</first_page>
						<last_page>3762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1535</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hosseiniasl18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik B.</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Auditory Filterbank Learning Using ConvRBM for Infant Cry Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>706</first_page>
						<last_page>710</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1536</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sailor18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nirmesh</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Effectiveness of Dynamic Features in INCA and Temporal Context-INCA</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>711</first_page>
						<last_page>715</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1538</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shah18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Max W. Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ka Ho</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Development of the CUHK Dysarthric Speech Recognition System for the UA Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2938</first_page>
						<last_page>2942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1541</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dieter</given_name>
<surname>Maurer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>d’Heureuse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidy</given_name>
<surname>Suter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Friedrichs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thayabaran</given_name>
<surname>Kathiresan</surname>
</person_name>
					</contributors>
					<titles><title>The Zurich Corpus of Vowel and Voice Quality, Version 1.0</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1417</first_page>
						<last_page>1421</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1542</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/maurer18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuma</given_name>
<surname>Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Compressing End-to-end ASR Networks by Tensor-Train Decomposition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>806</first_page>
						<last_page>810</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1543</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mori18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaorui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanyuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Gated Recurrent Unit Based Acoustic Modeling with Future Context</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1788</first_page>
						<last_page>1792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1544</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zili</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Angular Softmax for Short-Duration Text-independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3623</first_page>
						<last_page>3627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1545</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huang18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuankai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Monaural Multi-Talker Speech Recognition with Attention Mechanism and Gated Convolutional Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1586</first_page>
						<last_page>1590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1547</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Attentive Pooling for Acoustic Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1354</first_page>
						<last_page>1357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1552</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik B.</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maddala</given_name>
<surname>Venkata Siva Krishna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diksha</given_name>
<surname>Chhabra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur T.</given_name>
<surname>Patil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madhu</given_name>
<surname>Kamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>DA-IICT/IIITV System for Low Resource Speech Recognition Challenge 2018</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3187</first_page>
						<last_page>3191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1553</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sailor18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Savitha</given_name>
<surname>Murthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinkar</given_name>
<surname>Sitaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunayana</given_name>
<surname>Sitaram</surname>
</person_name>
					</contributors>
					<titles><title>Effect of TTS Generated Audio on OOV Detection and Word Error Rate in ASR for Low-resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1026</first_page>
						<last_page>1030</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1555</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/murthy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiusi</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Luan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Characteristics of L2 English Speech by Chinese Speakers: A Large-scale Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2593</first_page>
						<last_page>2597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1556</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yuan18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Machine Speech Chain with One-shot Speaker Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>887</first_page>
						<last_page>891</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1558</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tjandra18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoya</given_name>
<surname>Yanagita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Nakamura</surname>
</person_name>
					</contributors>
					<titles><title>Incremental TTS for Japanese Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>902</first_page>
						<last_page>906</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1561</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yanagita18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandeep</given_name>
<surname>Nallan Chakravarthula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Baucom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Panayiotis</given_name>
<surname>Georgiou</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2339</first_page>
						<last_page>2343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1562</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nallanchakravarthula18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masayuki</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tohru</given_name>
<surname>Nagano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Thomas</surname>
</person_name>
					</contributors>
					<titles><title>Inference-Invariant Transformation of Batch Normalization for Domain Adaptation of Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2893</first_page>
						<last_page>2897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1563</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/suzuki18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neil</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nirmesh</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Effectiveness of Generative Adversarial Network for Non-Audible Murmur-to-Whisper Speech Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3157</first_page>
						<last_page>3161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1565</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shah18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddique</given_name>
<surname>Latif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajib</given_name>
<surname>Rana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junaid</given_name>
<surname>Qadir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Variational Autoencoders for Learning Latent Representations of Speech Emotion: A Preliminary Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3107</first_page>
						<last_page>3111</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1568</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/latif18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandana</given_name>
<surname>S</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ritu</given_name>
<surname>Aggarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev Kumar</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kausthubha</given_name>
<surname>N K</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raseena</given_name>
<surname>K T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Astha</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Visual Augmentation for Concatenation Based Synthesized Articulatory Videos from Real-time MRI Data for Spoken Language Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3127</first_page>
						<last_page>3131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1570</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/s18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Buddhi</given_name>
<surname>Wickramasinghe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saad</given_name>
<surname>Irtza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Frequency Domain Linear Prediction Features for Replay Spoofing Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>661</first_page>
						<last_page>665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1574</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wickramasinghe18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juntae</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heejin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinuk</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsoo</given_name>
<surname>Hahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangjin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong-Jin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Korean Singing Voice Synthesis Based on an LSTM Recurrent Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1551</first_page>
						<last_page>1555</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1575</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raghav</given_name>
<surname>Menon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Quinn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Fast ASR-free and Almost Zero-resource Keyword Spotting Using DTW and CNNs for Humanitarian Monitoring</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2608</first_page>
						<last_page>2612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1580</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/menon18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joo-Kyung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young-Bum</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning of Domain Classification and Out-of-Domain Detection with Dynamic Class Weighting for Satisficing False Acceptance Rates</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>556</first_page>
						<last_page>560</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1581</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anil</given_name>
<surname>Ramakrishna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timothy</given_name>
<surname>Greer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Atkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Computational Modeling of Conversational Humor in Psychotherapy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2344</first_page>
						<last_page>2348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1583</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ramakrishna18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hari Krishna</given_name>
<surname>Vydana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Gurugubelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>V V V</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>An Exploration towards Joint Acoustic Modeling for Indian Languages: IIIT-H Submission for Low Resource Speech Recognition Challenge for Indian Languages, INTERSPEECH 2018</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3192</first_page>
						<last_page>3196</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1584</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vydana18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingkun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongbin</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation for Sequence Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3703</first_page>
						<last_page>3707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1589</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huang18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Min-Jae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin-Seob</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>A Unified Framework for the Generation of Glottal Signals in Deep Learning-based Parametric Speech Synthesis Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>912</first_page>
						<last_page>916</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1590</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hwang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhongxin</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingdong</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Cosine Metric Learning for Speaker Verification in the I-vector Space</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1126</first_page>
						<last_page>1130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1593</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Wenjie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengpei</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on the Combination of Batch Normalization and Dropout in BLSTM-based Acoustic Modeling for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2888</first_page>
						<last_page>2892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1597</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wenjie18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joun Yeop</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Jun</given_name>
<surname>Cheon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byoung Jin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nam Soo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Modeling Using Adversarially Trained Variational Recurrent Neural Network for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>917</first_page>
						<last_page>921</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1598</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lee18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mugdha</given_name>
<surname>Pandya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamini</given_name>
<surname>Sabu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanhaiya</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nandini</given_name>
<surname>Bondale</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Lexical and Prosodic Cues to Segmentation in a Hindi-English Code-switched Discourse</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1918</first_page>
						<last_page>1922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1600</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Stress Distribution of Given Information in Chinese Reading Texts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2171</first_page>
						<last_page>2175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1602</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jia18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Permutation Invariant Training of Generative Adversarial Network for Monaural Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>302</first_page>
						<last_page>306</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1603</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Enea</given_name>
<surname>Ceolini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendar</given_name>
<surname>Anumula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Huber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Kiselev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Chii</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Activity Detection and Minimum Variance Beamforming for Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>836</first_page>
						<last_page>840</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1606</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ceolini18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hee-soo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>IL-ho</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Avoiding Speaker Overfitting in End-to-End DNNs Using Raw Waveform for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3583</first_page>
						<last_page>3587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1608</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jung18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cristina</given_name>
<surname>Gorrostieta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kye</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avi</given_name>
<surname>Shapiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Moran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali</given_name>
<surname>Azarbayejani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Kane</surname>
</person_name>
					</contributors>
					<titles><title>Attention-based Sequence Classification for Affect Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>506</first_page>
						<last_page>510</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1610</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gorrostieta18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rini A</given_name>
<surname>Sharon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep Reddy</given_name>
<surname>Kothinti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Umesh</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>Correlational Networks for Speaker Normalization in Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>882</first_page>
						<last_page>886</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1612</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sharon18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>C M</given_name>
<surname>Vikram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Epoch Extraction from Pathological Children Speech Using Single Pole Filtering Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2310</first_page>
						<last_page>2314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1613</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vikram18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoke</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name>
					</contributors>
					<titles><title>Sparsity-Constrained Weight Mapping for Head-Related Transfer Functions Individualization from Anthropometric Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>841</first_page>
						<last_page>845</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1615</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/qi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuki</given_name>
<surname>Irie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Improved Training of End-to-end Attention Models for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>7</first_page>
						<last_page>11</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1616</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zeyer18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddique</given_name>
<surname>Latif</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajib</given_name>
<surname>Rana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahzad</given_name>
<surname>Younis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junaid</given_name>
<surname>Qadir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Transfer Learning for Improving Speech Emotion Classification Accuracy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>257</first_page>
						<last_page>261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1625</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/latif18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinfu</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Shiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Grapheme-to-Phoneme Conversion with Global Character Vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2823</first_page>
						<last_page>2827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1626</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ni18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Hershey</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2708</first_page>
						<last_page>2712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1629</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony Pak Hin</given_name>
<surname>Kong</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Assessment for People with Aphasia Using TDNN-BLSTM with Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3418</first_page>
						<last_page>3422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1630</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/qin18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>C M</given_name>
<surname>Vikram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayush</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of Hypernasality Scores from Cleft Lip and Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1701</first_page>
						<last_page>1705</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1631</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vikram18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Tsiaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bajibabu</given_name>
<surname>Bollepalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manu</given_name>
<surname>Airaksinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paavo</given_name>
<surname>Alku</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-independent Raw Waveform Model for Glottal Excitation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2012</first_page>
						<last_page>2016</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1635</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/juvela18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pankaj</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Digvijaysingh</given_name>
<surname>Gautam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ganesh</given_name>
<surname>Ramakrishnan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>Time Aggregation Operators for Multi-label Audio Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3309</first_page>
						<last_page>3313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1637</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/joshi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minghui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Pitch or Phonation: on the Glottalization in Tone Productions in the Ruokeng Hui Chinese Dialect</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1398</first_page>
						<last_page>1402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1638</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoon Seok</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyung Seo</given_name>
<surname>Ki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gahgene</given_name>
<surname>Gweon</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Miscue Detection Using RNN Based Models with Data Augmentation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1646</first_page>
						<last_page>1650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1644</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hong18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Protima</given_name>
<surname>Nomo Sudro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Processing Transition Regions of Glottal Stop Substituted /S/ for Intelligibility Enhancement of Cleft Palate Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1536</first_page>
						<last_page>1540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1646</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nomosudro18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Foulkes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>French</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colleen</given_name>
<surname>Kavanagh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugenia</given_name>
<surname>San Segundo Fernández</surname>
</person_name>
					</contributors>
					<titles><title>The Individual and the System: Assessing the Stability of the Output of a Semi-automatic Forensic Voice Comparison System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>227</first_page>
						<last_page>231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1649</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hughes18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Using Shifted Real Spectrum Mask as Training Target for Supervised Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1151</first_page>
						<last_page>1155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1650</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madhu</given_name>
<surname>Kamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Auditory Filterbank Learning for Temporal Modulation Features in Replay Spoof Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>666</first_page>
						<last_page>670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1651</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sailor18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Robust TDOA Estimation Based on Time-Frequency Masking and Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>322</first_page>
						<last_page>326</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1652</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Flemotomos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavlos</given_name>
<surname>Papadopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Combined Speaker Clustering and Role Recognition in Conversational Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1378</first_page>
						<last_page>1382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1654</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/flemotomos18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Hayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuya</given_name>
<surname>Takeda</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Head Decoder for End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>801</first_page>
						<last_page>805</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1655</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hayashi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ina</given_name>
<surname>Kodrasi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Single-channel Late Reverberation Power Spectral Density Estimation Using Denoising Autoencoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1319</first_page>
						<last_page>1323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1660</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kodrasi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prasad</given_name>
<surname>Tapkir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Novel Empirical Mode Decomposition Cepstral Features for Replay Spoof Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>721</first_page>
						<last_page>725</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1661</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tapkir18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Huai</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Te</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yichiao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Exemplar-Based Spectral Detail Compensation for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>486</first_page>
						<last_page>490</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1662</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/peng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>All-Neural Multi-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3234</first_page>
						<last_page>3238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1664</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>C M</given_name>
<surname>Vikram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ajish</given_name>
<surname>K Abraham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pushpavathi</given_name>
<surname>M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Girish</given_name>
<surname>K S</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Glottal Activity Errors in Production of Stop Consonants in Children with Cleft Lip and Palate</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>382</first_page>
						<last_page>386</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1665</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vikram18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianwu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimeng</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Text-Dependent Speech Enhancement for Small-Footprint Robust Keyword Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2613</first_page>
						<last_page>2617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1668</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yu18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akihiro</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>Waveform to Single Sinusoid Regression to Estimate the F0 Contour from Noisy Speech Using Recurrent Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>327</first_page>
						<last_page>331</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1671</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kato18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Madhu</given_name>
<surname>Kamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Effectiveness of Speech Demodulation-Based Features for Replay Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>641</first_page>
						<last_page>645</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1675</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kamble18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kimberley</given_name>
<surname>Mulder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>ten Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing EEG Signals in Auditory Speech Comprehension Using Temporal Response Functions and Generalized Additive Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1452</first_page>
						<last_page>1456</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1676</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mulder18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anita</given_name>
<surname>Szakay</surname>
</person_name>
					</contributors>
					<titles><title>Weighting of Coda Voicing Cues: Glottalisation and Vowel Duration</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1422</first_page>
						<last_page>1426</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1677</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/penney18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Norbert</given_name>
<surname>Braunschweiler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Papangelis</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of an End-to-end Trainable Dialogue System with a Modular Statistical Dialogue System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>576</first_page>
						<last_page>580</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1679</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/braunschweiler18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiacen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nakamasa</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3613</first_page>
						<last_page>3617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1680</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nam</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marc</given_name>
<surname>Odobez</surname>
</person_name>
					</contributors>
					<titles><title>Robust and Discriminative Speaker Embedding via Intra-Class Distance Variance Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2257</first_page>
						<last_page>2261</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1685</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/le18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Madhu</given_name>
<surname>Kamble</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Novel Variable Length Energy Separation Algorithm Using Instantaneous Amplitude Features for Replay Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>646</first_page>
						<last_page>650</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1687</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kamble18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwa</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Kenny</surname>
</person_name>
					</contributors>
					<titles><title>Deeply Fused Speaker Embeddings for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3588</first_page>
						<last_page>3592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1688</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bhattacharya18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohit</given_name>
<surname>M A</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-Prosodic Features of Tabla Bol Recitation and Correspondence with the Tabla Imitation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1229</first_page>
						<last_page>1233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1692</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ma18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jichen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhuai</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Feature with Complementarity of Statistics and Principal Information for Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>651</first_page>
						<last_page>655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1693</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yang18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Somnath</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shakuntala</given_name>
<surname>Mahanta</surname>
</person_name>
					</contributors>
					<titles><title>A Hybrid Approach to Grapheme to Phoneme Conversion in Assamese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2828</first_page>
						<last_page>2832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1694</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/roy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>Muckenhirn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastien</given_name>
<surname>Marcel</surname>
</person_name>
					</contributors>
					<titles><title>On Learning Vocal Tract System Related Speaker Discriminative Information from Raw Signal Using CNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1116</first_page>
						<last_page>1120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1696</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/muckenhirn18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Novel Linear Frequency Residual Cepstral Features for Replay Attack Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>726</first_page>
						<last_page>730</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1702</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tak18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anshul</given_name>
<surname>Thakur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pulkit</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Padmanabhan</given_name>
<surname>Rajan</surname>
</person_name>
					</contributors>
					<titles><title>Deep Convex Representations: Feature Representations for Bioacoustics Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2127</first_page>
						<last_page>2131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1705</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/thakur18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feilong</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guanglai</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghe</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological Embeddings with BiLSTM Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>57</first_page>
						<last_page>61</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1706</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Tiesmeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Visualizing Phoneme Category Adaptation in Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1482</first_page>
						<last_page>1486</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1707</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/scharenborg18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Astik</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Febe</given_name>
<surname>de Wet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewald</given_name>
<surname>van der Westhuizen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Neural Network Acoustic Modelling for ASR of Under-Resourced English-isiZulu Code-Switched Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2603</first_page>
						<last_page>2607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1711</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/biswas18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nirmesh</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maulik C.</given_name>
<surname>Madhavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Patil</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Vocal Tract Length Warped Posterior Features for Non-Parallel Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1968</first_page>
						<last_page>1972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1712</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shah18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tifani</given_name>
<surname>Warnita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nakamasa</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Alzheimer’s Disease Using Gated Convolutional Neural Network from Audio Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1706</first_page>
						<last_page>1710</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1713</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/warnita18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chia-Hsuan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Lin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Liang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3459</first_page>
						<last_page>3463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1714</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lee18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cong-Thanh</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>Weighting Time-Frequency Representation of Speech Using Auditory Saliency for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1591</first_page>
						<last_page>1595</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1721</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/do18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dhananjay</given_name>
<surname>Ram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lesly</given_name>
<surname>Miculicich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>CNN Based Query by Example Spoken Term Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>92</first_page>
						<last_page>96</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1722</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ram18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nassima</given_name>
<surname>Fezza</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Temporal Variation in Narrative Organization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2982</first_page>
						<last_page>2986</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1725</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fezza18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iksoo</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhwan</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonyong</given_name>
<surname>Sung</surname>
</person_name>
					</contributors>
					<titles><title>Character-level Language Modeling with Gated Hierarchical Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>411</first_page>
						<last_page>415</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1727</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/choi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis</given_name>
<surname>ten Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirjam</given_name>
<surname>Ernestus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Reaction Time Sequences from Human Participants in Auditory Experiments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>971</first_page>
						<last_page>975</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1728</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tenbosch18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pavan</given_name>
<surname>Karjol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement Using Deep Mixture of Experts Based on Hard Expectation Maximization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3254</first_page>
						<last_page>3258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1730</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/karjol18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>D.V.L.N Dheeraj</given_name>
<surname>Sai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. S.</given_name>
<surname>Kishor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>Speech Source Separation Using ICA in Constant Q Transform Domain</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>846</first_page>
						<last_page>850</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1732</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aciel</given_name>
<surname>Eshky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manuel Sam</given_name>
<surname>Ribeiro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanne</given_name>
<surname>Cleland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Roxburgh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James M</given_name>
<surname>Scobbie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>Wrench</surname>
</person_name>
					</contributors>
					<titles><title>UltraSuite: A Repository of Ultrasound and Acoustic Data from Child Speech Therapy Sessions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1888</first_page>
						<last_page>1892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1736</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/eshky18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lu</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziteng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Risheng</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghong</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Multi-talker Speech Separation Based on Permutation Invariant Training and Beamforming</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>851</first_page>
						<last_page>855</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1739</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yin18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueyang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization with Enhancing Speech for the First DIHARD Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2793</first_page>
						<last_page>2797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1742</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sun18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaocheng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dale</given_name>
<surname>Joachim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Depression Detection from Short Utterances via Diverse Smartphones in Natural Environmental Conditions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3393</first_page>
						<last_page>3397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1743</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huang18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaotong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingliang</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas Fang</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Imbalance Learning-based Framework for Fear Recognition in the MediaEval Emotional Impact of Movies Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3678</first_page>
						<last_page>3682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1744</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shigeki</given_name>
<surname>Karita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoharu</given_name>
<surname>Iwata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised End-to-End Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2</first_page>
						<last_page>6</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1746</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/karita18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marek</given_name>
<surname>Hrúz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleš</given_name>
<surname>Pražák</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michal</given_name>
<surname>Bušta</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Name Recognition in Live TV Subtitling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3529</first_page>
						<last_page>3532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1748</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hruz18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Landini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kateřina</given_name>
<surname>Žmolíková</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karel</given_name>
<surname>Veselý</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Glembek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Matějka</surname>
</person_name>
					</contributors>
					<titles><title>BUT System for DIHARD Speech Diarization Challenge 2018</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2798</first_page>
						<last_page>2802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1749</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/diez18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruiqing</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claude</given_name>
<surname>Barras</surname>
</person_name>
					</contributors>
					<titles><title>Neural Speech Turn Segmentation and Affinity Propagation for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1393</first_page>
						<last_page>1397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1750</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yin18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhavik</given_name>
<surname>Vachhani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chitralekha</given_name>
<surname>Bhat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation Using Healthy Speech for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>471</first_page>
						<last_page>475</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1751</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vachhani18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Toru</given_name>
<surname>Nakashika</surname>
</person_name>
					</contributors>
					<titles><title>LSTBM: A Novel Sequence Representation of Speech Spectra Using Restricted Boltzmann Machine with Long Short-Term Memory</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2529</first_page>
						<last_page>2533</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1753</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nakashika18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chitralekha</given_name>
<surname>Bhat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biswajit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhavik</given_name>
<surname>Vachhani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Recognition Using Time-delay Neural Network Based Denoising Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>451</first_page>
						<last_page>455</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1754</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bhat18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reine</given_name>
<surname>Asakawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosana</given_name>
<surname>Kamiyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Kobashikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Question Detection from Acoustic and Phonetic Features Using Feature-wise Pre-training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1731</first_page>
						<last_page>1735</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1755</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ando18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Balamurali</given_name>
<surname>B T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jer-Ming</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Automated Classification of Vowel-Gesture Parameters Using External Broadband Excitation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2315</first_page>
						<last_page>2318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1756</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bt18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank K.</given_name>
<surname>Soong</surname>
</person_name>
					</contributors>
					<titles><title>A New Glottal Neural Vocoder for Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2017</first_page>
						<last_page>2021</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1757</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cui18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rajath</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaishnavi</given_name>
<surname>Yeruva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>On Convolutional LSTM Modeling for Joint Wake-Word Detection and Text Dependent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1121</first_page>
						<last_page>1125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1759</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kumar18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiphaine</given_name>
<surname>Caudrelier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Perrier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Schwartz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amélie</given_name>
<surname>Rochet-Capellan</surname>
</person_name>
					</contributors>
					<titles><title>Picture Naming or Word Reading: Does the Modality Affect Speech Motor Adaptation and Its Transfer?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>956</first_page>
						<last_page>960</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1760</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/caudrelier18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Traci</given_name>
<surname>Walker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Annalena</given_name>
<surname>Venneri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Reuber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Signs of Dementia Using Word Vector Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1893</first_page>
						<last_page>1897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1764</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mirheidari18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kazuki</given_name>
<surname>Irie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liuhui</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on Estimation of Sentence Probability by Combining Forward, Backward and Bi-directional LSTM-RNNs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>392</first_page>
						<last_page>395</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1766</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/irie18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Trmal</surname>
</person_name>
					</contributors>
					<titles><title>The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1561</first_page>
						<last_page>1565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1768</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/barker18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Na</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deyi</given_name>
<surname>Tuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhifeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Deep Discriminative Embeddings for Duration Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2262</first_page>
						<last_page>2266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1769</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shahin</given_name>
<surname>Amiriparian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Baird</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahib</given_name>
<surname>Julka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alyssa</given_name>
<surname>Alcorn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Ottl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunčica</given_name>
<surname>Petrović</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eloise</given_name>
<surname>Ainger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Recognition of Echolalic Autistic Child Vocalisations Utilising Convolutional Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2334</first_page>
						<last_page>2338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1772</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/amiriparian18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoya</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Purvi</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nabarun</given_name>
<surname>Goswami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Mitsufuji</surname>
</person_name>
					</contributors>
					<titles><title>PhaseNet: Discretized Phase Modeling with Deep Neural Networks for Audio Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2713</first_page>
						<last_page>2717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1773</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/takahashi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shi-wook</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuyo</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Itoh</surname>
</person_name>
					</contributors>
					<titles><title>Empirical Analysis of Score Fusion Application to Combined Neural Networks for Open Vocabulary Spoken Term Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2062</first_page>
						<last_page>2066</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1776</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lee18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhao</given_name>
<surname>Shan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Attention-based End-to-End Models for Small-Footprint Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2037</first_page>
						<last_page>2041</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1777</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brecht</given_name>
<surname>Desplanques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name>
					</contributors>
					<titles><title>Cross-lingual Speech Emotion Recognition through Factor Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3648</first_page>
						<last_page>3652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1778</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/desplanques18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Generative Adversarial Networks Based Speech Dereverberation for Robust Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1581</first_page>
						<last_page>1585</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1780</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziwei</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runnan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianhong</given_name>
<surname>Cai</surname>
</person_name>
					</contributors>
					<titles><title>Siamese Recurrent Auto-Encoder Representation for Query-by-Example Spoken Term Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>102</first_page>
						<last_page>106</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1788</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hieu-Thi</given_name>
<surname>Luong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Speech Synthesis Architecture for Unsupervised Speaker Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2494</first_page>
						<last_page>2498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1791</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/luong18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takaaki</given_name>
<surname>Shochi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Rouas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marine</given_name>
<surname>Guerry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donna</given_name>
<surname>Erickson</surname>
</person_name>
					</contributors>
					<titles><title>Cultural Differences in Pattern Matching: Multisensory Recognition of Socio-affective Prosody</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2201</first_page>
						<last_page>2205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1795</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shochi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinhwan</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iksoo</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonho</given_name>
<surname>Boo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonyong</given_name>
<surname>Sung</surname>
</person_name>
					</contributors>
					<titles><title>Hierarchical Recurrent Neural Networks for Acoustic Modeling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3728</first_page>
						<last_page>3732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1797</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/park18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chris</given_name>
<surname>Davis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeesun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Characterizing Rhythm Differences between Strong and Weak Accented L2 Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2568</first_page>
						<last_page>2572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1798</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/davis18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Da-Rong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin-shan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3748</first_page>
						<last_page>3752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1800</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Szu-wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Te</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model Based on BLSTM</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1873</first_page>
						<last_page>1877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1802</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fu18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md Hafizur</given_name>
<surname>Rahman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Himawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clinton</given_name>
<surname>Fookes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sridha</given_name>
<surname>Sridharan</surname>
</person_name>
					</contributors>
					<titles><title>Employing Phonetic Information in DNN Speaker Embeddings to Improve Speaker Recognition Performance</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3593</first_page>
						<last_page>3597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1804</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rahman18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarith</given_name>
<surname>Fernando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Sub-band Envelope Features Using Frequency Domain Linear Prediction for Short Duration Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1818</first_page>
						<last_page>1822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1805</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fernando18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joana</given_name>
<surname>Correia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhiksha</given_name>
<surname>Raj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Teixeira</surname>
</person_name>
					</contributors>
					<titles><title>Mining Multimodal Repositories for Speech Affecting Diseases</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2963</first_page>
						<last_page>2967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1806</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/correia18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Teun</given_name>
<surname>Krikke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Broz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Lane</surname>
</person_name>
					</contributors>
					<titles><title>Who Said That? a Comparative Study of Non-negative Matrix Factorization Techniques</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1234</first_page>
						<last_page>1238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1807</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/krikke18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youhyun</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kang Min</given_name>
<surname>Yoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-goo</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Slot Filling with Delexicalized Sentence Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2082</first_page>
						<last_page>2086</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1808</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shin18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Promod</given_name>
<surname>Yenigalla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhay</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suraj</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chirag</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sibsambhu</given_name>
<surname>Kar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jithendra</given_name>
<surname>Vepa</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition Using Spectrogram &#38; Phoneme Embedding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3688</first_page>
						<last_page>3692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1811</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yenigalla18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeesun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonya</given_name>
<surname>Karisma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Aubanel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Davis</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Role of Familiar Face and Voice Cues in Speech Processing in Noise</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2276</first_page>
						<last_page>2279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1812</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaavya</given_name>
<surname>Sriskandaraja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Deep Siamese Architecture Based Replay Detection for Secure Voice Biometric</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>671</first_page>
						<last_page>675</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1819</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sriskandaraja18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Akagi</surname>
</person_name>
					</contributors>
					<titles><title>A Three-Layer Emotion Perception Model for Valence and Arousal-Based Detection from Multilingual Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3643</first_page>
						<last_page>3647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1820</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lam Dang</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramaswamy</given_name>
<surname>Palaniappan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Lang</surname>
</person_name>
					</contributors>
					<titles><title>Early Detection of Continuous and Partial Audio Events Using CNN</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3314</first_page>
						<last_page>3318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1821</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mcloughlin18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Max W. Y.</given_name>
<surname>Lam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoukang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shansong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Gaussian Process Neural Networks for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1778</first_page>
						<last_page>1782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1823</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lam18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yufan</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongying</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Measuring the Band Importance Function for Mandarin Chinese with a Bayesian Adaptive Procedure</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>961</first_page>
						<last_page>965</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1825</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/du18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiina</given_name>
<surname>Murtola</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jarmo</given_name>
<surname>Malinen</surname>
</person_name>
					</contributors>
					<titles><title>Interaction Mechanisms between Glottal Source and Vocal Tract in Pitch Glides</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2987</first_page>
						<last_page>2991</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1827</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/murtola18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ishwar Chandra</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avinash</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed</given_name>
<surname>Shahnawazuddin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gayadhar</given_name>
<surname>Pradhan</surname>
</person_name>
					</contributors>
					<titles><title>Non-Uniform Spectral Smoothing for Robust Children's Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1601</first_page>
						<last_page>1605</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1828</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yadav18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju-chieh</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-chieh</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin-shan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>501</first_page>
						<last_page>505</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1830</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chou18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danqing</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongyan</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on Joint Representation Learning for Robust Feature Extraction in Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>152</first_page>
						<last_page>156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1832</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/luo18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Mohanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajbabu</given_name>
<surname>Velmurugan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>A Non-convolutive NMF Model for Speech Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1324</first_page>
						<last_page>1328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1834</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mohanan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunxi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ondel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Harman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vimal</given_name>
<surname>Manohar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Trmal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongqiang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition and Topic Identification from Speech for Almost-Zero-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2052</first_page>
						<last_page>2056</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1836</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wiesner18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul</given_name>
<surname>Magron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuomas</given_name>
<surname>Virtanen</surname>
</person_name>
					</contributors>
					<titles><title>Expectation-Maximization Algorithms for Itakura-Saito Nonnegative Matrix Factorization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>856</first_page>
						<last_page>860</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1840</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/magron18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ignacio</given_name>
<surname>Viñals</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Gimeno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfonso</given_name>
<surname>Ortega</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Miguel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Lleida</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of the Number of Speakers with Variational Bayesian PLDA in the DIHARD Diarization Challenge.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2803</first_page>
						<last_page>2807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1841</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vinals18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Illa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Low Resource Acoustic-to-articulatory Inversion Using Bi-directional Long Short Term Memory</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3122</first_page>
						<last_page>3126</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1843</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/illa18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul</given_name>
<surname>Magron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Drossos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stylianos</given_name>
<surname>Ioannis Mimilakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuomas</given_name>
<surname>Virtanen</surname>
</person_name>
					</contributors>
					<titles><title>Reducing Interference with Phase Recovery in DNN-based Monaural Singing Voice Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>332</first_page>
						<last_page>336</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1845</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/magron18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gajan</given_name>
<surname>Suthokumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chamith</given_name>
<surname>Wijenayake</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name>
					</contributors>
					<titles><title>Modulation Dynamic Features for the Detection of Replay Attacks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>691</first_page>
						<last_page>695</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1846</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/suthokumar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lixia</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanlu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Preliminary Study on Tonal Coarticulation in Continuous Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3017</first_page>
						<last_page>3021</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1849</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Buschermöhle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>What Do Classifiers Actually Learn? a Case Study on Emotion Recognition Datasets</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>262</first_page>
						<last_page>266</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1851</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/meyer18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Watts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cassia</given_name>
<surname>Valentini-Botinhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felipe</given_name>
<surname>Espic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Exemplar-based Speech Waveform Generation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2022</first_page>
						<last_page>2026</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1857</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/watts18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huabin</given_name>
<surname>Ruan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaomin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhixiang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haifeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Towards Temporal Modelling of Categorical Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>932</first_page>
						<last_page>936</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1858</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/han18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suguru</given_name>
<surname>Kabashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kumi</given_name>
<surname>Kanamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutaka</given_name>
<surname>Yamauchi</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Objective Measurement of Comprehensibility through Native Speakers' Shadowing of Learners' Utterances</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1651</first_page>
						<last_page>1655</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1860</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/inoue18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Astha</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>G. Nisha</given_name>
<surname>Meenakshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Relating Articulatory Motions in Different Speaking Rates</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2992</first_page>
						<last_page>2996</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1862</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/singh18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minali</given_name>
<surname>Upreti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>Improved Accented Speech Recognition Using Accent Embeddings and Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2454</first_page>
						<last_page>2458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1864</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jain18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sei</given_name>
<surname>Ueno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshikazu</given_name>
<surname>Yamaguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Multi-task Learning with Augmentation Strategy for Acoustic-to-word Attention-based Encoder-decoder Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2399</first_page>
						<last_page>2403</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1866</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/moriya18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pallavi</given_name>
<surname>Baljekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>SaiKrishna</given_name>
<surname>Rallabandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Convolution Attention Based Models for Multilingual Speech Synthesis of Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2474</first_page>
						<last_page>2478</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1869</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/baljekar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karttikeya</given_name>
<surname>Mangalam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanaya</given_name>
<surname>Guha</surname>
</person_name>
					</contributors>
					<titles><title>Learning Spontaneity to Improve Emotion Recognition in Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>946</first_page>
						<last_page>950</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1872</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mangalam18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heini</given_name>
<surname>Kallio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antti</given_name>
<surname>Suni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Päivi</given_name>
<surname>Virkkunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name>
					</contributors>
					<titles><title>Prominence-based Evaluation of L2 Prosody</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1838</first_page>
						<last_page>1842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1873</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kallio18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Evdokia</given_name>
<surname>Kazimirova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Belyaev</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Multi-speaker Fragments with High Time Resolution</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1388</first_page>
						<last_page>1392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1878</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kazimirova18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Sahu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>On Enhancing Speech Emotion Recognition Using Generative Adversarial Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3693</first_page>
						<last_page>3697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1883</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sahu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rohith</given_name>
<surname>Aralikatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilip Kumar</given_name>
<surname>Margam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanay</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Thanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Venkatesan</surname>
</person_name>
					</contributors>
					<titles><title>Global SNR Estimation of Speech Signals Using Entropy and Uncertainty Estimates from Dropout Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1878</first_page>
						<last_page>1882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1884</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/aralikatti18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaesung</given_name>
<surname>Bae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dae-Shik</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Command Recognition with Capsule Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>776</first_page>
						<last_page>780</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1888</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bae18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas Søeborg</given_name>
<surname>Kirkedal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeon-Jun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Deep Neural Network Training Using Cyclical Learning Rate</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2933</first_page>
						<last_page>2937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1891</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kirkedal18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gregory</given_name>
<surname>Sell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan</given_name>
<surname>McCree</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Maciejewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vimal</given_name>
<surname>Manohar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name>
					</contributors>
					<titles><title>Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2808</first_page>
						<last_page>2812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1893</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sell18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis</given_name>
<surname>ten Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lou</given_name>
<surname>Boves</surname>
</person_name>
					</contributors>
					<titles><title>Information Encoding by Deep Neural Networks: What Can We Learn?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1457</first_page>
						<last_page>1461</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1896</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tenbosch18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Empirical Evaluation of Speaker Adaptation on DNN Based Acoustic Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2429</first_page>
						<last_page>2433</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1897</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohamed</given_name>
<surname>Morchid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiheb</given_name>
<surname>Trabelsi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georges</given_name>
<surname>Linares</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renato</given_name>
<surname>de Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshua</given_name>
<surname>Bengio</surname>
</person_name>
					</contributors>
					<titles><title>Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>22</first_page>
						<last_page>26</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1898</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/parcollet18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pamir</given_name>
<surname>Gogoi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parismita</given_name>
<surname>Gogoi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ratree</given_name>
<surname>Wayland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R Mahadeva</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Breathiness in Contextual Vowel of Voiceless Nasals in Mizo</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>237</first_page>
						<last_page>241</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1899</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gogoi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bastian</given_name>
<surname>Schnell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name>
					</contributors>
					<titles><title>A Neural Model to Predict Parameters for a Generalized Command Response Model of Intonation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3147</first_page>
						<last_page>3151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1904</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/schnell18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manjunath</given_name>
<surname>Mulimani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashidhar G</given_name>
<surname>Koolagudi</surname>
</person_name>
					</contributors>
					<titles><title>Robust Acoustic Event Classification Using Bag-of-Visual-Words</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3319</first_page>
						<last_page>3322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1905</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mulimani18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abinay Reddy</given_name>
<surname>N</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Achuth</given_name>
<surname>Rao MV</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>G. Nisha</given_name>
<surname>Meenakshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Reconstructing Neutral Speech from Tracheoesophageal Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1541</first_page>
						<last_page>1545</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1907</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/n18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinfeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaoyan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minbo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingjian</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Revealing Spatiotemporal Brain Dynamics of Speech Production Based on EEG and Eye Movement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1427</first_page>
						<last_page>1431</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1908</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhao18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Gómez Alanís</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio M.</given_name>
<surname>Peinado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose A.</given_name>
<surname>Gonzalez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel</given_name>
<surname>Gomez</surname>
</person_name>
					</contributors>
					<titles><title>A Deep Identity Representation for Noise Robust Spoofing Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>676</first_page>
						<last_page>680</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1909</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gomezalanis18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthias</given_name>
<surname>Sperber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Graham</given_name>
<surname>Neubig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stüker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Self-Attentional Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3723</first_page>
						<last_page>3727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1910</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sperber18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Evolving Learning for Analysing Mood-Related Infant Vocalisation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>142</first_page>
						<last_page>146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1914</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dravyansh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>On Training and Evaluation of Grapheme-to-Phoneme Mappings with Limited Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2858</first_page>
						<last_page>2862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1920</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sharma18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kumud</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sreenivasa</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of sparse representation based feature on speech mode classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>731</first_page>
						<last_page>735</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1921</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tripathi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nagapuri</given_name>
<surname>Srinivas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gayadhar</given_name>
<surname>Pradhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed</given_name>
<surname>Shahnawazuddin</surname>
</person_name>
					</contributors>
					<titles><title>Enhancement of Noisy Speech Signal by Non-Local Means Estimation of Variational Mode Functions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1156</first_page>
						<last_page>1160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1928</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/srinivas18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arsha</given_name>
<surname>Nagrani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Zisserman</surname>
</person_name>
					</contributors>
					<titles><title>VoxCeleb2: Deep Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1086</first_page>
						<last_page>1090</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1929</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chung18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mia</given_name>
<surname>Atcheson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Demonstrating and Modelling Systematic Time-varying Annotator Disagreement in Continuous Emotion Annotation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3668</first_page>
						<last_page>3672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1933</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/atcheson18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jitendra Kumar</given_name>
<surname>Dhiman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra Sekhar</given_name>
<surname>Seelamantula</surname>
</person_name>
					</contributors>
					<titles><title>Multicomponent 2-D AM-FM Modeling of Speech Spectrograms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>736</first_page>
						<last_page>740</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1937</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dhiman18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jacques</given_name>
<surname>Koreman</surname>
</person_name>
					</contributors>
					<titles><title>Category Similarity in Multilingual Pronunciation Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2578</first_page>
						<last_page>2582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1938</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/koreman18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valliappan</given_name>
<surname>CA</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renuka</given_name>
<surname>Mannem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Air-Tissue Boundary Segmentation in Real-Time Magnetic Resonance Imaging Video Using Semantic Segmentation with Fully Convolutional Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3132</first_page>
						<last_page>3136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1939</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ca18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Spectral and Spatial Features for Multi-Channel Speaker Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2718</first_page>
						<last_page>2722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1940</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhijeet</given_name>
<surname>Sangwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Joglekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmet E.</given_name>
<surname>Bulut</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lakshmish</given_name>
<surname>Kaushik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengzhu</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Fearless Steps: Apollo-11 Corpus Advancements for Speech Technologies from Earth to the Moon</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2758</first_page>
						<last_page>2762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1942</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hansen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Triantafyllos</given_name>
<surname>Afouras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Zisserman</surname>
</person_name>
					</contributors>
					<titles><title>Deep Lip Reading: A Comparison of Models and an Online Application</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3514</first_page>
						<last_page>3518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1943</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/afouras18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kate</given_name>
<surname>Earnshaw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Gold</surname>
</person_name>
					</contributors>
					<titles><title>Variation in the FACE Vowel across West Yorkshire: Implications for Forensic Speaker Comparisons</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2743</first_page>
						<last_page>2747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1944</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/earnshaw18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Surbhi</given_name>
<surname>Sakshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avinash</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gayadhar</given_name>
<surname>Pradhan</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Variational Mode Functions for Robust Detection of Vowels</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>756</first_page>
						<last_page>760</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1947</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sakshi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Toshiko</given_name>
<surname>Isei-Jaakkola</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keiko</given_name>
<surname>Ochi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keikichi</given_name>
<surname>Hirose</surname>
</person_name>
					</contributors>
					<titles><title>Respiratory and Respiratory Muscular Control in JL1’s and JL2’s Text Reading Utilizing 4-RSTs and a Soft Respiratory Mask with a Two-Way Bulb</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3012</first_page>
						<last_page>3016</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1948</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/iseijaakkola18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Priya</given_name>
<surname>Pallavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ch V Rama</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>Phase-locked Loop (PLL) Based Phase Estimation in Single Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1161</first_page>
						<last_page>1164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1950</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pallavi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aviv</given_name>
<surname>Gabbay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asaph</given_name>
<surname>Shamir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shmuel</given_name>
<surname>Peleg</surname>
</person_name>
					</contributors>
					<titles><title>Visual Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1170</first_page>
						<last_page>1174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1955</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gabbay18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>RaviShankar</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bayya</given_name>
<surname>Yegnanarayana</surname>
</person_name>
					</contributors>
					<titles><title>Identification and Classification of Fricatives in Speech Using Zero Time Windowing Method</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>187</first_page>
						<last_page>191</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1958</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/prasad18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mark</given_name>
<surname>Huckvale</surname>
</person_name>
					</contributors>
					<titles><title>Neural Network Architecture That Combines Temporal and Summative Features for Infant Cry Classification in the Interspeech 2018 Computational Paralinguistics Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>137</first_page>
						<last_page>141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1959</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huckvale18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniil</given_name>
<surname>Kocharov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alla</given_name>
<surname>Menshikova</surname>
</person_name>
					</contributors>
					<titles><title>Language-Dependent Melody Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2167</first_page>
						<last_page>2170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1962</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kocharov18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emre</given_name>
<surname>Yılmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Astik</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewald</given_name>
<surname>van der Westhuizen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Febe</given_name>
<surname>de Wet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Niesler</surname>
</person_name>
					</contributors>
					<titles><title>Building a Unified Code-Switching ASR System for South African Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1923</first_page>
						<last_page>1927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1966</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ylmaz18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanumay</given_name>
<surname>Mandal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sreenivasa</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjay Kumar</given_name>
<surname>Gupta</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Disorders in Vocal Folds Using Electroglottographic Signal</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3002</first_page>
						<last_page>3006</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1967</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mandal18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhua</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqi</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name>
					</contributors>
					<titles><title>On the Application and Compression of Deep Time Delay Neural Network for Embedded Statistical Parametric Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>922</first_page>
						<last_page>926</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1970</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zheng18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Purvi</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Unsupervised Modulation Filter Learning Methods for ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2908</first_page>
						<last_page>2912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1972</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/agrawal18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Afsaneh</given_name>
<surname>Asaei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhananjay</given_name>
<surname>Ram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Phonological Posterior Hashing for Query by Example Spoken Term Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2067</first_page>
						<last_page>2071</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1973</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/asaei18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengcheng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haihua</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Study of Semi-supervised Approaches to Improving English-Mandarin Code-Switching Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1928</first_page>
						<last_page>1932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1974</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/guo18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Myer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikrant Singh</given_name>
<surname>Tomar</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Keyword Spotting Using Time Delay Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1264</first_page>
						<last_page>1268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1979</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/myer18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Win Thuzar</given_name>
<surname>Kyaw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Sagisaka</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of L2 Learners’ Progress of Distinguishing Mandarin Tone 2 and Tone 3</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2549</first_page>
						<last_page>2553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1983</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sun18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abhilash</given_name>
<surname>Sainathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil</given_name>
<surname>Rudresh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra Sekhar</given_name>
<surname>Seelamantula</surname>
</person_name>
					</contributors>
					<titles><title>An Optimization Framework for Recovery of Speech from Phase-Encoded Spectrograms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>741</first_page>
						<last_page>745</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1987</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sainathan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juan Camilo</given_name>
<surname>Vásquez Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomas</given_name>
<surname>Arias</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>A Multitask Learning Approach to Assess the Dysarthria Severity in Patients with Parkinson's Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>456</first_page>
						<last_page>460</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1988</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vasquezcorrea18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sibo</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bourlard</surname>
</person_name>
					</contributors>
					<titles><title>Fast Language Adaptation Using Phonological Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2459</first_page>
						<last_page>2463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1990</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tong18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuewen</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songxiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyin</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Rapid Style Adaptation Using Residual Error Embedding for Expressive Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3072</first_page>
						<last_page>3076</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-1991</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wu18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dongbo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zeyan</given_name>
<surname>Oo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seiichi</given_name>
<surname>Nakagawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haotian</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multiple Phase Information Combination for Replay Attacks Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>656</first_page>
						<last_page>660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2001</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madhumita</given_name>
<surname>Harish</surname>
</person_name>
					</contributors>
					<titles><title>Far-Field Speech Recognition Using Multivariate Autoregressive Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3023</first_page>
						<last_page>3027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2003</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ganapathy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yasuhito</given_name>
<surname>Ohsugi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study of Statistical Conversion of Face to Voice Based on Their Subjective Impressions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1001</first_page>
						<last_page>1005</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2005</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ohsugi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yulun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis-Philippe</given_name>
<surname>Morency</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxine</given_name>
<surname>Eskenazi</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Polynomial Fusion for Detecting Driver Distraction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>611</first_page>
						<last_page>615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2011</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/du18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreyas</given_name>
<surname>Ramoji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Supervised I-vector Modeling - Theory and Applications</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1091</first_page>
						<last_page>1095</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2012</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ramoji18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hirak</given_name>
<surname>Dasgupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prem C.</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K S</given_name>
<surname>Nataraj</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Glottal Excitation Epochs in Speech Signal Using Hilbert Envelope</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2132</first_page>
						<last_page>2136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2014</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dasgupta18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pegah</given_name>
<surname>Ghahremani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phani Sankar</given_name>
<surname>Nidadavolu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Povey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end Deep Neural Network Age Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>277</first_page>
						<last_page>281</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2015</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ghahremani18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maren</given_name>
<surname>Kucza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Zenkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stüker</surname>
</person_name>
					</contributors>
					<titles><title>Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2072</first_page>
						<last_page>2076</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2017</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kucza18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zafi Sherhan</given_name>
<surname>Syed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Schroeter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirill</given_name>
<surname>Sidorov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Marshall</surname>
</person_name>
					</contributors>
					<titles><title>Computational Paralinguistics: Automatic Assessment of Emotions, Mood and Behavioural State from Acoustics of Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>511</first_page>
						<last_page>515</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2019</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/syed18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Markus</given_name>
<surname>Kitza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of BLSTM-Layer-Specific Affine Transformations for Speaker Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>877</first_page>
						<last_page>881</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2022</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kitza18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chong</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanlu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Interactions between Vowels and Nasal Codas in Mandarin Speakers’ Perception of Nasal Finals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3763</first_page>
						<last_page>3767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2025</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoguang</given_name>
<surname>Mao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Discovery of Non-native Phonetic Patterns in L2 English Speech for Mispronunciation Detection and Diagnosis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2554</first_page>
						<last_page>2558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2027</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sourish</given_name>
<surname>Chaudhuri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Roth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel P. W.</given_name>
<surname>Ellis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Gallagher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liat</given_name>
<surname>Kaver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Radhika</given_name>
<surname>Marvin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caroline</given_name>
<surname>Pantofaru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Reale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Loretta</given_name>
<surname>Guarino Reid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Wilson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhonghua</given_name>
<surname>Xi</surname>
</person_name>
					</contributors>
					<titles><title>AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1239</first_page>
						<last_page>1243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2028</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chaudhuri18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Perez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenyu</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noelle</given_name>
<surname>Carlozzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Praveen</given_name>
<surname>Dayalu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angela</given_name>
<surname>Roberts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Huntington Disease Using Acoustic and Lexical Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1898</first_page>
						<last_page>1902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2029</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/perez18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François</given_name>
<surname>Grondin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Enhancement, Augmentation and Autoencoder Methods for Domain Adaptation in Distant Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2928</first_page>
						<last_page>2932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2030</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tang18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saketh</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nitya</given_name>
<surname>Tiwari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prem C.</given_name>
<surname>Pandey</surname>
</person_name>
					</contributors>
					<titles><title>Implementation of Digital Hearing Aid as a Smartphone Application</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1175</first_page>
						<last_page>1179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2031</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sharma18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ferdinand</given_name>
<surname>Brasser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tommaso</given_name>
<surname>Frassetto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad-Reza</given_name>
<surname>Sadeghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schneider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Weinert</surname>
</person_name>
					</contributors>
					<titles><title>VoiceGuard: Secure and Private Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1303</first_page>
						<last_page>1307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2032</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/brasser18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eva-Maria</given_name>
<surname>Rathner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Djamali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannik</given_name>
<surname>Terhorst</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gudrun</given_name>
<surname>Salamon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christina</given_name>
<surname>Hunger-Schoppe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harald</given_name>
<surname>Baumeister</surname>
</person_name>
					</contributors>
					<titles><title>How Did You like 2017? Detection of Language Markers of Depression and Narcissism in Personal Narratives</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3388</first_page>
						<last_page>3392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2040</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rathner18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masaki</given_name>
<surname>Yokoyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nagata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroki</given_name>
<surname>Mori</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Dimensional Input on Paralinguistic Information Perceived from Synthesized Dialogue Speech with Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3053</first_page>
						<last_page>3056</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2042</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yokoyama18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eva-Maria</given_name>
<surname>Rathner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannik</given_name>
<surname>Terhorst</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harald</given_name>
<surname>Baumeister</surname>
</person_name>
					</contributors>
					<titles><title>State of Mind: Classification through Self-reported Affect and Word Use in Speech.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>267</first_page>
						<last_page>271</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2043</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rathner18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deepanway</given_name>
<surname>Ghosal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maheshkumar H.</given_name>
<surname>Kolekar</surname>
</person_name>
					</contributors>
					<titles><title>Music Genre Recognition Using Deep Neural Networks and Transfer Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2087</first_page>
						<last_page>2091</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2045</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ghosal18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moïra-Phoebé</given_name>
<surname>Huet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Micheyl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Etienne</given_name>
<surname>Gaudrain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Etienne</given_name>
<surname>Parizet</surname>
</person_name>
					</contributors>
					<titles><title>Who Are You Listening to? Towards a Dynamic Measure of Auditory Attention to Speech-on-speech.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2272</first_page>
						<last_page>2275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2053</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/huet18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Zenkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramon</given_name>
<surname>Sanabria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Subword and Crossword Units for CTC Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>396</first_page>
						<last_page>400</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2057</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zenkel18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Bruguier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Bakhtin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dravyansh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Dictionary Augmented Sequence-to-Sequence Neural Network for Grapheme to Phoneme Prediction</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3733</first_page>
						<last_page>3737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2061</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bruguier18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tsukasa</given_name>
<surname>Yoshida</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuho</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshikazu</given_name>
<surname>Yamaguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Automatic DNN Node Pruning Using Mixture Distribution-based Group Regularization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1269</first_page>
						<last_page>1273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2062</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yoshida18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arpita</given_name>
<surname>Gang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pravesh</given_name>
<surname>Biyani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshay</given_name>
<surname>Soni</surname>
</person_name>
					</contributors>
					<titles><title>Towards Automated Single Channel Source Separation Using Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3494</first_page>
						<last_page>3498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2065</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gang18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danny</given_name>
<surname>Websdale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ben</given_name>
<surname>Milner</surname>
</person_name>
					</contributors>
					<titles><title>The Effect of Real-Time Constraints on Automatic Speech Animation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2479</first_page>
						<last_page>2483</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2066</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/websdale18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koji</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divesh</given_name>
<surname>Lala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katsuya</given_name>
<surname>Takanashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Engagement Recognition in Spoken Dialogue via Neural Network by Aggregating Different Annotators' Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>616</first_page>
						<last_page>620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2067</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/inoue18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhadeep</given_name>
<surname>Dey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Language Dependent Front-End for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1101</first_page>
						<last_page>1105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2071</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/madikeri18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Boll-Avetisyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessie S.</given_name>
<surname>Nixon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomas O.</given_name>
<surname>Lentz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liquan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandrien</given_name>
<surname>van Ommen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Çağri</given_name>
<surname>Çöltekin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacolien</given_name>
<surname>van Rij</surname>
</person_name>
					</contributors>
					<titles><title>Neural Response Development During Distributional Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1432</first_page>
						<last_page>1436</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2072</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bollavetisyan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zack</given_name>
<surname>Hodari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Watts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Learning Interpretable Control Dimensions for Speech Synthesis by Using External Data</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>32</first_page>
						<last_page>36</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2075</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hodari18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shobhana</given_name>
<surname>Ganesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marisa</given_name>
<surname>Casillas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Talker Diarization in the Wild: the Case of Child-centered Daylong Audio-recordings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2583</first_page>
						<last_page>2587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2078</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cristia18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwa</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilles</given_name>
<surname>Boulianne</surname>
</person_name>
					</contributors>
					<titles><title>CRIM's System for the MGB-3 English Multi-Genre Broadcast Media Transcription</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2653</first_page>
						<last_page>2657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2079</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gupta18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenz</given_name>
<surname>Diener</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Objective Intelligibility in Real-Time EMG-to-Speech Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3162</first_page>
						<last_page>3166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2080</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/diener18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Filip</given_name>
<surname>Nenadić</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>ten Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin V.</given_name>
<surname>Tucker</surname>
</person_name>
					</contributors>
					<titles><title>Implementing DIANA to Model Isolated Auditory Word Recognition in English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3772</first_page>
						<last_page>3776</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2081</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nenadic18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeroen</given_name>
<surname>Zegers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>van Hamme</surname>
</person_name>
					</contributors>
					<titles><title>Memory Time Span in LSTMs for Multi-Speaker Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1477</first_page>
						<last_page>1481</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2082</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zegers18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shefali</given_name>
<surname>Waldekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Goutam</given_name>
<surname>Saha</surname>
</person_name>
					</contributors>
					<titles><title>Wavelet Transform Based Mel-scaled Features for Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3323</first_page>
						<last_page>3327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2083</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/waldekar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sankar</given_name>
<surname>Mukherjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thierry</given_name>
<surname>Legou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Lancia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Hilt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Tomassini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciano</given_name>
<surname>Fadiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>D'Ausilio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Badino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noël</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Vocal Tract Movements During Speech Accommodation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>561</first_page>
						<last_page>565</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2084</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mukherjee18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Parrell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikantan</given_name>
<surname>Nagarajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Houde</surname>
</person_name>
					</contributors>
					<titles><title>FACTS: A Hierarchical Task-based Control Model of Speech Incorporating Sensory Feedback</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1497</first_page>
						<last_page>1501</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2087</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/parrell18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julien</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanny</given_name>
<surname>Meunier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laure</given_name>
<surname>Dentel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noelia</given_name>
<surname>Do Carmo Blanco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frédéric</given_name>
<surname>Sèbe</surname>
</person_name>
					</contributors>
					<titles><title>Loud and Shouted Speech Perception at Variable Distances in a Forest</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2285</first_page>
						<last_page>2289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2089</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/meyer18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Panda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of the Effect of Speech-Laugh on Speaker Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1751</first_page>
						<last_page>1755</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2090</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dumpala18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arijit</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Per</given_name>
<surname>Hedelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lars</given_name>
<surname>Villemoes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Melkote</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Noise Shaping with Companding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3548</first_page>
						<last_page>3552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2096</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/biswas18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akshay Raj</given_name>
<surname>Maggu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick C. M.</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanjun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francis C. K.</given_name>
<surname>Wong</surname>
</person_name>
					</contributors>
					<titles><title>Experience-dependent Influence of Music and Language on Lexical Pitch Learning Is Not Additive</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3791</first_page>
						<last_page>3794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2104</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/maggu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwanchiva</given_name>
<surname>Thangthai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Harvey</surname>
</person_name>
					</contributors>
					<titles><title>Building Large-vocabulary Speaker-independent Lipreading Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2648</first_page>
						<last_page>2652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2112</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/thangthai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhamini</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Homophone Density on Spoken Word Recognition in Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3777</first_page>
						<last_page>3780</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2114</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sharma18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Thai Tone Distribution through Functional Data Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2137</first_page>
						<last_page>2141</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2115</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noor</given_name>
<surname>Fathima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahima</given_name>
<surname>C</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anuroop</given_name>
<surname>Iyengar</surname>
</person_name>
					</contributors>
					<titles><title>TDNN-based Multilingual Speech Recognition System for Low Resource Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3197</first_page>
						<last_page>3201</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2117</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fathima18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mauro</given_name>
<surname>Nicolao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Sanders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Improved Acoustic Modelling for Automatic Literacy Assessment of Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1666</first_page>
						<last_page>1670</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2118</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nicolao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammed Shifas</given_name>
<surname>PV</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Tsiaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Stylianou</surname>
</person_name>
					</contributors>
					<titles><title>Speech Intelligibility Enhancement Based on a Non-causal Wavenet-like Model</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1868</first_page>
						<last_page>1872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2119</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/pv18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debadatta</given_name>
<surname>Dash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myungjong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Teplansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition with Articulatory Information and a Unified Dictionary for Hindi, Marathi, Bengali and Oriya</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1046</first_page>
						<last_page>1050</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2122</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dash18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Roddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>586</first_page>
						<last_page>590</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2124</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/roddy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Dey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biswajit Dev</given_name>
<surname>Sarma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wendy</given_name>
<surname>Lalhminghlui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lalnunsiami</given_name>
<surname>Ngente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parismita</given_name>
<surname>Gogoi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R M</given_name>
<surname>Prasanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohit</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S R</given_name>
<surname>Nirmala</surname>
</person_name>
					</contributors>
					<titles><title>Robust Mizo Continuous Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1036</first_page>
						<last_page>1040</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2125</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dey18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niko</given_name>
<surname>Brümmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Garcia-Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Snyder</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Fast Variational Bayes for Heavy-tailed PLDA Applied to i-vectors and x-vectors</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>72</first_page>
						<last_page>76</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2128</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/silnova18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Wun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hen-Hsen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Hsi</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Discourse Marker Detection for Hesitation Events on Mandarin Conversation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1721</first_page>
						<last_page>1725</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2129</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akshay Raj</given_name>
<surname>Maggu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenqing</given_name>
<surname>Zong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vina</given_name>
<surname>Law</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick C. M.</given_name>
<surname>Wong</surname>
</person_name>
					</contributors>
					<titles><title>Learning Two Tone Languages Enhances the Brainstem Encoding of Lexical Tones</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1437</first_page>
						<last_page>1441</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2130</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/maggu18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>DN</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noor</given_name>
<surname>Fathima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nisar</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mahima</given_name>
<surname>C</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anuroop</given_name>
<surname>Iyengar</surname>
</person_name>
					</contributors>
					<titles><title>Development of Large Vocabulary Speech Recognition System with Keyword Search for Manipuri</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1031</first_page>
						<last_page>1035</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2133</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/patel18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dean</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunxiao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linzhong</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lixin</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Factorized Deep Neural Network Adaptation for Automatic Scoring of L2 Speech in English Speaking Tests</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1656</first_page>
						<last_page>1660</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2138</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/luo18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Glarner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Hanebrink</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Janek</given_name>
<surname>Ebbers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2688</first_page>
						<last_page>2692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2148</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/glarner18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>SaiKrishna</given_name>
<surname>Rallabandi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhavya</given_name>
<surname>Karki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carla</given_name>
<surname>Viegas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Nyberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alan W</given_name>
<surname>Black</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Utterance Level Representations for Detecting Intent from Acoustics</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>516</first_page>
						<last_page>520</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2149</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/rallabandi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sadeen</given_name>
<surname>Alharbi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madina</given_name>
<surname>Hasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony J H</given_name>
<surname>Simons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shelagh</given_name>
<surname>Brumfitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Phil</given_name>
<surname>Green</surname>
</person_name>
					</contributors>
					<titles><title>A Lightly Supervised Approach to Detect Stuttering in Children's Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3433</first_page>
						<last_page>3437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2155</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/alharbi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lili</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linjuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haotian</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Speech Emotion Recognition by Combining Amplitude and Phase Information Using Convolutional Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1611</first_page>
						<last_page>1615</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2156</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/guo18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Semi-tied Units for Efficient Gating in LSTM and Highway Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1773</first_page>
						<last_page>1777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2158</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Anastasopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Chiang</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Translations for Speech Transcription in Low-resource Settings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1279</first_page>
						<last_page>1283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2162</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/anastasopoulos18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kätlin</given_name>
<surname>Aare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pärtel</given_name>
<surname>Lippus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Wlodarczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mattias</given_name>
<surname>Heldner</surname>
</person_name>
					</contributors>
					<titles><title>Creak in the Respiratory Cycle</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1408</first_page>
						<last_page>1412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2165</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/aare18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yasin</given_name>
<surname>Özkanca</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cenk</given_name>
<surname>Demiroglu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aslı</given_name>
<surname>Besirli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Selime</given_name>
<surname>Celik</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Lingual Depression-Level Assessment from Conversational Speech Using Acoustic and Text Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3398</first_page>
						<last_page>3402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2169</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ozkanca18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jose</given_name>
<surname>Patino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Delgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>The EURECOM Submission to the First DIHARD Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2813</first_page>
						<last_page>2817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2172</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/patino18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karthik</given_name>
<surname>Girija Ramesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parth</given_name>
<surname>Suresh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Subband Weighting for Binaural Speech Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>861</first_page>
						<last_page>865</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2173</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/girijaramesan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shereen</given_name>
<surname>Oraby</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lena</given_name>
<surname>Reed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharath</given_name>
<surname>T.S.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubhangi</given_name>
<surname>Tandon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marilyn</given_name>
<surname>Walker</surname>
</person_name>
					</contributors>
					<titles><title>Neural MultiVoice Models for Expressing Novel Personalities in Dialog</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3057</first_page>
						<last_page>3061</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2174</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/oraby18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Masataki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Aono</surname>
</person_name>
					</contributors>
					<titles><title>Role Play Dialogue Aware Language Models Based on Conditional Hierarchical Recurrent Encoder-Decoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1259</first_page>
						<last_page>1263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2185</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/masumura18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name>
					</contributors>
					<titles><title>Patient Privacy in Paralinguistic Tasks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3428</first_page>
						<last_page>3432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2186</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/teixeira18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mehmet Ali Tuğtekin</given_name>
<surname>Turan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Engin</given_name>
<surname>Erzin</surname>
</person_name>
					</contributors>
					<titles><title>Monitoring Infant's Emotional Cry in Domestic Environments Using the Capsule Network Architecture</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>132</first_page>
						<last_page>136</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2187</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/turan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Medennikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Khokhlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksei</given_name>
<surname>Romanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Popov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Sorokin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Zatvornitskiy</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Mixup Training Strategies for Acoustic Models in ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2903</first_page>
						<last_page>2907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2191</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/medennikov18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Milde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Biemann</surname>
</person_name>
					</contributors>
					<titles><title>Unspeech: Unsupervised Speech Context Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2693</first_page>
						<last_page>2697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2194</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/milde18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raffaele</given_name>
<surname>Tavarone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Badino</surname>
</person_name>
					</contributors>
					<titles><title>Conditional-Computation-Based Recurrent Neural Networks for Computationally Efficient Acoustic Modelling</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1274</first_page>
						<last_page>1278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2195</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tavarone18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Drude</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Boeddeker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahn</given_name>
<surname>Heymann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Kinoshita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>Integrating Neural Network Based Beamforming and Weighted Prediction Error Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3043</first_page>
						<last_page>3047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2196</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/drude18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Sigtia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rob</given_name>
<surname>Haynes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hywel</given_name>
<surname>Richards</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Marchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Bridle</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Voice Trigger Detection for Low Resource Hardware</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2092</first_page>
						<last_page>2096</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2204</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sigtia18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuri</given_name>
<surname>Khokhlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Adaptive Training and Mixup Regularization for Neural Network Acoustic Models in Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2414</first_page>
						<last_page>2418</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2209</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tomashenko18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Ghannay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Camelin</surname>
</person_name>
					</contributors>
					<titles><title>Task Specific Sentence Embeddings for ASR Error Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1288</first_page>
						<last_page>1292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2211</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ghannay18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas D.</given_name>
<surname>Terissi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gonzalo</given_name>
<surname>Sad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mauricio</given_name>
<surname>Cerda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Ouni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Galvez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan C.</given_name>
<surname>Gómez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernard</given_name>
<surname>Girau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy</given_name>
<surname>Hitschfeld-Kahler</surname>
</person_name>
					</contributors>
					<titles><title>A French-Spanish Multimodal Speech Communication Corpus Incorporating Acoustic Data, Facial, Hands and Arms Gestures Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2778</first_page>
						<last_page>2782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2212</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/terissi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pramod</given_name>
<surname>Bachhav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name>
					</contributors>
					<titles><title>Artificial Bandwidth Extension with Memory Inclusion Using Semi-supervised Stacked Auto-encoders</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1185</first_page>
						<last_page>1189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2213</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bachhav18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bekir Berker</given_name>
<surname>Türker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Engin</given_name>
<surname>Erzin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yücel</given_name>
<surname>Yemez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Metin</given_name>
<surname>Sezgin</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Visual Prediction of Head-Nod and Turn-Taking Events in Dyadic Interactions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1741</first_page>
						<last_page>1745</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2215</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/turker18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mahesh Kumar</given_name>
<surname>Nandwana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>van Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitchell</given_name>
<surname>McLaren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Allen</given_name>
<surname>Stauffer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colleen</given_name>
<surname>Richey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aaron</given_name>
<surname>Lawson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Graciarena</surname>
</person_name>
					</contributors>
					<titles><title>Robust Speaker Recognition from Distant Speech under Real Reverberant Environments Using Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1106</first_page>
						<last_page>1110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2221</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nandwana18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jian</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jared</given_name>
<surname>Bernstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Rosenfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter W.</given_name>
<surname>Foltz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex S.</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Terje B.</given_name>
<surname>Holmlund</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brita</given_name>
<surname>Elvevåg</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Self-Reported and Observed Affect from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3653</first_page>
						<last_page>3657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2222</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cheng18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Wuxi Speakers’ Production and Perception of Coda Nasals in Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2559</first_page>
						<last_page>2562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2224</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Puyang</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wentao</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroya</given_name>
<surname>Fujisaki</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic and Perceptual Characteristics of Mandarin Speech in Homosexual and Heterosexual Male Speakers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1726</first_page>
						<last_page>1730</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2225</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/geng18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwas M.</given_name>
<surname>Shetty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rini A</given_name>
<surname>Sharon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Basil</given_name>
<surname>Abraham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejaswi</given_name>
<surname>Seeram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anusha</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithya</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S.</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory and Stacked Bottleneck Features for Low Resource Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3202</first_page>
						<last_page>3206</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2226</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shetty18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xi</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianhong</given_name>
<surname>Cai</surname>
</person_name>
					</contributors>
					<titles><title>Emotion Recognition from Variable-Length Speech Segments Using Deep Learning on Spectrograms</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3683</first_page>
						<last_page>3687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2228</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ma18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Guzewich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Zahorian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Corpora Convolutional Deep Neural Network Dereverberation Preprocessing for Speaker Verification and Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1329</first_page>
						<last_page>1333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2238</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/guzewich18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Khe Chai</given_name>
<surname>Sim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ananya</given_name>
<surname>Misra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshuman</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Golan</given_name>
<surname>Pundak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parisa</given_name>
<surname>Haghani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Bacchiani</surname>
</person_name>
					</contributors>
					<titles><title>Domain Adaptation Using Factorized Hidden Layer for Robust Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>892</first_page>
						<last_page>896</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2246</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sim18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Myungjong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beiming</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwanghoon</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Recognition Using Convolutional LSTM Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2948</first_page>
						<last_page>2952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2250</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Frédéric</given_name>
<surname>Béchet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Raymond</surname>
</person_name>
					</contributors>
					<titles><title>Is ATIS Too Shallow to Go Deeper for Benchmarking Spoken Language Understanding Models?</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3449</first_page>
						<last_page>3453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2256</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bechet18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chih Chi</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Lane</surname>
</person_name>
					</contributors>
					<titles><title>Online Incremental Learning for Speaker-Adaptive Language Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3363</first_page>
						<last_page>3367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2259</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung-Lin</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-Ya</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan-Yu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun-Chang</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeng-Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Self-Assessed Affect Recognition Using Fusion of Attentional BLSTM and Static Acoustic Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>536</first_page>
						<last_page>540</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2261</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/su18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guozhen</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rivka</given_name>
<surname>Levitan</surname>
</person_name>
					</contributors>
					<titles><title>Lexical and Acoustic Deep Learning Model for Personality Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1761</first_page>
						<last_page>1765</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2263</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/an18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guozhen</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah Ita</given_name>
<surname>Levitan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rivka</given_name>
<surname>Levitan</surname>
</person_name>
					</contributors>
					<titles><title>Deep Personality Recognition for Deception Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>421</first_page>
						<last_page>425</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2269</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/an18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danny</given_name>
<surname>Merkx</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Feature Classification Using Convolutional Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2142</first_page>
						<last_page>2146</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2275</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/merkx18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francis</given_name>
<surname>Tom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohit</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasenjit</given_name>
<surname>Dey</surname>
</person_name>
					</contributors>
					<titles><title>End-To-End Audio Replay Attack Detection Using Deep Convolutional Networks with Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>681</first_page>
						<last_page>685</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2279</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tom18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hakan</given_name>
<surname>Erdogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiong</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fil</given_name>
<surname>Alleva</surname>
</person_name>
					</contributors>
					<titles><title>Recognizing Overlapped Speech in Meetings: A Multichannel Separation Approach Using Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3038</first_page>
						<last_page>3042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2284</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yoshioka18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Tamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kento</given_name>
<surname>Horio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hajime</given_name>
<surname>Endo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoru</given_name>
<surname>Hayamizu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Audio-visual Voice Conversion Using Deep Canonical Correlation Analysis for Deep Bottleneck Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2469</first_page>
						<last_page>2473</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2286</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tamura18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marc Antony</given_name>
<surname>Hullebus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Tobin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adamantios</given_name>
<surname>Gafos</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-specific Structure in German Voiceless Stop Voice Onset Times</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1403</first_page>
						<last_page>1407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2288</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hullebus18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Delgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Integrated Presentation Attack Detection and Automatic Speaker Verification: Common Features and Gaussian Back-end Fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>77</first_page>
						<last_page>81</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2289</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/todisco18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>Real-time Single-channel Dereverberation and Separation with Time-domain Audio Separation Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>342</first_page>
						<last_page>346</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2290</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/luo18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Loren</given_name>
<surname>Lugosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikrant Singh</given_name>
<surname>Tomar</surname>
</person_name>
					</contributors>
					<titles><title>Tone Recognition Using Lifters and CTC</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2305</first_page>
						<last_page>2309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2293</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lugosch18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicanor</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Camilo</given_name>
<surname>Vásquez Correa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal I-vectors to Detect and Evaluate Parkinson's Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2349</first_page>
						<last_page>2353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2295</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/garcia18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gary</given_name>
<surname>Yeung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1661</first_page>
						<last_page>1665</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2297</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yeung18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heysem</given_name>
<surname>Kaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitrii</given_name>
<surname>Fedotov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali</given_name>
<surname>Yeşilkanat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oxana</given_name>
<surname>Verkholyak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>LSTM Based Cross-corpus and Cross-task Acoustic Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>521</first_page>
						<last_page>525</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2298</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kaya18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachel E.</given_name>
<surname>Bouserhal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philippe</given_name>
<surname>Chabot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milton</given_name>
<surname>Sarria-Paja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Cardinal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jérémie</given_name>
<surname>Voix</surname>
</person_name>
					</contributors>
					<titles><title>Classification of Nonverbal Human Produced Audio Events: A Pilot Study</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1512</first_page>
						<last_page>1516</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2299</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bouserhal18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Subhadeep</given_name>
<surname>Dey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name>
					</contributors>
					<titles><title>End-to-end Text-dependent Speaker Verification Using Novel Distance Measures</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3598</first_page>
						<last_page>3602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2300</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dey18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valter Akira</given_name>
<surname>Miasato Filho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diego Augusto</given_name>
<surname>Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis Gustavo</given_name>
<surname>Depra Cuozzo</surname>
</person_name>
					</contributors>
					<titles><title>Joint Discriminative Embedding Learning, Speech Activity and Overlap Detection for the DIHARD Speaker Diarization Challenge</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2818</first_page>
						<last_page>2822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2304</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/miasatofilho18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Megan</given_name>
<surname>Willi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jayaraman J.</given_name>
<surname>Thiagarajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Spanias</surname>
</person_name>
					</contributors>
					<titles><title>Triplet Network with Attention for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3608</first_page>
						<last_page>3612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2305</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/song18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Matějka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Novotný</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Dereverberation and Beamforming in Robust Far-Field Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1334</first_page>
						<last_page>1338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2306</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mosner18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fasih</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carl</given_name>
<surname>Vogel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nick</given_name>
<surname>Campbell</surname>
</person_name>
					</contributors>
					<titles><title>Improving Response Time of Active Speaker Detection Using Visual Prosody Information Prior to Articulation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1736</first_page>
						<last_page>1740</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2310</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/haider18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Wand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jürgen</given_name>
<surname>Schmidhuber</surname>
</person_name>
					</contributors>
					<titles><title>Domain-Adversarial Training for Session Independent EMG-based Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3167</first_page>
						<last_page>3171</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2318</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wand18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jilt</given_name>
<surname>Sebastian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manoj</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>D. S. Pavan</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema</given_name>
<surname>Murthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Denoising and Raw-waveform Networks for Weakly-Supervised Gender Identification on Noisy Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>292</first_page>
						<last_page>296</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2321</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sebastian18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chieh-Chi</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1358</first_page>
						<last_page>1362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2323</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adrien</given_name>
<surname>Le Franc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Riebling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Karadayi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Camila</given_name>
<surname>Scaff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>The ACLEW DiViMe: An Easy-to-use Diarization Tool</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1383</first_page>
						<last_page>1387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2324</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lefranc18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rajath</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>Music Source Activity Detection and Separation Using Deep Attractor Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>347</first_page>
						<last_page>351</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2326</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kumar18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Itshak</given_name>
<surname>Lapidot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Héctor</given_name>
<surname>Delgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Massimiliano</given_name>
<surname>Todisco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name>
					</contributors>
					<titles><title>Speech Database and Protocol Validation Using Waveform Entropy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2773</first_page>
						<last_page>2777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2330</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lapidot18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Volker</given_name>
<surname>Dellwo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thayabaran</given_name>
<surname>Kathiresan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisa</given_name>
<surname>Pellegrino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandra</given_name>
<surname>Schwab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dieter</given_name>
<surname>Maurer</surname>
</person_name>
					</contributors>
					<titles><title>Influences of Fundamental Oscillation on Speaker Identification in Vocalic Utterances by Humans and Computers</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3795</first_page>
						<last_page>3799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2331</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dellwo18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Bottleneck Features for Subword Modeling in Zero-resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2668</first_page>
						<last_page>2672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2334</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/hermann18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adnan</given_name>
<surname>Haider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Combining Natural Gradient with Hessian Free Methods for Sequence Training</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2918</first_page>
						<last_page>2922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2335</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/haider18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiran</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chieh-Chi</given_name>
<surname>Kao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Simple Model for Detection of Rare Sound Events</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1344</first_page>
						<last_page>1348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2338</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-An</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>811</first_page>
						<last_page>815</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2341</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chung18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anderson R.</given_name>
<surname>Avila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Douglas</given_name>
<surname>O'Shaughnessy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiago</given_name>
<surname>Falk</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Speech Enhancement and Perceptual Quality for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3663</first_page>
						<last_page>3667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2350</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/avila18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andy</given_name>
<surname>Murphy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irena</given_name>
<surname>Yanushevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailbhe</given_name>
<surname>Ní Chasaide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name>
					</contributors>
					<titles><title>Voice Source Contribution to Prominence Perception: Rd Implementation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>217</first_page>
						<last_page>221</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2352</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/murphy18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soheil</given_name>
<surname>Khorram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mimansa</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Gideon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melvin</given_name>
<surname>McInnis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Mower Provost</surname>
</person_name>
					</contributors>
					<titles><title>The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1903</first_page>
						<last_page>1907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2355</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/khorram18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avik</given_name>
<surname>Ray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilin</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxia</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Robust Spoken Language Understanding via Paraphrasing</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3454</first_page>
						<last_page>3458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2358</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ray18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leda</given_name>
<surname>Sari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kumaran</given_name>
<surname>S</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georg</given_name>
<surname>Stemmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishnakumar N</given_name>
<surname>Nair</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Adaptive Audio-Visual Fusion for the Open-Vocabulary Section of AVICAR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3524</first_page>
						<last_page>3528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2359</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sari18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Vlasenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jilt</given_name>
<surname>Sebastian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>D. S. Pavan</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew</given_name>
<surname>Magimai.-Doss</surname>
</person_name>
					</contributors>
					<titles><title>Implementing Fusion Techniques for the Classification of Paralinguistic Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>526</first_page>
						<last_page>530</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2360</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vlasenko18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karel</given_name>
<surname>Veselý</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Segura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Szöke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordi</given_name>
<surname>Luque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Lightly Supervised vs. Semi-supervised Training of Acoustic Model on Luxembourgish for Low-resource Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2883</first_page>
						<last_page>2887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2361</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/vesely18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keelan</given_name>
<surname>Evanini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Mulholland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rutuja</given_name>
<surname>Ubale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Pugh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aoife</given_name>
<surname>Cahill</surname>
</person_name>
					</contributors>
					<titles><title>Improvements to an Automated Content Scoring System for Spoken CALL Responses: the ETS Submission to the Second Spoken CALL Shared Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2379</first_page>
						<last_page>2383</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2362</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/evanini18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nils</given_name>
<surname>Holzenberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingxing</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Karadayi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Riad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>Learning Word Embeddings: Unsupervised Methods for Fixed-size Representations of Variable-length Speech Segments</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2683</first_page>
						<last_page>2687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2364</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/holzenberger18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vera</given_name>
<surname>Cabarrão</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fernando</given_name>
<surname>Batista</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Moniz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ana Isabel</given_name>
<surname>Mata</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-prosodic Entrainment in Structural Metadata Events</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2176</first_page>
						<last_page>2180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2366</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cabarrao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>João</given_name>
<surname>Cabral</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of the Asymmetry Parameter of the Glottal Flow Waveform Using the Electroglottographic Signal</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2997</first_page>
						<last_page>3001</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2371</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cabral18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Quatieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Lammert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Mitchell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Finkelstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Ekon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caitlin</given_name>
<surname>Dillon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Kenefick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Heaton</surname>
</person_name>
					</contributors>
					<titles><title>The Effect of Exposure to High Altitude and Heat on Speech Articulatory Coordination</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>297</first_page>
						<last_page>301</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2372</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/williamson18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Dyrenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Fuchs</surname>
</person_name>
					</contributors>
					<titles><title>The Diphthongs of Formal Nigerian English: A Preliminary Acoustic Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2563</first_page>
						<last_page>2567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2373</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dyrenko18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viet Anh</given_name>
<surname>Trinh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>McFee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael I</given_name>
<surname>Mandel</surname>
</person_name>
					</contributors>
					<titles><title>Bubble Cooperative Networks for Identifying Important Speech Cues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1616</first_page>
						<last_page>1620</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2377</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/trinh18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jane</given_name>
<surname>Wottawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amazouz</given_name>
<surname>Djegdjiga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Adda-Decker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name>
					</contributors>
					<titles><title>Studying Vowel Variation in French-Algerian Arabic Code-switched Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2753</first_page>
						<last_page>2757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2381</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wottawa18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soumi</given_name>
<surname>Maiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joey</given_name>
<surname>Ching</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Mandel</surname>
</person_name>
					</contributors>
					<titles><title>Large Vocabulary Concatenative Resynthesis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1190</first_page>
						<last_page>1194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2383</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/maiti18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Riad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corentin</given_name>
<surname>Dancette</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Karadayi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neil</given_name>
<surname>Zeghidour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Schatz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>Sampling Strategies in Siamese Networks for Unsupervised Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2658</first_page>
						<last_page>2662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2384</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/riad18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raquel</given_name>
<surname>Norel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mary</given_name>
<surname>Pietrowicz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carla</given_name>
<surname>Agurto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shay</given_name>
<surname>Rishoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillermo</given_name>
<surname>Cecchi</surname>
</person_name>
					</contributors>
					<titles><title>Detection of Amyotrophic Lateral Sclerosis (ALS) via Acoustic Analysis</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>377</first_page>
						<last_page>381</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2389</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/norel18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meredith</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemanth</given_name>
<surname>Venkateswara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sethuraman</given_name>
<surname>Panchanathan</surname>
</person_name>
					</contributors>
					<titles><title>Whistle-blowing ASRs: Evaluating the Need for More Inclusive Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>466</first_page>
						<last_page>470</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2391</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/moore18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phani Sankar</given_name>
<surname>Nidadavolu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng-I</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on Bandwidth Extension for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1111</first_page>
						<last_page>1115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2394</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/nidadavolu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixiaofan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Arousal and Valence from Waveforms and Spectrograms Using Deep Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3092</first_page>
						<last_page>3096</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2397</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yang18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jason</given_name>
<surname>Lilley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erin</given_name>
<surname>Crowgey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>H Timothy</given_name>
<surname>Bunnell</surname>
</person_name>
					</contributors>
					<titles><title>The Use of Machine Learning and Phonetic Endophenotypes to Discover Genetic Variants Associated with Speech Sound Disorder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>461</first_page>
						<last_page>465</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2398</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lilley18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Monika</given_name>
<surname>Podsiadło</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Ungureanu</surname>
</person_name>
					</contributors>
					<titles><title>Experiments with Training Corpora for Statistical Text-to-speech Systems.</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2002</first_page>
						<last_page>2006</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2400</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/podsiado18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raghav</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhinav</given_name>
<surname>Rastogi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dilek</given_name>
<surname>Hakkani-Tür</surname>
</person_name>
					</contributors>
					<titles><title>An Efficient Approach to Encoding Context for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3469</first_page>
						<last_page>3473</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2403</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gupta18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biing-Hwang (Fred)</given_name>
<surname>Juang</surname>
</person_name>
					</contributors>
					<titles><title>Cycle-Consistent Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1165</first_page>
						<last_page>1169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2409</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/meng18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Dmitriev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulia</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Matveeva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claude</given_name>
<surname>Montacié</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Boulard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yadviga</given_name>
<surname>Sinyavskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulia</given_name>
<surname>Zhukova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Zarazinski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Egor</given_name>
<surname>Akhanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Viksnin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Shlykov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Usova</surname>
</person_name>
					</contributors>
					<titles><title>LOCUST - Longitudinal Corpus and Toolset for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1096</first_page>
						<last_page>1100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2412</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/dmitriev18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed Imtiaz</given_name>
<surname>Humayun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md. Tauhiduzzaman</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shabnam</given_name>
<surname>Ghaffarzadegan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taufiq</given_name>
<surname>Hasan</surname>
</person_name>
					</contributors>
					<titles><title>An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>127</first_page>
						<last_page>131</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2413</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/humayun18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neil</given_name>
<surname>Zeghidour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Usunier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Synnaeve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronan</given_name>
<surname>Collobert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Recognition from the Raw Waveform</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>781</first_page>
						<last_page>785</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2414</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zeghidour18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ian</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjuli</given_name>
<surname>Kannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petar</given_name>
<surname>Aleksic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Rybach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>Sainath</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Speech Recognition in End-to-end Neural Network Systems Using Beam Search</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2227</first_page>
						<last_page>2231</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2416</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/williams18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Bonafonte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santiago</given_name>
<surname>Pascual</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgina</given_name>
<surname>Dorca</surname>
</person_name>
					</contributors>
					<titles><title>Spanish Statistical Parametric Speech Synthesis Using a Neural Vocoder</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1998</first_page>
						<last_page>2001</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2417</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bonafonte18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Sloboda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Lammert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Williamson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Smalt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daryush D.</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>COL Ian</given_name>
<surname>Curry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Heaton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeffrey</given_name>
<surname>Palmer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Quatieri</surname>
</person_name>
					</contributors>
					<titles><title>Vocal Biomarkers for Cognitive Performance Estimation in a Working Memory Task</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1756</first_page>
						<last_page>1760</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2418</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sloboda18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elnaz</given_name>
<surname>Shafaei-Bajestan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>R. Harald</given_name>
<surname>Baayen</surname>
</person_name>
					</contributors>
					<titles><title>Wide Learning for Auditory Comprehension</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>966</first_page>
						<last_page>970</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2420</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/shafaeibajestan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eva</given_name>
<surname>Fringi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Russell</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Phone Errors Attributable to Phonological Effects Associated With Language Acquisition Through Bottleneck Feature Visualisations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2573</first_page>
						<last_page>2577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2422</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/fringi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Stern</surname>
</person_name>
					</contributors>
					<titles><title>A Priori SNR Estimation Based on a Recurrent Neural Network for Robust Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3274</first_page>
						<last_page>3278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2423</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xia18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suliang</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunxin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meiyuh</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sining</given_name>
<surname>Sun</surname>
</person_name>
					</contributors>
					<titles><title>A Probability Weighted Beamformer for Noise Robust ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3048</first_page>
						<last_page>3052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2427</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bu18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yijia</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy</given_name>
<surname>McElwain</surname>
</person_name>
					</contributors>
					<titles><title>Infant Emotional Outbursts Detection in Infant-parent Spoken Interactions</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>242</first_page>
						<last_page>246</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2429</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xu18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Recognition with Nonlinear Distortion: Clipping Analysis and Impact</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>746</first_page>
						<last_page>750</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2430</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/xia18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Woodland</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Adaptation and Adaptive Training for Jointly Optimised Tandem Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>872</first_page>
						<last_page>876</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2432</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/wang18o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hueber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denis</given_name>
<surname>Beautemps</surname>
</person_name>
					</contributors>
					<titles><title>Visual Recognition of Continuous Cued Speech Using a Tandem CNN-HMM Approach</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2643</first_page>
						<last_page>2647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2434</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/liu18h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>A</given_name>
<surname>Padmasundari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivas</given_name>
<surname>Bangalore</surname>
</person_name>
					</contributors>
					<titles><title>Intent Discovery Through Unsupervised Semantic Text Clustering</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>606</first_page>
						<last_page>610</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2436</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/padmasundari18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ali Raza</given_name>
<surname>Syed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viet Anh</given_name>
<surname>Trinh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Mandel</surname>
</person_name>
					</contributors>
					<titles><title>Concatenative Resynthesis with Improved Training Signals for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1195</first_page>
						<last_page>1199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2439</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/syed18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aswin Shanmugam</given_name>
<surname>Subramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Student-Teacher Learning for BLSTM Mask-based Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3249</first_page>
						<last_page>3253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2440</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/subramanian18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hakan</given_name>
<surname>Erdogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Yoshioka</surname>
</person_name>
					</contributors>
					<titles><title>Investigations on Data Augmentation and Loss Functions for Deep Learning Based Speech-Background Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3499</first_page>
						<last_page>3503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2441</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/erdogan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarah Ita</given_name>
<surname>Levitan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel</given_name>
<surname>Maredia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic-Prosodic Indicators of Deception and Trust in Interview Dialogues</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>416</first_page>
						<last_page>420</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2443</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/levitan18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alessandra</given_name>
<surname>Cervone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evgeny</given_name>
<surname>Stepanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giuseppe</given_name>
<surname>Riccardi</surname>
</person_name>
					</contributors>
					<titles><title>Coherence Models for Dialogue</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1011</first_page>
						<last_page>1015</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2446</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cervone18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leonid</given_name>
<surname>Velikovich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Scheiner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petar</given_name>
<surname>Aleksic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Riley</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Lattice Processing in Contextual Automatic Speech Recognition for Google Assistant</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2222</first_page>
						<last_page>2226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2453</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/velikovich18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nick K</given_name>
<surname>Chibuye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Todd</given_name>
<surname>Rosenstock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>DeRenzi</surname>
</person_name>
					</contributors>
					<titles><title>Cross-language Phoneme Mapping for Low-resource Languages: An Exploration of Benefits and Trade-offs</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2623</first_page>
						<last_page>2627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2454</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chibuye18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adithya</given_name>
<surname>Renduchintala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuoyang</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Modal Data Augmentation for End-to-end ASR</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2394</first_page>
						<last_page>2398</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2456</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/renduchintala18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Karafiát</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Murali Karthick</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Szöke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimír</given_name>
<surname>Malenovský</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karel</given_name>
<surname>Veselý</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>František</given_name>
<surname>Grézl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>BUT OpenSAT 2017 Speech Recognition System</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2638</first_page>
						<last_page>2642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2457</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/karafiat18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter Sibbern</given_name>
<surname>Frederiksen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Effectiveness of Single-Channel BLSTM Enhancement for Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1823</first_page>
						<last_page>1827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2458</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/frederiksen18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biing-Hwang (Fred)</given_name>
<surname>Juang</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Feature-Mapping for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3259</first_page>
						<last_page>3263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2461</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/meng18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linxue</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Jančovič</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Russell</surname>
</person_name>
					</contributors>
					<titles><title>Exploring How Phone Classification Neural Networks Learn Phonetic Information by Visualising and Interpreting Bottleneck Features</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1472</first_page>
						<last_page>1476</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2462</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bai18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Reza</given_name>
<surname>Lotfian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Categorical Emotions by Jointly Learning Primary and Secondary Emotions through Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>951</first_page>
						<last_page>955</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2464</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lotfian18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaejin</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghavendra</given_name>
<surname>Pappagari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Purva</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yishay</given_name>
<surname>Carmiel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>247</first_page>
						<last_page>251</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2466</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cho18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Igor</given_name>
<surname>Jauk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaime</given_name>
<surname>Lorenzo-Trueba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Bonafonte</surname>
</person_name>
					</contributors>
					<titles><title>Expressive Speech Synthesis Using Sentiment Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3062</first_page>
						<last_page>3066</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2467</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/jauk18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jayadev</given_name>
<surname>Billa</surname>
</person_name>
					</contributors>
					<titles><title>ISI ASR System for the Low Resource Speech Recognition Challenge for Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3207</first_page>
						<last_page>3211</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2473</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/billa18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nanxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesús</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>An Investigation of Non-linear i-vectors for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>87</first_page>
						<last_page>91</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2474</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chen18j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Bandini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Richburg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yana</given_name>
<surname>Yunusova</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection of Orofacial Impairment in Stroke</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1711</first_page>
						<last_page>1715</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2475</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bandini18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zoltán</given_name>
<surname>Tüske</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Investigation on LSTM Recurrent N-gram Language Models for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3358</first_page>
						<last_page>3362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2476</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tuske18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Srinivas</given_name>
<surname>Parthasarathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Preference-Learning with Qualitative Agreement for Sentence Level Emotional Annotations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>252</first_page>
						<last_page>256</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2478</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/parthasarathy18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Beiming</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myungjong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun R.</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>van Santen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ted</given_name>
<surname>Mau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Articulation-to-Speech Synthesis Using Articulatory Flesh Point Sensors’ Orientation Information</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3152</first_page>
						<last_page>3156</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2484</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/cao18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maharajan</given_name>
<surname>Chellapriyadharshini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anoop</given_name>
<surname>Toffy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasa Raghavan</given_name>
<surname>K. M.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>V</given_name>
<surname>Ramasubramanian</surname>
</person_name>
					</contributors>
					<titles><title>Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model Refinement for a Low Resource Indian Language</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1041</first_page>
						<last_page>1045</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2486</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/chellapriyadharshini18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Audiovisual Speech Activity Detection with Advanced Long Short-Term Memory</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1244</first_page>
						<last_page>1248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2490</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tao18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bayya</given_name>
<surname>Yegnanarayana</surname>
</person_name>
					</contributors>
					<titles><title>Estimation of Fundamental Frequency from Singing Voice Using Harmonics of Impulse-like Excitation Source</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2319</first_page>
						<last_page>2323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2495</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kadiri18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwanghoon</given_name>
<surname>An</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myungjong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Teplansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan</given_name>
<surname>Green</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Campbell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yana</given_name>
<surname>Yunusova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daragh</given_name>
<surname>Heitzman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Early Detection of Amyotrophic Lateral Sclerosis from Intelligible Speech Using Convolutional Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1913</first_page>
						<last_page>1917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2496</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/an18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bayya</given_name>
<surname>Yegnanarayana</surname>
</person_name>
					</contributors>
					<titles><title>Breathy to Tense Voice Discrimination using Zero-Time Windowing Cepstral Coefficients (ZTWCCs)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>232</first_page>
						<last_page>236</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2498</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kadiri18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sudarsana Reddy</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bayya</given_name>
<surname>Yegnanarayana</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and Detection of Phonation Modes in Singing Voice using Excitation Source Features and Single Frequency Filtering Cepstral Coefficients (SFFCC)</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>441</first_page>
						<last_page>445</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2502</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kadiri18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Williams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paola</given_name>
<surname>Escudero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adamantios</given_name>
<surname>Gafos</surname>
</person_name>
					</contributors>
					<titles><title>Perceptual Sensitivity to Spectral Change in Australian English Close Front Vowels: An Electroencephalographic Investigation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1442</first_page>
						<last_page>1446</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2505</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/williams18b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kusha</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivas</given_name>
<surname>Parthasarathy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Role of Regularization in the Prediction of Valence from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>941</first_page>
						<last_page>945</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2508</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/sridhar18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maida</given_name>
<surname>Percival</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexei</given_name>
<surname>Kochetov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonjung</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>An Ultrasound Study of Gemination in Coronal Stops in Eastern Oromo</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1531</first_page>
						<last_page>1535</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2512</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/percival18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jenny</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Zahner</surname>
</person_name>
					</contributors>
					<titles><title>Truncation and Compression in Southern German and Australian English</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1833</first_page>
						<last_page>1837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2513</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yu18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mandar</given_name>
<surname>Gogate</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahsan</given_name>
<surname>Adeel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Barker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussain</surname>
</person_name>
					</contributors>
					<titles><title>DNN Driven Speaker Independent Audio-Visual Mask Estimation for Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2723</first_page>
						<last_page>2727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2516</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gogate18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Seltzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Improved Training for Online End-to-end Speech Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2913</first_page>
						<last_page>2917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2517</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuka</given_name>
<surname>Al Hanai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Ghassemi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Depression with Audio/Text Sequence Modeling of Interviews</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1716</first_page>
						<last_page>1720</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2522</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/alhanai18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Warlaumont</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Automated Classification of Children’s Linguistic versus Non-Linguistic Vocalisations</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2588</first_page>
						<last_page>2592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2523</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyed Hamidreza</given_name>
<surname>Mohammadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taehwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Investigation of Using Disentangled and Interpretable Representations for One-shot Cross-lingual Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2833</first_page>
						<last_page>2837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2525</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/mohammadi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandrakant</given_name>
<surname>Bothe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sven</given_name>
<surname>Magg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cornelius</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Wermter</surname>
</person_name>
					</contributors>
					<titles><title>Conversational Analysis Using Utterance-level Attention-based Bidirectional Recurrent Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>996</first_page>
						<last_page>1000</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2527</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/bothe18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manjunath</given_name>
<surname>K E</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sreenivasa</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinesh Babu</given_name>
<surname>Jayagopi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>V</given_name>
<surname>Ramasubramanian</surname>
</person_name>
					</contributors>
					<titles><title>Indian Languages ASR: A Multilingual Phone Recognition Framework with IPA Based Common Phone-set, Predicted Articulatory Features and Feature fusion</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1016</first_page>
						<last_page>1020</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2529</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ke18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arne</given_name>
<surname>Köhn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oskar</given_name>
<surname>Dörfler</surname>
</person_name>
					</contributors>
					<titles><title>An Empirical Analysis of the Correlation of Syntax and Prosody</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2157</first_page>
						<last_page>2161</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2530</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kohn18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christer</given_name>
<surname>Gobl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andy</given_name>
<surname>Murphy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irena</given_name>
<surname>Yanushevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ailbhe</given_name>
<surname>Ní Chasaide</surname>
</person_name>
					</contributors>
					<titles><title>On the Relationship between Glottal Pulse Shape and Its Spectrum: Correlations of Open Quotient, Pulse Skew and Peak Flow with Source Harmonic Amplitudes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>222</first_page>
						<last_page>226</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2532</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/gobl18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timo</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hussein</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Burkhard</given_name>
<surname>Meyer-Sickendiek</surname>
</person_name>
					</contributors>
					<titles><title>Analysing the Focus of a Hierarchical Attention Network: the Importance of Enjambments When Classifying Post-modern Poetry</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2162</first_page>
						<last_page>2166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2533</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/baumann18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pramit</given_name>
<surname>Saha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Praneeth</given_name>
<surname>Srungarapu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>Towards Automatic Speech Identification from Vocal Tract Shape Dynamics in Real-time MRI</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1249</first_page>
						<last_page>1253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2537</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/saha18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Keiko</given_name>
<surname>Ochi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Sakai</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Evaluation of Soft Articulatory Contact for Stuttering Treatment</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1546</first_page>
						<last_page>1550</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2544</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/ochi18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karan</given_name>
<surname>Singla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuohao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolaos</given_name>
<surname>Flemotomos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dogan</given_name>
<surname>Can</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Atkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Using Prosodic and Lexical Information for Learning Utterance-level Behaviors in Psychotherapy</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3413</first_page>
						<last_page>3417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2551</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/singla18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Constantinos</given_name>
<surname>Papayiannis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justice</given_name>
<surname>Amoh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viktor</given_name>
<surname>Rozgic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiva</given_name>
<surname>Sundaram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Detecting Media Sound Presence in Acoustic Scenes</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1363</first_page>
						<last_page>1367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2559</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/papayiannis18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longfei</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanlu</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsong</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Mandarin Tone Recognition Using Convolutional Bidirectional Long Short-Term Memory with Attention</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>352</first_page>
						<last_page>356</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2561</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/yang18d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Variani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michiel</given_name>
<surname>Bacchiani</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3028</first_page>
						<last_page>3032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2566</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/kim18g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Achuth</given_name>
<surname>Rao MV</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Krishnamurthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pebbili</given_name>
<surname>Gopikishore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veeramani</given_name>
<surname>Priyadharshini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Glottis Localization and Segmentation in Stroboscopic Videos Using Deep Neural Network</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3007</first_page>
						<last_page>3011</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2572</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/raomv18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eshete Derb</given_name>
<surname>Emiru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengwu</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yichang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multi-frame Quantization of LSF Parameters Using a Deep Autoencoder and Pyramid Vector Quantizer</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3553</first_page>
						<last_page>3557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2577</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengwu</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueming</given_name>
<surname>Ding</surname>
</person_name>
					</contributors>
					<titles><title>Multi-frame Coding of LSF Parameters Using Block-Constrained Trellis Coded Vector Quantization</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>3558</first_page>
						<last_page>3562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2578</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/li18q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dengke</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlin</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>An End-to-End Deep Learning Framework for Speech Emotion Recognition of Atypical Individuals</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>162</first_page>
						<last_page>166</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2581</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/tang18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Greenwood</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iain</given_name>
<surname>Matthews</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephen</given_name>
<surname>Laycock</surname>
</person_name>
					</contributors>
					<titles><title>Joint Learning of Facial Expression and Head Pose from Speech</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2484</first_page>
						<last_page>2488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2587</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/greenwood18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoufeng</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>A New Frequency Coverage Metric and a New Subband Encoding Model, with an Application in Pitch Estimation</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>2147</first_page>
						<last_page>2151</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2590</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/lin18c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William</given_name>
<surname>Katz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Reidy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divya</given_name>
<surname>Prabhakaran</surname>
</person_name>
					</contributors>
					<titles><title>Sensorimotor Response to Tongue Displacement Imagery by Talkers with Parkinson’s Disease</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1502</first_page>
						<last_page>1506</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2592</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/katz18_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cuiling</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Si</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Analysis of Whispery Voice Disguise in Mandarin Chinese</title></titles>
					<publication_date media_type='online'>
						<month>9</month>
						<day>2</day>
						<year>2018</year>
					</publication_date>
					<pages>
						<first_page>1413</first_page>
						<last_page>1416</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2018-2598</doi>
						<resource>https://www.isca-archive.org/interspeech_2018/zhang18p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>