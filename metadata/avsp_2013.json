{
 "location": "Annecy, France",
 "startDate": "29/8/2013",
 "endDate": "1/9/2013",
 "conf": "AVSP",
 "year": "2013",
 "name": "avsp_2013",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "29 August - 1 September 2013",
 "booklet": "avsp_2013.pdf",
 "papers": {
  "cangelosi13_avsp": {
   "authors": [
    [
     "Angelo",
     "Cangelosi"
    ]
   ],
   "title": "Embodied language learning with the humanoid robot icub",
   "original": "av13_001",
   "page_count": 2,
   "order": 1,
   "p1": "1",
   "pn": "",
   "abstract": [
    "Growing theoretical and experimental research on action and language processing and on number learning and space representation clearly demonstrates the role of embodiment in cognition. These studies have important implications for the design of communication and linguistic capabilities in cognitive systems and robots, and have led to the new interdisciplinary approach of Cognitive Developmental Robotics. In the European FP7 project “ITALK” (www.italkproject.org) and the Marie Curie ITN “RobotDoC” (www.robotdoc.org) we follow this integrated view of action and language to develop cognitive capabilities in the humanoid robot iCub. During the talk we will present ongoing results from iCub experiments on embodiment biases in early word acquisition studies, word order cues for lexical development and number and space interaction effects. The talk will also introduce the simulation software of the iCub robot, an open source software tool to perform cognitive modeling experiments in simulation\n",
    ""
   ]
  },
  "spence13_avsp": {
   "authors": [
    [
     "Charles",
     "Spence"
    ]
   ],
   "title": "Audiovisual speech integration: modulatory factors and the link to sound symbolism",
   "original": "av13_003",
   "page_count": 2,
   "order": 2,
   "p1": "3",
   "pn": "",
   "abstract": [
    "In this talk, I will review some of the latest findings from the burgeoning literature on the audiovisual integration of speech stimuli. I will focus on those factors that have been demonstrated to influence this form of multisensory integration (such as temporal coincidence, speaker/gender matching, and attention; Vatakis & Spence, 2007, 2010). I will also look at a few of the other factors that appear not to matter as much as some researchers have argued that they should, such as spatial coincidence (except under a particular subset of circumstances; Spence 2013). I will also look at the emerging literature that has documented the widespread existence of audiovisual crossmodal correspondences, and consider their putative link to the literature on embodied cognition (Spence & Deroy, 2013).\n",
    "s Spence, C. (2013). Just how important is spatial coincidence to multisensory integration? Evaluating the spatial rule. Annals of the New York Academy of Sciences. Spence, C., & Deroy, O. (2012). Hearing mouth shapes: Sound symbolism and the reverse McGurk effect. i- Perception, 3, 550-556. Vatakis, A., & Spence, C. (2007). Crossmodal binding: Evaluating the “unity assumption” using audiovisual speech stimuli. Perception & Psychophysics, 69, 744-756. Vatakis, A., & Spence, C. (2010). Audiovisual temporal integration for complex speech, object-action, animal call, and musical stimuli. In M. J. Naumer & J. Kaiser (Eds.), Multisensory object perception in the primate brain (pp. 95-121). New York: Springer.\n",
    ""
   ]
  },
  "visser13_avsp": {
   "authors": [
    [
     "Mandy",
     "Visser"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Who presents worst? a study on expressions of negative feedback in different intergroup contexts",
   "original": "av13_005",
   "page_count": 6,
   "order": 3,
   "p1": "5",
   "pn": "10",
   "abstract": [
    "We studied the effect of two intergroup contexts (in-group and out-group) on the way people use verbal and nonverbal signals when giving feedback. Participants evaluated another person’s low quality oral presentation by filling out evaluation forms and by making a personal video message for the speaker. Participants were led to believe that the addressee was either a fellow student (in-group condition) or a student from another university (outgroup condition). We found no intergroup effect on written feedback. Next, independent judges rated feedback givers’ appreciation in the video messages (spoken feedback) based on verbal and nonverbal expressions. When judging only the verbal content, messages addressed to out-group members were more appreciative than messages addressed to in-group members. When these messages were judged on the speaker’s nonverbal expressions, messages addressed to in-group members were more appreciative than the messages to out-group members.\n",
    "Index Terms: social interaction, nonverbal expressions, perception, intergroup communication, social identity theory, politeness.\n",
    ""
   ]
  },
  "barbulescu13_avsp": {
   "authors": [
    [
     "Adela",
     "Barbulescu"
    ],
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Remi",
     "Ronfard"
    ]
   ],
   "title": "Audio-visual speaker conversion using prosody features",
   "original": "av13_011",
   "page_count": 6,
   "order": 4,
   "p1": "11",
   "pn": "16",
   "abstract": [
    "The article presents a joint audio-video approach towards speaker identity conversion, based on statistical methods originally introduced for voice conversion. Using the experimental data from the 3D BIWI Audiovisual corpus of Affective Communication, mapping functions are built between each two speakers in order to convert speaker-specific features: speech signal and 3D facial expressions. The results obtained by combining audio and visual features are compared to corresponding results from earlier approaches, while outlining the improvements brought by introducing dynamic features and exploiting prosodic features.\n",
    "Index Terms: speaker identity conversion, gaussian mixture model, dynamic features, prosodic features\n",
    ""
   ]
  },
  "zelic13_avsp": {
   "authors": [
    [
     "Gregory",
     "Zelic"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Spontaneous synchronisation between repetitive speech and rhythmic gesture",
   "original": "av13_017",
   "page_count": 4,
   "order": 5,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "Although studies have described how motion in diverse biological systems may spontaneously synchronize it is not known whether speech and gesture exhibit such a property. Previous research on the coordination of speech and gesture has focused on pointing or tapping tasks, the structure of which may regulate speech and gesture dynamics. Here we examined whether synchronies might arise between a repetitive utterance and rhythmic finger movement oscillations in a non-intentional paradigm. Participants were instructed to repeatedly utter /ba/ or /sa/ syllables with/without vocalizing, while continuously moving their right index finger in flexion/extension. No instructions about synchronization were given; participants were only told to adopt the most comfortable motions. We expected that the larger amplitude of face motion for /ba/ syllables and vocalized speech would lead to greater influence on the gesture. In contrast, the results showed more synchronization for /sa/ and when syllables were articulated silently. Less perceptive feedback may lead to a reduction in the robustness of the speech component, making it more susceptible to gesture influence.\n",
    "Index Terms: non-intentional synchronization, speech, gesture\n",
    ""
   ]
  },
  "mui13_avsp": {
   "authors": [
    [
     "Phoebe",
     "Mui"
    ],
    [
     "Martijn",
     "Goudbeek"
    ],
    [
     "Marc",
     "Swerts"
    ],
    [
     "Per van der",
     "Wijst"
    ]
   ],
   "title": "Culture and nonverbal cues: how does power distance influence facial expressions in game contexts?",
   "original": "av13_021",
   "page_count": 6,
   "order": 6,
   "p1": "21",
   "pn": "26",
   "abstract": [
    "Power distance is one of the most prominent cultural dimensions underlying cultural differences in beliefs and values. However, how power distance is evident in the more tangible domain of behavior, such as nonverbal cues, has not been well-documented. In our study, we recruited Dutch (low power distance culture) and Chinese (high power distance culture) university students to play games against two opponents via an ostensibly real-time internet connection. Unbeknownst to the participants, the two opponents were in fact confederates, who assumed the role of either a highly successful full professor or a down-to-earth undergraduate freshman. Throughout the game, participants were videotaped. Analyses of the resulting recordings showed that Chinese students showed more submission in their nonverbal behavior than Dutch students in general; what was most remarkable was that such a cultural difference in submission was more evident when the opponent was a professor than when the opponent was a fellow student. To the best of our knowledge, these findings are the first to illustrate that the role of power distance could indeed be reflected in nonverbal behavior exhibited in a naturalistic social setting.\n",
    "Index Terms: nonverbal cues, facial expressions, culture, power distance, game contexts\n",
    ""
   ]
  },
  "honemann13_avsp": {
   "authors": [
    [
     "Angelika",
     "Hönemann"
    ],
    [
     "Diego",
     "Evin"
    ],
    [
     "Alejandro J.",
     "Hadad"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Predicting head motion from prosodic and linguistic features",
   "original": "av13_027",
   "page_count": 4,
   "order": 7,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "This paper describes an approach to predict non-verbal cues from speech-related features. Our previous investigations of audiovisual speech showed that there are strong correlations between the two modalities. In this work we developed two models using different kinds of Recurrent Artificial Neural Networks: Elman and NARX, to predict parameters of activity for head motion using linguistic and prosodic inputs, and compared their performance. Prosodic inputs included F0 and intensity, while linguistic parameters included the former plus additional information such as the type of syllables, phrases, and different relations between them. Using speaker specific models for six subjects, performance measures in terms of root mean square error (RMSE) showed that there are significant differences between the models with respect to the input parameters, and that NARX network outperformed the Elman network on the prediction task.\n",
    "Index Terms: predicting head motion, audiovisual speech, timedelayed NARX, Elman NN, linguistic vs. prosodic features\n",
    ""
   ]
  },
  "hollenstein13_avsp": {
   "authors": [
    [
     "Jakob",
     "Hollenstein"
    ],
    [
     "ichael",
     "Pucher"
    ],
    [
     "Dietmar",
     "Schabus"
    ]
   ],
   "title": "Visual control of hidden-semi-Markov-model based acoustic speech synthesis",
   "original": "av13_031",
   "page_count": 6,
   "order": 8,
   "p1": "31",
   "pn": "36",
   "abstract": [
    "We show how to visually control acoustic speech synthesis by modelling the dependency between visual and acoustic parameters within the Hidden-Semi-Markov-Model (HSMM) based speech synthesis framework. A joint audio-visual model is trained with 3D facial marker trajectories as visual features. Since the dependencies of acoustic features on visual features are only present for certain phones, we implemented a model where dependencies are estimated for a set of vowels only. A subjective evaluation consisting of a vowel identification task showed that we can transform some vowel trajectories in a phonetically meaningful way by controlling the visual parameters in PCA space. These visual parameters can also be interpreted as fundamental visual speech motion components, which leads to an intuitive control model.\n",
    "Index Terms: audio-visual speech synthesis, HMM-based speech synthesis, controllability\n",
    ""
   ]
  },
  "schabus13_avsp": {
   "authors": [
    [
     "Dietmar",
     "Schabus"
    ],
    [
     "Michael",
     "Pucher"
    ],
    [
     "Gregor",
     "Hofer"
    ]
   ],
   "title": "Objective and subjective feature evaluation for speaker-adaptive visual speech synthesis",
   "original": "av13_037",
   "page_count": 6,
   "order": 9,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "This paper describes an evaluation of a feature extraction method for visual speech synthesis that is suitable for speaker-adaptive training of a Hidden Semi-Markov Model (HSMM)-based visual speech synthesizer. An audio-visual corpus from three speakers was recorded. While the features used for the auditory modality are well understood, we propose to use a standard Principal Component Analysis (PCA) approach to extract suitable features for training and synthesis of the visual modality. A PCA-based approach provides dimensionality reduction and component de-correlation on the 3D facial marker data which was recorded using a facial motion capturing system. Enabling visual average “voice” training and speaker-adaptation brings a key strength of the HMM framework into both the visual and the audio-visual domain. An objective evaluation based on reconstruction error calculations, as well as a perceptual evaluation with 40 test subjects, show that PCA is well suited for feature extraction from multiple speakers, even in a challenging adaptation scenario where no data from the target speaker is available during PCA.\n",
    ""
   ]
  },
  "shen13_avsp": {
   "authors": [
    [
     "Peng",
     "Shen"
    ],
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Audio-visual interaction in sparse representation features for noise robust audio-visual speech recognition",
   "original": "av13_043",
   "page_count": 6,
   "order": 10,
   "p1": "43",
   "pn": "48",
   "abstract": [
    "In this paper, we investigate audio-visual interaction in sparse representation to obtain robust features for audio-visual speech recognition. Firstly, we introduce our system which uses sparse representation method for noise robust audio-visual speech recognition. Then, we introduce the dictionary matrix used in this paper, and consider the construction of audio-visual dictionary. Finally, we reformulate audio and visual signals as a group sparse representation problem in a combined featurespace domain, and then we improve the joint sparsity feature fusion method with the group sparse representation features and audio sparse representation features. The proposed methods are evaluated using CENSREC-1-AV database with both audio noise and visual noise. From the experimental results, we showed the effectiveness of our proposed method comparing with traditional methods.\n",
    "Index Terms: sparse representation, audio-visual speech recognition, feature fusion, noise reduction, joint sparsity model\n",
    ""
   ]
  },
  "parocosta13_avsp": {
   "authors": [
    [
     "Paula D.",
     "Paro Costa"
    ],
    [
     "José Mario De",
     "Martino"
    ]
   ],
   "title": "Assessing the visual speech perception of sampled-based talking heads",
   "original": "av13_049",
   "page_count": 6,
   "order": 11,
   "p1": "49",
   "pn": "54",
   "abstract": [
    "Focusing on flexible applications for limited computing devices, this paper investigates the improvement on the visual speech perception obtained by the implicitly modeling of coarticulation on a sample-based talking head that is characterized by a compact image database and a morphing visemes synthesis strategy. Speech intelligibility tests were applied to assess the effectiveness of the proposed context-dependent visemes (CDV) model, comparing it to a simpler model that does not handle coarticulation. The results show that, when compared to the simpler model, the CDV approach improves speech intelligibility in situations in which the audio is degraded by noise. Moreover the CDVmodel achieves 80%to 90% of visual speech intelligibility of video of a real talker in the tested cases. Additionally, when the audio is heavily degraded by noise, the results suggest that the mechanisms that explain visual speech perception depends on the quality of the audible information.\n",
    "Index Terms: facial animation, sample-based, 2D, speech intelligibility\n",
    ""
   ]
  },
  "steiner13_avsp": {
   "authors": [
    [
     "Ingmar",
     "Steiner"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Speech animation using electromagnetic articulography as motion capture data",
   "original": "av13_055",
   "page_count": 6,
   "order": 12,
   "p1": "55",
   "pn": "60",
   "abstract": [
    "Electromagnetic articulography (EMA) captures the position and orientation of a number of markers, attached to the articulators, during speech. As such, it performs the same function for speech that conventional motion capture does for full-body movements acquired with optical modalities, a long-time staple technique of the animation industry.   In this paper, EMA data is processed from a motion-capture perspective and applied to the visualization of an existing multimodal corpus of articulatory data, creating a kinematic 3D model of the tongue and teeth by adapting a conventional motion capture based animation paradigm. This is accomplished using off-the-shelf, open-source software. Such an animated model can then be easily integrated into multimedia applications as a digital asset, allowing the analysis of speech production in an intuitive and accessible manner.   The processing of the EMA data, its co-registration with 3D data from vocal tract magnetic resonance imaging (MRI) and dental scans, and the modeling workflow are presented in detail, and several issues discussed.\n",
    "Index Terms: speech production, articulatory data, electromagnetic articulography, vocal tract, motion capture, visualization\n",
    ""
   ]
  },
  "baart13_avsp": {
   "authors": [
    [
     "Martijn",
     "Baart"
    ],
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Kathleen E.",
     "Shaw"
    ],
    [
     "Heather",
     "Bortfeld"
    ]
   ],
   "title": "Phonetic information in audiovisual speech is more important for adults than for infants; preliminary findings.",
   "original": "av13_061",
   "page_count": 4,
   "order": 13,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "Infants and adults are able to match auditory and visual speech but the cues on which they rely may differ. Here we provide an initial assessment of the relative contribution of temporal- and phonetic cues available in the AV signal. Adults (N=52) and infants (N=18) matched 2 trisyllabic speech sounds, either natural speech or SWS, with visual speech information. Adults saw two articulating faces and matched a sound to one of these, while infants were presented with the same stimuli in a preferential looking paradigm. Adults’ performance was almost flawless with natural speech, but was significantly less accurate with SWS. In contrast, infants matched the sound to the articulating face, irrespective of whether it was natural speech or SWS. We propose that infants matched the AV signal based on temporal cues whereas adults relied more heavily on phonetic cues. This is in line with the idea that lipreading improves with age.\n",
    "Index Terms: Phonetic correspondence, temporal correspondence, audiovisual speech, sine-wave speech\n",
    ""
   ]
  },
  "irwin13_avsp": {
   "authors": [
    [
     "Julia R.",
     "Irwin"
    ],
    [
     "Lawrence",
     "Brancazio"
    ]
   ],
   "title": "Audiovisual speech perception in children with autism spectrum disorders and typical controls",
   "original": "av13_065",
   "page_count": 6,
   "order": 14,
   "p1": "65",
   "pn": "70",
   "abstract": [
    "This paper presents data comparing children with autism spectrum disorders (ASD) to those with typical development (TD) on auditory, visual and audiovisual speech perception. Using eye tracking methodology, we assessed group differences in visual influence on heard speech and pattern of gaze to speaking faces. There were no differences in perception of auditory syllables /ma/ and /na/ in clear listening conditions or in the presence of noise. In addition, there were no differences in perception of a non-speech, non-face control. However, children with ASD were significantly less visually influenced than TD controls in mismatched AV and speech reading conditions, and showed less visual gain (AV speech in the presence of auditory noise). Further, to examine whether differential patterns of gaze may underlie these findings, we examined participant gaze to the speaking faces. The children with ASD looked significantly less to the face of the speaker overall. When children with ASD looked at a speaker’s face, they looked less at the mouth of the speaker and more to non-focal areas of the face during the speech reading and AV speech in noise conditions. No group differences were observed for pattern of gaze to non-face, nonspeech controls.\n",
    "Index Terms: audiovisual speech perception, autism spectrum disorders, eye tracking.\n",
    ""
   ]
  },
  "fort13_avsp": {
   "authors": [
    [
     "Mathilde",
     "Fort"
    ],
    [
     "Alexa",
     "Weiß"
    ],
    [
     "Alexander",
     "Martin"
    ],
    [
     "Sharon",
     "Peperkamp"
    ]
   ],
   "title": "Looking for the bouba-kiki effect in prelexical infants",
   "original": "av13_071",
   "page_count": 6,
   "order": 15,
   "p1": "71",
   "pn": "76",
   "abstract": [
    "Adults and toddlers systematically associate certain pseudowords, such as `bouba´ and `kiki´, with round and spiky shapes, respectively. The ontological origin of this so-called bouba-kiki effect is unknown: it could be an unlearned aspect of perception, appear with language exposure, or only emerge with the ability to produce speech sounds (i.e., babbling). We report the results of three experiments with five- and six-month-olds that found no bouba-kiki effect at all. We discuss the consequences of these findings for the emergence of cross-modal associations in infant speech perception.\n",
    "Index Terms: sound symbolism, bouba-kiki effect, prelexical infants, early language acquisition\n",
    ""
   ]
  },
  "groen13_avsp": {
   "authors": [
    [
     "Margriet A.",
     "Groen"
    ],
    [
     "Alexandra",
     "Jesse"
    ]
   ],
   "title": "Audiovisual speech perception in children and adolescents with developmental dyslexia: no deficit with McGurk stimuli",
   "original": "av13_077",
   "page_count": 4,
   "order": 16,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "Developmental dyslexia could, at least partially, reflect an underlying problem in forming audiovisual associations, such as between graphemes and phonemes. Some of the few studies testing people with reading difficulties on McGurk stimuli report less sensitivity to visual information, and worse processing of visual-only speech. In this study, we tested Dutch children (M = 11.0 years) and adolescents (M = 13.7 years) with developmental dyslexia, and age-matched controls. Dyslexics and age-matched controls were similarly able to recognize the nonsense syllables “apa” and “aka” from hearing or seeing a speaker. Most critically, dyslexics and controls showed similar response patterns to McGurk stimuli, consisting of hearing “apa” combined with seeing a speaker say “aka”. Adolescents, however, perceived McGurk stimuli more often as /k/ and somewhat less often as /p/ than children, confirming earlier studies investigating age differences. Both groups did not differ in their number of fusion (/t/) responses. Concluding, audiovisual speech perception does not seem to be impaired in developmental dyslexia, if groups show similar unimodal speech perception.\n",
    "Index Terms: speech perception, dyslexia, McGurk effect, development\n",
    ""
   ]
  },
  "fecher13_avsp": {
   "authors": [
    [
     "Natalie",
     "Fecher"
    ],
    [
     "Dominic",
     "Watt"
    ]
   ],
   "title": "Effects of forensically-realistic facial concealment on auditory-visual consonant recognition in quiet and noise conditions",
   "original": "av13_081",
   "page_count": 6,
   "order": 17,
   "p1": "81",
   "pn": "86",
   "abstract": [
    "The study presented in this paper investigates auditory-only and auditory-visual (AV) consonant recognition where the talker’s face is obscured by various types of face-concealing garments and headgear. Observers’ consonant identification performance across the various ‘facewear’ conditions was tested both in quiet listening conditions (Experiment 1), and when the speech stimuli were embedded in 8-talker babble noise (Experiment 2). Statistical analysis of the responses collected from 82 phonetically-untrained subjects (N = 43, quiet; N = 39, noise) revealed a significant AV effect in both experiments. However, the strength of the effect varied considerably as a function of facewear type. The findings are discussed in the context of previous research on AV speech perception, which aims to identify the facial regions that are particularly important for the extraction of visual speech cues.\n",
    "Index Terms: auditory-visual speech perception, consonant identification, facial occlusion, forensic speech science\n",
    ""
   ]
  },
  "bayard13_avsp": {
   "authors": [
    [
     "Clémence",
     "Bayard"
    ],
    [
     "Cécile",
     "Colin"
    ],
    [
     "Jacqueline",
     "Leybaert"
    ]
   ],
   "title": "Impact of cued speech on audio-visual speech integration in deaf and hearing adults",
   "original": "av13_087",
   "page_count": 6,
   "order": 18,
   "p1": "87",
   "pn": "92",
   "abstract": [
    "For hearing and deaf people, speech perception involves an integrative process between auditory and lip read information. In order to disambiguate information from lips, manual cue may be added (Cued Speech). We examined how audio-visual integration is affected by the presence of manual cues. To address this issue, we designed an original experiment using audio-visual McGurk stimuli produced with manual cues. The manual cue was either congruent with auditory information, lip information or with the expected fusion. Our results suggest that manual cues can modify the audio-visual integration, and that their impact depends on auditory status.\n",
    "Index Terms: Deafness, Cued Speech perception, Cochlear implant, Binaural hearing aids, Multi signal integration\n",
    ""
   ]
  },
  "hazan13_avsp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Acoustic and visual adaptations in speech produced to counter adverse listening conditions",
   "original": "av13_093",
   "page_count": 6,
   "order": 19,
   "p1": "93",
   "pn": "98",
   "abstract": [
    "This study investigated whether communication modality affects talkers’ speech adaptation to an interlocutor exposed to background noise. It was predicted that adaptations to lip gestures would be greater and acoustic ones reduced when communicating face-to-face. We video recorded 14 Australian-English talkers (Talker A) speaking in a face-toface or auditory only setting with their interlocutors who were either in quiet or noise. Focusing on keyword productions, acoustic-phonetic adaptations were examined via measures of vowel intensity, pitch, keyword duration, vowel F1/F2 space and VOT, and visual adaptations via measures of vowel interlip area. The interlocutor adverse listening conditions lead Talker A to reduce speech rate, increase pitch and expand vowel space. These adaptations were not significantly reduced in the face-to-face setting although there was a trend for a smaller degree of vowel space expansion than in the auditory only setting. Visible lip gestures were more enhanced overall in the face-to-face setting, but also increased in the auditory only setting when countering the effects of noise. This study therefore showed only small effects of communication modality on speech adaptations.\n",
    "Index Terms: speech adaptation, audiovisual communication, speech in noise\n",
    ""
   ]
  },
  "barone13_avsp": {
   "authors": [
    [
     "Pascal",
     "Barone"
    ],
    [
     "Kuzma",
     "Strelnikov"
    ],
    [
     "Olivier",
     "Déguine"
    ]
   ],
   "title": "Role of audiovisual plasticity in speech recovery after adult cochlear implantation",
   "original": "av13_099",
   "page_count": 6,
   "order": 20,
   "p1": "99",
   "pn": "104",
   "abstract": [
    "The role of vision on the speech recovery after cochlear implantation is a subject of debate. In our study, we assessed the role of crossmodal reorganization and plasticity in auditory recovery in cochlear implanted deaf patients. Our results demonstrate that the initial functional level of the visual cortex leads to the greater proficiency in auditory recovery. Experienced patients had greater activity in the left middle temporal cortex known for audiovisual integration. The time course of temporal and visual activity in experienced patients was highly correlated meaning their synchronized integrative activity. Our data confirms the importance of visual activity and audiovisual integration in speech comprehension in cochlear implanted subjects by establishing the neural underpinnings for this integration.\n",
    "Index Terms: cochlear implantation, speech, vision\n",
    ""
   ]
  },
  "fitzpatrick13_avsp": {
   "authors": [
    [
     "Michael",
     "Fitzpatrick"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Auditory and auditory-visual Lombard speech perception by younger and older adults",
   "original": "av13_105",
   "page_count": 6,
   "order": 21,
   "p1": "105",
   "pn": "110",
   "abstract": [
    "The current study examined older and younger adults’ perception of auditory and auditory-visual Lombard speech. A staircase procedure was used to estimate the SNR required for participants to achieve 50% correct auditory identification of Quiet and Lombard speech (CVC and VCV stimuli). Stimuli were then presented in auditory only (AO), visual only (VO) and auditory visual (AV) conditions in a speech identification task in noise using the SNR set in the staircase procedure. Results showed that both groups received comparable benefit from the auditory Lombard speech modifications. Both age groups received significant benefit from the AV Lombard speech with the degree of AV Lombard benefit greater for the older adults on the CVC stimuli. In contrast, for the VO condition, older adults’ overall perception was relatively poor. Although Lombard speech improved their lip-reading ability on the CVC stimuli, they received no benefit on the VCV stimuli. The findings suggest that although lip-reading abilities may diminish with age, older adults can still receive substantial benefit from the integration of the auditory and visual Lombard speech in AV speech perception.\n",
    "Index Terms: Lombard speech, AV speech, Aging.\n",
    ""
   ]
  },
  "mixdorff13_avsp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Angelika",
     "Hönemann"
    ],
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Integration of acoustic and visual cues in prominence perception",
   "original": "av13_111",
   "page_count": 6,
   "order": 22,
   "p1": "111",
   "pn": "116",
   "abstract": [
    "This study concerns the perception of prominence in auditoryvisual speech perception. We constructed A/V stimuli from five-syllable sentences in which every syllable was a candidate for receiving stress. All syllables were of uniform length, and the F0 contours were manipulated using the Fujisaki model, moving a peak of F0 from the beginning to the end of the utterance. The peak was either aligned with the center of the syllable or the boundary between syllables, yielding a total of nine positions. Likewise, a video showing the upper part of a speaker’s face exhibiting one single raise of eyebrows was aligned with the audio, hence yielding nine positions for the visual cue, with the maximum displacement of the eyebrows coinciding with syllable centers or boundaries. Another series of stimuli was produced with head nods as the visual cue. In addition stimuli with constant F0 with or without video were created. 22 German native subjects rated the strength of each of the five syllables in a stimulus on a scale from 1-3. Results show that the acoustic prominence outweighs the visual one, and that the integration of both in a single syllable is the strongest when the movement as well as the F0 peak are aligned with the center of the syllable. However, F0 peaks aligned with the right boundary of the accented syllable, as well as visual peaks aligned with the left one also boost prominence considerably. Nods had an effect similar in magnitude as eye brow movements, however, results suggest that they rather have to be aligned with the right boundary of the syllable than the left one.\n",
    "Index Terms: Prominence, auditory-visual integration, F0 modeling\n",
    ""
   ]
  },
  "davis13_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Detecting auditory-visual speech synchrony: how precise?",
   "original": "av13_117",
   "page_count": 6,
   "order": 23,
   "p1": "117",
   "pn": "122",
   "abstract": [
    "Previous research suggests that people are rather poor at perceiving auditory-visual (AV) speech asynchrony, especially when the visual signal occurs first. However, estimates of AV synchrony detection depend on many factors and previous measures may have underestimated its precision. Here we used a synchrony-driven search task to examine how accurately an observer could detect AV speech synchrony. In this task on each trial a participant viewed four videos (positioned at the cardinal points of a circle) that showed the lower face of a talker while hearing a spoken /ba/ syllable. One video had the original AV timing, in the others the visual speech was shifted 100 ms, 200 ms or 300 ms earlier. Participants were required to conduct a speeded visual search for the synchronized face/voice token (the position of which was randomized). The results showed that the synchrony detection window was narrow with 82% of responses selecting either the original unaltered video (29%) or the video where the visual signal led by 100 ms (53%). These results suggest that an observer is able to judge AV speech synchrony with some precision.\n",
    "Index Terms: Auditory-visual speech synchrony; Synchrony search task; Inter-sensory timing\n",
    ""
   ]
  },
  "kim13_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "How far out? the effect of peripheral visual speech on speech perception",
   "original": "av13_123",
   "page_count": 6,
   "order": 24,
   "p1": "123",
   "pn": "128",
   "abstract": [
    "Seeing the talker’s moving face (visual speech) can facilitate or distort auditory speech perception. Our previous study showed that these effects occur even when visual speech was presented in the periphery and participants performed a central visual task. The current study examined the extent to which these effects were modulated by the eccentricity of visual speech: Visual speech presented at a visual angle of 10.40 (Exp 1) and 23.60 (Exp 2). In both experiments spoken /aba/ stimuli were presented in noise (-6 dB) with congruent or incongruent visual speech in full-face or upper-face (baseline) conditions. Other AV vCv syllables were also presented as filler items. Participants were to identify what they heard while performing a central visual task with their eyemovements monitored. Congruent visual speech facilitated speech perception; incongruent interfered. The sizes of the visual speech effects were smaller for the more eccentric presentation but were still significant. We discuss these results in terms of the form and timing cues that visual speech provides for incoming auditory speech and the robustness of the speech processes that use these cues.\n",
    "Index Terms: Visual speech; Auditory-visual speech; Auditory-visual congruency; Visual periphery; Attention\n",
    ""
   ]
  },
  "eg13_avsp": {
   "authors": [
    [
     "Ragnhild",
     "Eg"
    ],
    [
     "Dawn M.",
     "Behne"
    ]
   ],
   "title": "Temporal integration for live conversational speech",
   "original": "av13_129",
   "page_count": 6,
   "order": 25,
   "p1": "129",
   "pn": "134",
   "abstract": [
    "The difficulty in detecting short asynchronies between corresponding audio and video signals demonstrates the remarkable resilience of the perceptual system when integrating the senses. Thresholds for perceived synchrony vary depending on the complexity, congruency and predictability of the audiovisual event. For instance, asynchrony is typically detected sooner for simple flash and tone combinations than for speech stimuli. In applied scenarios, such as teleconference platforms, the thresholds themselves are of particular interest; since the transmission of audio and video streams can result in temporal misalignments, system providers need to establish how much delay they can allow. This study compares the perception of synchrony in speech for a live two-way teleconference scenario and a controlled experimental set-up. Although methodologies and measures differ, our explorative analysis indicates that the windows of temporal integration are similar for the two scenarios. Nevertheless, the direction of temporal tolerance differs; for the teleconference, audio lead asynchrony was more difficult to detect than for the experimental speech videos. While the windows of temporal integration are fairly independent of the context, the skew in the audio lead threshold may be a reflection of the natural diversion of attending to a conversation.\n",
    "Index Terms: audiovisual speech, temporal integration, synchrony perception, teleconference\n",
    ""
   ]
  },
  "miranda13_avsp": {
   "authors": [
    [
     "Jérémy",
     "Miranda"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Mixing faces and voices: a study of the influence of faces and voices on audiovisual intelligibility",
   "original": "av13_135",
   "page_count": 6,
   "order": 26,
   "p1": "135",
   "pn": "140",
   "abstract": [
    "This study examined the influence of mixing faces and voices on the audiovisual intelligibility. The goal is to study the effect of combining two sources of information on the audiovisual intelligibility. Cross-talker dubbing was performed between faces and voices of 10 meaningful sentences pronounced by 10 talkers: 5 females and 5 males. Human subjects were asked to rate the articulation of the output videos. Comparisons were made between results of original and dubbed video. Almost across all the combinations, the audiovisual intelligibility was acceptable. The intelligibility of the speakers varied, however. We observed an influence of the audio/visual channel on the overall intelligibility that can increase or decrease depending the intelligibility results of this channel.\n",
    ""
   ]
  },
  "treille13_avsp": {
   "authors": [
    [
     "Avril",
     "Treille"
    ],
    [
     "Camille",
     "Cordeboeuf"
    ],
    [
     "Coriandre",
     "Vilain"
    ],
    [
     "Marc",
     "Sato"
    ]
   ],
   "title": "The touch of your lips: haptic information speeds up auditory speech processing",
   "original": "av13_141",
   "page_count": 6,
   "order": 27,
   "p1": "141",
   "pn": "146",
   "abstract": [
    "The human ability to follow speech gestures through the visual modality is a core component of speech perception. Remarkably, speech can be perceived not only by the ear and by the eye but also by the hand, with speech gestures felt from manual tactile contact with the speaker’s face. In the present study, early cross-modal interactions were investigated by comparing early auditory evoked potentials during auditory, audio-visual and audio-haptic speech perception in natural dyadic interactions between a listener and a speaker. Although participants were not experienced with audio-haptic speech perception, shortened latencies of auditory evoked potentials were observed in both audio-visual and audio-tactile modalities compared to the auditory modality. These results demonstrate early cross-modal interactions during face-to-face and hand-to-face speech perception and highlight a predictive role of visual and haptic information on auditory speech processing in dyadic interactions.\n",
    "Index Terms: audio-visual speech perception, audio-haptic speech perception, EEG.\n",
    ""
   ]
  },
  "schwartz13_avsp": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "Data and simulations about audiovisual asynchrony and predictability in speech perception",
   "original": "av13_147",
   "page_count": 6,
   "order": 28,
   "p1": "147",
   "pn": "152",
   "abstract": [
    "Since a paper by Chandrasekaran et al. (2009), an increasing number of neuroscience papers capitalize on the assumption that visual speech would be typically 150 ms ahead of auditory speech. It happens that the estimation of audiovisual asynchrony by Chandrasekaran et al. is valid only in very specific cases, for isolated CV syllables or at the beginning of a speech utterance. We present simple audiovisual data on plosive-vowel syllables (pa, ta, ka, ba, da, ga, ma, na) showing that audiovisual synchrony is actually rather precise when syllables are chained in sequences, as they are typically in most parts of a natural speech utterance. Then we discuss on the way the natural coordination between sound and image (combining cases of lead and lag of the visual input) is reflected in the so-called temporal integration window for audiovisual speech perception (van Wassenhove et al., 2007). We conclude by a computational proposal about predictive coding in such sequences, showing that the visual input may actually provide and enhance predictions even if it is quite synchronous with the auditory input.\n",
    "Index Terms: audiovisual asynchrony, temporal integration window, predictive coding, visual lead/lag, visual prediction\n",
    ""
   ]
  },
  "tiippana13_avsp": {
   "authors": [
    [
     "Kaisa",
     "Tiippana"
    ],
    [
     "Kaupo",
     "Viitanen"
    ],
    [
     "Riia",
     "Kivimäki"
    ]
   ],
   "title": "The effect of musical aptitude on the integration of audiovisual speech and non-speech signals in children",
   "original": "av13_153",
   "page_count": 4,
   "order": 29,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Multisensory integration was assessed using two audiovisual illusions. In the McGurk effect, auditory speech perception is altered by incongruent visual speech. In the Shams illusion, the number of seen flashes is altered by an incongruent number of heard beeps. The illusions were tested in 10-year-old children, whose musical aptitude was also assessed. The strength of the McGurk effect was not linked to musical aptitude. However, children with high musical aptitude scores had a weaker Shams illusion and a narrower temporal window of integration, suggesting that they integrate non-speech information more selectively than children with low musical aptitude. These findings imply that musical aptitude influences multisensory integration selectively for rapid non-speech events.\n",
    "Index Terms: audiovisual speech, McGurk effect, Shams illusion, multisensory integration, musical aptitude.\n",
    ""
   ]
  },
  "treille13b_avsp": {
   "authors": [
    [
     "Avril",
     "Treille"
    ],
    [
     "Coriandre",
     "Vilain"
    ],
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Laurent",
     "Lamalle"
    ],
    [
     "Marc",
     "Sato"
    ]
   ],
   "title": "The sight of your tongue: neural correlates of audio-lingual speech perception",
   "original": "av13_157",
   "page_count": 6,
   "order": 30,
   "p1": "157",
   "pn": "162",
   "abstract": [
    "While functional neuroimaging studies demonstrate that multiple cortical regions play a key role in audio-visual integration of speech, whether cross-modal speech interactions only depend on well-known auditory and visuo-facial modalities or, rather, might also be triggered by other sensory sources remains unexplored. The present functional magnetic resonance imaging (fMRI) study examined the neural substrates of cross-modal binding during audio-visual speech perception in response to either seeing the facial/lip or tongue (tongue movement inside the mouth acquired by means of ultrason) movements of a speaker. To this aim, participants were exposed to auditory and/or visual speech stimuli in five different conditions: an auditory-only condition, and two visual-only and two audiovisual conditions that showed either the facial/lip or tongue movements of a speaker. Common overlapping activity between conditions were mainly observed in the posterior part of the superior temporal gyrus/sulcus, extending ventrally to the posterior middle temporal gyrus and dorsally to the parietal operculum, the supramarginal and angular gyri, as well as in the premotor cortex and in the inferior frontal gyrus. In addition, sub-additive neural responses were observed in the left posterior superior temporal gyrus/sulcus during audio-visual perception of both facial and tongue speech movements compared to unimodal auditory and visual speech perception. Altogether these results suggest that the left posterior superior temporal gyrus/sulcus is involved in multisensory processing of auditory speech signals and their accompanying facial/lip and tongue speech movements, and that multisensory speech perception is partly driven by listener’s knowledge of speech production.\n",
    "Index Terms: audio-visual speech perception, ultrasound, fMRI\n",
    ""
   ]
  },
  "kalantari13_avsp": {
   "authors": [
    [
     "Shahram",
     "Kalantari"
    ],
    [
     "Rajitha",
     "Navarathna"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Visual front-endwars: Viola-Jones face detector vs Fourier Lucas-Kanade",
   "original": "av13_163",
   "page_count": 6,
   "order": 31,
   "p1": "163",
   "pn": "168",
   "abstract": [
    "The performance of visual speech recognition (VSR) systems are significantly influenced by the accuracy of the visual front-end. The current state-of-the-art VSR systems use off-the-shelf face detectors such as Viola- Jones (VJ) which has limited reliability for changes in illumination and head poses. For a VSR system to perform well under these conditions, an accurate visual front end is required. This is an important problem to be solved in many practical implementations of audio visual speech recognition systems, for example in automotive environments for an efficient human-vehicle computer interface. In this paper, we re-examine the current state-of-the-art VSR by comparing off-the-shelf face detectors with the recently developed Fourier Lucas-Kanade (FLK) image alignment technique. A variety of image alignment and visual speech recognition experiments are performed on a clean dataset as well as with a challenging automotive audio-visual speech dataset. Our results indicate that the FLK image alignment technique can significantly outperform off-the shelf face detectors, but requires frequent fine-tuning.\n",
    "Index Terms: Visual Front-ends, Viola-Jones, Fourier Lucas-Kanade, Visual Speech Recognition\n",
    ""
   ]
  },
  "alexanderson13_avsp": {
   "authors": [
    [
     "Simon",
     "Alexanderson"
    ],
    [
     "David",
     "House"
    ],
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Aspects of co-occurring syllables and head nods in spontaneous dialogue",
   "original": "av13_169",
   "page_count": 4,
   "order": 32,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "This paper reports on the extraction and analysis of head nods taken from motion capture data of spontaneous dialogue in Swedish. The head nods were extracted automatically and then manually classified in terms of gestures having a beat function or multifunctional gestures. Prosodic features were extracted from syllables co-occurring with the beat gestures. While the peak rotation of the nod is on average aligned with the stressed syllable, the results show considerable variation in fine temporal synchronization. The syllables co-occurring with the gestures generally show greater intensity, higher F0, and greater F0 range when compared to the mean across the entire dialogue. A functional analysis shows that the majority of the syllables belong to words bearing a focal accent.\n",
    "Index Terms: Gestures, prosody, motion capture, beats, head nods, stressed syllable\n",
    ""
   ]
  },
  "fagel13_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Andreas",
     "Hilbert"
    ],
    [
     "Christopher",
     "Mayer"
    ],
    [
     "Martin",
     "Morandell"
    ],
    [
     "Matthias",
     "Gira"
    ],
    [
     "Martin",
     "Petzold"
    ]
   ],
   "title": "Avatar user interfaces in an OSGi-based system for health care services",
   "original": "av13_173",
   "page_count": 2,
   "order": 33,
   "p1": "173",
   "pn": "174",
   "abstract": [
    "We present a system to display text information by an animated talking avatar suitable for health care services. Speech animation parameters are calculated by a co-articulation model from the phone chain extracted from the text-to-speech processing step. An animation script that layers body movements and speech animation is generated and then rendered and converted into an h.264 video by a computer game engine. The animation system is attached to AALuis, an OSGi-based system for care services for older adults within a European research project.\n",
    "Index Terms: character animation; ambient assisted living; audiovisual speech synthesis; health care service; avatar\n",
    ""
   ]
  },
  "musti13_avsp": {
   "authors": [
    [
     "Utpala",
     "Musti"
    ],
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Slim",
     "Ouni"
    ],
    [
     "Caroline",
     "Lavecchia"
    ],
    [
     "Brigitte",
     "Wrobel-Dautcourt"
    ],
    [
     "Marie-Odile",
     "Berger"
    ]
   ],
   "title": "Automatic feature selection for acoustic-visual concatenative speech synthesis: towards a perceptual objective measure",
   "original": "av13_175",
   "page_count": 6,
   "order": 34,
   "p1": "175",
   "pn": "180",
   "abstract": [
    "We present an iterative algorithm for automatic feature selection and weight tuning of target cost in the context of unit selection based audio-visual speech synthesis. We perform feature selection and weight tuning for a given unit-selection corpus to make the ranking given by the target cost function consistent with the ordering given by an objective dissimilarity measure. We explicitly perform feature elimination to reduce the redundancy and noise in target cost calculation based on an objective metric. Finding an objective metric highly correlated to perception should improve the quality of tuning. This is the purpose of the second part where we are making an attempt to such goal. Firstly, we present the human-centered evaluation done of the synthesized audio-visual speech and secondly, its preliminary analysis in relation to the objective evaluation metrics. This analysis of correlation between objective and subjective evaluation results shows interesting patterns which might help in designing better tuning metrics and objective evaluation techniques. The key point is to find a link between objective and perceptual measures.\n",
    "Index Terms: Unit selection, audio-visual speech synthesis, target cost, target feature selection, weight tuning\n",
    ""
   ]
  },
  "nahorna13_avsp": {
   "authors": [
    [
     "Olha",
     "Nahorna"
    ],
    [
     "Ganesh Attigodu",
     "Chandrashekara"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Jean Luc",
     "Schwartz"
    ]
   ],
   "title": "Modulating fusion in the McGurk effect by binding processes and contextual noise",
   "original": "av13_181",
   "page_count": 6,
   "order": 35,
   "p1": "181",
   "pn": "186",
   "abstract": [
    "In a series of experiments we showed that the McGurk effect may be modulated by context: applying incoherent auditory and visual material before an audiovisual target made of an audio “ba” and a video “ga” significantly decreases the McGurk effect. We interpreted this as showing the existence of an audiovisual “binding” stage controlling the fusion process. Incoherence would produce “unbinding” and result in decreasing the weight of the visual input in the fusion process. In this study, we further explore this binding stage around two experiments. Firstly we test the “rebinding” process, by presenting a short period of either coherent material or silence after the incoherent ”unbinding” context. We show that coherence provides “rebinding”, resulting in a recovery of the McGurk effect. In contrary, silence provides no rebinding and hence “freezes” the unbinding process, resulting in no recovery of the McGurk effect. Capitalizing on this result, in a second experiment including an incoherent unbinding context followed by a coherent rebinding context before the target, we add noise all over the contextual period, though not in the McGurk target. It appears that noise uniformly increases the rate of McGurk responses compared to the silent condition. This suggests that contextual noise increases the weight of the visual input in fusion, even if there is no noise within the target stimulus where fusion is applied. We conclude on the role of audiovisual coherence and noise in the binding process, in the framework of audiovisual speech scene analysis and the cocktail party effect.\n",
    "Index Terms: audiovisual speech perception, McGurk effect, unbinding, rebinding, perception in noise\n",
    ""
   ]
  },
  "joosten13_avsp": {
   "authors": [
    [
     "Bart",
     "Joosten"
    ],
    [
     "Eric",
     "Postma"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "Visual voice activity detection at different speeds",
   "original": "av13_187",
   "page_count": 4,
   "order": 36,
   "p1": "187",
   "pn": "190",
   "abstract": [
    "Visual Voice Activity Detection (VVAD) refers to the detection of speech from a video sequence by means of visual cues. VVAD provides a useful addition to auditory voice activity detection, in particular in cases involving multiple speakers or background noise. This paper focusses explicitly on the measurement of facial movements at different speeds to determine which rates of movement contribute to VVAD. Facial movements in video sequences of talking faces are measured using a spatiotemporal Gabor transform. VVAD performances based on these measurements are determined for different speeds and compared to simple frame-differencing. In addition, performances are assessed for the entire frame, the head region, and the mouth region. The results obtained reveal an elevated VVAD performance for large speeds as compared to low speeds. In addition, frame differencing performs at a level comparable to that of the spatiotemporal Gabor method at the optimal speeds.\n",
    "Index Terms:visual active speech, frame differencing, Gabor transform, spatiotemporal Gabor transform\n",
    ""
   ]
  },
  "ming13_avsp": {
   "authors": [
    [
     "Zuheng",
     "Ming"
    ],
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Gang",
     "Feng"
    ]
   ],
   "title": "GMM mapping of visual features of cued speech from speech spectral features",
   "original": "av13_191",
   "page_count": 6,
   "order": 37,
   "p1": "191",
   "pn": "196",
   "abstract": [
    "In this paper, we present a statistical method based on GMM modeling to map the acoustic speech spectral features to visual features of Cued Speech in the regression criterion of Minimum Mean-Square Error (MMSE) in a low signal level which is innovative and different with the classic text-to-visual approach. Two different training methods for GMM, namely Expecting-Maximization (EM) approach and supervised training method were discussed respectively. In comparison with the GMM based mapping modeling we first present the results with the use of a Multiple-Linear Regression (MLR) model also at the low signal level and study the limitation of the approach. The experimental results demonstrate that the GMM based mapping method can significantly improve the mapping performance compared with the MLR mapping model especially in the sense of the weak linear correlation between the target #and the predictor such as the hand positions of Cued Speech and the acoustic speech spectral features.\n",
    "Index Terms: Cued Speech, LSP, MFCC, GMM mapping.\n",
    ""
   ]
  },
  "howell13_avsp": {
   "authors": [
    [
     "Dominic",
     "Howell"
    ],
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Confusion modelling for automated lip-reading using weighted finite-state transducers",
   "original": "av13_197",
   "page_count": 6,
   "order": 38,
   "p1": "197",
   "pn": "202",
   "abstract": [
    "Automated lip-reading involves recognising speech from only the visual signal. The accuracy of current state-of-the-art lip-reading systems is significantly lower than that obtained by acoustic speech recognisers. These poor results are most likely due to the lack of information about speech production that is available in the visual signal: for example, it is impossible to discriminate voiced and unvoiced sounds, or many places of articulation, from visual signals. Our approach to this problem is to regard the visual speech signal as having been produced by a speaker who has a reduced phonemic repertoire and to attempt to compensate for this. In this respect, visual speech is similar to dysarthric speech, which is produced by a speaker who has poor control over their articulators, leading them to speak with a reduced and distorted set of phonemes. In previous work, we found that the use of weighted finite-state transducers improved recognition performance on dysarthric speech considerably. In this paper, we report the results of applying this technique to lip-reading. The technique works, but our initial results are not as good as those obtained by using a conventional approach, and we discuss why this might be so and what the prospects for future investigation are.\n",
    "Index Terms: automated lip-reading, weighted finite-state transducers, visual speech recognition, confusion modelling\n",
    ""
   ]
  },
  "shaw13_avsp": {
   "authors": [
    [
     "Felix",
     "Shaw"
    ],
    [
     "Barry-John",
     "Theobald"
    ]
   ],
   "title": "Transforming neutral visual speech into expressive visual speech",
   "original": "av13_203",
   "page_count": 6,
   "order": 39,
   "p1": "203",
   "pn": "208",
   "abstract": [
    "We present a method for transforming neutral visual speech sequences into realistic expressive visual speech sequences. By applying Independent Component Analysis (ICA) to visual features extracted from time aligned neutral and equivalent expressive sequences, a model that separates speech from expression can be learned. Analyzing the behavior of different speaking styles in terms of this model provides both a means for identifying the component(s) responsible for expression, and for learning the correspondence between different speaking styles. Exploiting this correspondence to transform neutral visual speech into expressive visual speech creates sequences that have the same time varying expressive dynamics as the equivalent ground-truth sequences, and an objective analysis shows that the neutral ICA parameters are shifted into the appropriate ranges for expressive visual speech.\n",
    "Index Terms: expressive visual speech synthesis, independent component analysis, expressive style transformation\n",
    ""
   ]
  },
  "heckmann13_avsp": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Keisuke",
     "Nakamura"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ]
   ],
   "title": "Differences in the audio-visual detection of word prominence from Japanese and English speakers",
   "original": "av13_209",
   "page_count": 6,
   "order": 40,
   "p1": "209",
   "pn": "214",
   "abstract": [
    "We have previously shown that for English speakers information on the mouth shape of a speaker is a powerful feature for the machine based discrimination of prominent from nonprominent words. In this paper we extend our analysis to data from Japanese speakers. We compare the discrimination performance of the different acoustic and visual features we extract for the two languages. This comparison shows a much wider variability in discrimination scores for the different speakers and the different features in the English dataset than in the Japanese dataset. Despite previous hints that visual speech and word prominence perception by Japanese listeners can yield inferior performance compared to English listeners we see that our discrimination scores are high and very similar for the English and Japanese speakers which indicates that at least the speakers signal prominence with a similar level of consistency in both languages.\n",
    "Index Terms: prosody, prominence, visual, audio-visual, Japanese\n",
    ""
   ]
  },
  "khan13_avsp": {
   "authors": [
    [
     "Faheem",
     "Khan"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Speaker separation using visually-derived binary masks",
   "original": "av13_215",
   "page_count": 6,
   "order": 41,
   "p1": "215",
   "pn": "220",
   "abstract": [
    "This paper is concerned with the problem of single-channel speaker separation and exploits visual speech information to aid the separation process. Audio from a mixture of speakers is received from a single microphone and to supplement this, video from each speaker in the mixture is also captured. The visual features are used to create a time-frequency binary mask that identifies regions where the target speaker dominates. These regions are retained and form the estimate of the target speaker’s speech. Experimental results compare the visually-derived binary masks with ideal binary masks which shows a useful level of accuracy. The effectiveness of the visually-derived binary mask for speaker separation is then evaluated through estimates of speech quality and speech intelligibility and shows substantial gains over the original mixture.\n",
    "Index Terms: Speaker separation, binary masks, visual features, audio-visual correlation\n",
    ""
   ]
  },
  "seko13_avsp": {
   "authors": [
    [
     "Takumi",
     "Seko"
    ],
    [
     "Naoya",
     "Ukai"
    ],
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Improvement of lipreading performance using discriminative feature and speaker adaptation",
   "original": "av13_221",
   "page_count": 6,
   "order": 42,
   "p1": "221",
   "pn": "226",
   "abstract": [
    "In this paper, we apply a general and discriminative feature ”GIF” (Genetic Algorithm based Informative feature) to lipreading (visual speech recognition), and improve the lipreading performance using speaker adaptation. The feature extraction method consists of two transforms, which convert an input vector into GIF for recognition. In the speaker adaptation, MAP (Maximum A Posteriori) adaptation is used to adapt a recognition model to a target speaker. Recognition experiments of continuous digit utterances were conducted using an audio-visual corpus CENSREC-1-AV [1] including more than 268,000 lip images. At first, we compared the GIF-based method with the baseline method employing conventional eigenlip features, using two kinds of images: pictures in the database around speakers’ mouth, and extracted images only containing lips. Secondly, we evaluated the effectiveness of speaker adaptation for lipreading. The result of comparison shows that the GIFbased approach achieved slightly better than the baseline method. And it is found using the mouth-around images is more suitable than lip-only images. Furthermore, the result of speaker adaptation shows that speaker adaptation significantly improved recognition accuracy in the GIF-based method; after the adaptation, the recognition rate drastically increased from approximately 30% to 70%.\n",
    "",
    "",
    "S.Tamura et al., “CENSREC-1-AV: An audio-visual corpus for noisy bimodal speech recognition”, Proc. AVSP2010, pp.85-88 (2010).\n",
    "",
    "",
    "Index Terms: discriminative feature, lipreading, speaker adaptation, lip extraction, CENSREC\n",
    ""
   ]
  },
  "saitoh13_avsp": {
   "authors": [
    [
     "Takeshi",
     "Saitoh"
    ]
   ],
   "title": "Efficient face model for lip reading",
   "original": "av13_227",
   "page_count": 6,
   "order": 43,
   "p1": "227",
   "pn": "232",
   "abstract": [
    "There is number of researches on the lip reading. However, there is little discussion about which face model is effect for lip reading. This paper builds various face models which changes the combination of a face part, and changes the feature points. Various experiments were conducted on the conditions which change only model and do not change other algorithms. We apply the active appearance model. The CUAVE database which the utterance scene of ten digits is contained was used for the recognition experiment. As a result, the model which combined the external lip contour, nose, and outline acquired the highest recognition accuracy using HMM. We found that the face model which contains eyes and/or eyebrows is not effective for lip reading.\n",
    "Index Terms: lip reading, word recognition, face models, active appearance model\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Invited Papers",
   "papers": [
    "cangelosi13_avsp",
    "spence13_avsp"
   ]
  },
  {
   "title": "Audiovisual Prosody",
   "papers": [
    "visser13_avsp",
    "barbulescu13_avsp",
    "zelic13_avsp",
    "mui13_avsp",
    "honemann13_avsp"
   ]
  },
  {
   "title": "Audiovisual Speech by Machines",
   "papers": [
    "hollenstein13_avsp",
    "schabus13_avsp",
    "shen13_avsp",
    "parocosta13_avsp",
    "steiner13_avsp"
   ]
  },
  {
   "title": "Development of Audiovisual Speech Perception",
   "papers": [
    "baart13_avsp",
    "irwin13_avsp",
    "fort13_avsp",
    "groen13_avsp"
   ]
  },
  {
   "title": "Audiovisual Speech Perception in Adverse Listening",
   "papers": [
    "fecher13_avsp",
    "bayard13_avsp",
    "hazan13_avsp",
    "barone13_avsp",
    "fitzpatrick13_avsp"
   ]
  },
  {
   "title": "Binding of Audiovisual Speech Information",
   "papers": [
    "mixdorff13_avsp",
    "davis13_avsp",
    "kim13_avsp",
    "eg13_avsp",
    "miranda13_avsp"
   ]
  },
  {
   "title": "Neuropsychology and Multimodality",
   "papers": [
    "treille13_avsp",
    "schwartz13_avsp",
    "tiippana13_avsp",
    "treille13b_avsp"
   ]
  },
  {
   "title": "Poster Sessions",
   "papers": [
    "kalantari13_avsp",
    "alexanderson13_avsp",
    "fagel13_avsp",
    "musti13_avsp",
    "nahorna13_avsp",
    "joosten13_avsp",
    "ming13_avsp",
    "howell13_avsp",
    "shaw13_avsp",
    "heckmann13_avsp",
    "khan13_avsp",
    "seko13_avsp",
    "saitoh13_avsp"
   ]
  }
 ]
}