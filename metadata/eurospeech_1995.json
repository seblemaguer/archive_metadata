{
 "title": "4th European Conference on Speech Communication and Technology (Eurospeech 1995)",
 "location": "Madrid, Spain",
 "startDate": "18/9/1995",
 "endDate": "21/9/1995",
 "conf": "Eurospeech",
 "year": "1995",
 "name": "eurospeech_1995",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "4th European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 1995)",
 "date": "18-21 September 1995",
 "papers": {
  "stevens95_eurospeech": {
   "authors": [
    [
     "Kenneth N.",
     "Stevens"
    ]
   ],
   "title": "Applying phonetic knowledge to lexical access",
   "original": "e95_0003",
   "page_count": 9,
   "order": 1,
   "p1": "3",
   "pn": "14",
   "abstract": [
    "Methods are proposed for incorporating phonetic and phonological knowledge into models for lexical access. These include representing the lexicon in terms of linguistically-motivated segments and features, so that dialectal and context-conditioned modifications of features can be accounted for in a direct way. Acoustic analysis to retrieve the segments and features from an utterance requires the detection of landmarks in the signal, followed by measurement of several cues for each feature. These cues are combined to produce an integrated property that determines the feature that is implemented by the talker. Examples of the required acoustic cues are given. Comparison is made with current approaches to lexical access based on statistically-motivated models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-1"
  },
  "ainsworth95_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Auditory mechanisms for speech perception",
   "original": "e95_0171",
   "page_count": 8,
   "order": 2,
   "p1": "171",
   "pn": "178",
   "abstract": [
    "One way towards an understanding of speech perception is by means of computational models based on neuroanatomical and neurophysiological data. These can be used to study the processing of speech sounds in the auditory system and the results can be compared with data on human perception. A model of the cochlear nerve and cochlear nucleus is described. It is suggested that some units in the cochlear nucleus, choppers, selectively extract a good frequency representation of the incoming signals whereas others, onset units, form a good time representation. The output of the onset units has been used to estimate the pitch of speech sounds. An extension of the model has been used to generate amplitude modulation maps similar to those found in the inferior colliculus. This has been employed in a vowel segregation experiment and gave similar results to those obtained by human perception of the same stimuli.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-2"
  },
  "bourlard95_eurospeech": {
   "authors": [
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Towards increasing speech recognition error rates",
   "original": "e95_0883",
   "page_count": 12,
   "order": 3,
   "p1": "883",
   "pn": "894",
   "abstract": [
    "In the field of Automatic Speech Recognition (ASR) research, it is conventional to pursue those approaches that reduce the word error rate. However, it is the author's belief that this seemingly sensible strategy often effectively leads to the suppression of innovation. This is the case when the leading approaches have been tuned for years, effectively optimizing for a local minimum in the space of all possible techniques. In this case, almost any sufficiently new approach will necessarily hurt the accuracy of existing systems and thus increase the error rate. However, if progress is to be made against the remaining difficult problems, new approaches will most likely be necessary. In this paper, I discuss a few research issues for ASR which, when investigated, will most probably first lead to (significant) increase of error rate, but hold some promise for ultimately improving performance in the end-applications. Some of these examples will illustrate cases of successful improvement of ASR through increasing error rates (in one case, to 130%!), while other examples will just describe ongoing work (which are still increasing error rates) and, finally, some discussion of new directions. Problems that will be addressed in this paper include: merging the language and acoustic models, role of prior information in speech recognition, Markov models, discrimination, signal analysis and temporal information, and decoding procedures reflecting human perceptual properties.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-3"
  },
  "furui95_eurospeech": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Flexible speech recognition",
   "original": "e95_1595",
   "page_count": 9,
   "order": 4,
   "p1": "1595",
   "pn": "1604",
   "abstract": [
    "This paper overviews the main methods that have recently been investigated for making speech recognition systems more flexible at both the acoustic and linguistic processing levels. Improved flexibility will enable such systems to work well over a wide range of unexpected and adverse conditions by helping them to cope with variations between training and testing speech utterances. This paper focuses on the Bayesian adaptive learning approach, the minimum classification error (MCE) approach, the HMM composition technique, and spontaneous speech recognition techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-4"
  },
  "jitsuhiro95_eurospeech": {
   "authors": [
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Tomokazu",
     "Yamada"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Syllabic duration control for vocabulary-free speech recognition",
   "original": "e95_0015",
   "page_count": 4,
   "order": 5,
   "p1": "15",
   "pn": "18",
   "abstract": [
    "This paper describes the use of syllabic duration control in vocabulary-free speaker-independent speech recognition (i.e. phonetic typewriter) to significantly reduce the frequency of insertion errors. The duration control algorithm used in this paper is applicable to arbitrary unit length and is particularly effective when applied to mora-timed languages such as Japanese speech. Experimental results show that syllabic duration control can reduce the frequency of insertion errors, thus improving syllable accuracy from 47.7% to 75.6% and word accuracy from 19.5% to 41.7%. It has been also found that syllabic duration control is more effective than phonemic duration control. A combination of syllabic duration control and syllable bigram probabilities further improved syllable accuracy to 78.5% and word accuracy to 48.4%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-5"
  },
  "takagi95_eurospeech": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "Effectiveness of pause information in the content word detection of spoken dialogues",
   "original": "e95_0019",
   "page_count": 4,
   "order": 6,
   "p1": "19",
   "pn": "22",
   "abstract": [
    "One of the difficulties in spontaneous speech processing lies in the pausal phenomena. This paper describes an effect of incorporating pausal information into the syntactic rules, and an advantage of using an \"utterance unit\" as a processing unit in spoken dialogue recognition. A speech recognizer was implemented with utterance unit grammar and pausal information as word bigram probability. Recognition experiments were conducted using various goaloriented spoken dialogues. The method employing utterance unit grammar and pausal information showed best performance: content word detection rate is 50.4% for spontaneous speech, which was 8.8% better than the method based on sentence recognition without the constraints by pause information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-6"
  },
  "kondo95_eurospeech": {
   "authors": [
    [
     "Kazuhiro",
     "Kondo"
    ]
   ],
   "title": "Connected Japanese digit recognition with pitch accent-dependent models",
   "original": "e95_0023",
   "page_count": 4,
   "order": 7,
   "p1": "23",
   "pn": "26",
   "abstract": [
    "This paper describes our first attempt to model accents in Japanese. We chose connected-digit recognition as our task since its simple accent structure will easily enable us to model its accents. We propose to use two distinct models for each digit according to two possible accent positions. Analysis of the trained models seems to show that the models represent alterations in the acoustics introduced by accents, even though current models only utilize intensity to distinguish accent. Connected digit recognition tests show that word errors can be decreased by up to 42% with the proposed models compared to conventional accent-independent models. Error analysis seems to suggest that not all digits require accent-dependent models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-7"
  },
  "barras95_eurospeech": {
   "authors": [
    [
     "C.",
     "Barras"
    ],
    [
     "M.-J.",
     "Caraty"
    ],
    [
     "C.",
     "Montacie"
    ]
   ],
   "title": "Temporal control and training selection for HMM-based system",
   "original": "e95_0027",
   "page_count": 4,
   "order": 8,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "Most speaker-independent acoustic-phonetic decoding systems are based on hidden Markov models. Such systems lack a real temporal control for the phonetic models. Furthermore, inter-speaker variability makes speaker adaptation necessary. In order to solve these problems, we introduce two original approaches. On the one hand, discontinuities detected with the Forward-Backward Divergence method are used to constrain phonetic transitions and to perform a more accurate temporal control. On the other hand, an efficient inter-speaker measure, based on AR-vector models, allows the selection of a speaker neighbourhood and the adaptation of the phonetic models. The contribution of these two methods is estimated on the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-8"
  },
  "hirose95_eurospeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Xinhni",
     "Hu"
    ]
   ],
   "title": "HMM-based tone recognition of Chinese trisyllables using double codebooks on fundamental frequency and waveform power",
   "original": "e95_0031",
   "page_count": 4,
   "order": 9,
   "p1": "31",
   "pn": "34",
   "abstract": [
    "An HMM-based method has been developed for the tone recognition of trisyllabic speech of standard Chinese. Several schemes developed for the tone recognition of dissyllabic speech were also incorporated: use of macroscopic and microscopic parameters of fundamental frequency contours, speaker normalization with fundamental frequency offset, coping with tone sandhi effect by additional tone models, concatenated learning of HMM, etc. Adding to these, a scheme of double codebooks on fundamental frequency and power was newly incorporated to the method to cope with the rather low recognition rate for light tones. By further discriminating light tones from third tones using syllabic durations, the recognition rate of 75% was obtained for the light tone. The total recognition rate was reached 97.2%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-9"
  },
  "murgia95_eurospeech": {
   "authors": [
    [
     "C.",
     "Murgia"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "C.",
     "Quinquis"
    ],
    [
     "A. Le",
     "Guyader"
    ]
   ],
   "title": "Very low delay and high quality coding of 20 hz -15 khz speech at 64 kbit/S",
   "original": "e95_0037",
   "page_count": 4,
   "order": 10,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "In this paper, we propose an algorithm for coding 20 Hz -15 kHz signals at 64 kbit/s with a very low delay (frame 0.16 ms). To achieve a quality near to transparency, we propose to adapt the Low-Delay CELP coder [1] to the 15 kHz bandwidth and we suggest a new noise shaping method coming from a psycho-acoustic model. In this way we take advantage of linear predictive coding and masking properties of the human hearing system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-10"
  },
  "sasaki95_eurospeech": {
   "authors": [
    [
     "Shigeaki",
     "Sasaki"
    ],
    [
     "Akitoshi",
     "Kataoka"
    ],
    [
     "Takehiro",
     "Moriya"
    ]
   ],
   "title": "Wideband CELP coder at 16-kbit/s with 10-ms frame",
   "original": "e95_0041",
   "page_count": 4,
   "order": 11,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "A 16-kbit/s Code Excited Linear Prediction (CELP) coder with a 10-ms frame is proposed for wideband speech. The main features of this coder are line spectrum pairs quantization using inter-frame moving average prediction, a fixed excitation codebook with a two-stage, two-channel conjugate structure, and pre-selection in both adaptive codebook search and fixed codebook search. To solve two problems peculiar to the wideband CELP coding, i.e., computational complexity and computational precision, adaptive codebook pre-selection in the residual domain, a trained sparse codebook, and square root LPC coefficients are introduced. The quality of the coder meets the standard for 56-kbit/s coding specified in ITU-T G.722 in terms of mean opinion scores.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-11"
  },
  "black95_eurospeech": {
   "authors": [
    [
     "A. W.",
     "Black"
    ],
    [
     "I. A.",
     "Atkinson"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "High quality 14.1kb/s wideband speech coder",
   "original": "e95_0045",
   "page_count": 4,
   "order": 12,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "This paper describes the application of the Pulsed Residual Excited Linear Prediction (PRELP), a variation of the well known CELP algorithm to coding 7kHz wideband speech. The coder is to be used in the audio description of television. The PRELP algorithm operates at the source rate of 14.1 kb/s and uses an innovative excitation which is adapted for wideband speech. In addition to producing high quality speech, the algorithm simplifies the complexity of the encoder/decoder. This ensures that a cost effective hardware implementation can be achieved. The coder also employs a novel codebook gain quantiser, which is adaptive to the energy of the LPC synthesis filter response.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-12"
  },
  "abreusernandez95_eurospeech": {
   "authors": [
    [
     "V.",
     "Abreu-Sernandez"
    ],
    [
     "D.",
     "Docampo-Amoedo"
    ]
   ],
   "title": "A multipulse-deconvolution codec for wideband speech",
   "original": "e95_0049",
   "page_count": 4,
   "order": 13,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "The wideband speech coder proposed in this paper is based on a MultiPulse scheme developed in our Department [1] in 1992. The first wideband MP version is a full-band scheme which makes forward linear prediction analysis every frame of 16 ms. The adaptive codebook search and the spiky deconvolution algorithm are carried out every 2 ms. Five different bit rates can be assigned from 16 kbps to 33 kbps. Employing the objective segmental SNR measure, the obtained quality is lower than the quality of the standard G.722 at 64 kbps. However, the subjective quality evaluated using MOS tests and Paired Comparison tests is comparable to the standard. We also evaluate a two-bands structure in which each subband is coded with a MP-Deconvolution scheme.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-13"
  },
  "chonavel95_eurospeech": {
   "authors": [
    [
     "T.",
     "Chonavel"
    ],
    [
     "S.",
     "Saoudi"
    ]
   ],
   "title": "Multi-channel linear predictive coding of audio signals",
   "original": "e95_0053",
   "page_count": 3,
   "order": 14,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "The problem of speech coding of sounds obtained from several microphones is adressed here. In view to improve the coding performances by accounting for the coupling that exists among the microphones, a. multichannel linear prediction approach is proposed. Encoding of the linear prediction coefficients from multichannel reflection coefficients is presented. In view to extend the concept of LSP representations to the vector case, some results about the multichannel line spectrum decompositions are mentionned. The relevance of the multichannel approach is discussed and checked on simulations and real signals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-14"
  },
  "rohwer95_eurospeech": {
   "authors": [
    [
     "E.",
     "Rohwer"
    ]
   ],
   "title": "An advanced multi-DSP platform for speech technology integration in computer telephony applications",
   "original": "e95_0058",
   "page_count": 5,
   "order": 15,
   "p1": "58",
   "pn": "62",
   "abstract": [
    "This paper presents an advanced platform for commercial use of speech technology products in multimedia applications. It covers the general concept of the hardware and software design of a powerful multi DSP communication board, based on the DSP32C processor, with an upgradable modular structure and universal port - one DSP processing one telephone line, and up to 16 DSPs simultaneously running different tasks on different channels on one board. A range of speech processing modules can be accessed through each port. They include TTS, SIR, SDR and SV and are provided by different speech technology suppliers. The paper gives information about the porting process and aids understanding the integration procedures of a single speech module into a complex UNIX based open system architecture.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-15"
  },
  "ciria95_eurospeech": {
   "authors": [
    [
     "Rafael",
     "Ciria"
    ],
    [
     "Rafael",
     "Sarmiento de Sotomayor"
    ],
    [
     "Cristina",
     "Aguila"
    ],
    [
     "José",
     "Parera"
    ],
    [
     "Juan",
     "Santos"
    ]
   ],
   "title": "Voice processing architecture for computer-telephony integration",
   "original": "e95_0063",
   "page_count": 4,
   "order": 16,
   "p1": "63",
   "pn": "66",
   "abstract": [
    "Integration of computers with the telephone network into commercial equipments has become feasible due to the proliferation of inexpensive Personal Computers and advances in speech processing. Now, Computer Telephony Integration (CTI), helps users to take advantage of new telephony services built-up using speech technologies such as coding, recognition and text-to-speech conversion. This article presents a hardware/software platform based on PC-Compatibles for CTI systems. Hardware comprises boards based on the latest Digital Signal Processors (DSPs) technology, allowing the real-time implementation of fairly complex signal processing algorithms. Software offers CTI application developers an Application Programming Interface (API) to manage and monitor speech processing functions running in the DSPs as well as multiprogramming support for DOS and Windows(R) that permits different CTI applications to be executed on the same or on a different board, each running associated to a phone port.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-16"
  },
  "ortizbalbuena95_eurospeech": {
   "authors": [
    [
     "L.",
     "Ortiz-Balbuena"
    ],
    [
     "H.",
     "Perez-Meana"
    ],
    [
     "A.",
     "Martinez-Gonzalez"
    ],
    [
     "L. Nino de",
     "Rivera"
    ],
    [
     "M.",
     "Nakano-Miyatake"
    ]
   ],
   "title": "Fast convergent analog adaptive filter",
   "original": "e95_0067",
   "page_count": 4,
   "order": 17,
   "p1": "67",
   "pn": "70",
   "abstract": [
    "The adaptive filters has been traditional developed in digital environment which involves several and large, computations time to get the coefficients that make the desired approximation. Most of the times, this calculations required a great capacity machines and there is no practical for some applications like channel equalization in cellular systems. This article shows an alternative way to develop adaptive filter using Low-Pass function and applying the LMS algorithm to reduce the error between the desired signal and the approximation and an application of this filter in a Decision Feedback Equalizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-17"
  },
  "leandro95_eurospeech": {
   "authors": [
    [
     "Manuel A.",
     "Leandro"
    ],
    [
     "Alvaro",
     "Villegas"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Efficient isolated word recognition in Spanish based on static modeling",
   "original": "e95_0071",
   "page_count": 4,
   "order": 18,
   "p1": "71",
   "pn": "74",
   "abstract": [
    "The use of well-suited algorithms to a particular language can lead to efficient systems. In this paper we show how static modelling of phonemes, combined with phonetic heuristic knowledge, is a good approach for isolated word speech recognition in Spanish. The system developed is specially indicated in applications where low computational load and easy speaker adaptation is important, while maintaining an acceptable performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-18"
  },
  "cochard95_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Cochard"
    ],
    [
     "Olivier",
     "Oppizzi"
    ]
   ],
   "title": "Reliability in a multi-agent spoken language recognition system",
   "original": "e95_0075",
   "page_count": 4,
   "order": 19,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "This paper is describing a continuous spoken language recognition system under development at IDIAP. A particular attention is paid to two important topics under investigation. Firstly, the system characteristics, namely its structure as a multi-agent system and its search mechanism of a global solution as an evolution dynamics, are explained. Secondly, in order to provide a means of evaluating pertinence of local solutions, a uniform reliability measure is defined. The computation of this measure is based on tests collection. A description of the different steps that lead to the definition of this measure is supplied.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-19"
  },
  "matsui95_eurospeech": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "A study of speaker adaptation based on minimum classification error training",
   "original": "e95_0081",
   "page_count": 4,
   "order": 20,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "This paper studies a speaker adaptation method based on minimum-classification-error (MCE) training applied to a hidden Markov model (HMM) based speaker-independent speech recognition system. In this method, the HMM parameters are adapted to a new speaker using the combination of maximum a posteriori (MAP) and MCE estimation. MAP estimation maximizes the a posteriori probability that the HMMs generate the data of the speaker, but this does not always guarantee the highest performance for reducing the recognition error. On the other hand, MCE estimation directly aims at minimizing the recognition error and has recently started to be used for speaker adaptation. Here MCE estimation is applied to the HMM parameters adapted by MAP estimation so that they fall into one of the local minima near the HMM parameters adapted by MAP estimation. In phoneme recognition experiments, we compare the performance of our combination of MAP and MCE estimation against the individual performances of MAP and MCE estimation. We find that the combination of MAP and MCE estimation is the most effective.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-20"
  },
  "nogueirasrodriguez95_eurospeech": {
   "authors": [
    [
     "Albino",
     "Nogueiras-Rodriguez"
    ],
    [
     "José B.",
     "Marino"
    ]
   ],
   "title": "Maximum likelihood based discriminative training of acoustic models",
   "original": "e95_0085",
   "page_count": 4,
   "order": 21,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "In this paper, a framework for discriminative training of acoustic models based on Generalised Probabilistic Descent (GPD) method is presented. The key feature of our proposal, Maximum Likelihood based Discriminative Training of Acoustic Models (MLDT), is the use of maximum likelihood trained HMM's instead of the original speech signal. We focus our attention in performing discriminative training applied to a discrete hidden Markov models continuos speech recogniser, achieving a 4.6% error rate reduction on a Spanish speaker-independent phoneme recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-21"
  },
  "leprieur95_eurospeech": {
   "authors": [
    [
     "Hugues",
     "Leprieur"
    ],
    [
     "Patrick",
     "Haffner"
    ]
   ],
   "title": "Discriminant learning with minimum memory loss for improved non-vocabulary rejection",
   "original": "e95_0089",
   "page_count": 4,
   "order": 22,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "A limitation to current, HMM-based Speech Recognition approaches lies in the modeling of non-vocabulary utterances. Improved rejection is a key research direction in Interactive Voice Response Services (IVR), where field evaluations show that many users do not only utter the requested keywords. This paper compares several discriminant training criteria on this problem and applies a novel optimization technique which can be used to improve rejection, without seriously disturbing HMM modeling assumptions. A 23% reduction in the error rate is observed on field data recorded during the operation of an IVR service.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-22"
  },
  "martindelalamo95_eurospeech": {
   "authors": [
    [
     "Cesar",
     "Martin del Álamo"
    ],
    [
     "F. Javier",
     "Caminero-Gil"
    ],
    [
     "Celinda de la",
     "Torre-Munilla"
    ],
    [
     "Lúis",
     "Hernandez-Gomez"
    ]
   ],
   "title": "Codebook weights adaptation for discriminative training of SCHMM-based speech recognition systems",
   "original": "e95_0093",
   "page_count": 4,
   "order": 23,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "Over the past few years, there have been several published reports on the use of Maximum Mutual Information (MMIE) for training HMM parameters. Lately, some reports have appeared, proposing different solutions to avoid the computational cost associated with the low convergence of the optimization technique. This paper proposes a new method for increasing the velocity of convergence, by pre-adapting the Codebook Weights of multiple codebooks in Semi-Continuous HMM (SCHMM). Experimental results on a small vocabulary recognition task show not only a fast convergence but also a 21% error rate reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-23"
  },
  "na95_eurospeech": {
   "authors": [
    [
     "Kyungmin",
     "Na"
    ],
    [
     "Bumki",
     "Jeon"
    ],
    [
     "Dong-Il",
     "Chang"
    ],
    [
     "Soo-Ik",
     "Chae"
    ],
    [
     "Souguil",
     "Ann"
    ]
   ],
   "title": "Discriminative training of hidden Markov models using overall risk criterion and reduced gradient method",
   "original": "e95_0097",
   "page_count": 4,
   "order": 24,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "In this paper, we propose an overall risk criterion for discriminative training of HMM-based speech recognizers using the minimum-error-rate classification of the Bayes decision theory. A reduced gradient method is applied to the proposed criterion for discriminative HMM parameter estimation. The resulting algorithm consists of gradient descent terms for competing classes and gradient ascent term for the correct class. In Korean digit recognition experiments, about 20 % reduction of the number of errors has been achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-24"
  },
  "huo95_eurospeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Chorkin",
     "Chan"
    ]
   ],
   "title": "Discriminative training of HMM based speech recognizer with gradient projection method",
   "original": "e95_0101",
   "page_count": 4,
   "order": 25,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "In this paper, in order to examine the viability and characteristics of our previously proposed gradient projection method for HMM training, the method is applied to discriminatively train the DHMM parameters with the objective of minimizing the recognition error rate and a series of experiments have been conducted. The experiments involve speaker independent recognition of the highly confusable E-set of the English alphabet. Three kinds of training schemes, namely, batch training, sequential block training and corrective training sue studied. The experimental results show that the minimum recognition error objective function is a viable formulation that can be optimized by the gradient projection method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-25"
  },
  "hernando95_eurospeech": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "J.",
     "Ayarte"
    ],
    [
     "E.",
     "Monte"
    ]
   ],
   "title": "Optimization of speech parameter weighting for CDHMM word recognition",
   "original": "e95_0105",
   "page_count": 4,
   "order": 26,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "Speech dynamic feature are routinely used in current speech recognition systems in combination with short-term (static) spectral features. The aim of this paper is to propose a method to automatically estimate the optimum ponderation of static and dynamic features in a speech recognition system. The recognition system considered in this paper is based on Continuous-Density Hidden Markov Modelling (CDHMM), widely used in speech recognition. Our approach consists basically in 1) adding two new parameters for each state of each model that weight both kinds of speech features, and 2) estimating those parameters by means of a discriminative training algorithm that minimizes the recognition error using the recently proposed Generalized Probabilistic Descent (GPD) method. Experimental results in speaker independent digit recognition show an important increase of recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-26"
  },
  "rahim95_eurospeech": {
   "authors": [
    [
     "Mazin G.",
     "Rahim"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Discriminative utterance verification for connected digits recognition",
   "original": "e95_0529",
   "page_count": 4,
   "order": 27,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "This paper describes a hidden Markov model (HMM) based utterance verification system designed in the frame-work of statistical hypothesis testing. The two major issues of how to design ke}'word and string scoring criteria are addressed. We motivate the need for discriminative hypothesis testing for verification. One such approach based on minimum classification error is investigated. When the proposed verification technique was integrated into a state-of-the-art connected digit recognition system, the string error rate for valid digit strings was found to decrease by 57% when setting the rejection rate to 5%. Furthermore, the system was able to correctly reject over 99.9% of non-vocabulary word strings.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-27"
  },
  "peinado95_eurospeech": {
   "authors": [
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "Victoria",
     "Sanchez"
    ],
    [
     "Jesus E.",
     "Diaz"
    ]
   ],
   "title": "MCE estimation of VQ parameters for MVQHMM speech recognition",
   "original": "e95_0533",
   "page_count": 4,
   "order": 28,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "Recent research on Multiple Vector Quantization (MVQ) has shown the suitability of such technique to Speech Recognition. Basically, MVQ proposes the use of one separated VQ code-book for each recognition unit. Thus, a MVQ HMM model is composed of a VQ codebook and a discrete HMM model. This technique allows the incorporation in the recognition dynamics of the input sequence information wasted by discrete HMM models in the VQ process. The use of distinct codebooks also allows to train them in a discriminative manner. In this paper, we propose a new VQ codebook design method for M VQ-based systems that provides meaningful error reductions and is performed independently from the estimation of the discrete HMM part of the MVQ model. This codebook design uses a Minimum Classification Error scheme and have certain similarities with the LVQ techniques proposed by Kohonen, but overcoming any time alignment requisite.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-28"
  },
  "reichl95_eurospeech": {
   "authors": [
    [
     "Wolfgang",
     "Reichl"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Discriminative training for continuous speech recognition",
   "original": "e95_0537",
   "page_count": 4,
   "order": 29,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "Discriminative training techniques for Hidden-Markov Models were recently proposed and successfully applied for automatic speech recognition. In this paper a discussion of the Minimum Classification Error and the Maximum Mutual Information objective is presented. An extended reestimation formula is used for the HMM parameter update for both objective functions. The discriminative training methods were utilized in speaker independent phoneme recognition experiments and improved the phoneme recognition rates for both discriminative training techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-29"
  },
  "paliwal95_eurospeech": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "M.",
     "Bacchiani"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Minimum classification error training algorithm for feature extractor and pattern classifier in speech recognition",
   "original": "e95_0541",
   "page_count": 4,
   "order": 30,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "Recently, a minimum classification error training algorithm has been proposed for minimizing the misclassification probability based on a given set of training samples using a generalized probabilistic descent method. This algorithm is a type of discriminative learning algorithm, but it approaches the objective of minimum classification error in a more direct manner than the conventional discriminative training algorithms. We apply this algorithm for simultaneous design pf feature extractor and pattern classifier, and demonstrate some of its properties and advantages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-30"
  },
  "euler95_eurospeech": {
   "authors": [
    [
     "Stephan",
     "Euler"
    ]
   ],
   "title": "Integrated optimization of feature transformation for speech recognition",
   "original": "e95_0109",
   "page_count": 4,
   "order": 31,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "In this paper we present an new approach for obtaining an optimal linear transformation of feature vectors. The generalized probabilistic descent method is used in order to optimize the elements of a transformation matrix with respect to a functional approximation of the recognition rate of the training data. The approach is tested in a speaker dependent recognizer for spelled names.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-31"
  },
  "morris95_eurospeech": {
   "authors": [
    [
     "Andrew C.",
     "Morris"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Phoneme transition detection and broad classification using a simple model based on the function of onset detector cells found in the cochlear nucleus",
   "original": "e95_0115",
   "page_count": 4,
   "order": 32,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "We present a simple model for onset and offset detection which is based on the broad functionality of onset cells in the cochlear nucleus, the first auditory brain centre. We show that the clusters of transition events detected by this model in the spectrogram can be used to both locate and broad-classify phoneme transitions. A preliminary Isolated Word Recognition system is described which bases recognition solely on evidence from detected transition clusters together with short spectral samples taken from each cluster centre. Recognition performance is compared with that for two other IWR systems of a similar complexity which process the whole signal uniformly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-32"
  },
  "fragniere95_eurospeech": {
   "authors": [
    [
     "Eric",
     "Fragniere"
    ],
    [
     "Andre van",
     "Schaik"
    ],
    [
     "Eric",
     "Vittoz"
    ]
   ],
   "title": "Linear predictive coding of speech using an analogue cochlear model",
   "original": "e95_0119",
   "page_count": 4,
   "order": 33,
   "p1": "119",
   "pn": "122",
   "abstract": [
    "An analogue electronic model of a cochlea was developed some years ago by R. F. Lyon using a cascade of filters. Combined with an analogue gradient descent circuit, such an artificial cochlea can be used to extract the Linear Predictive Coding (LPC) of the speech signal in continuous time. We propose in this article an analogue VLSI circuit implementing a so-called 'Cochlear LPC (CLPC). We discuss the expected advantages of the CLPC, such as an optimal time-frequency resolution and the continuous time processing. Furthermore we present some speech recognition results obtained using a computer model of the CLPC pre-processing combined with an HMM classifier. These results compare favourably with those obtained from a standard LPC/HMM system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-33"
  },
  "jones95_eurospeech": {
   "authors": [
    [
     "E.",
     "Jones"
    ],
    [
     "E.",
     "Ambikairajah"
    ]
   ],
   "title": "Pitch extraction of telephone bandwidth speech using a place-temporal approach",
   "original": "e95_0123",
   "page_count": 4,
   "order": 34,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "This paper describes pitch extraction of telephone-bandwidth speech using a combined place-temporal approach. The method makes use of a nonlinear auditory model which regenerates harmonics below 300 Hz which have been removed from the speech by the bandlimiting effect of a telephone channel. The spectral representation produced by the nonlinear auditory model is further processed using the Harmonic Product Spectrum to give an initial estimate of the fundamental frequency, using only place information. This initial estimate is then refined by temporal analysis on the detailed time response of a single section of the auditory model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-34"
  },
  "bodden95_eurospeech": {
   "authors": [
    [
     "Markus",
     "Bodden"
    ],
    [
     "Timothy R.",
     "Anderson"
    ]
   ],
   "title": "A binaural selectivity model for speech recognition",
   "original": "e95_0127",
   "page_count": 4,
   "order": 35,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "Neural networks that employed unsupervised learning were used on the output of a binaural auditory model, know as the Cocktail-Party-Processor, to perform context-independent phoneme recognition. Experiments which compared the performance of the binaural model representation to that of a monaural version showed that the binaural model performed significantly better in terms of phoneme recognition accuracy under the conditions tested (low signal-to-noise ratios (SNR) and a small database of speakers). The binaural model representations' performance has approximately a 20 dB SNR advantage over the monaural representation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-35"
  },
  "dobrin95_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Dobrin"
    ],
    [
     "Petri",
     "Haavisto"
    ],
    [
     "Kari",
     "Laurila"
    ],
    [
     "Jaakko",
     "Astola"
    ]
   ],
   "title": "Speech recognition experiments in a noisy environment using auditory system modelling",
   "original": "e95_0131",
   "page_count": 4,
   "order": 36,
   "p1": "131",
   "pn": "134",
   "abstract": [
    "The recognition performance of FFT or LPC front-ends decreases dramatically when the signal-to-noise ratio (SNR) in the input speech signal is approaching 0 dB. An alternative solution is to model the auditory system, which is the best recognition system that we know. This paper analyses the speech recognition performance of an auditory-based front-end versus an FFT front-end for a large number of speakers in natural environments. First, the auditory models that we used for experiments are presented. The instantaneous firing rate is then processed by a model of central auditory system, introduced by Wu. We extended Wu's model focusing more on maximizing of recognition performance, rather than modelling subtle auditory phenomena. The particular test conditions are then described and the processing stages are illustrated on isolated words. At last, the conclusions from our experiments are drawn.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-36"
  },
  "berthommier95_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Georg F.",
     "Meyer"
    ]
   ],
   "title": "Source separation by a functional model of amplitude demodulation",
   "original": "e95_0135",
   "page_count": 4,
   "order": 37,
   "p1": "135",
   "pn": "138",
   "abstract": [
    "The aim of this work is to separate complex sounds characterised by their fundamental frequency (F0) and their spectrum. We propose an elementary 'cocktail party' processor working with these two features. The model consists of a DFT based processing of the output of a gammatone filterbank channel by channel, having three stages : pitch estimation, recovery of source spectrum using amplitude modulation (AM) frequency, and pattern matching, using neural networks. The signal is demodulated by rectification, and the amplitude modulation frequency is given by evaluation of the Fourier transform module. We build a two-dimensional tonotopic/AMtopic map where the complex sound components are well resolved and we group them in order to recover separate spectra, using the sieve estimate of the fundamental frequency. Performances are shown for the vowel/masker and vowel/vowel recognition task. We show that it is a good alternative method to the autocorrelogram proposed by Assmann and Summerfield [1] for performing the periodicity analysis and the complex sound separation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-37"
  },
  "davidek95_eurospeech": {
   "authors": [
    [
     "V.",
     "Davidek"
    ],
    [
     "P.",
     "Sovka"
    ],
    [
     "J.",
     "Sika"
    ]
   ],
   "title": "Real-time implementation of spectral subtraction algorithm for suppression of acoustic noise in speech",
   "original": "e95_0141",
   "page_count": 4,
   "order": 38,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "Spectral subtraction algorithm offers a powerfully robust and computationally efficient approach to suppression of acoustic noise in speech. Real-time implementation of modified spectral subtraction method on floating-point signal processor TMS320C30 is described. The computational and memory requirements of an implemented procedures are analysed. Two types of implemented speech activity detectors, Harison's energy detector and cepstral detector are compared on real noisy speech signal from a car.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-38"
  },
  "coile95_eurospeech": {
   "authors": [
    [
     "Bert Van",
     "Coile"
    ],
    [
     "Hans-Wilhelm",
     "Rühl"
    ],
    [
     "L.",
     "Vogten"
    ],
    [
     "M.",
     "Thoone"
    ],
    [
     "S.",
     "Goß"
    ],
    [
     "D.",
     "Delaey"
    ],
    [
     "E.",
     "Moons"
    ],
    [
     "Jacques M. B.",
     "Terken"
    ],
    [
     "Jan Roelof de",
     "Pijper"
    ],
    [
     "M.",
     "Kugler"
    ],
    [
     "P.",
     "Kaufholz"
    ],
    [
     "R.",
     "Krüger"
    ],
    [
     "S.",
     "Leys"
    ],
    [
     "S.",
     "Willems"
    ]
   ],
   "title": "Speech synthesis for the new pan-european traffic message control system RDS-TMC",
   "original": "e95_0145",
   "page_count": 4,
   "order": 39,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "This paper reports on the speech synthesis module used within the Pan-European Traffic Message Control system RDS-TMC to present spoken traffic messages through the car radio. A central concern has been to generate spoken traffic messages in a language selected by the user, independent of the geographical area. Another feature is the combination of text-to-speech techniques with phonetic input which has been optimized off-line, resulting in a flexible system with natural sounding speech output. A prototype has been developed for German, and more languages are to follow soon.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-39"
  },
  "pacifici95_eurospeech": {
   "authors": [
    [
     "R.",
     "Pacifici"
    ],
    [
     "G.",
     "Manca"
    ]
   ],
   "title": "Echo cancelling in speech recognition systems",
   "original": "e95_0149",
   "page_count": 4,
   "order": 40,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "Man-machine dialogue systems using speech recognizers allow a more friendly asynchronous interaction, in such a way that the speaker is not forced to wait the end of previous prompts or messages uttered by the machine. In order to achieve that, man-machine systems accessed through the telephone network require an echo canceller, for eliminating the interference of the echo of the machine messages; this echo in unavoidable, given the mismatch of the hybrid circuit. Echo cancellers used in man-machine dialogue systems have different requirements from those used in man to man transmission systems; moreover their implementation adds a significant computational overhead to recognizers, inducing some possible degradation in recognition performance as well. Here we describe our specific implementation of an echo canceller for speech recognizers, not too heavy from the computational point of view and exhibiting good recognition accuracy as well.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-40"
  },
  "calonge95_eurospeech": {
   "authors": [
    [
     "T.",
     "Calonge"
    ],
    [
     "L.",
     "Alonso"
    ],
    [
     "R.",
     "Ralha"
    ],
    [
     "A. L.",
     "Sanchez"
    ]
   ],
   "title": "Parallel implementation of an hybrid neural network used for speech recognition task",
   "original": "e95_0153",
   "page_count": 4,
   "order": 41,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "In this paper a parallel implementation of a learning process for the two main non recurrent neural networks will be presented: Multilayer Perceptron (MLP) and Self-Organising Map of Kohonen (SOM). They will be integrated in just one system used for isolated word recognition task, speaker independent, with a restricted vocabulary, made up of the ten Spanish words for the decimal digits. The implementation was carried out on a distributed memory multiprocessor system with a synchronous message passing communication network. A detailed explanation about the computational tasks distribution and the subsequent communication requirements will be given. In addition, several experiments with different numbers of processors were made in order to study both the granularity and the overhead due to the processors communication. In this way, we obtained the speedup curve showing the load balance and efficiency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-41"
  },
  "li95_eurospeech": {
   "authors": [
    [
     "M.",
     "Li"
    ],
    [
     "J. T.",
     "Proudfoot"
    ]
   ],
   "title": "Hardware design of LPC coding for speech feature extraction",
   "original": "e95_0157",
   "page_count": 4,
   "order": 42,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "This paper describes the high-level design and specification of an Application-Specific Integrated Circuit (ASIC) to implement a speech feature extraction algorithm based on Linear Predictive Coding (LPC). The ASIC uses a fully synchronous top-down design methodology with a hardware description language ELLA. To suit the hardware design, the LPC algorithm is reformulated. A register-transfer level structure is used to meet requirements for suitability and efficiency of hardware, and a hierarchical design methodology used to implement the algorithm in low cost ASIC form. The approach can be applied to a wide range of digital signal processing functions. The goal of this effort is to develop a speaker recognition system based on the LPC algorithm which can be used as an alternative to the currently used software system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-42"
  },
  "bergmann95_eurospeech": {
   "authors": [
    [
     "Henning",
     "Bergmann"
    ],
    [
     "Hans-Hermann",
     "Hamer"
    ],
    [
     "Andreas",
     "Noll"
    ],
    [
     "Annedore",
     "Paeseler"
    ],
    [
     "Horst",
     "Tomaschewski"
    ]
   ],
   "title": "Modularization in task-specific language modelling",
   "original": "e95_0161",
   "page_count": 4,
   "order": 43,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "In the SpeechMaster speech recognition system the grammar serves a two-fold purpose, (1) as the primary source for the creation of the language model, and (2) as the basis for semantic interpretation rules used to derive the task-specific semantics in a syntax-directed manner. After outlining the baseline approach some extensions are motivated by an overview of selected applications. The enhancements then described keep the simplicity and flexibility of the grammar based-approach, while allowing for. a more modular composition of a production language model from predefined parts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-43"
  },
  "avendano95_eurospeech": {
   "authors": [
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Eric A.",
     "Wan"
    ]
   ],
   "title": "Beyond NYQUIST: towards the recovery of broad-bandwidth speech from narrow-bandwidth speech",
   "original": "e95_0165",
   "page_count": 4,
   "order": 44,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "A new technique is presented which improves the subjective quality of band-limited speech. The approach is based on a linear model of speech production, in which we independently estimate the spectral envelope and excitation function for a broad-bandwidth speech signal to reconstruct missing frequency components in narrow-bandwidth speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-44"
  },
  "pye95_eurospeech": {
   "authors": [
    [
     "D.",
     "Pye"
    ],
    [
     "Phil C.",
     "Woodland"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Large vocabulary multilingual speech recognition using HTK",
   "original": "e95_0181",
   "page_count": 4,
   "order": 45,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "The HTK large vocabulary speech recognition system has been shown to produce state-of-the-art results on American English data. The system uses decision tree state-clustered mixture-density cross-word triphones and statistical N-gram language modelling. Recently, as part of the EC-funded SQALE project, versions of the system have been developed in several European languages. The paper gives an overview of the HTK speech recognition system with American English baseline results, and then describes the progress made in developing British English, French and German versions. The official SQALE evaluation results are reported for each of these four languages and their relative performance is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-45"
  },
  "lamel95_eurospeech": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "M.",
     "Adda-Decker"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Issues in large vocabulary, multilingual speech recognition",
   "original": "e95_0185",
   "page_count": 4,
   "order": 46,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "In this paper we report on our activities in multilingual, speaker-independent, large vocabulary continuous speech recognition. The multilingual aspect of this work is of particular importance in Europe, where each country has its own national language. Our existing recognizer for American English and French, has been ported to British English and German. It has been assessed in the context of the LRE Sqale project whose objective was to experiment with installing in Europe a multilingual evaluation paradigm for the assessment of large vocabulary, continuous speech recognition systems. The recognizer makes use of phone-based continuous density HMM for acoustic modeling and n-gram statistics estimated on newspaper texts for language modeling. The system has been evaluated on a dictation task with read, newspaper-based corpora, the ARPA Wall Street Journal corpus of American English, the WSJCAMO corpus of British English, the BREF-Le Monde corpus of French and the PHONDXT-Frankfurter Rundschau corpus of German. Under closely matched conditions, the average word accuracy across all 4 languages is 85%, obtained with an open-vocabulary test and 20k trigram systems (64k system German).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-46"
  },
  "barnett95_eurospeech": {
   "authors": [
    [
     "James",
     "Barnett"
    ],
    [
     "Paul",
     "Bamberg"
    ],
    [
     "Martin",
     "Held"
    ],
    [
     "Juan",
     "Huerta"
    ],
    [
     "Linda",
     "Manganaro"
    ],
    [
     "Adam",
     "Weiss"
    ]
   ],
   "title": "Comparative performance in large-vocabulary isolated-word recognition in five european languages",
   "original": "e95_0189",
   "page_count": 4,
   "order": 47,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "Dragon Systems' DragonDictate® for Windows Version 1.0 (DDWin) large-vocabulary isolated-word dictation system is available in several languages. This paper reports on comparative recognition performance in five of them: French (pre-release), German, Italian, Spanish, and English. The tests are based on a variety of different document styles, including a work of philosophy, a popular novel, and a software user's manual. The results show statistically significant differences between the languages, with Italian being the easiest to recognize, and German and French the hardest. An error analysis shows the importance of vocabulary coverage in inflected languages, as English shows an out-of-vocabulary error rate of less than .5%, while the other four languages have rates above 2%. French showed a much higher rate of homophone errors than the other languages. However, recognition performance is uniformly better than that reported in a similar test in 1991.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-47"
  },
  "brousseau95_eurospeech": {
   "authors": [
    [
     "Julie",
     "Brousseau"
    ],
    [
     "Caroline",
     "Drouin"
    ],
    [
     "George",
     "Foster"
    ],
    [
     "Pierre",
     "Isabelle"
    ],
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Yves",
     "Normandin"
    ],
    [
     "Pierre",
     "Plamondon"
    ]
   ],
   "title": "French speech recognition in an automatic dictation system for translators: the transtalk project",
   "original": "e95_0193",
   "page_count": 4,
   "order": 48,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "This paper describes a system designed for use by professional translators that enables them to dictate their translation. Because the speech recognizer has access to the source text as well as the spoken translation, a statistical translation model can guide recognition. This can be done in many different ways - which is best? We discuss the experiments that led to integration of the translation model in a way that improves both speed and performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-48"
  },
  "dugast95_eurospeech": {
   "authors": [
    [
     "Christian",
     "Dugast"
    ],
    [
     "Xavier",
     "Aubert"
    ],
    [
     "Reinhard",
     "Kneser"
    ]
   ],
   "title": "The Philips large-vocabulary recognition system for american English, French, and German",
   "original": "e95_0197",
   "page_count": 4,
   "order": 49,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "This paper presents the work done at Philips Research to extend our American-English large-vocabulary continuous speech recognition system to two new languages, French and German. The tasks on which the system is tested are very similar for each language: a speaker-independent, continuous-speech, national newspaper reading task with a high-quality microphone. However, an important factor for German is the extension of the recognition vocabulary from 20k to 64k words to achieve comparable out-of-vocabulary rates. A comparison between the different databases used to train the system for the three languages is made, results are accordingly interpreted and the characteristics of each language are pointed out.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-49"
  },
  "lin95_eurospeech": {
   "authors": [
    [
     "Sung-Chien",
     "Lin"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Keh-Jiann",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "A syllable-based very-large-vocabulary voice retrieval system for Chinese databases with textual attributes",
   "original": "e95_0203",
   "page_count": 4,
   "order": 50,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "In this paper a syllable-based voice retrieval approach for Chinese textual databases retrieval is presented. The presented approach can reduce most of difficulties of Chinese voice retrieval and easily integrate with the continuous speech recognition technology of the Mandarin dictation machine, Golden Mandarin (m). The experimental results show that the presented approach is easy to implement and systems based on it can allow users to retrieve Chinese textual databases using spoken queries and unconstrained vocabulary. Although the proposed approach is statistics-based and has some restrictions in linguistic analysis, the achieved results are very encouraging and have shown its feasibility in creating practical applications which demand the recognition ability of very large vocabulary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-50"
  },
  "riley95_eurospeech": {
   "authors": [
    [
     "Michael",
     "Riley"
    ],
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Donald",
     "Hindle"
    ],
    [
     "Fernando",
     "Pereira"
    ]
   ],
   "title": "The AT&t 60,000 word speech-to-text system",
   "original": "e95_0207",
   "page_count": 4,
   "order": 51,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "The AT&T Speech-To-Text System is a large vocabulary, continuous speech recognition system with high-accuracy performance and a flexible, modular architecture. We will describe the acoustic, lexical and grammatical modeling and the overall system architecture and search strategy used in this system. In the 1994 ARPA North American Business (NAB) News Evaluation, the system used a 60,000 word vocabulary and thirty-four million 1-5 grams and achieved a word error rate of 10% (H1-P0). We also report results on the ATIS task using the identical system and acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-51"
  },
  "ho95_eurospeech": {
   "authors": [
    [
     "Tai-hsuan",
     "Ho"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Lee-feng",
     "Chien"
    ],
    [
     "Keh-Jiann",
     "Chen"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Fast and accurate continuous speech recognition for Chinese language with very large vocabulary",
   "original": "e95_0211",
   "page_count": 4,
   "order": 52,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "This paper presents a fast and accurate approach to continuous speech recognition for Chinese language with very large vocabulary. Since Chinese language is not alphabetic and the input of Chinese characters into computers through keyboards is difficult, such speech input techniques for dictation purposes are highly desired. Considering the special characteristics of Chinese language, the approach proposed in this paper is based on a two-stage recognition concept. The first fast matching stage utilizes the monosyllabic structure of Chinese language and provides a word lattice to constrain the search space of the subsequent stage, while the second detailed matching stage successfully integrates the acoustic word models and the Chinese language model. This approach is therefore much more accurate and faster than our previous version for such a task[l]. The current experimental system runs in real-time on SPARC-10 workstation with Chinese character accuracy on the order of 90.2%, while the previous version takes more than twice of the time with accuracy of 88.7%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-52"
  },
  "wang95_eurospeech": {
   "authors": [
    [
     "Zuoying",
     "Wang"
    ],
    [
     "Jun",
     "Wu"
    ],
    [
     "Xi",
     "Xiao"
    ],
    [
     "Jin",
     "Quo"
    ]
   ],
   "title": "Methods towards the very large vocabulary Chinese speech recognition",
   "original": "e95_0215",
   "page_count": 4,
   "order": 53,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "Chinese is not an alphabetic language, and there is no convenient input method. Inputting by speech is one of the best ways to solve this problem. In this paper, we introduced our Chinese dictation machine, the JANET system, which is a very large vocabulary isolated speech recognition system. It could recognize all 1254 Chinese syllables as well as convert syllable strings into texts. The syllable recognition rate is 89% and the word accuracy is more than 95%. The system is implemented on PC and is commercially available now.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-53"
  },
  "cook95_eurospeech": {
   "authors": [
    [
     "G. D.",
     "Cook"
    ],
    [
     "A. J.",
     "Robinson"
    ]
   ],
   "title": "Utterance clustering for large vocabulary continuous speech recognition",
   "original": "e95_0219",
   "page_count": 4,
   "order": 54,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "Conventional speaker independent speech recognition systems are trained using data from many different speakers. Inter-speaker variability is a major problem because parametric representations of speech are highly speaker dependent. This paper describes a technique which allows speaker dependent parameters to be considered when building a speaker independent speech recognition system. The technique is based on utterance clustering, where subsets of the training data are formed and the variability within each subset minimized. Cluster dependent connectionist models are then used to estimate phone probabilities as part of a hybrid connectionist hidden Markov model based large vocabulary talker independent speech recognition system. The system has been evaluated on the ARPA Wall Street Journal continuous speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-54"
  },
  "eriksson95_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Eriksson"
    ],
    [
     "Jan",
     "Linden"
    ],
    [
     "Jan",
     "Skoglund"
    ]
   ],
   "title": "Vector quantization of glottal pulses",
   "original": "e95_0225",
   "page_count": 4,
   "order": 55,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "An efficient codebook driven voiced excitation coding method producing natural sounding speech is proposed. It can be incorporated as an essential part in a complete speech coder working at low bit rates. The interpulse correlation of such a coding scheme is investigated and exploited using linear predictive vector quantization and finite state vector quantization (FSVQ). A new and more robust FSVQ method that is able to more efficiently exploit this correlation is proposed. The new method dynamically combines two memoryless vector quantizers. This dynamic combination method is not restricted to pulse codebooks but can be employed for any coding scheme with intervector correlation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-55"
  },
  "festa95_eurospeech": {
   "authors": [
    [
     "Michele",
     "Festa"
    ],
    [
     "Daniele",
     "Sereno"
    ]
   ],
   "title": "A speech coding algorithm based on prototypes interpolation with critical bands and phase coding",
   "original": "e95_0229",
   "page_count": 4,
   "order": 56,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "Coding schemes based on PWI (Prototype Waveform Interpolation) [1] can supply good voiced speech quality at low bit rate (4 kbit/s). PWI algorithms generate the excitation signal for the LPC filter by interpolating prototype pitch waveforms taken from the previous and the current frame. Starting from this idea we have developed one algorithm called CB-PC (Critical Band - Prototype Coding) which allows to provide excellent quality, for voiced speech segments, at 3.3 kbit/s. This result has been achieved by means of an efficient coding and quantization of the prototype waveforms based on VQ of the critical band magnitude components as well as of a novel technique for phase coding. The work has been developed in the framework of the RACE/MAVT (Mobile Audio-Visual Terminal) project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-56"
  },
  "tsoukalas95_eurospeech": {
   "authors": [
    [
     "D.",
     "Tsoukalas"
    ],
    [
     "Jiannis",
     "Mouropoulos"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Very low-bitrate speech coding using perceptually-derived spectral data",
   "original": "e95_0233",
   "page_count": 4,
   "order": 57,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "A new family of very low bitrate speech coders employing models of human perception is presented. The coding methodology is based on non-linear modulation of a random broadband noise source with signals derived from speech, following two main strategies for representing coded speech: one using the minimum audible difference between the original and the modulated speech signals, and another using a minimum log-error criterion along with some perceptually derived harmonic information. Depending on the methodology and the degree of accuracy employed for coding several implementations are allowed starting from 1 kb/s. High intelligibility is achieved even for the lower bitrate implementations, although, some increase in the bitrate is required for high-quality speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-57"
  },
  "piazzo95_eurospeech": {
   "authors": [
    [
     "Lorenzo",
     "Piazzo"
    ]
   ],
   "title": "A new very low bit rate speech coder: the step decomposition vocoder",
   "original": "e95_0237",
   "page_count": 4,
   "order": 58,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "An extension of the classical LPC model of speech production is proposed, where the LPC spectral trajectories are viewed as the output of linear shift-invariant filters (articulator filters: representing the articulator responses to a brain command), excited by piece-wise constant signals (command signals: representing brain commands). Efficient procedures for the articulator filters estimation and for the command signals computation from the speech signal are outlined. The model is then used to design a very low bit rate, low computational burden vocoder, the Step Decomposition Vocoder, from which comprehensible speech can be obtained with about 650 bps, and with a 250 millisec. algorithmic delay.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-58"
  },
  "atkinson95_eurospeech": {
   "authors": [
    [
     "I. A.",
     "Atkinson"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "Time envelope LP vocoder: a new coding technique at very low bit rates",
   "original": "e95_0241",
   "page_count": 4,
   "order": 59,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "This paper presents a linear prediction (LP) based vocoder in which speech waveforms are considered as having a 'time envelope', the shape of which contains important perceptual information. By ensuring that the time envelope of the synthetic speech closely matches that of the original, natural sounding synthetic speech can be produced. The advantage over more traditional linear prediction vocoders is that the amplitude time envelope is preserved in addition to the spectral envelope, allowing the rapid amplitude transitions associated with onsets to be retained in the synthetic speech, resulting in a more intelligible output. This paper presents a complete vocoder scheme including details of techniques such as parameter interpolation, quantisation, spectrum shaping and pitch detection which have proven necessary to produce natural sounding synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-59"
  },
  "stefanoiu95_eurospeech": {
   "authors": [
    [
     "Dan",
     "Stefanoiu"
    ],
    [
     "Radwan",
     "Kastantin"
    ],
    [
     "Gang",
     "Feng"
    ]
   ],
   "title": "Speech coding based on the discrete-time wavelet transform and human auditory system properties",
   "original": "e95_0661",
   "page_count": 4,
   "order": 60,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "In this paper, we present a new speech coding algorithm with variable bit rate for telephonic domain. The properties of the Discrete-Time Wavelet Transform and several human psycho-acoustic features are integrated in order to obtain a high quality of coded signal, close to original. This approach allows to reduce the mean bit rate in the range of 10 kbit/s. We describe there the main steps of the coding algorithm. More specifically, we provide the solutions to the following specific problems: choosing the type of the Wavelet Transform, selecting the wavelets coefficients to send, quantifying all sending information (coefficients values and auxiliary) and generating the associated codes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-60"
  },
  "ancin95_eurospeech": {
   "authors": [
    [
     "F. J.",
     "Ancin"
    ],
    [
     "M. L.",
     "Larreategui"
    ],
    [
     "B. L.",
     "Burrows"
    ],
    [
     "R. A.",
     "Carrasco"
    ]
   ],
   "title": "Wavelets for low bit rate speech coding applications",
   "original": "e95_0665",
   "page_count": 4,
   "order": 61,
   "p1": "665",
   "pn": "669",
   "abstract": [
    "Adaptive transform coding (ATC) and Analysis-by-Synthesis (AbS) have been shown to provide high quality speech above 10 kb/s and below 8 kb/s respectively. The application of the discrete wavelet packet transform (DWPT) and the finite interscale basis coefficient sequences are investigated and evaluated for speech coding applications. A fully quantised DWPT based ATC is presented. Objective and subjective measurements prove the superior WT performance in the adaptive quantisation and bit allocation processes in comparison with other transform based ATC algorithms. Moreover, a hybrid Wavelet-Binary pulse excitation (WBPE) model for CELP speech coding is also presented. Results of a WBPE targeted at 6.3 kb/s are also compared with the conventional CELP coder at 8 kb/s using Gaussian, sparse and ternary codebook population.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-61"
  },
  "mandridake95_eurospeech": {
   "authors": [
    [
     "E.",
     "Mandridake"
    ],
    [
     "R.",
     "Atay"
    ],
    [
     "M.",
     "Najim"
    ]
   ],
   "title": "Adaptive speech vector coding with a multiresolution hierarchical codebook",
   "original": "e95_0669",
   "page_count": 4,
   "order": 62,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "We have recently proposed a new approach for speech coding based on the use of the Discrete Wavelet Transform and Vector Quantization called the Discrete Wavelet Vector Transform Coding (DWVTC). This approach, although it provides results close to the optimal multiresolution codebook, suffers from the fixed size of the used codebook. In this paper, we propose a new scheme based on a multiresolution hierarchical codebook structure which gives better results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-62"
  },
  "popescu95_eurospeech": {
   "authors": [
    [
     "Andrei",
     "Popescu"
    ],
    [
     "Nicolas",
     "Moreau"
    ]
   ],
   "title": "Subband analysis-by-synthesis coding",
   "original": "e95_0673",
   "page_count": 4,
   "order": 63,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "We propose a coder structure, called SB AS (\"Subband Analysis-by-Synthesis\"), for use in medium- and low-rate coding of speech and audio. We use analysis-by-synthesis to quantize the parameters of a subband signal model. For speech coding applications, long-term prediction can be easily integrated with this structure. The evaluation of SBAS for the coding of telephone-band speech at 8 kb/s gave promising results. A relatively low coding delay (10...50 ms) can be achieved with this type of coder, and its computational complexity is comparable to that of a CELP coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-63"
  },
  "parris95_eurospeech": {
   "authors": [
    [
     "Clifford I.",
     "Parris"
    ],
    [
     "Danny",
     "Wong"
    ],
    [
     "Francois",
     "Chambon"
    ]
   ],
   "title": "A robust 2.4kb/s LP-MBE with iterative LP modelling",
   "original": "e95_0677",
   "page_count": 4,
   "order": 64,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "In order to reduce the transmission bandwidth requirement for the spectral envelope parameters for Multiband Excitation (MBE) coders [1] Linear Prediction (LP) modelling has been widely investigated [2, 3, 4]. In this paper we compare the direct methods for evaluating the linear prediction filter [2, 3] with a novel iterative LP modelling technique. A novel switched Vector Quantizer (VQ) is employed to transmit 12 binary V/UV harmonic decisions [5] using only 4 bits which achieves only 5% errors. To enhance robustness joint source and channel coding codebook design and search has been incorporated into the coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-64"
  },
  "torresguijarro95_eurospeech": {
   "authors": [
    [
     "M. S.",
     "Torres-Guijarro"
    ],
    [
     "F. J.",
     "Casajus-Quiros"
    ]
   ],
   "title": "Improved transient representation and quantization for sinusoidal speech coders",
   "original": "e95_0681",
   "page_count": 4,
   "order": 65,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "In this contribution we propose an improvement of the multiband excitation vococoder on the basis of preserving temporal and spectral features of transients. Proper representation of unvoiced-voiced transitions is achieved by means of reliable detection of the point where harmonics are born and application of specific analysis and synthesis procedures for the frames placed before and after this point. Description of temporal features requires the definition of new parameters, their extraction and quantization having been studied. Additional methods for parameter reduction and quantization will also be addressed:\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-65"
  },
  "yu95_eurospeech": {
   "authors": [
    [
     "W. M. E.",
     "Yu"
    ],
    [
     "Cheung-Fat",
     "Chan"
    ]
   ],
   "title": "Efficient multiband excitation linear predictive coding of speech at 1.6 kbps",
   "original": "e95_0685",
   "page_count": 4,
   "order": 66,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "A 1.6 kbps MBE speech coder is proposed in this paper. Several improvements on the conventional MBE coder have been made in order to maintain the quality and intelligibility of the original speech at 1.6 kbps. We developed an improved pitch detection algorithm which results in more accurate pitch estimation for natural speech synthesis. The number of bits for coding V/UV information is significantly reduced by employing a simplified V/UV mixture function which requires coding of the position of a V/UV transition frequency only. Band magnitudes are coded via linear predictive model. An efficient recursive procedure was developed for computing and quantizing the 2DdLSP residuals simultaneously. Subjective speech quality was further improved by the application of a postfilter in the frequency domain. The speech coder was implemented in real-time on the TMS320C30 DSP chip.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-66"
  },
  "wery95_eurospeech": {
   "authors": [
    [
     "Bruno",
     "Wery"
    ],
    [
     "Stephane",
     "Deketelaere"
    ]
   ],
   "title": "Voice coding in the MSBN satellite communication system",
   "original": "e95_0689",
   "page_count": 4,
   "order": 67,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "In this paper, we present the voice coding techniques that are proposed for the new Mobile Satellite Business Network (or MSBN) system. The paper describes not only some coding techniques that may be applied, but also the considerations that lead to them, hi the paper, we give a brief of the MSBN system, and its requirements for the voice coding sub-system. The choice of an MBE based vocoder is then discussed. Last, two key techniques that are involved in this implementation are presented: the LPC + correction representation of the spectrum envelope, and the Vector Quantisation technique that is used for transmission of the Linear Prediction parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-67"
  },
  "cheetham95_eurospeech": {
   "authors": [
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "X. Q.",
     "Sun"
    ],
    [
     "W. T. K.",
     "Wong"
    ]
   ],
   "title": "Spectral envelope estimation for low bit-rate sinusoidal speech coders",
   "original": "e95_0693",
   "page_count": 4,
   "order": 68,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "Many low bit-rate sinusoidal coding techniques, such as sinusoidal transform coding, require a short term spectral envelope to be estimated for each frame of speech. Suitably encoded, this determines the amplitudes of the sinusoids used to model the speech, hi some coders, phase information is also derived from the envelope under the assumption that it represents the gain response of a minimum phase vocal tract transfer function. The phase spectrum, thus derived, is dependent on how the envelope is interpolated between pitch frequency harmonics especially in the vicinity of formants. The use of optimisation to improve the shape of the envelope in such regions and to compensate for the inadequacy of the minimum phase assumption has been shown to be capable of reducing phase discrepancies. Frequency spreading due to pitch frequency variation also distorts the spectral envelope, and can be compensated for. Experiments have shown that the shape of the reconstructed waveform can be made closer to that of the original by spectral envelope modification, and improvement in perceived speech quality is obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-68"
  },
  "cohen95_eurospeech": {
   "authors": [
    [
     "Israel",
     "Cohen"
    ],
    [
     "Shalom",
     "Raz"
    ],
    [
     "David",
     "Malah"
    ]
   ],
   "title": "Shift-invariant adaptive local trigonometric decomposition",
   "original": "e95_0247",
   "page_count": 4,
   "order": 69,
   "p1": "247",
   "pn": "250",
   "abstract": [
    "A general formulation of shift-invariant \"best-basis\" expansions is presented. Specifically, we construct an extended library of smooth local trigonometric bases, and introduce a suitable \"best-basis\" search algorithm. We prove that the resultant decomposition is shift-invariant, orthonormal and characterized by a reduced information cost. The shift-invariance is derived from an adaptive relative shift of expansions in distinct resolution levels. We show that at any resolution level £ it suffices to examine and select one of two relative shift options - a zero shift or a 2-l-1 shift. A variable folding operator, whose polarity is locally adapted to the parity properties of the signal, extra enhances the representation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-69"
  },
  "micallef95_eurospeech": {
   "authors": [
    [
     "Paul",
     "Micallef"
    ],
    [
     "Edward",
     "Chilton"
    ]
   ],
   "title": "Spectral envelope of speech using wavelets",
   "original": "e95_0251",
   "page_count": 4,
   "order": 70,
   "p1": "251",
   "pn": "254",
   "abstract": [
    "The dyadic wavelet transform can be used to separate out the energy of a signal in various bands. The transform makes use of smoothing functions and wavelet functions that result in a very efficient method of signal decomposition and reconstruction. In this paper wavelet transform techniques are applied to separate out the spectral envelope from a Short Time Fourier Transform of a frame of speech. The results are compared to the spectral envelope estimation using LPC and cepstral methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-70"
  },
  "drygajlo95_eurospeech": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ],
    [
     "Nicolas",
     "Thevoz"
    ]
   ],
   "title": "Multiresolution speech analysis using fast time-varying orthogonal wavelet packet transform algorithms",
   "original": "e95_0255",
   "page_count": 4,
   "order": 71,
   "p1": "255",
   "pn": "258",
   "abstract": [
    "Various decomposition techniques have been employed in signal processing for exploiting and high-lighting the characteristics of a given signal. In this paper we present orthogonal overlapping block transforms as a signal analysis tool with the capability of variable multiresolution time-spectral decomposition of speech signals. Our prime interest is in the representation of nonstationary discrete-time signals in terms of wavelet packets used as time-varying discrete orthogonal systems, and we concentrate on the fast transform algorithms for such systems. Much of our current knowledge and intuition of speech is derived from analysis involving assumptions of short-time stationarity (e.g., the DFT-based speech spectrogram). Such methods are, by their very nature, incapable of revealing the true nonstationary nature of speech. A careful consideration of the theory of time-varying orthogonal transforms, however, allows the construction of dynamic multiresolution spectrograms that reveal far more of the nonstationarities of speech, thereby highlighting just what it is that conventional approaches miss. The fast orthogonal overlapping block transform algorithms represent an elegant and efficient solution to the implementation of time-varying wavelet packet transforms, since their FFT-like lattice block structure provides all possible multiresolution time-spectral coefficients.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-71"
  },
  "rangoussi95_eurospeech": {
   "authors": [
    [
     "Maria",
     "Rangoussi"
    ],
    [
     "Flemming",
     "Pedersen"
    ]
   ],
   "title": "Second- and third-order wigner distributions in hierarchical recognition of speech phonemes",
   "original": "e95_0259",
   "page_count": 4,
   "order": 72,
   "p1": "259",
   "pn": "262",
   "abstract": [
    "In this paper we investigate the use of the time-frequency representation of speech signals, and more specifically of the second- and third-order Wigner distributions, for the hierarchical classification of speech phonemes. We focus on the obstruent rather than the sonorant class of speech phonemes. Our aim is to obtain a new set of recognition features, that are both well-suited to the non-stationary nature of these sounds and robust to noise present when speech is produced under real conditions. A three-level hierarchical classification scheme for obstruent sounds is implemented and tested on real speech data obtained from TIMIT/DARPA, either noise-free or with additive noise, recorded in the interior of a moving car. Satisfactory classification scores are obtained at all three levels, from both distributions; the advantage of third-order over second-order Wigner, however, is marginal, not justifying the extra computations required.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-72"
  },
  "saleh95_eurospeech": {
   "authors": [
    [
     "G. M. K.",
     "Saleh"
    ],
    [
     "M.",
     "Niranjan"
    ],
    [
     "W. J.",
     "Fitzgerald"
    ]
   ],
   "title": "The use of maximum a posteriori parameters in linear prediction of speech",
   "original": "e95_0263",
   "page_count": 4,
   "order": 73,
   "p1": "263",
   "pn": "268",
   "abstract": [
    "We present an approach to linear prediction parameter estimation and model order selection that utilises Bayesian inference. The addition of a penalty term, or regulariser, to the conventional linear prediction data error term prior to minimising it facilitates the estimation of the maximum a posteriori parameters. A direct equivalence can be drawn between the type of regulariser used and the prior assumptions regarding the solution to a linear prediction problem. Mackay's Bayesian Evidence framework is used for the estimation of linear prediction parameters that reflect the role that prior assumptions play during the analysis of a speech segment. Quadratic regularisers are utilised to parametrise speech signals and the results are demonstrated with formant tracking and analysis-synthesis examples.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-73"
  },
  "ortel95_eurospeech": {
   "authors": [
    [
     "William C. G.",
     "Ortel"
    ]
   ],
   "title": "Observed long-term changes in customer calling patterns in a telephone application using automatic speech recognition",
   "original": "e95_0269",
   "page_count": 4,
   "order": 74,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "This paper presents some statistics regarding cutomer usage of an application of speaker-independent, continuous-speech automatic speech recognition (ASR) technology in an automated telephone operator application. In this application, ASR technology was used as a default alternative for customers who did not demonstrate a willingness to use tone signalling (DTMF). Over the four-year period, the use of DTMF increased, but the success rate per DTMF-processed call fell from 82% to 66%. However, although the use of ASR decreased, the success rate per ASR-processed call rose from 16% to 21%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-74"
  },
  "asadi95_eurospeech": {
   "authors": [
    [
     "Ayman",
     "Asadi"
    ],
    [
     "David",
     "Lubensky"
    ],
    [
     "L.",
     "Madhavrao"
    ],
    [
     "Jayant",
     "Naik"
    ],
    [
     "Vijay",
     "Raman"
    ],
    [
     "George",
     "Vysotsky"
    ]
   ],
   "title": "Combining speech algorithms into a \"natural\" application of speech technology for telephone network services",
   "original": "e95_0273",
   "page_count": 4,
   "order": 75,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "This paper describes the Intelligent Telephonic Recognition and Personal Identification (INTREPID) system developed at NYNEX Science and Technology. INTREPID combines, in one application, several speech recognition and speaker verification algorithms. The task was undertaken as a response to the request of many telephone service providers who currently offer or are going to offer voice dialing services. The goal is to have a limited, speaker independent vocabulary of Custom Calling Features (like Redial, Call Return, etc.) for voice control of these telephone network services. Although the paper describes in some detail different parts of the INTREPID system and gives some performance figures, the main goal is to demonstrate a structure in which this and a number of similar applications may be implemented and deployed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-75"
  },
  "ludovik95_eurospeech": {
   "authors": [
    [
     "Ye.",
     "Ludovik"
    ],
    [
     "V.",
     "Sibirtsev"
    ]
   ],
   "title": "Intelligent answering machine-secretary",
   "original": "e95_0277",
   "page_count": 4,
   "order": 76,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "Application of speech recognition technology seems to have very good prospects in creating intelligent answering machines. A system of that kind could be able to perform most of routine duties of a secretary connected with telephone calls, e.g., to hold pre-programmed dialogs initiated either by some caller or by the answering machine itself. The flow of the dialog could depend on the interlocutor and on his answers. The system described below is based on a PC and hardware, now commonly available, like modem and sound card connected to telephone line via specially developed simple interface device. Speech recognition is carried out by the quasi-linear method, using critical bands log spectrum processed to emphasise speaker-dependent formant information or on the contrary to make it more speaker-independent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-76"
  },
  "lokenkim95_eurospeech": {
   "authors": [
    [
     "Kyung-ho",
     "Loken-Kim"
    ],
    [
     "Young-duk",
     "Park"
    ],
    [
     "Suguru",
     "Mizunashi"
    ],
    [
     "Laurel",
     "Fais"
    ],
    [
     "Tsyuoshi",
     "Morimoto"
    ]
   ],
   "title": "Verbal-gestural behaviors in multimodal spoken language interpreting telecommunications",
   "original": "e95_0281",
   "page_count": 4,
   "order": 77,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "The study reported here is an attempt to understand human verbal-gestural behavior in a multimodal bilingual setting. Specific questions addressed are: 1) What kind of deictic gestures people use in a machine-mediated condition, and how these differ from those used in a human-mediated condition, 2) How significant the use of gestures is in each condition, and 3) How verbal and gestural behaviors are interrelated, 4) What the implications of our findings are for a multimodal spoken language interpretation system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-77"
  },
  "chen95_eurospeech": {
   "authors": [
    [
     "Jung-Kuei",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Large vocabulary, word-based Mandarin dictation system",
   "original": "e95_0285",
   "page_count": 4,
   "order": 78,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "In this paper we present a large vocabulary, word-based, automatic Mandarin dictation system. In this system we use isolated words as input to alleviate the unnaturalness of syllable input used by many existing systems and to reduce the acoustic confusions among 414 Mandarin syllables in many existing isolated syllable input system. There are two stages of processing in the word-based dictation system. The first, or the acoustic processing, stage includes two modules: a large vocabulary word recognizer and a tone recognizer. The word recognizer generates N-best word candidates based on the tree-trellis fast search. Then the tone recognizer reduces the homonym number of those word candidates. In the second, or the linguistic processing, stage a statistical language model is applied to a word lattice generated in the first stage. The most likely word sequence and the corresponding Chinese character string are decoded via a Viterbi search. The system has been evaluated on a speaker-trained database, a word accuracy of 85.5% and character accuracy of 88.3% were obtained. A real-time demo system has also been implemented on an HP-9000/735 workstation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-78"
  },
  "goff95_eurospeech": {
   "authors": [
    [
     "Bertrand Le",
     "Goff"
    ],
    [
     "Thierry",
     "Guiard-Marigny"
    ],
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "Read my lips... and my jaw! how intelligible are the components of a speaker's face?",
   "original": "e95_0291",
   "page_count": 4,
   "order": 79,
   "p1": "291",
   "pn": "294",
   "abstract": [
    "In adverse acoustic condition, speech intelligibility is dramatically improved when the speaker's face can be seen. Which components of the face best convey the visual information associated with speech gestures? We answer this question through a series of experiments where normal hearers had to identify three vowels [a,i,y] and six consonants [b, v, z, 3, R, 1] in French nonsense words of the form VCVCV, presented audiovisually under five conditions of background noise and five conditions of visual displays (the real face of the reference speaker, his lips alone, and three synthetic displays of facial components: lips, jaw and lips, whole face.)\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-79"
  },
  "fusterduran95_eurospeech": {
   "authors": [
    [
     "Angela",
     "Fuster Duran"
    ]
   ],
   "title": "Mcgurk effect in Spanish and German listeners: influences of visual cues in the perception of Spanish and German conflicting audio-visual stimuli",
   "original": "e95_0295",
   "page_count": 4,
   "order": 80,
   "p1": "295",
   "pn": "298",
   "abstract": [
    "A corpus of discrepant AV German and Spanish syllables was created in order to examine how audio-visual integration functions in those languages and to investigate possible interlanguage differences in the magnitude and type of visual effects. 15 syllables /ba, ma, da, na, ga, Na, la, ra, rra, bra, dra, gra, bla, dla, gla/ were produced by one German speaker and one Spanish speaker. Both sets of syllables were audiovisually recorded and combined^ with , , and video sequences resulting in 45 AV-stimuli. German AV-stimuli were presented to 30 German and 30 Spanish subjects; Spanish AV- stimuli to 30 German and 30, Spanish subjects. Subjects were asked to report what they perceived. Results showed perceptual differences across the 4 conditions. The influence of visual cues was stronger with the Spanish speaker than with the German one. Germans showed a higher amount of visual effects in the non-native language. Keywords: Bimodal Speech Perception, Audio-visual integration, McGurk effect, Interlanguage difference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-80"
  },
  "beskow95_eurospeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Rule-based visual speech synthesis",
   "original": "e95_0299",
   "page_count": 4,
   "order": 81,
   "p1": "299",
   "pn": "302",
   "abstract": [
    "A system for rule based audiovisual text-to-speech synthesis has been created. The system is based on the KTH text-to-speech system which has been complemented with a three-dimensional parameterized model of a human face. The face can be animated in real time, synchronized with the auditory speech. The facial model is controlled by the same synthesis software as the auditory speech synthesizer. A set of rules that takes coarticulation into account has been developed. The audiovisual text-to-speech system has also been incorporated into a spoken man-machine dialogue system that is being developed at the department.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-81"
  },
  "lavagetto95_eurospeech": {
   "authors": [
    [
     "Fabio",
     "Lavagetto"
    ],
    [
     "Paolo",
     "Lavagetto"
    ]
   ],
   "title": "A new algorithm for visual synthesis of speech",
   "original": "e95_0303",
   "page_count": 4,
   "order": 82,
   "p1": "303",
   "pn": "306",
   "abstract": [
    "In this paper an integrated software system is described for the conversion of speech into graphic animation suitable to lipreading. The objective of the work has been that of integrating the unimodal audio information conveyed through acoustic speeech with coherent visual information associated to the movements of the speaker's mouth. Thanks to this integration, the occurrence of perceptive impairments affecting the auditory channel, i.e. environmental noise, age or handicaps, can be bypassed effectively through the visual modality. The described system implements a direct mapping from audio to visual parameters through two processing stages responsible of speech analysis and visual synthesis, respectively. The specific architecture employed for implementing the conversion structure, based on Time-Delay neural networks, has been chosen in consideration of the bimodal nature of speech and, in particular, of the complex coarticulation phenomena which require the integration of a given amount of past acoustic information before performing the visual conversion. Preliminary experimental results have reported showing the concrete possibility of providing new services in telecommunication and rehabilitation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-82"
  },
  "kabre95_eurospeech": {
   "authors": [
    [
     "H.",
     "Kabre"
    ]
   ],
   "title": "Audiovisual speech recognition using the fuzzy shape filters model",
   "original": "e95_0307",
   "page_count": 4,
   "order": 83,
   "p1": "307",
   "pn": "310",
   "abstract": [
    "This paper presents an application of a Fuzzy Logic based Model for audiovisual speech recognition. The speech signal is analyzed with the Perceptual Linear Predictive Analysis and the speaker's face image is processed to extract some lip-opening parameters. Two sets of fuzzy filters are trained to extract some features from both acoustic and visual paths. These features are integrated by a second level of processing based on a feedforward neural network. Results obtained with a Signal to Noise Ratio between 0 and 18 dB are in the range of 63-95% and are compared to the Kohonen feature extractor.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-83"
  },
  "he95_eurospeech": {
   "authors": [
    [
     "Jialong",
     "He"
    ],
    [
     "Li",
     "Liu"
    ],
    [
     "Günther",
     "Palm"
    ]
   ],
   "title": "On the use of features from prediction residual signals in speaker identification",
   "original": "e95_0313",
   "page_count": 4,
   "order": 84,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "A by-product of the LPC analysis is the generation of a prediction residual signal e(n). e(n) is usually ignored in the major applications of speech analysis, and only the LPC coefficients or parameters derived from the LPC coefficients are used to compose feature vectors. Since e(n) carries all information that has not been captured by the LPC coefficients, an algorithm was proposed to calculate parameters from this prediction residual signal. The effectiveness of these features (named as RCEP coefficients) for speaker identification was evaluated. This approach yielded promising results. In an evaluation experiment in which the learning vector quantization (LVQ) networks served as classifiers, the correct identification rate for 112 male speakers was 88.8% for feature vectors composed of LPC based cepstrum (LPCC) alone, but reached 96.9% when the LPCC coefficients were combined with the RCEP coefficients.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-84"
  },
  "ng95_eurospeech": {
   "authors": [
    [
     "Kai Tat",
     "Ng"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Some nonparametric distance measures in speaker verification",
   "original": "e95_0317",
   "page_count": 4,
   "order": 85,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "This paper presents a nonparametric technique of speaker recognition and verification. Some statistics and their distance measures of speech are evaluated for short utterances. The proposed distance measures are different from existing ones in their straightforward symmetricity. The computational efficiency and effectiveness in performance are demonstrated by a set of experiments. A new cohort selection is proposed and a 99.6% verification rate is reported on a database of 200 French speakers. From a nonparametric viewpoint, the facts are revealed that the statistics of covariance carries more speaker information than the sample mean, short term dynamics of speech is also of important speaker discriminative characteristics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-85"
  },
  "carey95_eurospeech": {
   "authors": [
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Graham D.",
     "Tattersall"
    ],
    [
     "Eluned S.",
     "Parris"
    ]
   ],
   "title": "Adaptive transforms for speaker recognition",
   "original": "e95_0321",
   "page_count": 4,
   "order": 86,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "We present, in this paper, new algorithms designed to estimate linear and non-linear transformations of the feature vectors describing the short term spectrum of the speech signal. Two algorithms are investigated: the Class Difference Maximisation Algorithm (CDMA) and Class Target Algorithm (CTA) are designed to minimise the distance between a speaker's transformed feature vectors and maximise the distance between these feature vectors and feature vectors from other speakers. This is achieved by a modified form of iterative gradient algorithm designed to maintain orthogonality between the features in the transform space. We demonstrate that the algorithm is capable of improving the separation between classes taken from different speakers. We describe the application of the transformation to the features of a Hidden Markov Model based speaker recognition experiment using broad class models. Experimental results show that when the transform is used in the system the likelihood of utterances given the true speakers models is increased while that of utterances from other speakers decreases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-86"
  },
  "ng95b_eurospeech": {
   "authors": [
    [
     "Kai Tat",
     "Ng"
    ],
    [
     "Jian",
     "Su"
    ],
    [
     "Bingzheng",
     "Xu"
    ]
   ],
   "title": "Speaker recognition with discriminative speaker VQ models",
   "original": "e95_0325",
   "page_count": 4,
   "order": 87,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "In this paper, we propose a discriminative VQ model for speaker recognition. The VQ training algorithm is developed within the framework of gradient descent learning. Unlike the conventional VQ scheme, which considers only the minimum distance to a speaker codebook, the new algorithm takes account of the distances to all competing classes and all codewords in a speaker codebook, and aims directly at minimizing the recognition error. Two sets of speaker recognition experiments based on the conventional learning scheme and the DVQ are conducted respectively on a database of 200 French speakers. We obtained 1.75% performance improvement in speaker recognition and 0.45% in verification over the conventional VQ algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-87"
  },
  "federico95_eurospeech": {
   "authors": [
    [
     "A.",
     "Federico"
    ],
    [
     "Andrea",
     "Paoloni"
    ]
   ],
   "title": "Parametric speaker recognition over large population of telephonic voices",
   "original": "e95_0329",
   "page_count": 4,
   "order": 88,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "The voice parametric features extraction followed by some statistical identification test is the widespread approach to the speaker recognition. The problem reaches its maximal complexity when a voice sample must be attributed to a set of speakers being non-zero the a-priori probability that the sample does not belong to the given set. Usually these open set tests are related to the highest level of responsibility like in the forensic applications. This paper is addressed to present a balanced solution to the speaker recognition problem and to give the right statistical foundation to the decision task. All the related issues are restated, the modelization method is reconsidered for sparse experimental matrices and the algorithms for a suitable bayesan approach to the decision are derived following a more consistent theory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-88"
  },
  "altosaar95_eurospeech": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Einar",
     "Meister"
    ]
   ],
   "title": "Speaker recognition experiments in Estonian using multi-layer feed-forward neural nets",
   "original": "e95_0333",
   "page_count": 4,
   "order": 89,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "In this paper a general strategy towards robust and efficient speaker recognition is presented. Emphasis is placed on comparing the usefulness of different features calculated from the speech signal at different temporal and spectral resolutions. Specifically, three spectral features are evaluated in a neural network environment: linear frequency loudness scaled spectra, auditory spectra from an auditory model, and the lattice coefficients from a warped linear predictor. These features are tested with four different neural network topologies ranging from speaker identification to verification configurations. Variations in the neural net dimensions are also performed to gain an understanding of the complexity of the problem. The tests are based on 40 minutes of speech recorded from a set of 20 native Estonian speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-89"
  },
  "magrinchagnolleau95_eurospeech": {
   "authors": [
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Jean-Frangois",
     "Bonastre"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Effect of utterance duration and phonetic content on speaker identification using second-order statistical methods",
   "original": "e95_0337",
   "page_count": 4,
   "order": 90,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "Second-order statistical methods show very good results for automatic speaker identification in controlled recording conditions [2]. These approaches are generally used on the entire speech material available. In this paper, we study the influence of the content of the test speech material on the performances of such methods, i.e. under a more analytical approach [3]. The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal. Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker specific: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identification results than phonetically balanced test utterances, even though the training is done, in both cases, with 15 seconds of phonetically balanced speech. Nevertheless, results with other phoneme classes are never dramatically poor. These results tend to show that the speaker-dependent information captured by long-term second-order statistics is consistently common to all phonetic classes, and that the homogeneity of the test material may improve the quality of the estimates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-90"
  },
  "labulin95_eurospeech": {
   "authors": [
    [
     "Pavel V.",
     "Labulin"
    ],
    [
     "Sergey L.",
     "Koval"
    ],
    [
     "Andrej N.",
     "Raev"
    ]
   ],
   "title": "Automatic speaker recognition using formants-based nearest-neighbour distance measure",
   "original": "e95_0341",
   "page_count": 4,
   "order": 91,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "We describe the investigation to find more reliable way to recognize speakers in the field. As primary features we use instant formants frequencies (for frames, where formants exist) and average for utterance pitch. In comparing two utterances modified nearest neighbour distance is used. It is discovered that 1ms inter-frame shift gives some noticeable advantage in recognition score. For verification task (30 speakers*6times during 4months) this algorithm showed 2-6% of errors in noisy environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-91"
  },
  "homayounpour95_eurospeech": {
   "authors": [
    [
     "M. Mehdi",
     "Homayounpour"
    ],
    [
     "Gerard",
     "Chollet"
    ]
   ],
   "title": "Discrimination of voices of twins and siblings for speaker verification",
   "original": "e95_0345",
   "page_count": 4,
   "order": 92,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "Some experiments were conducted to study the possibility of discriminating voices of identical twins in text-independent speaker verification. Three approaches to speaker verification were examined: i) by human listeners, ii) by comparison of long-term spectra, and iii) by automatic methods [1]. Speaker verification experiments were carried out using LVQ3 and a Second Order Statistical Measure (SOSM). The results showed that listeners familiar with the voices of identical twins outperform our automatic speaker verification systems in discriminating between them. On the other hand, listeners unfamiliar with the twins perform worse than our automatic systems. This result may be explained by the fact that twins' relatives and their friends have received much more speech material for training than our automatic systems. A LVQ3-based system provides a more robust capacity than SOSM for discriminating two speakers wih similar voices such as twins.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-92"
  },
  "berkling95_eurospeech": {
   "authors": [
    [
     "Kay M.",
     "Berkling"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Theoretical error prediction for a language identification system using optimal phoneme clustering",
   "original": "e95_0351",
   "page_count": 4,
   "order": 93,
   "p1": "351",
   "pn": "354",
   "abstract": [
    "A neural network based language identification system is described, which uses language independent phoneme clusters as speech units to recognize the language spoken by native speakers over the telephone. We extend our previous work comparing phoneme-cluster and phoneme based approaches to language identification [l]. By creating a new speech unit valid across all languages in a theoretically motivated manner, we circumvent problems that are associated with fine phone-mic modelling such as high complexity [4], extensive training requirements [2], and the linguistically arbitrary reduction to subsets of phonemes [4]. A common set of speech units across languages allows us to automatically derive discriminating sequences of any length and theoretically estimate the language identification error. We demonstrate our implemented system for German vs. English on the OGI-TS database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-93"
  },
  "olsen95_eurospeech": {
   "authors": [
    [
     "Jesper O.",
     "Olsen"
    ]
   ],
   "title": "Separation of speakers in audio data",
   "original": "e95_0355",
   "page_count": 4,
   "order": 94,
   "p1": "355",
   "pn": "358",
   "abstract": [
    "Speaker separation is a technique with potentially many applications, for instance as an aid in browsing audio documents. This paper describes a novel speaker separation method, where speaker models are created without having any training data available in advance. The method was tested on realistic unconstrained telephone conversations, and ergodic Hidden Markov Models used for speaker modelling. The overall results were sequence and duration accuracies of respectively 87% and 94%, when no prior knowledge of the speakers was used (i.e. training data). Keywords: Speaker Separation, Speaker Recognition, Hidden Markov Models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-94"
  },
  "bonifas95_eurospeech": {
   "authors": [
    [
     "J.-L.",
     "Bonifas"
    ],
    [
     "I.",
     "Hernaez Rioja"
    ],
    [
     "B.",
     "Etxebarria Gonzalez"
    ],
    [
     "S.",
     "Saoudi"
    ]
   ],
   "title": "Text-dependent speaker verification using dynamic time warping and vector quantization of LSF",
   "original": "e95_0359",
   "page_count": 4,
   "order": 95,
   "p1": "359",
   "pn": "362",
   "abstract": [
    "This paper describes a text-dependent speaker verification system based on the use of DTW and VQ connected in series. It describes the algorithms and the extraction of the used LSF parameters. This way of using two classical algorithms makes it possible to reach high discrimination rates and particularly to eliminate the most dangerous category of impostors: those who could enter without knowing someone else's key. By this means, the system performances obtain a 95% recognition rate versus 2.3% false admission rate for a database recorded over telephone lines.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-95"
  },
  "li95b_eurospeech": {
   "authors": [
    [
     "Haizhou",
     "Li"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "On MMI learning of Gaussian mixture for speaker models",
   "original": "e95_0363",
   "page_count": 4,
   "order": 96,
   "p1": "363",
   "pn": "366",
   "abstract": [
    "In this paper, a general framework of maximum mutual information (MMI) learning of mixture densities is developed based on the discriminative learning strategy, within which a family of probabilistic classifiers can be trained. Two case studies are presented concerning class dependent Gaussian mixture model (GMM) and its extension to the case of tying kernel across classes. The related learning algorithms are derived. In the speaker recognition experiment, each speaker is represented by a GMM. The algorithms train the models aiming at minimum error rate. A normalized distance is also introduced to speaker verification. Five algorithms are evaluated for comparison and 100% of speaker verification rate is obtained on a database of 200 French speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-96"
  },
  "gong95_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Evaluation of Bayes decision approach to automatic determination of thresholds for speaker verification",
   "original": "e95_0367",
   "page_count": 4,
   "order": 97,
   "p1": "367",
   "pn": "370",
   "abstract": [
    "Under Bayes statistical decision framework, this paper addresses statistical modelling and determination of thresholds for speaker verification sj^stems. It is pointed out that speaker-dependent between-speaker score distribution is bi-modal, as opposed to common believe that the distribution is normal. Previous mono-modal modelling of between-speaker score distribution is then extended to bi-modal modelling. For a text-dependent application, experiments are reported which compare verification results with speaker-independent unique threshold, speaker-dependent mono-modal distributions and speaker-dependent bi-modal distributions. It is observed that speaker-dependent thresholds give dramatic error reduction, as compared to unique threshold and that bimodal and mono-modal distribution models give very close verification results. For a 200 speaker database, using 1 sec of test speech, the resulting System resulted in a 0.65% mean verification error.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-97"
  },
  "falavigna95_eurospeech": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ]
   ],
   "title": "Comparison of different HMM based methods for speaker verification",
   "original": "e95_0371",
   "page_count": 4,
   "order": 98,
   "p1": "371",
   "pn": "374",
   "abstract": [
    "Three different speaker verification methods are described. All of them are based on Hidden Markov Models (HMM); the first one is of type text independent the other two are of type text prompted. The text independent method makes use of a single state Continuous HMM, to represent each customer in the system, while the text prompted methods require to use speaker dependent phoneme models. To model phonemes both Continuous HMMs and Semi-Continuous HMMs are used. Two different normalization methods for the likelihood values provided by the various HMMs are considered: one is based on the posterior probability, the other is based on the application of a mapping function.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-98"
  },
  "sheikhzadegan95_eurospeech": {
   "authors": [
    [
     "J.",
     "Sheikhzadegan"
    ],
    [
     "M.",
     "Tebiani"
    ],
    [
     "M.",
     "Lotfizad"
    ],
    [
     "M. R.",
     "Roohani"
    ]
   ],
   "title": "Speaker classification by neural network for short utteranses using phoneme groups in Farsi",
   "original": "e95_0375",
   "page_count": 4,
   "order": 99,
   "p1": "375",
   "pn": "378",
   "abstract": [
    "In this work we propose a method for automatic text-independent speaker classification for short utterances in Farsi (persian). An efficient speaker classifier was designed and implemented by neural networks (MLPs) and just made use of only the effective phonemes divided into different groups , i.e. long vowels short vowels , nasals and some of the fricatives. This classifier was evaluated with the FARSDAT speech database, from 40 native speakers with 26 males and 14 females. The results showed that the accuracy of the speaker classification with just the subclassifier related to the long vowels was over 82%, with the subclassifiers related to all vowels was over 92%, and with taking account of all of the whole classifier and considerable amount of testing process data was about 100%. Keywords: speaker classification, neural network, effective phonemes, phoneme groups, Farsi.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-99"
  },
  "floch95_eurospeech": {
   "authors": [
    [
     "J.-L. Le",
     "Floch"
    ],
    [
     "C.",
     "Montacie"
    ],
    [
     "M.-J.",
     "Caraty"
    ]
   ],
   "title": "Speaker recognition experiments on the NTIMIT database",
   "original": "e95_0379",
   "page_count": 4,
   "order": 100,
   "p1": "379",
   "pn": "382",
   "abstract": [
    "In this paper, we compare three speaker recognition systems results (i.e. GMM, AHSM, ARVM) on the TIMIT and NTIMIT databases. In order to improve the results on the NTIMIT database, we present two more sophisticated systems: the first one is based on ARMA-Vector model, the second one is based on the utilisation of several AR-Vector models per speaker. We investigate the ability to recognize speaker using phonetic segments labels. We test the cooperation of several AR-Vector models, each one being learned on a distinct phonetic cluster. For this cooperation, we develop a segmental and an analytic approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-100"
  },
  "wagner95_eurospeech": {
   "authors": [
    [
     "Michael",
     "Wagner"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "J. Bruce",
     "Millar"
    ]
   ],
   "title": "Speaker identification using vector quantisation with codeword-specific derivative coding",
   "original": "e95_0383",
   "page_count": 4,
   "order": 101,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "This paper investigates improvements to the vector quantisation (VQ) distortion method of text-independent speaker identification, using a conventional codebook of instantaneous cepstral vectors from each speaker's training data, and one second-level codebook of transitional cepstral vectors for each codeword of the instantaneous codebook. Results on a 20-speaker database of 30 phonetically rich utterances show a reduction of the error rate from 6.5% for a conventional codebook of size 128 to 5.5% for a code-book which contains 16 transitional codewords for each of the 128 instantaneous codewords (128x16). Results on a 20-speaker database of spoken digits show a reduction of error rate from 3.1% for a conventional (128xO)-codebook to 0.9% for a (128x4)-codebook. Alternatively, a constant error rate can be maintained at a reduced number of codeword comparisons using codeword-specific transitional code-books. Results also show that, given a sufficient size of transitional codebook, transitional distortion scores after instantaneous preclassification can be superior to purely instantaneous distortion scores.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-101"
  },
  "li95c_eurospeech": {
   "authors": [
    [
     "Haizhou",
     "Li"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Jian",
     "Su"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Speaker recognition with temporal transition models",
   "original": "e95_0617",
   "page_count": 4,
   "order": 102,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "In this paper, a temporal transition model (TTM) of speech is proposed for speaker recognition and verification. The TTM is introduced to encode the short time dynamics of speech. The issues on the model building, the distance measures and the implementation are addressed. A set of experiments were conducted based on TTM, which gave a 98.9% recognition rate and 99.5% verification rate on a database of 72 French speakers. The fact is confirmed that temporal dynamics of utterance encodes well speaker specificity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-102"
  },
  "matsui95b_eurospeech": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Tomohito",
     "Kanno"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Speaker recognition using HMM composition in noisy environments",
   "original": "e95_0621",
   "page_count": 4,
   "order": 103,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition by the noise-and-voice (NOVO) transform. The HMM composition combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR exactly for non-stationary noise, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-103"
  },
  "che95_eurospeech": {
   "authors": [
    [
     "ChiWei",
     "Che"
    ],
    [
     "Qiguang",
     "Lin"
    ]
   ],
   "title": "Speaker recognition using HMM with experiments on the yoho database",
   "original": "e95_0625",
   "page_count": 4,
   "order": 104,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "In this paper, a Hidden Markov Model (HMM) based speaker recognition system is presented. The system utilizes concatenated phoneme HMMs and works in a text-prompted mode. Each registered speaker has a separate set of HMMs which are trained using the Baum-Welch algorithm. The speaker recognition system has been evaluated with the YOHO voice verification corpus in terms of both speaker verification and closed-set speaker identification. It is shown that by using 10 seconds of testing speech, an error rate of 0.09% for male and 0.31% for female are obtained for speaker identification with a total population of 138 talkers. For speaker verification, under the 0% false rejection condition, the system achieves a false acceptance rate of 0.09% for male and 0% for female. This paper also explores effects of various factors (such as the mixture number and cohort selection) on the performance of speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-104"
  },
  "yu95b_eurospeech": {
   "authors": [
    [
     "Kin",
     "Yu"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "John",
     "Oglesby"
    ]
   ],
   "title": "Speaker recognition models",
   "original": "e95_0629",
   "page_count": 4,
   "order": 105,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "This paper evaluates continuous density hidden Markov models (CDHMM), dynamic time warping (DTW) and distortion-based vector quantisation (VQ) for speaker recognition, across incremental amounts of training data. In comparing VQ and CDHMMs for text-independent (TI) speaker recognition, it is shown that VQ performs better than an equivalent CDHMM with one training version, but is outperformed by the CDHMM when trained with ten training versions. In text-dependent (TD) experiments, a comparison of DTW, VQ and CDHMMs shows that DTW outperforms VQ and CDHMMs for sparse amounts of training data, but with more data, the performance of each model is indistinguishable. Further analysis shows TD to be superior to TI architecture for speaker recognition, and TD digit performance illustrates zero, 1 and 9 to be good discriminators.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-105"
  },
  "artieres95_eurospeech": {
   "authors": [
    [
     "T.",
     "Artieres"
    ],
    [
     "Patrick",
     "Gallinari"
    ]
   ],
   "title": "Multi-state predictive neural networks for text-independent speaker recognition",
   "original": "e95_0633",
   "page_count": 4,
   "order": 106,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "Both Hidden Markov Models and Neural Networks have already been used as production systems for speaker identification or verification. Recently [9] has shown that ergodic multi-state hidden Markov Models do not outperform one-state \"hidden\" Markov Models, i.e. Gaussian Mixture Models, for speaker recognition. She put in evidence that the important characteristic of these models is the total number of mixtures and not the number of states. These HMMs are thus unable to make use of temporal information for performing speaker recognition. On the other hand, recent experiments have shown that, for neural predictive systems, modelization of non stationarity allowed to significantly improve the performances [6]. We are interested here in the development of such models which will be refereed to as multi-state predictive neural networks (MSPNNs). We study the ability of these systems for speaker identification and discuss the superiority of multi-state upon one-state models. We provide results on 15 talkers from the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-106"
  },
  "beritelli95_eurospeech": {
   "authors": [
    [
     "Francesco",
     "Beritelli"
    ],
    [
     "Salvatore",
     "Casale"
    ],
    [
     "Marco",
     "Russo"
    ]
   ],
   "title": "A voiced/unvoiced speech discrimination technique based on fuzzy logic",
   "original": "e95_0389",
   "page_count": 4,
   "order": 107,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "A new criterion for speech frame classification based on Fuzzy Logic is presented. An algorithm which correctly discriminates between sounds allows a better trade-off between quality and bit rate in the recent generation of vocoders. Extracting from speech waveforms a set of significant parameters for voiced/unvoiced classification, we trained a Fuzzy Voicing Detector (FVD) system, obtaining a fuzzy knowledge base of only 5 rules. The new voicing system has certain advantages over the main Voicing Determination Algorithms (VDAs). With a notoriously low level of complexity, on account of its fuzzy nature, the FVD is extremely robust and maintains good performance even in the presence of background noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-107"
  },
  "darsinos95_eurospeech": {
   "authors": [
    [
     "V.",
     "Darsinos"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Evaluation of a periodic/aperiodic speech decomposition algorithm",
   "original": "e95_0393",
   "page_count": 4,
   "order": 108,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "In this paper an evaluation method for periodic/aperiodic speech decomposition algorithms is proposed and the results of its application on a recently presented algorithm are discussed. The performance and the characteristics of the algorithm are examined both in the time domain, through HNR measurements and in the frequency domain, through spectral distance measurements criteria. As stimuli, synthetic steady-vowels distorted by natural-like sources of pseudoperiodic and noise excitation, with controlled varying time-frequency characteristics, were used. The analysis was used to test the influence of the fundamental frequency, perturbation type and perturbation level at the efficiency of the decomposition algorithm. The results demonstrated that the new algorithm is a powerful tool for analysing the aperiodic component of speech signals, under some conditions that have been explicited.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-108"
  },
  "rouat95_eurospeech": {
   "authors": [
    [
     "Jean",
     "Rouat"
    ],
    [
     "Yong Chun",
     "Liu"
    ],
    [
     "Daniel",
     "Morissette"
    ]
   ],
   "title": "A pitch determination and voiced/unvoiced decision algorithm for noisy speech",
   "original": "e95_0397",
   "page_count": 4,
   "order": 109,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "We propose a multi-channel pitch determination algorithm (PDA) that has been tested on three speech databases (OdB SNR telephone speech, speech recorded in a car and clean speech) involving fifty-eight speakers. The system has been compared to AMPEX [9], to hand-labelled and laryngograph pitch contours. Our PDA comprises an automatic channel selection module and a pitch extraction module that relies on a pseudo-periodic histogram (combination of normalised scalar products for the less corrupted channels) in order to find pitch. It outperformed the reference system on OdB telephone and car speech. The automatic selection of channels was effective on the very noisy telephone speech (OdB) but not on the car speech where the robustness of the system is mainly due to the pitch extraction module in comparison to AMPEX. The paper reports in details the V/UV, UV/V performance and pitch estimation errors for the PDA and the reference system on the three databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-109"
  },
  "janer95_eurospeech": {
   "authors": [
    [
     "Leonard",
     "Janer"
    ]
   ],
   "title": "Modulated Gaussian wavelet transform based speech analyser (MGWTSA) pitch detection algorithm (PDA)",
   "original": "e95_0401",
   "page_count": 4,
   "order": 110,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "In this paper, a new Pitch Detection Algorithm (PDA) based on a Wavelet Transform (WT) Analyser of speech signals is proposed. It provides a value for the fundamental frequency at a pitch period rate. The method is described and evaluated in this paper. The algorithm uses both time and frequency information from the front-end analyser to detect relevant events for the estimation of the fundamental frequency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-110"
  },
  "navarromesa95_eurospeech": {
   "authors": [
    [
     "Juan L.",
     "Navarro-Mesa"
    ],
    [
     "Ignasi",
     "Esquerra-Llucia"
    ]
   ],
   "title": "A time-frequency approach to epoch detection",
   "original": "e95_0405",
   "page_count": 4,
   "order": 111,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "The aim of this paper is to find a reliable function using Time-Frequency Representations (TFR) for epoch detection, making no assumptions about the speech production model. This is achieved by adapting the optimum time-frequency detector to the problem of glottal closure instant (GCI) determination. An ideal function for such purpose may show only one maximum per period. However, since this situation is not assured because secondary maxima may appear, we also propose a post-processing method based on morphological filtering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-111"
  },
  "larreategui95_eurospeech": {
   "authors": [
    [
     "M. L.",
     "Larreategui"
    ],
    [
     "F. J.",
     "Ancin"
    ],
    [
     "R. A.",
     "Carrasco"
    ]
   ],
   "title": "An improved epoch detection algorithm based on sinusoidal modelling of speech",
   "original": "e95_0409",
   "page_count": 4,
   "order": 112,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "Reliable and automatic estimation of the Glottal Closure Instant (GCI), also referred to as epoch or pitch-event, has increasingly become an essential requirement in many speech processing applications, such as high-quality speech synthesis [1][2] and speech coding [3]. Although several methods exist [4][5][6], GCI detection still remains a difficult task. In this paper a robust pitch-event detection algorithm based on the sinusoidal modelling of speech is proposed. The improvement in performance over other well-known epoch detectors demonstrates the powerfulness of the sinusoidal technique for GCI determination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-112"
  },
  "darsinos95b_eurospeech": {
   "authors": [
    [
     "V.",
     "Darsinos"
    ],
    [
     "D.",
     "Galanis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "A method for fully automatic analysis and modelling of voice source characteristics",
   "original": "e95_0413",
   "page_count": 4,
   "order": 113,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "In this paper, a fully automatic pitch synchronous method to derive the voice source characteristics starting from the speech signal itself, and fitting them to a parametrized model, is presented. It does not require additional signals for the closed-phase alignment, like the ECG, and uses a fitting procedure that guarantees a good convergence. Further- more the method seems to work reliably even for the closed phase where the analysis interval becomes short. For the automatic finding of the glottal closure instants the method uses a modified version of a previous epoch-detection method. This modification consists of the application of linear detectors to improve performance and reliability mainly in the high-frequency energy speech signals. The method does not require user interaction during the analysis execution. In addition, the required computational load permits its integration in a personal computer environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-113"
  },
  "pfitzinger95_eurospeech": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "Dynamic vowel quality: a new determination formalism based on perceptual experiments",
   "original": "e95_0417",
   "page_count": 4,
   "order": 114,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "The acoustic properties of phonologically equal vowels show a large range of variability, so that automatic classification methods have achieved limited success in identifying vowels. The only secure basis for obtaining absolute vowel quality is to rely on judgements of a jury of phoneticians being well experienced in Daniel Jones's familiar Cardinal Vowel diagram. Because of the exclusiveness of this procedure a desire to replace the expert jury by automatic methods existed for a long time (Fant, 1978 [4]). In this paper an algorithm (WVT) is proposed for calculating absolute vowel quality from the speech signal. Since the results of this algorithm are equivalent to those of the jury and meaningful in phonetic terms, it can replace the jury. The algorithm can be applied to complex utterances and can display which vowel targets are really reached in the Cardinal Vowel diagram and how the trajectories run. So this tool can be used for research on diphthongs, undershoot/overshoot, coarticulation, assimilation and reduction, as some preliminary investigations show.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-114"
  },
  "ohno95_eurospeech": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A method for quantitative analysis of the local speech rate",
   "original": "e95_0421",
   "page_count": 4,
   "order": 115,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "Conventional methods for studying the temporal organization of speech generally suffer from inaccuracies in finding segmental boundaries. The present paper proposes a new method that allows one to measure the temporal variations in speech rate of a given target utterance relative to another utterance chosen as reference. By the DP matching procedure commonly used in speech recognition, a warping function is found for mapping the time axis of the target onto that of the reference, and the relative local speech rate is defined as the reciprocal of the slope of this time-axis warping function. Analysis of both Japanese and English utterances has revealed the structure of the local speech rate variations, and has proved the potential utility of the method in speech synthesis of various styles and speech rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-115"
  },
  "lee95_eurospeech": {
   "authors": [
    [
     "Ki Seung",
     "Lee"
    ],
    [
     "Dae Hee",
     "Youn"
    ],
    [
     "Il Whan",
     "Cha"
    ]
   ],
   "title": "Voice personality transformation using an orthogonal vector space conversion",
   "original": "e95_0427",
   "page_count": 4,
   "order": 116,
   "p1": "427",
   "pn": "430",
   "abstract": [
    "A newly developed voice personality transformation algorithm is introduced in this paper. Voice personality transformation is the process of changing one person's acoustic features (source) to those of another person (target). In this paper, personality transformation is achieved by changing the LPC cepstrum coefficients, excitation spectrum and global/local pitch contour. An orthogonal vector space conversion technique is proposed to transform LPC the cepstrum coefficients. This technique consists of principle component decomposition by applying the Karhunen-Loeve(KL) transformation and minimum mean-square error coordinate transformation(MSECT). To transform prosodic characteristics, we propose a simple pitch contour modification method. The experimental results show the effectiveness of the proposed algorithm in both subjective and objective evaluations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-116"
  },
  "hashimoto95_eurospeech": {
   "authors": [
    [
     "Makoto",
     "Hashimoto"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Spectral mapping for voice conversion using speaker selection and vector field smoothing",
   "original": "e95_0431",
   "page_count": 4,
   "order": 117,
   "p1": "431",
   "pn": "435",
   "abstract": [
    "This paper proposes a spectral mapping method for voice conversion using speaker selection and vector field smoothing. With this method, the spectral distance between transformed speech and the speech of a target speaker was reduced by 25% in mean value for eight target speakers (four males and four females) in comparison with the distance between the speech of a speaker who was selected from among multiple reference speakers and that of a target speaker using only one word as training data. Transformed speech samples of one male and one female were judged as closer to their target speakers than their selected speakers by 67% and 65%, respectively, in a hearing test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-117"
  },
  "higuchi95_eurospeech": {
   "authors": [
    [
     "Norio",
     "Higuchi"
    ],
    [
     "Makoto",
     "Hashimoto"
    ]
   ],
   "title": "Analysis of acoustic features affecting speaker identification",
   "original": "e95_0435",
   "page_count": 4,
   "order": 118,
   "p1": "435",
   "pn": "438",
   "abstract": [
    "In order to investigate the influence of individual acoustic features in the human process of speaker identification, the authors propose a new model to predict the perceptual contribution rate of each acoustic feature. The perceptual contribution rate is measured by a hearing test using speech resynthesized by swapping the acoustic features of natural speech, while its predicted value is calculated using a model based on differences in the acoustic features, that is, a cepstral distance representing spectral information and a difference of the mean logarithmic voice fundamental frequency representing information of the voice fundamental frequency. It is demonstrated that contribution rates can be predicted with an error of 7.71% in RMS after optimization of the acoustic features' weighting factors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-118"
  },
  "akagi95_eurospeech": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ],
    [
     "Taw",
     "Ienaga"
    ]
   ],
   "title": "Speaker individualities in fundamental frequency contours and its control",
   "original": "e95_0439",
   "page_count": 4,
   "order": 119,
   "p1": "439",
   "pn": "442",
   "abstract": [
    "Speaker individualities in F0 contours are investigated through analyses of several speakers uttered speech and psychoacoustic experiments. The stimuli for the experiments are re-synthesized with manipulated F0 contours and spectral envelopes averaged overall speakers by using the Log Magnitude Approximation analysis-synthesis system. The analysis and experimental results indicate that (1) there are speaker individualities in the F0 contours, (2) some specific parameters related to the dynamics of F0 contours have many speaker individuality features and the speaker individualities can be controlled by manipulating these parameters, and (3) although there are speaker individuality features in the time-averaged F0 contours, they help improve speaker identification less than the dynamics of the F0 contours.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-119"
  },
  "lam95_eurospeech": {
   "authors": [
    [
     "King-fai",
     "Lam"
    ],
    [
     "Cheung-fat",
     "Chan"
    ]
   ],
   "title": "Interpolating MBE v/UV mixture function for high quality synthesis of speech",
   "original": "e95_0443",
   "page_count": 4,
   "order": 120,
   "p1": "443",
   "pn": "447",
   "abstract": [
    "A high quality speech synthesis method based on interpolating the voiced/unvoiced (V/UV) mixture functions [1] of the multiband excitation model (MBE) [2,3] is proposed. In MBE model, each harmonic band of fundamental frequency in an excitation spectrum is rigidly declared as either voiced or unvoiced while it should be a mixture of two. In the proposed method, each harmonic band in a short time spectrum is synthesized by mixing both voiced and unvoiced energies. The ratio of the V/UV energy in a spectrum is determined by the V/UV mixture function which is subsequently parametized by an all-zero model. Smooth transition of excitation between phonetic units can be achieved by linearly interpolating the V/UV mixture functions of adjacent frames.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-120"
  },
  "stylianou95_eurospeech": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Olivier",
     "Cappe"
    ],
    [
     "Eric",
     "Moulines"
    ]
   ],
   "title": "Statistical methods for voice quality transformation",
   "original": "e95_0447",
   "page_count": 4,
   "order": 121,
   "p1": "447",
   "pn": "450",
   "abstract": [
    "This paper presents a new method for the statistical learning of the correspondence between spectral parameters measured from two different speakers uttering the same text. This method is based on the use of a gaussian mixture model of the speaker's spectral parameters. It is shown to be more efficient and robust than previously known techniques based on the use of vector quantization. The results obtained on large speech database demonstrate effective high-quality transformations of the voice characteristics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-121"
  },
  "stylianou95b_eurospeech": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Jean",
     "Laroche"
    ],
    [
     "Eric",
     "Moulines"
    ]
   ],
   "title": "High-quality speech modification based on a harmonic + noise model",
   "original": "e95_0451",
   "page_count": 4,
   "order": 122,
   "p1": "451",
   "pn": "454",
   "abstract": [
    "In this contribution a novel method enabling high-quality speech modifications is presented; it is based on a harmonic + noise, (HNM), representation of the speech signal. The harmonic part is modelled by harmonically related sinewaves with slowly varying amplitudes, and frequencies. The noise part is modelled by an AR model and it is modulated by a time-domain amplitude envelope. Both the analysis and the synthesis are pitch-synchronous. Thanks to this twofold representation (Harmonic + Noise part), and to the pitch-synchronous scheme, a flexible technique for time-scale and pitch-scale modifications, inspired by PSOLA methods, can be applied, yielding high-quality modification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-122"
  },
  "boughazah95_eurospeech": {
   "authors": [
    [
     "Sahar E.",
     "Bou-Ghazah"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Source generator based stressed speech perturbation",
   "original": "e95_0455",
   "page_count": 4,
   "order": 123,
   "p1": "455",
   "pn": "458",
   "abstract": [
    "The objective of this study is to generate stressed synthetic speech from neutral speech using a source, generator framework previously employed for stressed speech recognition. This is achieved by formulating speech parameter models for the various stressed speaking conditions and perturbing the parameters of neutral speech. The stress modeling scheme is applied to an existing low-bit rate CELP speech coder in order to investigate (i) the coder's ability and limitations in reproducing stressed synthetic speech, and (ii) our ability to perturb coded neutral speech parameters at the synthesis stage of CELP so that the resulting speech is perceived as being under stress. Four different stress perturbation algorithms are proposed and evaluated. Results from formal listener evaluations of the stress perturbed neutral speech show successful classification rates of 87% for angry speech, 75%. for Lombard effect speech, and 92% for loud speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-123"
  },
  "yoma95_eurospeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Improved algorithms for speech recognition in noise using lateral inhibition and SNR weighting",
   "original": "e95_0461",
   "page_count": 4,
   "order": 124,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "This paper addresses the problem of speech recognition with signals corrupted by white Gaussian additive noise at moderate SNR. The energy of the noise is not required. A technique based on a lateral inhibition process approximation with multilayer neural nets, and SNR weighting in acoustic pattern matching algorithms is proposed. At the recognition procedure, the local SNR is estimated by means of the autocorrelation function and is taken into account as a weight in a pattern matching algorithm. A general criterion based on weighting the frame influence in decisions according to local SNR is suggested, and modified versions of both HMM arid DTW algorithms have been designed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-124"
  },
  "siohan95_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Noise adaptation using linear regression for continuous noisy speech recognition",
   "original": "e95_0465",
   "page_count": 4,
   "order": 125,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "We present an approach for recognising continuous speech in the presence of an additive noise, based on model adaptation. The method consists in transforming the parameters of acoustic models to reduce the acoustic mismatch between a test utterance and a set of clean speech models. We assume that speech is modelled by a set of Stochastic Trajectory Models (STM). The mean vectors of STMs are adapted using a set of linear transformations. The transformations are derived from a small labelled adaptation corpus, so that the likelihood of the adaptation corpus given the adapted models is maximised. Experiments performed on different additive noises and for various signal-to-noise ratio (SNR) show that the adaptation scheme significantly increases the accuracy. For SNR from 12dB to 36dB, we observed that the performance of the linear regression is similar or better than the performance obtained when training and testing the recogniser in noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-125"
  },
  "yang95_eurospeech": {
   "authors": [
    [
     "Ruikang",
     "Yang"
    ],
    [
     "Markku",
     "Majaniemi"
    ],
    [
     "Petri",
     "Haavisto"
    ]
   ],
   "title": "Dynamic parameter compensation for speech recognition in noise",
   "original": "e95_0469",
   "page_count": 4,
   "order": 126,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "In this paper we present an algorithm to compensate the dynamic cepstral coefficients. This extends the parallel model combination scheme to more general cases where dynamic cepstral coefficients are calculated by linear regression of any length. The algorithm has been applied to a speech database which was recorded in a car and improvement was observed. The performance is improved further by using prefiltering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-126"
  },
  "drygajlo95b_eurospeech": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ],
    [
     "Nathalie",
     "Virag"
    ],
    [
     "Gregoire",
     "Cosendai"
    ]
   ],
   "title": "Robust speech recognition in noise using speech enhancement based on masking properties of the auditory system and adaptive HMM",
   "original": "e95_0473",
   "page_count": 4,
   "order": 127,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "This paper describes a new method of adapting a HMM speaker independent recogniser trained on telephone quality speech data to make it robust to background speech-like noise. The method is based on the parallel combination of a speech enhancement scheme based on masking properties of the auditory system and the HMM adaptation technique recently described by Nolazco Flores and Young [1]. The proposed enhancement algorithm uses a variation of the generalized spectral subtraction method and incorporates in it a criterion based on the human perception. Previous work [2] has demonstrated that this algorithm succeeds in finding the best trade-off between noise reduction and speech distortion in a perceptual sense. In this paper we show that this method improves also noise compensation techniques in which the parameters of corresponding pairs of speech and noise states are combined to yield a set of compensated parameters. The originality of the proposed algorithm resides in the fact that instead of using fixed parameters for the noise compensation, the noise masking threshold is used to control the enhancement and recognition processes adaptively, frame-by-frame, hence helping to find the best trade-off. The paper describes an evaluation of the presented scheme using the \"Polyphone Swiss Romand\" and Noisex-92 databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-127"
  },
  "yu95c_eurospeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Canonical correlation based compensation approach for robust speech recognition in noisy environment",
   "original": "e95_0477",
   "page_count": 4,
   "order": 128,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "In this paper we propose a novel compensation approach named Canonical Correlation Based Compensation(CCBC) to improve the performance of recognizers under noisy environment. In practical speech recognition applications, the mismatching between training and testing environments often seriously diminish recognition accuracy. Because the testing environments are not always known beforehand, The adaptive compensation framework is a practical approach to cope with this problem. While other compensation methods often deal with only some of the cepstrum changes, and is effective only for specific condition, the new approach proposed here is noise independent and can compensate all of the three main differences between two environments, i.e. mean value shift, norm shrink and the bad correlation of each dimension between training and testing speech. The experimental results show that our method has very good compensation effect.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-128"
  },
  "moreno95_eurospeech": {
   "authors": [
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "A unified approach for robust speech recognition",
   "original": "e95_0481",
   "page_count": 4,
   "order": 129,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "There are two major structural approaches to robust speech recognition. In the first approach to the problem, compensation is performed by modifying the incoming cepstral stream using ML or MMSE methods to estimate parameters characterizing environmental degradation, from direct frame-by-frame comparisons between speech recorded in high-quality and degraded acoustical environments, or by signal processing techniques such as spectral subtraction. The second approach tackles the problem by modifying the statistics of the internal representation of speech cepstra in the classifier to make them more closely resemble the statistics of degraded speech. This paper attempts to unify these approaches to robust speech recognition by presenting three techniques that share the same basic assumptions and internal structure but differ in whether they modify the incoming speech cepstra or whether they modify the classifier statistics. We present SNR-dependent multi-vaRiate gAussian-based cepsTral normalization (SNR-RATZ) and SNR-based Blind RATZ (SNR-BRATZ), which modify incoming cepstra, along with STAR (STAtistical Re-estimation), which modifies the internal statistics of the classifier. The algorithms were tested using the SPHINX-II speech recognition system on the CENSUS database, a database of strings of letters and numbers to which unknown added and unknown linear filtering was introduced artificially. While all the algorithms showed good performance, STAR was observed to provide lower error rates as SNR decreases than any of the algorithms that modify incoming cepstra.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-129"
  },
  "singer95_eurospeech": {
   "authors": [
    [
     "Harald",
     "Singer"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Tomohiko",
     "Beppu"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Effect of rasta-type processing for speech recognition with speaking-rate mismatches",
   "original": "e95_0487",
   "page_count": 4,
   "order": 130,
   "p1": "487",
   "pn": "490",
   "abstract": [
    "In the present paper, we investigate the use of \"Rasta-type cepstral processing techniques\", for speech recognition under mismatched speaking rate conditions. The acoustic models are trained on an isolated-word speech data base and and then tested on a continuous speech data base. The speaking rates in the two data bases are significantly different. Using high resolution phoneme-context dependent models, the high-pass cepstrum is shown to perform comparable for matched conditions and outperforms the other techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-130"
  },
  "mirghafori95_eurospeech": {
   "authors": [
    [
     "Nikki",
     "Mirghafori"
    ],
    [
     "Eric",
     "Foster"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Fast speakers in large vocabulary continuous speech recognition: analysis & antidotes",
   "original": "e95_0491",
   "page_count": 4,
   "order": 131,
   "p1": "491",
   "pn": "494",
   "abstract": [
    "The performance of automatic speech recognizers (ASR) typically degrades for test speakers with \"outlier\" characteristics, for example, speakers with foreign accent and fast speaking rate. In this work, we concentrate on the latter. Consistent with other researchers, we have observed that for speakers with exceptionally high speaking rate, the word recognition error is significantly higher. We have investigated two possible causes for this effect. Inherent spectral differences may cause the extracted features for these outliers to be significantly different from that of normal speech. Also, due to phone omissions and duration reduction, the normal word-models may not be suitable for fast speech. Based on our exploratory experiments on TIMIT and WSJ corpora, we believe the spectral differences and duration reduction are both significant sources of the increased error. By adapting our MLP phonetic probability estimator to fast speech, and employing fast speaker word-models, we have been able to eliminate about 16% of the fast speaker word recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-131"
  },
  "chou95_eurospeech": {
   "authors": [
    [
     "Wu",
     "Chou"
    ],
    [
     "Mazin G.",
     "Rahim"
    ],
    [
     "Eric",
     "Buhrke"
    ]
   ],
   "title": "Signal conditioned minimum error rate training",
   "original": "e95_0495",
   "page_count": 4,
   "order": 132,
   "p1": "495",
   "pn": "498",
   "abstract": [
    "In this paper, a new approach, signal conditioned minimum error training, is proposed, where signal conditioning and minimum string error rate training are integrated into one process. The signal conditioning in this approach is based on hierarchical signal bias removal (HSBR), a novel extension of the signal bias removal algorithm. The HSBR is applied in conjunction with minimum string error rate training. In contrast to using a fixed codebook, the HSBR codebook used in our approach is derived from HMM parameters and updated with the HMMs during the process of minimum error rate training. As such, both HSBR signal conditioning and string model based minimum error rate training are based on the same set of HMMs. Experiments are performed on a connected digit database collected from the telephone network. The database covers various analog and digital network channels, different regional areas and a variety of telephone handsets. It is found that the proposed approach of signal conditioned minimum error rate training can lead to a significant reduction in recognition error rate. Based on a sub-word model consisting of various inter-word and intra-word context dependent (head-body-tail) model units, a 47% word error rate reduction is obtained through the proposed approach comparing with the model obtained from the conventional maximum likelihood (ML) training. This corresponds to an additional 27% word error rate reduction comparing with the inter-word context dependent sub-word model obtained from minimum error rate training where signal conditioning is not incorporated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-132"
  },
  "matsumura95_eurospeech": {
   "authors": [
    [
     "Takeshi",
     "Matsumura"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Non-uniform unit HMMS for speech recognition",
   "original": "e95_0499",
   "page_count": 4,
   "order": 133,
   "p1": "499",
   "pn": "502",
   "abstract": [
    "A novel acoustic modeling algorithm that generates non-uniform unit HMMs to effectively cope with spectral variations in fluent speech is proposed. The algorithm is devised for the automatic iterative generation of long-span units for non-uniform modeling. This generation algorithm is based on an entropy reduction criterion using text data and a maximum likelihood criterion using speech data. The effectiveness of the non-uniform unit model is confirmed by a phrase recognition test using an LR parser. Recognition results show that non-uniform unit HMMs achieve higher performance than conventional phoneme-unit HMMs and suggest the potential capacity of non-uniform unit HMMs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-133"
  },
  "sankar95_eurospeech": {
   "authors": [
    [
     "Ananth",
     "Sankar"
    ],
    [
     "Frangoise",
     "Beaufays"
    ],
    [
     "Vassilios",
     "Digalakis"
    ]
   ],
   "title": "Training data clustering for improved speech recognition",
   "original": "e95_0503",
   "page_count": 4,
   "order": 134,
   "p1": "503",
   "pn": "506",
   "abstract": [
    "We present an approach to cluster the training data for automatic speech recognition (ASR). A relative-entropy based distance metric between training data clusters is defined. This metric is used to hierarchically cluster the training data. The metric can also be used to select the closest training data clusters given a small amount of data from the test speaker. The selected clusters are then used to estimate a set of hidden Markov models (HMMs) for recognizing the speech from the test speaker. We present preliminary experimental results of the clustering algorithm and its application to ASR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-134"
  },
  "huo95b_eurospeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Chorkin",
     "Chan"
    ]
   ],
   "title": "On the use of bi-directional contextual dependence in acoustic modeling for speech recognition",
   "original": "e95_0507",
   "page_count": 4,
   "order": 135,
   "p1": "507",
   "pn": "510",
   "abstract": [
    "With the motivation of utilizing bi-directional contextual dependence in acoustic modeling, in this paper, a bidirectional hidden Markov modeling approach for speech recognition is studied and the importance of bi-directional contextual dependence for speech recognition is identified by a series of comparative experiments. Furthermore, hidden Markov random field based acoustic modeling techniques using our previously proposed contextual vector quantization method and iterated conditional modes algorithm which is very suitable for the parallel processing implementation are also attempted. Their viability is confirmed by a series of preliminary experiments in a speaker independent isolated English letter recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-135"
  },
  "beppu95_eurospeech": {
   "authors": [
    [
     "Tomohiko",
     "Beppu"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Spontaneous speech recognition using dynamic CEPSTRA incorporating forward and backward masking effect",
   "original": "e95_0511",
   "page_count": 4,
   "order": 136,
   "p1": "511",
   "pn": "514",
   "abstract": [
    "The spectral parameters for spontaneous speech recognition need to more clearly emphasize the relevant spectral dynamics of speech sounds. This paper discusses the dynamic cepstra, which is a spectral representation simulating the time-frequency characteristics of auditory forward masking. In this paper, we propose a new dynamic cepstrum that incorporates both forward and backward masking to improve robustness against noise and mismatch utterance style. The dynamic characteristics of signals at a time point is efficiently extracted using the information from both sides of a time axis. The new dynamic cepstrum expresses the dynamic features of speech more efficiently than the dynamic cepstrum incorporating only one direction of masking with the same masking duration. The new dynamic cepstrum improved the phoneme recognition error using a phonetic typewriter over 10% for both of two utterance styles; 1) the same as and 2) different from the utterance style of the training data base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-136"
  },
  "afify95_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Stochastic trajectory models for speech recognition: an extension to modelling time correlation",
   "original": "e95_0515",
   "page_count": 4,
   "order": 137,
   "p1": "515",
   "pn": "518",
   "abstract": [
    "Based on experimental evidence from other research work, we assume that speech signals in consecutive analysis frames are correlated and we explore the correlation for speech recognition in the framework of stochastic trajectory models (STM). In this paper, we extend our previously proposed stochastic mixture trajectory models to model such a time correlation. Specifically, we consider the observation sequence of a speech signal as being generated by a mixture of random trajectory generators, and explicitly model the time evolution of the mean vector of each trajectory as the sum of a first order non-observable AR process and a state dependent random process. We describe the main results of the formulation, parameter estimation and sentence recognition. Evaluated on a speaker-dependent continuous speech recognition task, the proposed approach reduced average word error rate by about 25%, compared to a baseline STM system in which frames are assumed to be independent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-137"
  },
  "milner95_eurospeech": {
   "authors": [
    [
     "Ben P.",
     "Milner"
    ],
    [
     "Saeed V.",
     "Vaseghi"
    ]
   ],
   "title": "An analysis of cepstral-time matrices for noise and channel robust speech recognition",
   "original": "e95_0519",
   "page_count": 4,
   "order": 138,
   "p1": "519",
   "pn": "522",
   "abstract": [
    "This paper presents an analysis of the cepstral-time matrix. The coefficients of the cepstral-time matrix are found to be similar to the standard cepstral vector with differential features augmented on. It is also shown that the cepstral-time matrix is inherently robust to convolutional channel distortion. Spectral-subtraction, Wiener filtering and model combination are extended into two-dimensions where improved noise robustness is achieved. Experimental results using the NOISEX database with noise and channel distorted speech are presented.,\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-138"
  },
  "rivlin95_eurospeech": {
   "authors": [
    [
     "Ze'ev",
     "Rivlin"
    ]
   ],
   "title": "A confidence measure for acoustic likelihood scores",
   "original": "e95_0523",
   "page_count": 4,
   "order": 139,
   "p1": "523",
   "pn": "526",
   "abstract": [
    "Performance of acoustic phoneme models varies in a phoneme-based recognition system. Some phonemes may be modeled very well, providing generally higher acoustic likelihood scores; phonemes that are not modeled well will typically produce poorer scores. To calibrate the diverse performance of the phoneme models, an acoustic confidence score (ACS) has been derived which uses probability distributions of acoustic likelihood scores associated with each of the phoneme models in a recognition system to determine an objective measure of acoustic match. Preliminary results correlate the ACS of a hypothesized sentence and its correctness. One potential application of ACS is in the task of rejection of a recognized sentence due to its poor score. A sentence containing many of the phonemes that were not modeled well in the system will unfairly get a poor overall score when using the raw acoustic scores, and may be falsely rejected. ACS provides an objective measure of acoustic match, independent of the recognized phonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-139"
  },
  "biem95_eurospeech": {
   "authors": [
    [
     "Alain",
     "Biem"
    ],
    [
     "Erik",
     "McDermott"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "A discriminative filter bank model for speech recognition",
   "original": "e95_0545",
   "page_count": 4,
   "order": 140,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "This paper investigates the realization of a filter bank model that achieves minimum classification error. A bank-of-filter feature extractor module is jointly optimized with the classifier's parameters so as to minimize the errors occurring at the back-end classifier, in the framework of Minimum Classification Error /Generalized Probabilistic Descent Method (MCE/GPD). The method was first applied to readjusting various parameters of filter banks linearly spaced on the Mel-scale for the Japanese vowel recognition task. Analysis of the feature extraction process shows how those parts of the spectrum that are relevant to discrimination are captured. Then the method was applied to a multi-speaker word recognition system, which resulted in an word error rate reduction of more than 20 %.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-140"
  },
  "stahl95_eurospeech": {
   "authors": [
    [
     "Holger",
     "Stahl"
    ],
    [
     "Johannes",
     "Müller"
    ]
   ],
   "title": "A stochastic grammar for isolated representation of syntactic and semantic knowledge",
   "original": "e95_0551",
   "page_count": 4,
   "order": 141,
   "p1": "551",
   "pn": "554",
   "abstract": [
    "A new form of a grammar is described, which provides two separate sets of stochastic parameters for representing both the semantic and the syntactic knowledge, required for automatic speech understanding. The semantic structure is introduced as an adequate representation of natural spoken, one-sentence command utterances. The constraints and probabilities delivered by the grammar can be integrated into the framework of a stochastic topdown parser to decode the semantic content of an utterance directly from its observation sequence. The performance of the developed methods is proved for the domain of a speech understanding graphic editor, which can be controlled solely by natural spoken commands. Keywords: speech understanding, context-free grammar, stochastic models, syntactic and semantic knowledge\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-141"
  },
  "levin95_eurospeech": {
   "authors": [
    [
     "Esther",
     "Levin"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "Concept-based spontaneous speech understanding system",
   "original": "e95_0555",
   "page_count": 4,
   "order": 142,
   "p1": "555",
   "pn": "558",
   "abstract": [
    "In this paper we describe the issues in the design of CHRONUS - a spontaneous speech understanding system. The CHRONUS system is based on the approach we proposed in 1991, which formalizes the understanding problem as a communication problem. The model assumes that a spoken sentence is generated by a HMM like process whose hidden states correspond to elemental meaning units called concepts. Understanding therefore consists in decoding the hidden concepts given a spoken utterance [2]. The CHRONUS system was used in the ARPA ATIS task, a spontaneous speech database interface. In the 1994 official evaluation it achieved a state of the art accuracy. In the following paper we discuss the features of the system that contributed to this success.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-142"
  },
  "bonafonte95_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "José B.",
     "Marino"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Semantic decoding of speech in constrained domains",
   "original": "e95_0559",
   "page_count": 4,
   "order": 143,
   "p1": "559",
   "pn": "562",
   "abstract": [
    "In this paper we present a system to translate speech into its semantic representation. The system consists of a semantic model, a representation of each semantic unit and phone HMM. All the models of the basic system are infered automatically from samples. However, some improvements are achieved if a keyword list is provided to the inference algorithm of the semantic units. The system decodes correctly 85.5% of the semantic units. 60.0% of the sentences are transcribed exactly as the reference labels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-143"
  },
  "federico95b_eurospeech": {
   "authors": [
    [
     "Marcello",
     "Federico"
    ],
    [
     "Fabrizio",
     "Vernesoni"
    ]
   ],
   "title": "A speech understanding architecture for an information query system",
   "original": "e95_0563",
   "page_count": 4,
   "order": 144,
   "p1": "563",
   "pn": "566",
   "abstract": [
    "This paper presents an architecture for speech understanding that was implemented for an information query application in Italian. The architecture employs speech recognition and stochastic grammar algorithms developed at IRST. Interpretation of an utterance is computed in three steps: first, a phonetic-unit sequence is produced by a speech recognizer with the help of a bigram language model; second, a sequence of semantic units is computed by a Viterbi decoder with the help of a semantic language model; and, finally, a semantic parser computes the most probable semantic interpretation of the query. Experimental results are reported for a test set of 154 queries which were never seen during the architecture design and training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-144"
  },
  "bauer95_eurospeech": {
   "authors": [
    [
     "Josef G.",
     "Bauer"
    ],
    [
     "Holger",
     "Stahl"
    ],
    [
     "Johannes",
     "Müller"
    ]
   ],
   "title": "A one-pass search algorithm for understanding natural spoken time utterances by stochastic models",
   "original": "e95_0567",
   "page_count": 4,
   "order": 145,
   "p1": "567",
   "pn": "570",
   "abstract": [
    "A system for understanding time utterances spoken in German language is presented. Stochastic models contain the knowledge in the semantic, syntactic and acoustic-phonetic levels. An adequate semantic representation allows the integration of these models within a one-pass Viterbi search. The simultaneous use of all knowledge sources for the search procedure results in the smallest possible search space for the determination of the most probable semantic content accurately following the Bayes classification rule. Both the recognition accuracy and the computing speed facilitate a realistic application. Keywords: speech recognition, language understanding, spoken man-machine-dialogue, stochastic models, one-pass search, representation of syntactic and semantic knowledge\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-145"
  },
  "donovan95_eurospeech": {
   "authors": [
    [
     "R. E.",
     "Donovan"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "Improvements in an HMM-based speech synthesiser",
   "original": "e95_0573",
   "page_count": 4,
   "order": 146,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "Improvements are presented to the performance of a speech synthesis system which learns it's parameters through training on a speech database. The system uses a set of cross-word decision-tree state-clustered triphone HMMs to segment the database into approximately 4000 clustered states, which are then used as the sub-word units for synthesis. The system is fully automatic, and can be retrained on a new voice in under 48 hours. The synthetic voice mimics the voice used in training. The improvements in segmentation presented in this paper, together with the adoption of the PSOLA synthesis technique have enabled the system to produce speech with high levels of intelligibility and naturalness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-146"
  },
  "itoh95_eurospeech": {
   "authors": [
    [
     "Yoshiharu",
     "Itoh"
    ],
    [
     "Makoto",
     "Hashimoto"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Sub-phonemic optimal path search for concatenative speech synthesis",
   "original": "e95_0577",
   "page_count": 4,
   "order": 147,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "This paper proposes a new scheme in which subphoneme units are used to reduce spectral distortion at concatenation points, thereby achieving high-quality concatenative speech synthesis. Spectral distortion is minimized by a dynamic programming technique based on a concatenation point table. The validity of the proposed method has been shown in objective and subjective evaluation tests. The objective test confirmed that the maximal spectral distortion at concatenation points was reduced by 16-44%. In the subjective test, speech synthesized using the proposed method got 60% of the preference score in comparison with that using phoneme units.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-147"
  },
  "black95b_eurospeech": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Optimising selection of units from speech databases for concatenative synthesis",
   "original": "e95_0581",
   "page_count": 4,
   "order": 148,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "Concatenating units of natural speech is one method of speech synthesis1. Most such systems use an inventory of fixed length units, typically diphones or triphones with one instance of each type. An alternative is to use more varied, non-uniform units extracted from large speech databases containing multiple instances of each. The greater variability in such natural speech segments allows closer modeling of naturalness and differences in speaking styles, and eliminates the need for specially-recorded, single-use databases. However, with the greater variability comes the problem of how to select between the many instances of units in the database. This paper addresses that issue and presents a general method for unit selection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-148"
  },
  "lopezgonzalo95_eurospeech": {
   "authors": [
    [
     "E.",
     "Lopez-Gonzalo"
    ],
    [
     "Lúis",
     "Hernandez-Gomez"
    ]
   ],
   "title": "Automatic data-driven prosodic modeling for text-to-speech",
   "original": "e95_0585",
   "page_count": 4,
   "order": 149,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "Prosodic modeling is very important for improved naturalness in text to speech synthesis systems. This paper describes an automatic data-driven methodology for prosodic modeling that can be incorporated into a text-to-speech system. This methodology models both fundamental frequency and suprasegmental duration from a monospeaker recorded corpus. The proposed automatic methodology has the advantage that can be adapted to a specific corpus or a particular speaker. The results of the automatic methodology are compared with our previous manual methodology with the same prosodic data. A greater variability in prosodic contourns are obtained as the main result.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-149"
  },
  "mana95_eurospeech": {
   "authors": [
    [
     "F.",
     "Mana"
    ],
    [
     "Silvia",
     "Quazza"
    ]
   ],
   "title": "Text-to-speech oriented automatic learning of Italian prosody",
   "original": "e95_0589",
   "page_count": 4,
   "order": 150,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "The work described in the paper compared different techniques for learning prosodic regularities from natural speech databases, in view of future developments of ELOQUENS®, the CSELT text-to-speech system for Italian. As an alternative to explicit modelling by rules, the adaptive algorithms ANN (Artificial Neural Nets) and CART (Classification And Regression Trees), have been applied to predict the prosodic parameters of phoneme duration and fundamental frequency. The first experiment, in which the algorithms were trained on a limited-size corpus of neutrally-read declarative sentences, gave encouraging results. The paper argues that, despite of their limits in providing accurate modelling of linguistic phenomena, automatic learning techniques may be considered a promising methodological framework for developing multi-voice, multi-style and multi-language text-to-speech systems, a task requiring research tools to analyze large speech databases and implementation devices to speed up computations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-150"
  },
  "breen95_eurospeech": {
   "authors": [
    [
     "Andrew P.",
     "Breen"
    ]
   ],
   "title": "A simple method of predicting the duration of syllables",
   "original": "e95_0595",
   "page_count": 4,
   "order": 151,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "The accurate specification and prediction of segmental durations in speech has attracted a large number of research publications over the years. Such publications may be divided into two types, those which attempt to catalogue the various statistical properties of durations [1] or describe observable duration trends in speech [2], and those which explicitly attempt to predict the duration of segments [3] [4] [5]. In recent years, many papers attempting to understand the reasoning behind particular segment durations have included language structure in their descriptions [2]. The causes of variation in the duration of a segment are now seen as a consequence of higher linguistic structure rather than just close proximity contextual effects. This drive to non-linear phonological descriptions has occurred in part due to a lack of success in improving upon the traditional context sensitive segment duration rules as advocated most notably by Klatt [6].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-151"
  },
  "riedi95_eurospeech": {
   "authors": [
    [
     "Marcel",
     "Riedi"
    ]
   ],
   "title": "A neural-network-based model of segmental duration for speech synthesis",
   "original": "e95_0599",
   "page_count": 4,
   "order": 152,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "This paper presents a neural-network-based model of segmental duration. It was developed with the intention of applying it to speech synthesis for German. Given a set of factors influencing the duration of a phone-sized segment a neural network is used to predict the segment duration. Different mappings of these factors to values suitable for networks with binary and analog input nodes have been applied. So far? the highest correlation coefficient between the observed and predicted segment durations of a test set is 0.886. Informal acoustical tests with this model in combination with a speech synthesis system further demonstrated the feasibility of this approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-152"
  },
  "beaugendre95_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Beaugendre"
    ]
   ],
   "title": "Generating French intonation at different speaking rates",
   "original": "e95_0603",
   "page_count": 4,
   "order": 153,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "This paper presents a series of experiments which aim at answering the question of how speakers adapt their prosodic strategy according to speaking rate. The results are interpreted both at a linguistic and a phonetic level. Furthermore, a set of rules was established that introduce speech rate as a new input parameter of the LIMSI Text-To-Speech synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-153"
  },
  "tzoukermann95_eurospeech": {
   "authors": [
    [
     "Evelyne",
     "Tzoukermann"
    ],
    [
     "Olivier",
     "Soumoy"
    ]
   ],
   "title": "Segmental duration in French text-to-speech synthesis",
   "original": "e95_0607",
   "page_count": 4,
   "order": 154,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "This paper presents the methodology and the results of a duration study performed for French and motivated by the construction of a French text-to-speech system [7]. Steps necessary for the construction of a statistically based duration system are described, with special attention on specific aspects of French, such as liaison and schwa deletion. The scientific contribution of this research is to add acoustic evidence to the knowledge of the complex effects of contextual duration phenomena on phonemic duration in French, extending perceptually determined knowledge.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-154"
  },
  "home95_eurospeech": {
   "authors": [
    [
     "M.",
     "Home"
    ],
    [
     "M.",
     "Filipsson"
    ]
   ],
   "title": "Developing the prosodic component for Swedish speech synthesis",
   "original": "e95_0611",
   "page_count": 4,
   "order": 155,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "A proposal is made for the design of an algorithm for determining when pronouns and 'Given' content words should be assigned accents due to a shift in grammatical function.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-155"
  },
  "setlur95_eurospeech": {
   "authors": [
    [
     "Anand",
     "Setlur"
    ],
    [
     "Thomas",
     "Jacobs"
    ]
   ],
   "title": "Results of a speaker verification service trial using HMM models",
   "original": "e95_0639",
   "page_count": 4,
   "order": 156,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "We report results from a speaker verification trial which used hidden Markov models (HMMs). In this trial, enrolled users of two self-service teller machines were asked to repeat random 4-digit phrases to gain access to their accounts. Use of a single speaker-specific HMM model resulted in a mean individual equal error rate (EER) of 18.4% for a population of 50 English speakers. This decreased to an EER of 10% when raw scores were normalized using a single cohort model built from an independent population. The performance is further improved to 4.5% by constraining the individual feature variances for all models to a fixed set of values. By doubling the training set size, the cohort normalized fixed variance system had a verification accuracy of 1.5% EER. For comparative purposes, we used the same technique on the publicly available YOHO speech corpus and obtained a mean EER of 0.4%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-156"
  },
  "liou95_eurospeech": {
   "authors": [
    [
     "Han-Sheng",
     "Liou"
    ],
    [
     "Richard J.",
     "Mammone"
    ]
   ],
   "title": "Application of phonetic weighting to the neural tree network based speaker recognition system",
   "original": "e95_0643",
   "page_count": 4,
   "order": 157,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "This paper presents a new scoring method for text-dependent speaker recognition. The scoring method is used to combine the scores measured by the phoneme-based neural tree network (NTN) classifiers. In contrast to the conventional method that combines the output scores of speech frames by averaging them over the utterance, the new method uses phoneme-dependent weighted combination. Since the phonemes are different in their effectiveness for speaker discrimination, the phonetic weights are chosen according to their abilities of speaker discrimination. The proposed method is evaluated by experiments on the YOHO database. Performance improvements are obtained over conventional techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-157"
  },
  "gauvain95_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "B.",
     "Prouts"
    ]
   ],
   "title": "Experiments with speaker verification over the telephone",
   "original": "e95_0651",
   "page_count": 4,
   "order": 158,
   "p1": "651",
   "pn": "654",
   "abstract": [
    "In this paper we present a study on speaker verification showing achievable performance levels for both high quality speech and telephone speech and for two operational modes, i.e. text-dependent and text-independent speaker verification. A statistical modeling approach is taken, where for text independent verification the talker is viewed as a source of phones, modeled by a fully connected Markov chain, where the lexical and syntactic structures of the language are approximated by local phonotactic constraints. A first series of experiments were carried out on high quality speech from the BREF corpus to validate this approach and resulted in an a posteriori equal error rate of 0.3% in text-dependent as well as in text-independent mode. A second series of experiments were carried out on a telephone corpus recorded specifically for speaker verification algorithm development. On this data, the lowest equal error rate is 2.9% for the text-dependent mode when 2 trials are allowed per attempt and with a minimum of 2s of speech per trial.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-158"
  },
  "morenoperez95_eurospeech": {
   "authors": [
    [
     "Sofia",
     "Moreno Perez"
    ],
    [
     "Ramon",
     "Garcia Gomez"
    ]
   ],
   "title": "Improved CELP algorithm suited for various speech coding applications",
   "original": "e95_0699",
   "page_count": 4,
   "order": 159,
   "p1": "699",
   "pn": "702",
   "abstract": [
    "This is another \"CELP based fantastic speech coding scheme\", and in this case we have solved a problem that will make your speech coding real time implementation really easier. We combine the efficiency of the open loop analysis with the robustness of the closed loop one. In this way, we obtain a significant computational save in the search of the optimum excitation vector with no loss of speech quality. We developed a real time implementation of this coder for low bit rate transmission (from 4.8 to 14.4 KBPS) and for wideband speech coding (from 16.0 to 28.8 KBPS).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-159"
  },
  "atungsiri95_eurospeech": {
   "authors": [
    [
     "S. A.",
     "Atungsiri"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "Comparative study of two codecs for an enhanced GSM system",
   "original": "e95_0703",
   "page_count": 4,
   "order": 160,
   "p1": "703",
   "pn": "707",
   "abstract": [
    "The European Telecommunication Standards Institute's SMG 1 recently finalised the requirements for an envisaged enhanced full rate (EFR) codec to supplement the current GSM TCH-FS codec [1]. In this paper, we assess the performance of two modern speech codecs vis-a-vis the published performance requirements for the envisaged EFR. We conclude that a significant improvement in speech quality both under clear and degraded channel conditions is achievable within the framework of the SMG performance requirements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-160"
  },
  "chui95_eurospeech": {
   "authors": [
    [
     "Siu-pun",
     "Chui"
    ],
    [
     "Cheung-fat",
     "Chan"
    ]
   ],
   "title": "Fast low-delay CELP coding of speech at 8kbps",
   "original": "e95_0707",
   "page_count": 4,
   "order": 161,
   "p1": "707",
   "pn": "710",
   "abstract": [
    "This paper presents a 2.5ms low-delay CELP coder at 8kbps with low complexity. Two major contributions were made in order to achieve this goal The first contribution is based on using hybrid input/output block adaptive linear prediction scheme for short-term analysis. In this scheme, LSP coefficients are used as parameters for spectral adaptation and they are partitioned into two segments; LSPs corresponding to low frequency regions are backward adapted from the reproduction speech while LSPs in high frequency regions are adapted from input speech and quantized as side information to decoder. The rationale behind this scheme is to compensate the spectrum distortion introduced by conventional backward adaptation scheme. The second contribution is a fast method for stochastic codebook search. This method relies on using a vector-sum codebook where the crosscorrelation of any pair of basis vectors is odd-symmetric. As a result of this odd-symmetric crosscorrelation (OSC) property the energy term of the cost function for codebook search is a constant with respect to the search and the optimum codeword can be easily determined by using a sign detection procedure with a complexity almost independent of the codebook size.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-161"
  },
  "kang95_eurospeech": {
   "authors": [
    [
     "Hong Goo",
     "Kang"
    ],
    [
     "Jeong Tae",
     "Seo"
    ],
    [
     "Il Whan",
     "Cha"
    ],
    [
     "Dae Hee",
     "Youn"
    ]
   ],
   "title": "A low bit-rate speech coder using the perceptual properties of the human ear",
   "original": "e95_0711",
   "page_count": 4,
   "order": 162,
   "p1": "711",
   "pn": "714",
   "abstract": [
    "This study describes a low bit-rate speech coding algorithm which can be applied to half-rate digital cellular and storage systems. The coding algorithm employs the temporal integration effect of the human ear that perceives short time signals as occurring concurrently. Using the result of listening tests, the length of the analysis frame and the types of excitation signals were chosen to enhance the integration effect. Informal listening tests verified that the proposed method at the rate of 4kbit/s yields better subjective quality than the 4.8kbit/s DoD-CELP(Department of Defence-CELP).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-162"
  },
  "cucchi95_eurospeech": {
   "authors": [
    [
     "Silvio",
     "Cucchi"
    ],
    [
     "Marco",
     "Fratti"
    ]
   ],
   "title": "Very fast CELP coding using stochastic innovations",
   "original": "e95_0715",
   "page_count": 4,
   "order": 163,
   "p1": "715",
   "pn": "718",
   "abstract": [
    "In this paper, we describe improvements to the CELP excitation model for speech coding at medium-low bit rates. The innovation signal in our coder is modeled by addition/subtraction of two codewords derived from the same stochastic codebook. An extremely efficient pre-selection procedure allows to choose the 'best' candidate codewords. The best couple of codewords is then selected among the pre-selected ones. The proposed method is amenable to the implementation of low complexity CELP coders based on stochastic excitation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-163"
  },
  "bouraoui95_eurospeech": {
   "authors": [
    [
     "M.",
     "Bouraoui"
    ],
    [
     "W.",
     "Glass"
    ],
    [
     "Gang",
     "Feng"
    ]
   ],
   "title": "Fast codebook search algorithm based on hamming ECC for algebraic CELP speech coding",
   "original": "e95_0719",
   "page_count": 4,
   "order": 164,
   "p1": "719",
   "pn": "722",
   "abstract": [
    "The complexity of a CELP algorithm is mainly concentrated in the codebook search procedure. Many studies show that the algebraic approach for codebook structures considerably reduces the computational load with a slight degradation compared to stochastic or trained codebook performances. We propose a novel search algorithm based on a Hamming single-error-correcting -code (ECC) structure. The optimum codevector is directly circumscribed using the error correction ability of the ECC. This algorithm requires only 0.7 MIPS which represents a complexity reduction by a factor of 3 or 4 compared to search algorithms based on current algebraic codebooks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-164"
  },
  "fingscheidt95_eurospeech": {
   "authors": [
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Thomas",
     "Wiechers"
    ],
    [
     "Eckhard",
     "Delfs"
    ]
   ],
   "title": "Implementation aspects of the GSM half-rate speech codec",
   "original": "e95_0723",
   "page_count": 4,
   "order": 165,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "In this paper the GSM Half-Rate Speech Codec is examined with respect to implementation complexity taking the bit exactness requirements as well as differences in DSP-architectures into consideration. While the codec specification [1] assumes some \"ideal DSP\" with a certain architecture, it turns out that the implementation on a real-world DSP could require about 50 MIPS. The computational load can be reduced by minor modifications of the DSP instruction set (appropriate saturation logic and barrel shifter) down to 25 MIPS. However, by exploiting detailed knowledge of the codec algorithm, a processor load of less than 25 MIPS can be achieved too, even if the DSP does not provide the required saturation logic. This is shown by way of example, using a single NEC PD77018 DSP for a full duplex real time implementation. The potential for complexity reduction is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-165"
  },
  "watson95_eurospeech": {
   "authors": [
    [
     "S. D.",
     "Watson"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "W. T. K.",
     "Wong"
    ],
    [
     "A. V.",
     "Lewis"
    ]
   ],
   "title": "Low and variable bit-rate speech coding for ATM networks",
   "original": "e95_0727",
   "page_count": 4,
   "order": 166,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "This paper investigates the use of a variable low bit-rate speech transmission method for Asynchronous Transfer Mode (ATM) networks, A modified version of an existing 8kbit/s ACELP speech coder, making it an on-off source controlled variable bit-rate coder, referred to as AV-ACELP, has been adopted for this investigation. A possible adaptation of this coder to ATM is proposed and the effects of cell loss, cell delay and congestion control are examined. This paper discusses a suitable packetization method, ATM adaptation layer function and cell loss recovery scheme. Results obtained from this study are compared with results obtained from a standard 32kbit/s embedded ADPCM coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-166"
  },
  "popescu95b_eurospeech": {
   "authors": [
    [
     "Andrei",
     "Popescu"
    ],
    [
     "Nicolas",
     "Moreau"
    ],
    [
     "Claude",
     "Lamblin"
    ]
   ],
   "title": "A differential encoding method for the LTP delay in CELP coders",
   "original": "e95_0731",
   "page_count": 3,
   "order": 167,
   "p1": "731",
   "pn": "733",
   "abstract": [
    "We propose a differential encoding method for the long term predictor (LTP) delay in CELP coders. This method permits saving bits while preserving the coded speech quality. Fast delay changes between successive LTP updates are allowed and no buffering of long signal frames is necessary, which makes our method suited for use in low-and medium-delay coders.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-167"
  },
  "schmid95_eurospeech": {
   "authors": [
    [
     "Philipp",
     "Schmid"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Robust, n-best formant tracking",
   "original": "e95_0737",
   "page_count": 4,
   "order": 168,
   "p1": "737",
   "pn": "740",
   "abstract": [
    "We describe a robust, N-best formant tracker. The 2 stage algorithm initially finds single formants or parts thereof. In the second stage a robust dynamic programming search with a wild card mechanism is employed to find the N best consistent interpretation of the initial formant information. The selection of the correct formant tracks is delayed until after the phonetic search, thus overcoming the lack of robustness of traditional formant trackers by delaying the final decision until after phonemic classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-168"
  },
  "plante95_eurospeech": {
   "authors": [
    [
     "F.",
     "Plante"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Formant tracking using reassigned spectrum",
   "original": "e95_0741",
   "page_count": 4,
   "order": 169,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "For accurate formant tracking, Linear Prediction (LP) and Fourier Transform (FT) analysis have some limitations. For the first, the limitation is the validity of the speech production model. For the second it is the limited time and frequency resolution. In this paper we focus on the improvment of - the FT using the reassignment technique. This method allows a much more accurate representation taking into account the phase of the FT. Comparison of this method with the normal FT and LP is carried out on Vowel-Vowel transitions, using respectively a pitch synchronous analyisis and a closed phase analysis. For the first case, the reassigned spectrum performs better than the LP, and slightly worse for the closed phase. In both conditions the results are significantly improved compared with the normal FT. The efficiency of the method with regard to the Formant definition is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-169"
  },
  "schoentgen95_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "S.",
     "Ciocca"
    ]
   ],
   "title": "Direct calculation of the vocal tract area function from measured formant frequencies",
   "original": "e95_0745",
   "page_count": 4,
   "order": 170,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "This article presents a method for directly calculating the area function of a vocal tract model by means of measured formant frequencies. The model is a concatenation of uniform tubelets whose cross-sections and lengths may vary in time. The proposed method uses a mathematical expression that relates time derivatives of tubelet cross-sections, lengths and formant frequencies. The derivatives are numerically integrated to arrive at cross-section and length trajectories. When more than one model area function is compatible with observed formant frequencies, additional constraints are used to select a unique area function. Results show that form ant-matched trajectories of tubelet cross-sections arid lengths are smooth. The agreement between observed and model-generated formant frequencies is better than 0.01 Hz\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-170"
  },
  "sun95_eurospeech": {
   "authors": [
    [
     "Don X.",
     "Sun"
    ]
   ],
   "title": "Robust estimation of spectral center-of-gravity trajectories using mixture spline models",
   "original": "e95_0749",
   "page_count": 4,
   "order": 171,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "This paper presents a novel approach to the estimation of the trajectories of spectral center-of-gravity using robust statistical models with penalized weighted spline smoothers. Most of the existing methods for tracking speech formant trajectories are based on dynamic programming algorithms with certain continuity constraints on the formant frequencies ([6, 8, 9]). The objective functions (or loss functions) in these approaches are usually ad hoc and have very complex expressions that are difficult to optimize. Also, many existing methods rely on the accuracy of the LPC spectral peaks and are not very robust against possible missing or spurious peaks. In this paper, instead of using the peaks of the LPC spectral functions, we propose a new approach to the estimation of the \"center-of-gravities\" in spectrogram using mixture models of spline smoothers ([5, 10]). There are two major advantages in this new approach: 1) it is robust against missing peaks and spurious peaks that may occur in LPC peak finding algorithms; 2) trajectory smoothness is guaranteed by the properties of spline regression models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-171"
  },
  "pan95_eurospeech": {
   "authors": [
    [
     "J. S.",
     "Pan"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Bound for minkowski metric based on LP distortion measure",
   "original": "e95_0753",
   "page_count": 4,
   "order": 172,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "A bound for Minkowski metric based on Lp distortion measure is proposed. This bound provides a better criterion than the absolute error inequality (AEI) elimination rule on the Euclidean distortion measure. For the Minkowski metric of order n} this bound contributes the elimination criterion from L1 metric to Ln metric.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-172"
  },
  "tokuda95_eurospeech": {
   "authors": [
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Tetsuya",
     "Yamada"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "An algorithm for speech parameter generation from continuous mixture HMMs with dynamic features",
   "original": "e95_0757",
   "page_count": 4,
   "order": 173,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "This paper proposes an algorithm for speech parameter generation from continuous mixture HMMs which include dynamic features, i.e., delta and delta-delta parameters of speech. We show that the parameter generation from HMMs using the dynamic features results in searching for the optimal state sequence and solving a set of linear equations for each possible state sequence. To solve the problem, we derive a fast algorithm on the analogy of the RLS algorithm for adaptive filtering. We show that the generated speech parameter vectors reflect not only the means of static and dynamic feature vectors but also the co-variances of those. An example presenting effectiveness of the proposed algorithm in speech synthesis is given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-173"
  },
  "richards95_eurospeech": {
   "authors": [
    [
     "Hywel B.",
     "Richards"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "Melvyn J.",
     "Hunt"
    ],
    [
     "John S.",
     "Bridle"
    ]
   ],
   "title": "Deriving articulatory representations of speech",
   "original": "e95_0761",
   "page_count": 4,
   "order": 174,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "Compared with the usual acoustic representations, articulatory models offer potential benefits in giving a compact, slowly changing representation, having a closer relationship with the phonetic domain and allowing a straightforward treatment of coarticulation and transitional effects. A long-standing difficulty preventing the realisation of these benefits is the estimation of appropriate parameters from the speech signal. This paper reviews recent progress in this area and describes some initial experiments that attempt to estimate the parameters of the recently proposed Distinctive Regions Model (DRM) [1], using a dynamic programming search of an articulatory codebook.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-174"
  },
  "aubert95_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Aubert"
    ],
    [
     "Christian",
     "Dugast"
    ]
   ],
   "title": "Improved acoustic-phonetic modeling in philips' dictation system by handling liaisons and multiple pronunciations",
   "original": "e95_0767",
   "page_count": 4,
   "order": 175,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "We address the use of multiple pronunciations to improve large-vocabulary continuous-speech recognition. Based on extensive tests with WSJ material, our results show that more consistent transcriptions and alternate pronunciations lead to an error reduction of 9% while at the same time reducing the number of mixture parameters. Next, we explain how the problem of cross-word liaisons has been treated when extending our system to dictation in French. Our solution consists in using phonological rules that are optionally applied both in training and recognition. Using the BREF corpus, it is shown that proper handling of liaisons improves the accuracy by about 10% for a 20K speaker-independent task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-175"
  },
  "morgan95_eurospeech": {
   "authors": [
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Su-Lin",
     "Wu"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Digit recognition with stochastic perceptual speech models",
   "original": "e95_0771",
   "page_count": 4,
   "order": 176,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "We have recently developed a statistical model of speech that focuses statistical modeling power on phonetic transitions. These are the perceptually-dominant and information-rich portions of the speech signal, which may also be the parts of the speech signal with a better chance to withstand adverse acoustical conditions. We describe here some of the concepts, along with some preliminary experiments on digit recognition. These experiments show that the new models, when used in combination with our more standard models, can significantly improve performance in the presence of noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-176"
  },
  "sitaram95_eurospeech": {
   "authors": [
    [
     "R. N. V.",
     "Sitaram"
    ],
    [
     "Thippur",
     "Sreenivas"
    ]
   ],
   "title": "On incorporating phonemic constraints in hidden Markov models for speech recognition",
   "original": "e95_0775",
   "page_count": 4,
   "order": 177,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "Phonemes have characteristic properties stich as unique temporal structure, context sensitive behaviour and specific duration etc. Phoneme models should incorporate such constraints to provide better classification accuracy. In this paper these phonemic properties are incorporated into a HMM based phoneme recognizer with the addition of several degrees of freedom to the HMM state. The resulting models have shown improved performance on the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-177"
  },
  "chang95_eurospeech": {
   "authors": [
    [
     "Saga",
     "Chang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "An improvement on syllable-based continuous Mandarin speech recognition via using inter-syllable boundary models",
   "original": "e95_0779",
   "page_count": 4,
   "order": 178,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "One of the most important problems in continuous speech recognition is to accurately model the acoustic variabilities caused by coarticulation in order to obtain high recognition accuracy. In this paper, a novel approach to solve the problem for compensating inter-syllable coarticulation effect on syllable-based continuous Mandarin speech recognition is proposed. Inter-syllable coarticulation is directly modelled by supplementing the context-independent syllable HMM models with some inter-syllable boundary models. Experimental results showed that the syllable recognition errors of a speaker-dependent continuous speech recognition task was reduced by 24%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-178"
  },
  "svendsen95_eurospeech": {
   "authors": [
    [
     "Torbjorn",
     "Svendsen"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Heiko",
     "Purnhagen"
    ]
   ],
   "title": "Optimizing baseforms for HMM-based speech recognition",
   "original": "e95_0783",
   "page_count": 4,
   "order": 179,
   "p1": "783",
   "pn": "787",
   "abstract": [
    "In this paper we propose a new, automatic optimal baseform determination algorithm. Given a set of subword Hidden Markov Models (HMMs) and acoustic tokens of a specific word, we apply the tree-trellis N-best search algorithm to find the optimal base-forms (transcriptions) in the maximum likelihood sense. Different token preselection algorithms have been investigated to facilitate fast search for representative baseforms and to alleviate the problem of representing vastly different pronounciations with a single baseform. The DARPA Resource Management database was used for evaluating the new baseform optimization algorithrty, improvements of recognition rates using different token selection algorithms and the tree-trellis search have been consistently obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-179"
  },
  "alvarezcercadillo95_eurospeech": {
   "authors": [
    [
     "J.",
     "Alvarez-Cercadillo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Luis",
     "Hernandez-Gomez"
    ]
   ],
   "title": "Acoustic modeling of context dependent units, for large vocabulary speech recognition in Spanish",
   "original": "e95_0787",
   "page_count": 4,
   "order": 180,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "We present a study on acoustic modeling of Spanish phonetic units. Bootstrap with a set of English phonetic models, we first obtain context-independent unit models for Spanish. We then compare context-dependent modeling techniques based on the conventional maximum likelihood (ML) and the maximum a posteriori (MAP) criteria. We found the MAP-based context adaptation approach produces a better result than the ML approach when a large number of units need to be modeled but the amount of training data is limited.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-180"
  },
  "okawa95_eurospeech": {
   "authors": [
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Estimation of statistical phoneme center and its application to accurate phoneme modelling",
   "original": "e95_0791",
   "page_count": 4,
   "order": 181,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "In this paper we propose a new approach of speech recognition based on an idea of Statistical Phoneme Center. The Statistical Phoneme Center has several properties that are feasible to realize a high-reliable phoneme extraction. First, in every phoneme we assume that there is the fictitious center which can be estimated statistically. The center is determined by an iterative procedure to maximize the local likelihood using a large amount of speech data. Next, HMMs are trained for every segment between the centers and are concatenated to make a word HMM. The word recognition is realized by an optimal algorithm considering the center likelihood. As the experimental result, 97.7% recognition accuracy is obtained for 216 word vocabularies in the speaker independent condition. The result demonstrates the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-181"
  },
  "li95d_eurospeech": {
   "authors": [
    [
     "Z.",
     "Li"
    ],
    [
     "P.",
     "Kenny"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Hybrid hidden Markov models in speech recognition",
   "original": "e95_0795",
   "page_count": 4,
   "order": 182,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "In speech recognition systems based on Hidden Markov Modeling, the computation of the likelihoods in detailed models is intensive, while the performance of crude models is poor. A hybrid model which combines the detailed and crude models is proposed to take advantage of the performance of the detailed model and the speed of the crude model. Experimental results show that a significant (up to a factor of 20) likelihood computation reduction has been obtained, with almost the same recognition accuracy as the baseline models on both the speaker-dependent and speaker-independent systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-182"
  },
  "fissore95_eurospeech": {
   "authors": [
    [
     "L.",
     "Fissore"
    ],
    [
     "F.",
     "Ravera"
    ],
    [
     "Pietro",
     "Laface"
    ]
   ],
   "title": "Acoustic-phonetic modeling for flexible vocabulary speech recognition",
   "original": "e95_0799",
   "page_count": 4,
   "order": 183,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "This paper focuses on the definition and modeling of robust context-dependent units for flexible vocabulary-recognition. It proposes a new technique for tuning the acoustic resolution of the models, and discusses the advantages of representing phonetic transcriptions in terms of a sequence of stationary context-independent phonemes and diphone-transition coarticulation units rather than with the classical diphone or triphone units. Combining these two techniques, the recognition rate of a speaker-independent recognizer with a vocabulary of 600 surnames increases from 91.2% to 96% using less than one third of the densities of the original models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-183"
  },
  "dumouchel95_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Dumouchel"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Segmental duration and HMM modeling",
   "original": "e95_0803",
   "page_count": 4,
   "order": 184,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "We propose to use a stochastic segmental duration model independent of the HMM model in INRS's large vocabulary speech continuous speech recognizer. First, we examine how to insert this model into the search algorithm without violating the optimality constraints of this algorithm. Second, we propose and test the performance of four different duration models. The training and testing of the models is done on a studio quality speaker-dependent speech corpus. The first model is a rule-based model which imposes minimum and maximum phone durations. The second model is a Gaussian mixture phone duration model independent of the phonemic context. The third model is a Gaussian mixture phone duration model dependent on the right or left phoneme context. Finally, the last model is a Gaussian mixture duration model based on the variation of duration within a diphone. Performance comparisons show that the best model is the first one which imposes hard constraints on duration. This model improves the percentage of word recognition from 89.58% (no duration modeling) to 90.11%. Keywords: segmental duration, prosody, suprasegmental features, Markov source-based continuous speech recognizer, large vocabulary\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-184"
  },
  "ghio95_eurospeech": {
   "authors": [
    [
     "A.",
     "Ghio"
    ],
    [
     "Mario",
     "Rossi"
    ]
   ],
   "title": "A knowledge-based model for speaker-independent, acoustic-phonetic decoding",
   "original": "e95_0807",
   "page_count": 4,
   "order": 185,
   "p1": "807",
   "pn": "810",
   "abstract": [
    "We examine to what extent a knowledge-based model can recognise segmental structure without feedback from semantic information and without stochastic modelling. The system proposed is inspired by some features of human cognitive processing in that the speech signal activates parallel distributed processes of decoding. The modules, conceptually different, are: an automatic segmentation module. a first analytic recognition based on oriented graphs with state transitions. a second analytic recognition module based on phonetic rules. a global recognition based on metric methods. Finally, scrutiny of all the parallel results and access to a dictionary allow the inference rules to propose ranked word candidates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-185"
  },
  "jan95_eurospeech": {
   "authors": [
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Piergiorgio",
     "Svaizer"
    ],
    [
     "James L.",
     "Flanagan"
    ]
   ],
   "title": "A database for microphone array experimentation",
   "original": "e95_0813",
   "page_count": 4,
   "order": 186,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "An extensive speech database was collected in two experimental enclosures for research on microphone arrays. The first enclosure is a digitally controlled variable acoustics facility where the reverberation time can be adjusted from 1.7 s to 0.1 s, and the second is a regular hard-walled laboratory. Two different sets of data were collected to address source location and spatial volume selectivity issues. Preliminary results on Matched-filter processing and on Source location are reported for the new database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-186"
  },
  "lander95_eurospeech": {
   "authors": [
    [
     "T.",
     "Lander"
    ],
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "B. T.",
     "Oshika"
    ],
    [
     "M.",
     "Noel"
    ]
   ],
   "title": "The OGI 22 language telephone speech corpus",
   "original": "e95_0817",
   "page_count": 4,
   "order": 187,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "The 22 Language Telephone Speech Corpus is the newest effort in CSLU multi-language corpus development. Since November of 1994 we have been collecting calls from speakers of 22 languages. The completed corpus will contain at least 200 speakers per language. All calls are verified by native speakers to insure that callers followed instructions. In addition a subset of the calls have been transcribed by native speakers of the different languages. Conventions are being developed for phonetic transcription of a portion of the calls in each language. This corpus is useful for language identification research, as well as research into spoken language systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-187"
  },
  "cole95_eurospeech": {
   "authors": [
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "M.",
     "Noel"
    ],
    [
     "T.",
     "Lander"
    ],
    [
     "T.",
     "Durham"
    ]
   ],
   "title": "New telephone speech corpora at CSLU",
   "original": "e95_0821",
   "page_count": 4,
   "order": 188,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "The Center for Spoken Language Understanding (CSLU) collects, annotates and distributes telephone speech data to enable research in spoken language understanding and automatic language identification. This paper gives a brief overview of recent activities in pursuit of this mission. We summarize corpus development activities at CSLU and describe new corpora useful for research on specific tasks: alphabet recognition, numbers recognition, large vocabulary word recognition, and yes/no recognition. We then discuss our two newest data collection efforts, Cellular Speech and the 22-Language Telephone Speech Corpus. All CSLU corpora are available at no charge to academic institutions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-188"
  },
  "os95_eurospeech": {
   "authors": [
    [
     "E. A. den",
     "Os"
    ],
    [
     "T. I.",
     "Boogaart"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Esther",
     "Klabbers"
    ]
   ],
   "title": "The Dutch polyphone corpus",
   "original": "e95_0825",
   "page_count": 4,
   "order": 189,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "This paper first summarizes the work done to design, record, transcribe, and produce the Dutch Polyphone corpus. In addition, figures related to frequency of occurrence of diphones and triphones in the phonetically rich sentences are presented. Furthermore, we describe one way of using the corpus, viz. to derive information about the way callers pronounced telephone numbers, postal codes (both read and spontaneously spoken), amounts of money, times, and bank accounts. Finally, it is shown that hesitation phenomena occur most frequently within longer items.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-189"
  },
  "bertenstam95_eurospeech": {
   "authors": [
    [
     "J.",
     "Bertenstam"
    ],
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Sheri",
     "Hunnicutt"
    ],
    [
     "J.",
     "Hogberg"
    ],
    [
     "R.",
     "Lindell"
    ],
    [
     "L.",
     "Neovius"
    ],
    [
     "Lennart",
     "Nord"
    ],
    [
     "Antonio de",
     "Serpa-Leitao"
    ],
    [
     "N.",
     "Strom"
    ]
   ],
   "title": "The waxholm application database",
   "original": "e95_0833",
   "page_count": 4,
   "order": 190,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "This paper describes an application database collected in Wizard-of-Oz experiments in a spoken dialogue system, WAXHOLM. The system provides information on boat traffic in the Stockholm archipelago. The database consists of utterance-length speech files, their corresponding transcriptions, and log files of the dialogue sessions. In addition to the spontaneous dialogue speech, the material also comprise recordings of phonetically balanced reference sentences uttered by all 66 subjects. In the paper the recording procedure is described as well as some characteristics of the speech data and the dialogue.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-190"
  },
  "plante95b_eurospeech": {
   "authors": [
    [
     "F.",
     "Plante"
    ],
    [
     "Georg F.",
     "Meyer"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "A pitch extraction reference database",
   "original": "e95_0837",
   "page_count": 4,
   "order": 191,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "Many pitch extraction algorithms have been proposed in the past. The comparison of these algorithms is difficult because each study tends to be carried out on a unique data set. The purpose of this project is to develop a database for the comparison of these algorithms. This database is based on a core speech module and several additonal modules. The core module contains speech and laryngograph data for 15 speakers reading a phonetically balanced text. A voiced/unvoiced reference file is provided with the speech data. Currently a psychophysics module is available to test the performance of pitch extraction stages on commonly used pitch perception stimuli.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-191"
  },
  "winski95_eurospeech": {
   "authors": [
    [
     "Richard",
     "Winski"
    ],
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Dafydd",
     "Gibbon"
    ]
   ],
   "title": "Eagles spoken language working group: overview and results",
   "original": "e95_0841",
   "page_count": 4,
   "order": 192,
   "p1": "841",
   "pn": "845",
   "abstract": [
    "In this paper a brief overview is provided of the EAGLES project specifically with reference to progress: achieved in the Spoken Language Working Group. The goals and achievements are presented primarily with respect to the production of a handbook documenting existing working practices and guidelines for spoken language resource creation and description in Europe. Future prospects include further extension and development of the handbook material to cover present activities more adequately, to extend the language coverage and to create a more widely representative consultation base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-192"
  },
  "torremunilla95_eurospeech": {
   "authors": [
    [
     "Celinda de la",
     "Torre-Munilla"
    ],
    [
     "Luis",
     "Hernandez-Gomez"
    ],
    [
     "Daniel",
     "Tapias"
    ]
   ],
   "title": "CEUDEX: a data base oriented to context-dependent units training in Spanish for continuous speech recognition",
   "original": "e95_0845",
   "page_count": 4,
   "order": 193,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "In this paper we describe the design and recording process of the new telephone speech database recorded in Telefonica Investigation y Desarrollo, designed for research in large vocabulary speaker independent continuous speech recognition, speaker adaptation and speaker verification in Spanish over the telephone line. The database is composed of two sets: (a) CEUDEX, the main set, with a corpus of 400 phonetically balanced sentences, and (b) SPATIS: a task oriented set which was inspired in the ATIS (Air Travel Information System) [9] standard application for English. It will be used for Task-Independent tests of the Continuous Speech Recognizer. In the first stage of the recording procedure, a total of 21500 sentences from nearly 300 speakers were collected.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-193"
  },
  "lopezdeipina95_eurospeech": {
   "authors": [
    [
     "K.",
     "Lopez de Ipina"
    ],
    [
     "I.",
     "Torres"
    ],
    [
     "L.",
     "Onederra"
    ]
   ],
   "title": "Design of a phonetic corpus for a speech database in basque language",
   "original": "e95_0851",
   "page_count": 4,
   "order": 194,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "The design of Continuous Speech Recognition System requires to select a large amount of spoken data for each specific language. The goal of this work was the design of a Phonetic Corpus for a Speech Database in Basque language. Several samples of nowadays narrative, spoken language and newspaper language were previously analysed under a phonetic point of view. The Speech Database finally designed consisted of a Phonetic Corpus including 300 sentences phonetically balanced uttered twice by 40 speakers resulting in about 900.000 allophones. Two additional corpora of digits and short words completed the database. This database includes the adequate distribution of allophones and contexts to model Basque phones in both, Speech Recognition Systems and Linguistic analysis frameworks. Keywords: Speech Databases, Basque language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-194"
  },
  "pirrelli95_eurospeech": {
   "authors": [
    [
     "V.",
     "Pirrelli"
    ],
    [
     "S.",
     "Federici"
    ]
   ],
   "title": "you'd better say nothing than say something wrong: analogy, accuracy and text-to-speech applications",
   "original": "e95_0855",
   "page_count": 4,
   "order": 195,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "Over the last decade, learning how to pronounce written words by analogy has received considerable attention in psycholinguistic circles thanks to its cognitive plausibility and flexibility [1,2]. Computational models of analogy-based learning have been developed to probe the realism of this hypothesis, and discover the nature and function of analogising factors [3,4,5]. However, comparatively little effort has been put into an overall assessment of how well analogy works in dealing with specific NLP tasks, in particular with respect to its computational tractability and complexity [6], and its level of accuracy. Although it is well known that some form of analogy-based reasoning is at the root of how children learn to read written words aloud, at the moment we do not yet know with the same certainty how correct pronunciation by analogy is when compared with rule-governed pronunciation. In this paper we intend to show that pronunciation by analogy is not only a psycholinguistically realistic cognitive hypothesis, but also an extremely reliable alternative to rule-based approaches to text-to-speech conversion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-195"
  },
  "misheva95_eurospeech": {
   "authors": [
    [
     "A.",
     "Misheva"
    ],
    [
     "S.",
     "Dimitrova"
    ],
    [
     "V.",
     "Filipov"
    ],
    [
     "E.",
     "Grigoreva"
    ],
    [
     "M.",
     "Nikov"
    ],
    [
     "Peter",
     "Roach"
    ],
    [
     "S.",
     "Arnfield"
    ]
   ],
   "title": "Bulgarian speech database: a pilot study",
   "original": "e95_0859",
   "page_count": 4,
   "order": 196,
   "p1": "859",
   "pn": "863",
   "abstract": [
    "The paper describes the construction of a database of Bulgarian speech which follows the protocols which were established by the ESPRIT SAM project and have become an accepted European standard for such work. A number of different transcription systems were attempted. Future work on other languages of Central and Eastern Europe will build on the foundations laid by the work reported here.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-196"
  },
  "hess95_eurospeech": {
   "authors": [
    [
     "Wolfgang J.",
     "Hess"
    ],
    [
     "Klaus J.",
     "Kohler"
    ],
    [
     "Hans-Günther",
     "Tillmann"
    ]
   ],
   "title": "The Phondat-verbmobil speech corpus",
   "original": "e95_0863",
   "page_count": 4,
   "order": 197,
   "p1": "863",
   "pn": "866",
   "abstract": [
    "This paper presents the PhonDat data collection and processing within the Verbmobil project. It gives an overview of the current size of the corpus and the tools and manuals that have been developed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-197"
  },
  "chan95_eurospeech": {
   "authors": [
    [
     "Dominic",
     "Chan"
    ],
    [
     "Adrian",
     "Fourcin"
    ],
    [
     "Dafydd",
     "Gibbon"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Mark",
     "Huckvale"
    ],
    [
     "George",
     "Kokkinakis"
    ],
    [
     "Knut",
     "Kvale"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Borge",
     "Lindberg"
    ],
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Jiannis",
     "Mouropoulos"
    ],
    [
     "Franco",
     "Senia"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Corin 't",
     "Veld"
    ],
    [
     "Jerome",
     "Zeiliger"
    ]
   ],
   "title": "EUROM - a spoken language resource for the EU - the SAM projects",
   "original": "e95_0867",
   "page_count": 4,
   "order": 198,
   "p1": "867",
   "pn": "870",
   "abstract": [
    "A summary of the progress of development and the current realisation of a CDrom based spoken language resource for 11 languages of the European Union is given; the physical conditions basic to its acquisition are defined and the criteria guiding its poly-language structures briefly outlined.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-198"
  },
  "fink95_eurospeech": {
   "authors": [
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Michaela",
     "Johanntokrax"
    ],
    [
     "Brigitte",
     "Schaffranietz"
    ]
   ],
   "title": "A flexible formal language for the orthographic transcription of spontaneous spoken dialogues",
   "original": "e95_0871",
   "page_count": 4,
   "order": 199,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "Orthographic transcriptions of speech are important in most fields of research concerned with spoken language. For spontaneous speech they have to be created manually, resulting potentially in inconsistent or erroneous transcriptions. We propose a new flexible and easy-to-use formal language for the orthographic transcription of spontaneous speech. All relevant phenomena introduced by spontaneous spoken dialogues are covered. The transcription serves as a meta-language from which various representations for different research purposes can be generated automatically.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-199"
  },
  "wang95b_eurospeech": {
   "authors": [
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Design and implementation of Mandarin speech database in taiwan",
   "original": "e95_0875",
   "page_count": 3,
   "order": 200,
   "p1": "875",
   "pn": "877",
   "abstract": [
    "A project to collect Mandarin speech data across Taiwan (MAT) is conducted by a group of researchers working in the area of speech signal processing. Several universities and research institutes in Taiwan will contribute to this cooperative project. The speech data of 5000 speakers will be collected through telephone system. Four types of speech databases will be produced for different application purposes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-200"
  },
  "morin95_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Morin"
    ],
    [
     "Ted H.",
     "Applebaum"
    ]
   ],
   "title": "Word hypothesizer based on reliably detected phoneme similarity regions",
   "original": "e95_0897",
   "page_count": 4,
   "order": 201,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "This paper presents a time and memory efficient multistage word candidate hypothesizer suitable for medium-size vocabulary applications on small hardware. It is based on a novel compact speech representation: regions of high phoneme similarity values. The processing stages of the word hypothesizer are applied in sequence to reduce the search space for a more computationally expensive fine match word recognition system. The paper also presents a scoring procedure for combining information from each stage of the hypothesizer with the output of the fine match procedure to produce the final word decision. On a 100 word task, use of the word hypothesizer reduced alignment complexity by 93% (compared to exhaustive search by the fine match alone), with significant error rate reduction for clean and noisy test data due to score combination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-201"
  },
  "ortmanns95_eurospeech": {
   "authors": [
    [
     "S.",
     "Ortmanns"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Experimental analysis of the search space for 20 000-word speech recognition",
   "original": "e95_0901",
   "page_count": 4,
   "order": 202,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "In this paper we investigate the search effort for large vocabulary continuous speech recognition. In particular, we study the effect of different pruning techniques on the search effort and on search errors. The experimental results show that it is much more efficient in the search procedure to use a tree lexicon than a linear lexicon. For the tree search method, we study the search space in detail. For the 20 000-word task under consideration, a reasonable compromise between the search effort and the recognition accuracy can be achieved by an average number of 13 000 state hypotheses per time frame. This effort is five orders of magnitude lower than the potential size of the search space. All experiments are based on our phoneme-based large vocabulary speech recognition system used in the 1994 ARPA benchmark test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-202"
  },
  "wang95c_eurospeech": {
   "authors": [
    [
     "Ming-Sheng",
     "Wang"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "Speech parsing by downward request search based on the divide and conquer method",
   "original": "e95_0905",
   "page_count": 4,
   "order": 203,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "In this paper, we report a speech parsing scheme with a new search method. The approach is based on the Divide and Conquer(DC) strategy. By dividing a given phonetic lattice, we construct a PD-tree to represent all the possible parsing paths generated from the lattice. Using the PD-tree, we introduce a parsing procedure called DC parsing, propose a new search method called DR search. This approach gives us the optimal or iV-best sentence candidates, and generates only a small hypothesis space, for a given phonetic lattice. Effectiveness of the approach was proved by the simulation experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-203"
  },
  "waast95_eurospeech": {
   "authors": [
    [
     "Claire",
     "Waast"
    ],
    [
     "Lalit",
     "Bahl"
    ],
    [
     "Marc",
     "El-Beze"
    ]
   ],
   "title": "Fast match based on decision tree",
   "original": "e95_0909",
   "page_count": 4,
   "order": 204,
   "p1": "909",
   "pn": "913",
   "abstract": [
    "In a large vocabulary speech recognition system using hidden Markov models, calculating the likelihood of an acoustic signal segment for all words in the vocabulary involves a large amount of computation. We describe in this paper a scheme to rapidly obtaining an approximate acoustic match for all words in the vocabulary in such a way as to ensure that the correct word is one of a small number of words examined in detail. Using a decision tree method we obtain a matching algorithm that is much faster than common acoustic likelihood computation on all the words. This method has been tested on isolated syllables.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-204"
  },
  "noda95_eurospeech": {
   "authors": [
    [
     "Yoshiaki",
     "Noda"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Fast and accurate beam search using forward heuristic functions in HMM-LR speech recognition",
   "original": "e95_0913",
   "page_count": 4,
   "order": 205,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "This paper describes the utilization of a forward heuristic function to achieve fast and accurate beam search in HMM-LR continuous speech recognition. We also discuss \"dynamic rejection\" as a means to reject out-of-grammar utterances during the search in the same framework. The key idea is utilizing the forward heuristic function to compensate for the time differences among the partial hypotheses. It has been experimentally shown that this approach drastically reduces computational cost. Moreover, it offers parallel processing capability with speech input to realize a real-time system. The new concept in dynamic rejection is the \"0th hypothesis\", whose the accumulated likelihood function is defined as the forward heuristic function. By using the \"0th hypothesis\" score as the base score, dynamic rejection operates by pruning in the framework of beam search. The ideas presented in this paper are widely applicable to time-asynchronous approaches to speech recognition with reduced computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-205"
  },
  "colton95_eurospeech": {
   "authors": [
    [
     "Don",
     "Colton"
    ],
    [
     "Mark",
     "Fanty"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Utterance verification improves closed-set recognition and out-of-vocabulary rejection",
   "original": "e95_1067",
   "page_count": 4,
   "order": 206,
   "p1": "1067",
   "pn": "1071",
   "abstract": [
    "We report on utterance verification of putative recognitions in both open-set and closed-set recognition tasks using telephone speech. For open-set recognition, we report on rejection of out-of-vocabulary utterances. In a two-keyword task (\"male\" and \"female\") using 50% out-of-vocabulary utterances, utterance verification reduced errors by 60%, from 12% to 4.8% compared to our baseline rejection strategy. For closed-set recognition, we report on re-ordering the N-best hypotheses. In a 58-phrase task, utterance verification reduced closed-set recognition errors by 30%, from 6.5% to 4.5%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-206"
  },
  "jimenez95_eurospeech": {
   "authors": [
    [
     "V. M.",
     "Jimenez"
    ],
    [
     "A.",
     "Marzal"
    ],
    [
     "J.",
     "Monné"
    ]
   ],
   "title": "A comparison of two exact algorithms for finding the n-best sentence hypotheses in continuous speech recognition",
   "original": "e95_1071",
   "page_count": 4,
   "order": 207,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "Two efficient and exact algorithms for computing the N best sentence hypotheses in continuous speech recognition are studied and compared. Both procedures are first stated as algorithmic solutions to the N shortest paths problem in graphs. The first algorithm is based on the general A* search strategy. The second one is based on a recursive procedure that enumerates shortest paths ending at a given node and is referred to as the Recursive Enumeration Algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-207"
  },
  "takeda95_eurospeech": {
   "authors": [
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Masaki",
     "Naito"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Top-down speech detection and n-best meaning search in a voice activated telephone extension system",
   "original": "e95_1075",
   "page_count": 4,
   "order": 208,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "In this paper, a robust speech detection method and an effective N-best search method are proposed. In the proposed speech endpoint detection method, the robustness to varying speech level is improved by using the likelihood of partially matched word sequences in contrast with short time speech level used in conventional methods. As a result, degradation of recognition accuracy due to failure of endpoint detection is very small even at the SNR of 7 dB, where speech detection using speech level does not work at all. In the proposed N-best search method, the effectiveness of keeping candidates is improved by merging the word sequences whose meanings are identical. By reducing the number of candidates, the time for reordering the N-best candidates can be reduced to one fourth without any degradation of recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-208"
  },
  "seide95_eurospeech": {
   "authors": [
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Fast likelihood computation for continuous-mixture densities using a tree-based nearest neighbor search",
   "original": "e95_1079",
   "page_count": 4,
   "order": 209,
   "p1": "1079",
   "pn": "1083",
   "abstract": [
    "The Philips automatic train timetable information system AIS provides accurate information about train connections between more than 1100 German cities over the telephone [2]. Its speaker-independent speech recognizer is monophone-based and uses continuous-mixture densities. Most of the CPU time is spent on log-likelihood computation. For realtime operation, the number of densities had to be limited, sacrificing accuracy. To overcome this restriction, we developed a fast hierarchical within-mixture nearest neighbor search with logarithmic computational effort. The method degrades recognition accuracy by roughly 2-7% rel., but on the other hand allows for a larger number of densities to be processed. With the new method, the AIS log-likelihood computation was accelerated by a factor of nine retaining optimal accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-209"
  },
  "beyerlein95_eurospeech": {
   "authors": [
    [
     "Peter",
     "Beyerlein"
    ],
    [
     "Meinhard",
     "Ullrich"
    ]
   ],
   "title": "Hamming distance approximation for a fast log-likelihood computation for mixture densities",
   "original": "e95_1083",
   "page_count": 4,
   "order": 210,
   "p1": "1083",
   "pn": "1086",
   "abstract": [
    "A computationally very expensive task arising within speech recognition systems using continuous mixture density HMMs is the log-likelihood computation. In the Philips large-vocabulary continuous-speech recognition system it consumes 50% - 75% of the decoding time. In our system the log-likelihood computation amounts to a nearest-neighbor search, i.e. to a search for the component density of a mixture density whose mean vector has a minimal distance to the observed feature vector. In this paper, we show that a Hamming Distance Approximation (HDA) of the angles between the vectors leads to a powerful nearest-neighbor search technique with negligible memory demands. Thus the likelihood-computation was sped up by a factor of 10 without significant increase in the word error rate of our large vocabulary speech recognizer. Since the likelihood-computation in this system consumed 66% of the recognition runtime, the overall decoding runtime could be reduced by a factor of 2.5. We also report results on Tl-digits and the WSJ task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-210"
  },
  "komori95_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Masayuki",
     "Yamada"
    ],
    [
     "Hiroki",
     "Yamamoto"
    ],
    [
     "Yasunori",
     "Ohora"
    ]
   ],
   "title": "An efficient output probability computation for continuous HMM using rough and detail models",
   "original": "e95_1087",
   "page_count": 4,
   "order": 211,
   "p1": "1087",
   "pn": "1090",
   "abstract": [
    "The paper presents an efficient computation algorithm of the output probability for a continuous HMM speech recognizer using a rough and detail HMM combination. In general, the more number of mixtures or the more number of contextual classes, the better accuracy with the heavier computation, and vice versa. The proposed algorithm first estimates the state output probabilities using the rough HMMs and then re-estimates those of the probable states using detail HMMs. We proposed two realizations for the algorithm and carried out experiments for each. Both results showed about 60% or 70% reduction of the output probability calculation with no reduction of recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-211"
  },
  "fritsch95_eurospeech": {
   "authors": [
    [
     "J.",
     "Fritsch"
    ],
    [
     "I.",
     "Rogina"
    ],
    [
     "Tilo",
     "Sloboda"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Speeding up the score computation of HMM speech regognizers with the bucket voronoi intersection algorithm",
   "original": "e95_1091",
   "page_count": 4,
   "order": 212,
   "p1": "1091",
   "pn": "1094",
   "abstract": [
    "With increasing sizes of speech databases, speech recognizers with huge parameter spaces have become trainable. However, the time and memory requirements for high accuracy realtime speaker-independent continuous speech recognition will probably not be met by the available hardware for a reasonable price for the next few years. This paper describes the application of the Bucket Voronoi Intersection algorithm to the JANUS-2 speech recognizer, which reduces the time for the computation of HMM emission probabilities with large Gaussian mixtures by 50% to 80%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-212"
  },
  "nouza95_eurospeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "On the speech feature selection problem: are dynamic features more important than the static ones?",
   "original": "e95_0919",
   "page_count": 4,
   "order": 213,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "The problem of the optimal speech feature selection with respect to CDHMM discrete-utterance recognition is addressed in the paper. Two sequential search algorithms - the Sequential Forward Search (SFS) and Sequential Floating Forward Search (SFFS) have been investigated and applied to several speech databases. It was observed that the dynamic (delta) parameters derived from frame energy and cepstrum were identified as more important than the static ones. The hypothesis about the high discriminative power of the dynamic features has been confirmed in a series of speaker-independent tests. We have even demonstrated that the recognition rate achieved with only the dynamic features closely approached that of the complete feature set. As a practical application of the study, we have proposed a two-level HMM classification scheme that may significantly reduce recognition time without a loss of accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-213"
  },
  "nadeu95_eurospeech": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Pau",
     "Paches-Leal"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Filtering the time sequence of spectral parameters for speaker-independent CDHMM word recognition",
   "original": "e95_0923",
   "page_count": 4,
   "order": 214,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "In this work, we show how speaker-independent CDHMM word recognition performance can be significantly improved for clean speech by filtering the time sequence of spectral parameters to enhance its time dynamics. Experimental results with the standard TI connected digits database show the filter can achieve more than 30% reduction of string recognition error. As shown in this paper, that improvement is partially due to the speaker variability reduction obtained by attenuating the very low modulation frequencies. The widely used cepstral mean subtraction technique also improves the recognition rate, but it can not achieve such a noticeable improvement as the parameter filter. In fact, the best results are obtained when the peak of the long-term spectrum of the filter output is at around 3 Hz, a frequency which corresponds to the average syllable rate of the employed database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-214"
  },
  "tambakas95_eurospeech": {
   "authors": [
    [
     "Dimitris",
     "Tambakas"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Robust phoneme prototype extraction for speech recognition",
   "original": "e95_0927",
   "page_count": 4,
   "order": 215,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "In this paper we describe the procedure for establishing a robust set of phoneme prototypes suitable for Speaker Adaptable Isolated Word Speech Recognition (IWSR) systems. The procedure is based on a K-Means cluster analysis of a large multispeaker phoneme data base. The prototypes were tested on a large vocabulary IWSR system for Greek developed in the framework of the ESPRIT 2104 project, POLYGLOT-I [1], and the results obtained showed a significant improvement in the accuracy of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-215"
  },
  "hunt95_eurospeech": {
   "authors": [
    [
     "Melvyn J.",
     "Hunt"
    ]
   ],
   "title": "The even transform: a variance-equalizing orthogonal transformation and its application to speech recognition",
   "original": "e95_0931",
   "page_count": 4,
   "order": 216,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "A linear transformation is described that turns a set of variables into a new set all having the same variance. The number of new variables is at least that of the original set and optionally more. This reversible transformation can be used to encode data efficiently when the precision available is limited and is the same for all variables. In tests with synthetic data, it encoded information more efficiently than simple scaling methods. Also, because the transformation is orthonormal, it can be used directly in speech recognition systems that use unweighted Euclidean distances. In speaker-independent alphabet recognition tests with telephone speech, scaling the variables into four bits caused a 34% increase in error rate. Applying the transformation before scaling, however, resulted in only a 5% rise. Finally, spreading the information in the original 16 variables over 32 transformed variables before scaling resulted in an increase of just 3%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-216"
  },
  "hubener95_eurospeech": {
   "authors": [
    [
     "Kai",
     "Hübener"
    ]
   ],
   "title": "Using segmental coefficients in HMM speech recognition",
   "original": "e95_0935",
   "page_count": 4,
   "order": 217,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "This paper presents a new kind of acoustic features for HMM speech recognition. These features try to capture phone-specific segmentation information using multiple temporal resolutions. Experiments show that word accuracy is improved by 7% when combining these features with traditional mel-cepstral coefficients in a speaker-independent word recogniser. This improvement is mostly due to a reduced number of segmentation errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-217"
  },
  "freitag95_eurospeech": {
   "authors": [
    [
     "F.",
     "Freitag"
    ],
    [
     "E.",
     "Monte"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "On the use of the derivative of the pole trajectories of the LPC analysis parameter sequence as an alternative to delta parameters",
   "original": "e95_1373",
   "page_count": 3,
   "order": 218,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "In this paper a new approach for modelling time variations in the speech spectra is presented. We propose to approximate the trajectories of the frequency and amplitude of the poles of the LPC spectra with exponential functions. The obtained time constants of the exponential functions are incorporated in the observation vector and used for recognition in an HMM based recognition system. The performance of the new parameters is tested using a database of connected digits. The recognition rates obtained with the new parameters are compared to results obtained with delta parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-218"
  },
  "rao95_eurospeech": {
   "authors": [
    [
     "P. V. S.",
     "Rao"
    ],
    [
     "R.",
     "Raveendran"
    ]
   ],
   "title": "On the dual role of sequence directionality and coherence in a spectral predictive discrimination model",
   "original": "e95_1377",
   "page_count": 4,
   "order": 219,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "A spectral prediction model is studied in the context of discriminating between the stop consonants by place of articulation. We study the predictor in both scalar and vector modes and show the sensitivity of both these models to sequence direction. The paper highlights the role of coherence in regulating the directionality dependence of performance. The predictor model is discussed from the multivariate autoregressive process point of view to throw some light on the observed interesting behaviour of the model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-219"
  },
  "nadeu95b_eurospeech": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Monica",
     "Gorricho"
    ]
   ],
   "title": "On the decorrelation of filter-bank energies in speech recognition",
   "original": "e95_1381",
   "page_count": 4,
   "order": 220,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "Cepstral coefficients are widely used in speech recognition. In this paper, we claim that they are not the best way of representing the spectral envelope, at least for some usual speech recognition systems. In fact, cepstrum has several disadvantages: poor physical meaning, need of transformation, and low capacity of adaptation to some recognition systems. In this paper, we propose a new representation that significantly outperforms both mel-cepstrum and LPC-cepstrum techniques in both recognition rate and computational cost. It consists of filtering the frequency sequence of filter-bank energies with an extremely simple filter that equalizes the variance of the cepstral coefficients. Excellent results of the new technique using a continuous observation density HMM recognition system and two very different recognition tasks, connected digits and phone recognition, are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-220"
  },
  "junqua95_eurospeech": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "J.-F.",
     "Mari"
    ],
    [
     "Ted H.",
     "Applebaum"
    ],
    [
     "Brian A.",
     "Hanson"
    ]
   ],
   "title": "Time derivatives, cepstrai normaiization, and spectral parameter filtering for continuously spelled names over the telephone",
   "original": "e95_1385",
   "page_count": 4,
   "order": 221,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "In this paper, we focus on spectral parameter filtering for reducing the mismatch between training and testing and report experimental results on a continuously spelled name recognition task over the telephone. We studied various time derivative feature combinations, the influence of RASTA processing, short-term and long-term cepstrai mean normalization, and the influence of the amount of training data on recognition performance. Based on the results of these experiments, we derived a new front-end for our task, leading to an error rate improvement of almost 30% in name retrieval as compared to previous published results. We also discuss the interaction between the different techniques studied when used in combination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-221"
  },
  "djezzar95_eurospeech": {
   "authors": [
    [
     "Linda",
     "Djezzar"
    ]
   ],
   "title": "Some new considerations about the spectral form of French stop bursts",
   "original": "e95_1389",
   "page_count": 4,
   "order": 222,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "This paper explores the claim that the shape of the onset spectrum provides invariant acoustic properties for stop place of articulation, but for French stop consonants. To this end, we carried out an acoustic analysis of 750 burst stimuli extracted from natural speech. Results showed that, contrary to the invariance theory, the global form of the spectrum is not invariant (i.e. context-independent). The acoustic properties residing in this gross shape must be extracted and exploited according to the vowel nature. In order to distinguish the palatovelars from the labials and the dentals, we implemented a contextual method which extracts the compactness property. In 94% of the time, our method appropriately discriminated between the two stop categories.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-222"
  },
  "rao95b_eurospeech": {
   "authors": [
    [
     "P. V. S.",
     "Rao"
    ],
    [
     "R.",
     "Raveendran"
    ]
   ],
   "title": "Characterization of spectral transition region by various prediction approaches for discriminating stop consonants",
   "original": "e95_1393",
   "page_count": 4,
   "order": 223,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "A comparative study of a scalar waveform modelling approaches based on linear, quadratic and neural prediction techniques was performed. A linear spectral predictor model was also studied on two spectral representations. For this, we consider the task of discriminating the places of articulation of stop consonants using their spectral transition information. A modified error measure has been defined and shown to be effective in reducing the pitch interference effect that arise in directly modelling the waveform. The importance of complete spectral information of the vocal tract system dynamics and the significance of higher order correlation in the recognition performance have been highlighted.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-223"
  },
  "vorstermanst95_eurospeech": {
   "authors": [
    [
     "A.",
     "Vorstermanst"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Fast automatic segmentation and labeling: results on TIMIT and EUROMO",
   "original": "e95_1397",
   "page_count": 4,
   "order": 224,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "In this article, a fast automatic segmentation and labeling system is presented. The system is capable of labeling speech originating from most languages without requiring extensive linguistic knowledge or large (manually segmented and labeled) databases of that language. The system comprises small neural networks for the segmentation and the broad phonetic classification. They were originally trained on one task, and automatically adapted to a new task. Due to the limited size of the neural networks, the segmentation and labeling strategy requires but a limited amount of computations, and the adaptation to a new task can be accomplished very quickly. The performance of our system on TIMIT and on the English, Danish and Italian portions of the EUROMO compares favourably to that of other systems reported in the literature.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-224"
  },
  "ramsay95_eurospeech": {
   "authors": [
    [
     "Gordon",
     "Ramsay"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Maximum-likelihood estimation for articulatory speech recognition using a stochastic target model",
   "original": "e95_1401",
   "page_count": 4,
   "order": 225,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "A stochastic framework for articulatory speech recognition is presented. Utterances are described in terms of overlapping phonological units built into a Markov chain, where each state is identified with a set of acoustic/articulatory correlates represented by a target distribution on an articulatory space. Articulator motion is modelled by a Markov-modulated stochastic linear dynamical system, and observations of the articulatory state are generated in an acoustic space through a non-linear mapping. Procedures for state and parameter estimation are outlined based on the EM algorithm and extended Kalman filtering techniques, and illustrated using artificial data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-225"
  },
  "sirigos95_eurospeech": {
   "authors": [
    [
     "J.",
     "Sirigos"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "A comparison of several speech parameters for speaker independent speech recognition and speaker recognition",
   "original": "e95_1407",
   "page_count": 3,
   "order": 226,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "In this paper we present a comparison of several parameters that are commonly used in the fields of speech and speaker recognition. Analysis of the variance was the evaluation method employed. The parameters that were tested were the Mel CEPSTRAL [1] parameters, the LPC parameters derived from a PLP2] analysis, the LPC and Mel CEPSTRAL parameters derived from RASTA-PLP [3] analysis and all the differential parameters of the above (A-Mel Cepstral, e.t.c). The testing procedure used a 500-word speech database of the Greek language. We concluded with several results which show the performance of the different parameters for speech and speaker recognition, for each phoneme category and for each sex.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-226"
  },
  "bitar95_eurospeech": {
   "authors": [
    [
     "Nabil N.",
     "Bitar"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Speech parameterization based on phonetic features: application to speech recognition",
   "original": "e95_1411",
   "page_count": 4,
   "order": 227,
   "p1": "1411",
   "pn": "1414",
   "abstract": [
    "This paper presents a feature-based representation (FBR) of speech that is motivated by phonetic-feature theory. Presently, only the manner features: sono-rant, syllabic, nonsyllabic, non-continuant and fricated are considered. The objectives of such a representation are to directly target the linguistic information in the signal and to minimize other extra-linguistic information that may yield large speech variability. To aid in this goal, FBR was defined in a relational manner across time and frequency. Preliminary results using FBR and the Hidden Markov model (HMM) approach in a broad-class recognition task suggest that FBR was able to target much of the phonetically relevant information. Results comparable to an HMM using a cepstral-based representation (CBR) were obtained when one-mixture probability density functions were used. As the number of mixtures was increased, CBR outperformed FBR primarily because fine phonetic details, that go beyond the manner features, were captured in the multi-mixture CBR-HMM's.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-227"
  },
  "beulen95_eurospeech": {
   "authors": [
    [
     "K.",
     "Beulen"
    ],
    [
     "L.",
     "Welling"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Experiments with linear feature extraction in speech recognition",
   "original": "e95_1415",
   "page_count": 4,
   "order": 228,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "In this paper we investigate Linear Discriminant Analysis (LDA) for the TI connected digit recognition task (TI task) and the Wall Street Journal large vocabulary recognition task (WSJ task). In addition to previous variants of LDA implementations, we avoided the explicit incorporation of derivatives in the acoustic vector. Instead a sliding window without derivatives was used. This large-sized vector was then taken to extract the features by an LDA transformation. Tests for this feature generation were performed both for Laplacian and Gaussian densities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-228"
  },
  "westendorf95_eurospeech": {
   "authors": [
    [
     "Christian-M.",
     "Westendorf"
    ]
   ],
   "title": "Nonlinear Feature Transformation Based On Statistical Phoneme Modeling",
   "original": "e95_1419",
   "page_count": 4,
   "order": 229,
   "p1": "1419",
   "pn": "1422",
   "abstract": [
    "This paper deals with the problem of robust nonlinear feature transformation of speech signal. Starting with a conventional signal analysis a traina-ble nonlinear mapping based on statistical density modeling is proposed. The statistical model was represented by the parametric Gaussian distribution as well as by a nonparameteric histogram-based approach. The result of the transformation is a vector containing the a-posteriori-probabilities of a set of selected phonemic categories. Experiments were carried out using fluently spoken sentences from the German PHONDAT-I database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-229"
  },
  "navarromesa95b_eurospeech": {
   "authors": [
    [
     "Juan L.",
     "Navarro-Mesa"
    ],
    [
     "Asunción",
     "Moreno"
    ]
   ],
   "title": "Skewness and nonstationarity measures applied to reliable speech endpoint detection",
   "original": "e95_1423",
   "page_count": 4,
   "order": 230,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "In this paper we are addressed to the problem of speech presence and silences detection. Speech signals possess properties to which higher order statistics are sensitive. Bispectral-based statistics have been proved a good alternative [3] to energy-based statistics in a speech presence detector scheme, but at the cost of computations. Three alternative methods are proposed for a reliable and more computationally efficient detection. Two, lag-and frequency-domain, methods for the computation of the IT and the OT contribution to total skew while keeping the ability of the statistic in [3]. And a method based in the integrated polyspectrum (IP).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-230"
  },
  "sarukkai95_eurospeech": {
   "authors": [
    [
     "Ramesh R.",
     "Sarukkai"
    ],
    [
     "Dana H.",
     "Bollard"
    ]
   ],
   "title": "The distance set representation of speech segments",
   "original": "e95_1427",
   "page_count": 4,
   "order": 231,
   "p1": "1427",
   "pn": "1430",
   "abstract": [
    "This paper evaluates the discriminative ability of nonsequential representations of speech segments. Variable duration speech segments are represented as fixed dimensional set representations, which are then clustered using LVQ2.1 to provide nonsequential templates. Several set representations are evaluated: 1) quantized vector set representation which indicate the presence/ absence of a vector quantized index in the speech segment; 2) a multi-set counting set representation which encodes the number of occurrences of the quantized features within the speech segments; 3) the distance set representation wherein each feature present in a particular speech segment is represented as a Gaussian probability distribution based on the Euclidean distance to every codebook vector; these distributions are then averaged to provide a template distance set representation for that particular speech token.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-231"
  },
  "dobrisek95_eurospeech": {
   "authors": [
    [
     "S.",
     "Dobrisek"
    ],
    [
     "R.",
     "Mihelic"
    ],
    [
     "N.",
     "Pavesic"
    ]
   ],
   "title": "Multi-variate mixture probability density modelling of VQ codebook using gradient descent algorithm",
   "original": "e95_1431",
   "page_count": 4,
   "order": 232,
   "p1": "1431",
   "pn": "1434",
   "abstract": [
    "A vector quantisation codebook can be modelled as a set of probability density functions. The problem of estimating the parameters determining mixture probability density models can be solved using a log-likelihood based re-estimation procedure. On the other hand, this problem can be also viewed as a conventional optimisation problem. Consequently, gradient descent techniques may be used to obtain values of the model parameters. The main advantage of these techniques over the re-estimation procedure is higher robustness due to an initial estimation of the model parameters. In the paper, we describe a descent algorithm along with a criterion function, we propose. We obtained some promising results by applying this algorithm to one and two-variate pseudo Gaussian mixture probability density functions and further to signal vectors of a continuous speech database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-232"
  },
  "sluijter95_eurospeech": {
   "authors": [
    [
     "Agaath M. C.",
     "Sluijter"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Intensity and vocal effort as cues in the perception of stress",
   "original": "e95_0941",
   "page_count": 4,
   "order": 233,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "In this study, the traditional claim that loudness is a weak cue in the perception of linguistic stress is reconsidered. This claim is based on perception experiments in which loudness was varied in a naive way: all parts of the spectrum were increased by the same amount of energy. In an earlier study we found that if a speaker produces stressed syllables higher frequencies increase more than lower frequencies. Varying loudness in this way would therefore be more realistic, and should bring its true cue value to the surface. Results of a perception experiment bear out that realistic loudness manipulations (i.e., concentrated in the higher frequency bands) provide stronger stress cues than traditional intensity differences, and are close in strength to duration differences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-233"
  },
  "owsianny95_eurospeech": {
   "authors": [
    [
     "Mariusz",
     "Owsianny"
    ]
   ],
   "title": "The effect of voice pitch on perception of synthetic Polish vowels",
   "original": "e95_0945",
   "page_count": 4,
   "order": 234,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "The effect of interrelation between voice pitch and formant frequencies on the perception of synthetic vowels was investigated. Using a software cascade parallel formant speech synthesizer SMOK, modelled on the system of D. H. Klatt and capable of producing high quality speech, six Polish vowels /i/, /i/, /e/, /a/, /o/, /u/ spoken by male and female speakers were resynthesised. Those prototypical vowels were subsequently modified by varying fundamental frequency F0 in the range of ±1 octave in relation to the original values. 15 subject (10 male and 5 female) participated in the listening experiments. Their task was to identify vowel stimuli presented in random order. The results indicate that it is not formant frequencies alone, but also their relation to F0. That is of crucial importance in the perceptual identification of vowels. Discrepancy between formant frequencies and voice pith may lead to a shift in phonetic category.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-234"
  },
  "house95_eurospeech": {
   "authors": [
    [
     "David",
     "House"
    ]
   ],
   "title": "Perception of prepausal tonal contours: implications for automatic stylization of intonation",
   "original": "e95_0949",
   "page_count": 4,
   "order": 235,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "Interactive adjustment tests were carried out to determine how a silent interval influences the perception of the preceding tonal contour. Results from 16 subjects show a strong influence of silence on tonal perception indicating that silence increases sensitivity for the preceding tonal endpoint with subjects showing greatest response consistency for the stimuli with the longest pause where adjustment is based on endpoint frequency in a nasal consonant before the pause. The implications of these results for automatic stylization and models of intonation are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-235"
  },
  "takara95_eurospeech": {
   "authors": [
    [
     "Tomio",
     "Takara"
    ]
   ],
   "title": "Experimental study on perception of the glottal explosive of the Japanese ryukyu dialect",
   "original": "e95_0953",
   "page_count": 4,
   "order": 236,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "This paper reports on the study of the phonetic features of the glottal explosive (?] ol* the Japanese Ryukyu dialect. First, we analyzed wave forms of minimal pair words concerned with the glottal explosive (/?/) and without (expressed by II). It was found that the pitch and the power rose in 10ms to 20ms at the beginning of the words with /?/ and 120ms to 130ms of the words with /7. Next, we performed listening tests using synthesized speech to evaluate the perceptional contribution of pitch, spectrum and power patterns to the phonetic discrimination. It was found that the contribution was greater in the order of pitch, spectrum and power. A further listening test was performed, and it was found that the discriminative threshold between the phonemes of the rising pitch was about 50ms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-236"
  },
  "dalessandro95_eurospeech": {
   "authors": [
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "S.",
     "Rosset"
    ],
    [
     "O.",
     "Piot"
    ]
   ],
   "title": "Measurement of pitch perception for F0 glides",
   "original": "e95_0957",
   "page_count": 4,
   "order": 237,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "Measurements of pitch perception for F0 glides are described. The method of adjustment was prefered. Stimuli were FO glides centered at 220 Hz. The parameters under study were: the pitch glide extent (0, 0.2, 0.4, 0.8, 1.5, 3, 6, and 12 SemiTones (ST), i.e. 0, 2.5, 5, 10.17, 18.74, 38.17, 76.63, 155.56 Hz), the tone duration (50, 100, 200 and 300 ms), the pitch glide direction (rising or falling), and the extremity of pitch glide matched (beginning or end). The interpretation of the results shows that: 1/ perception correspond to a time-average of the frequencies present; 2/ the the higher extremity of the glides seems more important; 3/ there is an assymetry between rises and falls. The experimental results are also compared to models of pitch integration for F0 glides, namely Rossi's 2/3 rule, and d'Alessandro and Castellengo's weighted time-average model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-237"
  },
  "wauquiergravelines95_eurospeech": {
   "authors": [
    [
     "S.",
     "Wauquier-Gravelines"
    ]
   ],
   "title": "Interferences between phonemes: evidence for \"perceptual domains\" in continuous speech perception",
   "original": "e95_0963",
   "page_count": 4,
   "order": 238,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "This paper presents 3 experiments in which the detection of initial vowel monitoring was performed on words preceded by critical words containing a phoneme similar to the target. Results show that in this case, the detection time of the target is increased. But we observe that this \"interference effect\" only emerges when the two words appear in the same prosodic group. This suggests that the interference effect seems to be sensitive to prosodic information. Thus, it could be tool to investigate the area of prosodic segmentation of speech. Key-words: Psycholingui sties, Speech Segmentation, Prosody.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-238"
  },
  "son95_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The influence of local context on the identification of vowels and consonants",
   "original": "e95_0967",
   "page_count": 4,
   "order": 239,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "Theories on the mechanisms of phoneme identification generally involve only the actual segment and the transitions to and from neighbouring segments. In two listening experiments we tested the importance for vowel and consonant identification of the presence of speech segments beyond the transition parts. The results clearly show that identification continues to improve when speech is added beyond the boundaries of the transitions to neighbouring phonemes. Adding speech in front of the target segment improved identification more than adding speech at the back of the target segment, even if the sound added in front was actually not part of the target phoneme itself. We also show that the identification of consonantal segments depends on the correct identification of the vowel, and vice versa, in CV-type stimuli, but not in VC-type stimuli. From these results, we conclude that context beyond the CV- and VC-transitions is used for both consonant and vowel identification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-239"
  },
  "ainsworth95b_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Effect of preceding noise duration on the perception of voiced plosives and vowels",
   "original": "e95_0971",
   "page_count": 4,
   "order": 240,
   "p1": "971",
   "pn": "974",
   "abstract": [
    "Previous work has shown that speech syllables in continuous noise are easier to recognise than the same syllables in noise which is turned on at the beginning of the syllable and off at the end (co-gated noise). A possible explanation of this is that the firing thresholds of neurones in the cochlear nerve are adapted by the continuous noise so that their dynamic ranges are more appropriate for the speech in noise ([1], [2]). If this is so noise of different durations preceding the syllables should adapt these thresholds by different amounts and so affect the recognition scores of the syllables. In order to test this hypothesis experiments have been performed to measure the recognition rates of plosive-vowel syllables as a function of preceding noise duration. It was found that noise of relatively short durations (of the order of 100 ms) decreases the recognition rate of both the plosives and the vowels but noise of longer duration increases the recognition rate. In order to account for these findings it is suggested that perceptual streaming takes place as well as noise adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-240"
  },
  "bonneau95_eurospeech": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Linda",
     "Djezzar"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Influence of a prior knowledge of the vocalic context on stop burst perception",
   "original": "e95_0975",
   "page_count": 4,
   "order": 241,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "The aim of this paper is to investigate the influence of a prior knowledge of the vocalic context on stop burst perception. A previous experiment [2], during which the listeners were unaware of the identity of the following vowel, showed that bursts provided very reliable information about stop place but also that the performance variation significantly depended on the syllable and on the following vowel. We used fixed-length burst stimuli of approximately 25 ms in which all traces of vocalic segment have been set off. We report another experiment concerning the perception of the same stimuli but during which the identity of the following vowel was specified to the subjects. Results showed that the specification of the vowel identity led to a slight but statistically significant improvement in stop recognition. Nevertheless, the effects of this knowledge were selective and varied with the context.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-241"
  },
  "donselaar95_eurospeech": {
   "authors": [
    [
     "Wilma van",
     "Donselaar"
    ]
   ],
   "title": "Listeners' use of the 'information-accentuation' interdependence in processing implicit and explicit references",
   "original": "e95_0979",
   "page_count": 4,
   "order": 242,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "As 'new' words are usually accented and 'given' words unaccented, listeners may use de-accentuation as a cue referring to earlier given information. An auditory verification experiment was carried out to investigate whether listeners benefit from de-accentation in solving anaphoric references during sentence processing. Both implicit, synonymous and explicit, identical referring expressions were investigated. The results of the experiment showed that unaccented synonyms were verified significantly faster than accented synonyms. However, latencies showed no difference between the verification of accented and unaccented identical words. This suggests that de-accentuation of referring expressions may be more important to listeners in solving more implicit anaphoric references than in solving explicit ones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-242"
  },
  "fujisaki95_eurospeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ]
   ],
   "title": "Analysis and modeling of fundamental frequency contours of English utterances",
   "original": "e95_0985",
   "page_count": 4,
   "order": 243,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "Following a review of various approaches to the modeling of F0 contours and a brief description of the authors' model that has been successfully applied to the analysis of F0 contours of Japanese and several other languages, an experiment is described on the application of the model to the analysis of F0 contours of a corpus of 200 English declarative sentences uttered by four native speakers of British English. The results confirmed the model's applicability to British English, at least as far as the present material is concerned, but revealed a considerable amount of individual differences in the patterns of phrasing and accentuation. The implication of the results on the relationship of prosody and syntax is also discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-243"
  },
  "nicolas95_eurospeech": {
   "authors": [
    [
     "P.",
     "Nicolas"
    ],
    [
     "Daniel J.",
     "Hirst"
    ]
   ],
   "title": "Symbolic coding of higher-level characteristics of fundamental frequency curves",
   "original": "e95_0989",
   "page_count": 4,
   "order": 244,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "In this paper we present the results of a study of high-level effects on fundamental frequency curves in French. To observe these we analysed four readings lasting approximately one minute of a text composed of three paragraphs. We used a modelling technique which allowed us to capture a number of features of individual Intonation Units embedded in a sentence, in a paragraph and finally in a text. In our data, we find that the highest intonation structure seems to be the paragraph and we suggest coding this structure for TTS systems which generally do not take into account levels of structure above that of the sentence. Finally, we propose an extension to the INTSINT transcription system in order to cover such paratone effects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-244"
  },
  "ross95_eurospeech": {
   "authors": [
    [
     "Ken",
     "Ross"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "A dynamical system model for recognizing intonation patterns",
   "original": "e95_0993",
   "page_count": 4,
   "order": 245,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "This paper describes a new computational model of prosody for recognizing symbolic intonation patterns, specifically the different tones that mark pitch accents and phrase boundaries. The model represents intonation at multiple levels (segmental, syllable, and phrase levels) to capture acoustic feature dependence at different time scales. We take a probabilistic approach to intonation label recognition that utilizes a state-space dynamical system model at the syllable level. Recognition and training algorithms are described and results are reported from experiments on prosodic labeling of radio news speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-245"
  },
  "hunt95b_eurospeech": {
   "authors": [
    [
     "Andrew J.",
     "Hunt"
    ]
   ],
   "title": "Syntactic influence on prosodic phrasing in the framework of the link grammar",
   "original": "e95_0997",
   "page_count": 4,
   "order": 246,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "This paper presents a new theory of the influence of syntax upon prosodic phrasing. Unlike previous theories which have used constituent tree structures in the analysis of syntax, this theory utilises the syntactic framework provided by either the Link Grammar or a Dependency Grammar. The use of the different syntactic framework leads to other important differences; the role of syntactic function is more important than that of syntactic structure, the relationship between syntax and prosodic phrasing is more direct, and the syntactic influence operates primarily from left to right. The paper presents evidence in support of the theory from experimental tests on two professionally read speech corpora; these tests show that the theory has significant predictive capability and is also effective in resolving syntactic ambiguities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-246"
  },
  "caspers95_eurospeech": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Effects of time pressure on the choice of accent-lending and boundary-marking pitch configurations in dutch",
   "original": "e95_1001",
   "page_count": 4,
   "order": 247,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "In a corpus of normal and fast read-aloud speech material, the phonological choice of pitch accents and melodic boundary markings was compared. Starting from the assumption that speakers will preserve the more important melodic characteristics under time pressure, this comparison may provide insights into the relative importance of the choice of specific pitch configurations. The following questions were asked: is the phonological form of pitch markings altered under time pressure? And if so, is there any evidence for a strategy for phonologically simplifying pitch accents and/or melodic boundary markings when speech rate is increased? Results show that the choice of pitch markings is altered under time pressure in a substantial minority of the cases. The shifts occurring may be interpreted in terms of a semantic simplification strategy: shades of meaning are sacrificed by replacing more marked configurations by unmarked ones under time pressure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-247"
  },
  "sakata95_eurospeech": {
   "authors": [
    [
     "Mayumi",
     "Sakata"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Analysis and synthesis of prosodic features in spoken dialogue of Japanese",
   "original": "e95_1007",
   "page_count": 4,
   "order": 248,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "Prosodic features of spoken dialogue were investigated by comparing them with those of read speech. Analyses were conducted on the fundamental frequency contours with a special focus on their relationship to the linguistic information. Results showed that the increase in the amplitude of accent command for word, usually observable in dialogue speech as compared with read speech, largely depends on the role of word in discourse. The increase were also showed to depend on other linguistic factors, such as the accent type and the position in the sentence. Analyses were conducted also for the speech rate and its dynamical change in a simple sentence was clarified. Prosodic rules were then derived for the synthesis of dialogue speech by modifying those formerly developed for read speech. Although all the findings were not incorporated yet in the rules, results of listening test indicated the appropriateness of the modification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-248"
  },
  "campbell95_eurospeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Prosodic influence on segmental quality",
   "original": "e95_1011",
   "page_count": 4,
   "order": 249,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "This paper shows that the addition of segmental acoustic information can be beneficial to the recognition of prosodic events in English speech. It builds on previous work from Campbell '92 [2] that used measures of segmental duration and energy to detect stress and focus. The present study shows that an increase in detection rate can be gained by incorporation of spectral tilt information and determines the degree to which automatic segmentation and acoustic extraction can be used to detect prominences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-249"
  },
  "petek95_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Petek"
    ]
   ],
   "title": "Towards voice-interactive telephone services in slovenia: on prosody of digits using the sociolinguistic framework",
   "original": "e95_1015",
   "page_count": 4,
   "order": 250,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "This paper presents original results on recognition, confusability and prosodic analyses of spoken digits over the local and long distance telephone lines. A database for the recognition experiments consisted of 300 speaker records of 10 digits from all across Slovenia. Confusion matrix analysis of the best test set recognition performance identified the most confusable word pairs Furthermore, the prosody of 10 Slovenian digits was studied in detail in the context of dialectal structure of Slovenian language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-250"
  },
  "mohler95_eurospeech": {
   "authors": [
    [
     "Gregor",
     "Möhler"
    ],
    [
     "Grzegorz",
     "Dogil"
    ]
   ],
   "title": "Test environment for the two level model of Germanic prominence",
   "original": "e95_1019",
   "page_count": 4,
   "order": 251,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "In this work we present a test bed designed to verify the two level model of Germanic prominence. We give an introduction to the linguistic background of the model and derive the features that the test environment should possess. Finally we describe the details of the implementation in the ESPS/xwaves1 environment. The implementation is based on resynthesis using PSOLA algorithm. As a linguistic application the Tone Sequence Model (TSM) is implemented and tested. Keywords: Prosody, Modelling of F0, PSOLA, Tone-Sequence-Model (TSM)\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-251"
  },
  "dalykelly95_eurospeech": {
   "authors": [
    [
     "Nancy A.",
     "Daly-Kelly"
    ]
   ],
   "title": "Linguistic and acoustic characteristics of pause intervals in spontaneous speech",
   "original": "e95_1023",
   "page_count": 4,
   "order": 252,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "This paper describes linguistic and acoustic characteristics of common disfluencies found in spontaneous speech taken from human/machine dialogues. A new linguistic unit, the pause interval, consisting of hesitated speech, silence and filled pauses, is proposed. Syntactic and acoustic analyses of spontaneous speech support the notion of pause intervals, and their constituents are identifiable with high accuracy in classification experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-252"
  },
  "yamashita95_eurospeech": {
   "authors": [
    [
     "Y.",
     "Yamashita"
    ],
    [
     "R.",
     "Mizoguchi"
    ]
   ],
   "title": "Modeling the contextual effects on prosody in dialog",
   "original": "e95_1329",
   "page_count": 4,
   "order": 253,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "This paper proposes a new method of stochastic modeling of prosodic contextual effects in dialog for the purpose of generating natural spoken language in dialog systems. F0 parameters are quantitatively predicted by two sets of rules; T-rule and D-rule. The T-rule is a conventional rule set for text-to-speech conversion. It models characteristics of isolated utterances without dialog context. The D-rule compensates the T-rule by modifying F0 parameters in accordance with the dialog context after application of the T-rule. The D-rule was inductively learned from the prediction error data which the T-rule resulted for the dialog utterances. The linear regressive method modeled the prediction errors with 7 dialog features manually annotated. The evaluation results of the D-rule are also shown for dialog utterances recorded under two different conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-253"
  },
  "kompe95_eurospeech": {
   "authors": [
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Andreas",
     "Kießling"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Ernst Günter",
     "Schukat-Talamazzini"
    ],
    [
     "A.",
     "Zottmann"
    ],
    [
     "Anton",
     "Batliner"
    ]
   ],
   "title": "Prosodic scoring of word hypotheses graphs",
   "original": "e95_1333",
   "page_count": 4,
   "order": 254,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "Prosodic boundary detection is important to disam-biguate parsing, especially in spontaneous speech, where elliptic sentences occur frequently. Word graphs are an efficient interface between word recognition and parser. Prosodic classification of word chains has been published earlier. The adjustments necessary for applying these classification techniques to word graphs are discussed in this paper. When classifying a word hypothesis a set of context words has to be determined appropriately. A method has been developed to use stochastic language models for prosodic classification. This as well has been adopted for the use on word graphs. We also improved the set of acoustic-prosodic features with which the recognition errors were reduced by about 60% on the read speech we were working on previously, now achieving 10% error rate for 3 boundary classes and 5% for 2 accent classes. Moving to spontaneous speech the recognition error increases significantly (e.g. 16% for a 2-class boundary task). We show that even on word graphs the combination of language models which model a larger context with acoustic-prosodic classifiers reduces the recognition error by up to 50%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-254"
  },
  "harbeck95_eurospeech": {
   "authors": [
    [
     "S.",
     "Harbeck"
    ],
    [
     "Andreas",
     "Kießling"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Robust pitch period detection using dynamic programming with an ANN cost function",
   "original": "e95_1337",
   "page_count": 4,
   "order": 255,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "In this paper, a new pitch synchronous FO-algorithm is described. The task of detecting pitch periods in the speech signal is solved with a search for an optimal path through a space of pitch period hypotheses. The search is efficiently implemented by dynamic programming (DP). The DP cost function is computed with an automatically trained artificial neural network (ANN) which combines the outputs of heuristic functions measuring the similarity of adjacent period hypotheses. With this algorithm a coarse error rate of 4,75% on a German speech database is achieved. It outperforms the DPFO algorithm, which itselfs outperforms two \"conventional\" algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-255"
  },
  "hirai95_eurospeech": {
   "authors": [
    [
     "Toshio",
     "Hirai"
    ],
    [
     "Norio",
     "Higuchi"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Automatic detection of major phrase boundaries using statistical properties of superpositional F0 control model parameters",
   "original": "e95_1341",
   "page_count": 4,
   "order": 256,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "This paper proposes a method for automatic detection of major phrase boundaries as a first step toward full automatic annotation of prosodic structure. The modeling error of fundamental frequency (F0) and an F0 control modeling appropriateness measure (AIC) are employed to evaluate a prosodic major phrase boundary hypotheses. Experimental results are presented showing correct prediction of 66% of major phrase boundaries in Japanese read speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-256"
  },
  "taylor95_eurospeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Using neural networks to locate pitch accents",
   "original": "e95_1345",
   "page_count": 4,
   "order": 257,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This paper describes a technique for producing an auto-segmental representation of an utterances intonation. The technique works in a bottom-up manner by using a recurrent neural network to perform a classification of each framen in the input waveform. The technique correctly identifies 87.5% of pitch accents and boundary tones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-257"
  },
  "strik95_eurospeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "The relation between physiological signals and F0: a quantitative analysis method",
   "original": "e95_2027",
   "page_count": 4,
   "order": 258,
   "p1": "2027",
   "pn": "2030",
   "abstract": [
    "Measurements were obtained of several physiological mechanisms which are known to be important in the control of fundamental frequency (F0). The data were analysed by means of a multiple regression analysis in which F0 is the criterion and the physiological signals are the predictors. Separate analyses were carried out for statements and questions, and for falling and rising F0. The results reveal no considerable differences in the control of F0 for the various datasets.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-258"
  },
  "abe95_eurospeech": {
   "authors": [
    [
     "Masanobu",
     "Abe"
    ]
   ],
   "title": "Analysis of prosodic characteristics in speech advisories and their application to speech output",
   "original": "e95_2031",
   "page_count": 4,
   "order": 259,
   "p1": "2031",
   "pn": "2034",
   "abstract": [
    "We investigate characteristics and benefits of speech advisories with the goal of enhancing human-machine interaction through their use. Spontaneous speech data was collected using a simple task. Characteristics of advisory speech are (1) utterances usually include redundant information, (2) when redundant information is included, main information is emphasized by changing F0 value, (3) when communication goes well, utterances often include only necessary information. To confirm the benefits of speech advisories, we designed an experiment in which a computer monitors a human operator and issues verbal advice when the operator makes a mistake. Results are (1) operators were found to divide into two groups; i.e., preferred silence and preferred audio output, (2)they preferred synthetic speech to human speech; human speech made them feel as if they were being personally monitored.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-259"
  },
  "caminerogil95_eurospeech": {
   "authors": [
    [
     "M. GuittonF. Javier",
     "Caminero-Gil"
    ],
    [
     "Joel",
     "Crestel"
    ],
    [
     "Laure",
     "Charonnat"
    ]
   ],
   "title": "Pitch and elocution rate of diverts speech",
   "original": "e95_2035",
   "page_count": 4,
   "order": 260,
   "p1": "2035",
   "pn": "2038",
   "abstract": [
    "This paper aims to quantify the evolution of voice fundamental frequency and the slowing down of the speech tempo of 3 divers during the simulated dive HYDRA X at COMEX (Marseille, Nov-Dec 92). The average voice fundamental frequency which is higher in hyperbaric conditions than in free atmosphere conditions appears to be unrelated to breathing mixtures. The higher fundamental frequency shows a steady decrease along the saturation dive. Our study highlights a good stability of pitch frequency for the 2 divers who are in good condition and a bad stability for the other diver who lacks calmness and self confidence. The slowing down of the speech tempo is very important (20% to 40%) during the first week of the dive, then the tempo steadily increases toward a standard value. The data related to the speech tempo is difficult to explain. One can think that it is correlated with some difficulties to breathe and to behavioural problems for the divers. The evolution of pitch frequency during the dive can also be related to psychological and behavioural problems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-260"
  },
  "strom95_eurospeech": {
   "authors": [
    [
     "Volker",
     "Strom"
    ]
   ],
   "title": "Detection of accents, phrase boundaries and sentence modality in German with prosodic features",
   "original": "e95_2039",
   "page_count": 3,
   "order": 261,
   "p1": "2039",
   "pn": "2042",
   "abstract": [
    "In this paper detectors for accents, phrase boundaries, and sentence modality are described which derive prosodic features only from the speech signal and its fundamental frequency to support other modules of a speech understanding system in an early analysis stage, or in cases where no word hypotheses are available. A new method for interpolating and decomposing the fundamental frequency is suggested. The detectors' underlying Gaussian distribution classifiers were trained and tested with approximately 50 minutes of spontaneous speech, yielding recognition rates of 78 percent for accents, 81 percent for phrase boundaries, and 85 percent for sentence modality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-261"
  },
  "morlec95_eurospeech": {
   "authors": [
    [
     "Yann",
     "Morlec"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Véronique",
     "Aubergé"
    ]
   ],
   "title": "Synthesis and evaluation of intonation with a superposition model",
   "original": "e95_2043",
   "page_count": 4,
   "order": 262,
   "p1": "2043",
   "pn": "2046",
   "abstract": [
    "A data-driven method based on a new paradigm is introduced in this paper. We assume that cognitive representations of the discourse are prosodically encoded by means of global multiparametric prototypes. The generation of adequate prosodic contours is then obtained by retrieving and combining these elementary prototypic contours accessed by linguistic or paralinguistic keys. We examine here F0 generation. Two procedures have been applied: the first consists of a superposition model via a structured lexicon, the second uses a recurrent neural network. Preliminary experiments shows that both methods can lead to good quality F0 contours.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-262"
  },
  "fach95_eurospeech": {
   "authors": [
    [
     "Marcus",
     "Fach"
    ],
    [
     "Wolfgang",
     "Wokurek"
    ]
   ],
   "title": "Pitch accent classification of fundamental frequency contours by hidden Markov models",
   "original": "e95_2047",
   "page_count": 4,
   "order": 263,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "In this study Hidden Markov models are employed for the automatic classification of pitch accents in German utterances. A simplified version of the tone sequence model (i.e. a linguistic theory of pitch accents) is applied. In this approach only two of the nuclear tones (rise, fall) are used. They are represented by continuous density Hidden Markov models. The classifier works on the sequence of feature vectors defined by N-tuples of succeeding fundamental frequency estimates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-263"
  },
  "hermes95_eurospeech": {
   "authors": [
    [
     "Dik J.",
     "Hermes"
    ]
   ],
   "title": "Measuring the perceptual similarity of pitch contours",
   "original": "e95_2051",
   "page_count": 4,
   "order": 264,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "It has been shown that visual display systems of intonation can be employed to advantage in teaching intonation to deaf persons and in teaching the intonation of a foreign language. In current training situations, the correctness of a reproduced pitch contour is either rated by the teacher or automatically. In the latter case, an algorithm often estimates the maximum deviation from an example contour or something equivalent. Two issues are addressed in this paper. The first issue is concerned with the question whether visually conspicuous differences between displayed pitch contours correspond with important audible differences. Second, in automatic training, the ratings of the correctness of an imitation must be supplied by an algorithm. These automatically obtained ratings must correspond with perceptual similarity ratings of pitch contours. It is shown that visual and auditory similarity of pitch contours correspond well if pitch contours are displayed in such a way that only perceptually relevant features are represented. Furthermore, for automatic rating, the measure obtained by calculating the maximum deviation from an example contour had a worse correspondence with similarity ratings by phoneticians than three other physical similarity measures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-264"
  },
  "langlais95_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Langlais"
    ]
   ],
   "title": "Microprosodic study of isolated French word corpora",
   "original": "e95_2055",
   "page_count": 4,
   "order": 265,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "The present contribution aims at inventoring the microprosodic phenomena already abundantly described in the past for languages such as English and French (see [1] for a commented review of these studies). We propose to measure their usefulness in an automatic processing and also to estimate the expected confidency of microprosodic \"corrections\" - at least in an automatic way - by studying each parameter separately and by presenting statistical distributions of the different phenomena measured on isolated french word corpora uttered by several speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-265"
  },
  "paliwal95b_eurospeech": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Interpolation properties of linear prediction parametric representations",
   "original": "e95_1029",
   "page_count": 4,
   "order": 266,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "In this paper, interpolation of linear predictive coding (LPC) parameters in terms of the following representations is investigated: linear prediction coefficient representation, reflection coefficient representation, log-area-ratio representation, arc-sine reflection coefficient representation, cepstral coefficient representation, line spectral frequency representation, autocorrelation coefficient representation and impulse response representation. Though each of these representations provides equivalent information about the LPC spectral envelope, their interpolation performance is found to be different. It is shown that the line spectral frequency representation results in the best interpolation performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-266"
  },
  "choi95_eurospeech": {
   "authors": [
    [
     "H. B.",
     "Choi"
    ],
    [
     "W. T. K.",
     "Wong"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "C. C.",
     "Goodyear"
    ]
   ],
   "title": "Interpolation of spectral information for low bit rate speech coding",
   "original": "e95_1033",
   "page_count": 4,
   "order": 267,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "It may be demonstrated that interpolating, on a sample-by-sample basis, the short-term spectral envelope parameters at both the analysis and synthesis stages of a low-bit rate speech coder can significantly improve the decoded speech quality. Results obtained using two alternative short-term spectral representations are compared, i.e. Linear Prediction (LP) filter coefficients and Line Spectral Frequencies (LSF's). LSF's have the advantage of preserving the stability of the synthesis filter throughout the interpolation process and lower computational complexity. The representations have been compared for a PWI coder and objective measurements and informal listening tests have shown that marginally better subjective performance is obtained from the computationally simpler LSF interpolation process. An interpolated LSF synthesis filter has been implemented in a CELP coder. About 0.4 dB improvement in the SNR measure was obtained as compared with a conventional blockwise interpolated CELP coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-267"
  },
  "baghairavary95_eurospeech": {
   "authors": [
    [
     "Ladan",
     "Baghai-Ravary"
    ],
    [
     "Steve",
     "Beet"
    ],
    [
     "Osman",
     "Tokhit"
    ]
   ],
   "title": "Adaptive flux interpolation, flow-based prediction, delta or delta-delta coefficients: which is best?",
   "original": "e95_1037",
   "page_count": 4,
   "order": 268,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "Four models of speech dynamics are compared for robustness and accuracy: Adaptive Flux Interpolation, Flow-Based Prediction, Delta and Delta-Delta Coefficients. These models are applied to a number of speech representations. The interpolative approach is shown to be superior to prediction in all cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-268"
  },
  "kovesi95_eurospeech": {
   "authors": [
    [
     "B.",
     "Kovesi"
    ],
    [
     "S.",
     "Saoudi"
    ],
    [
     "J. M.",
     "Boucher"
    ],
    [
     "Z.",
     "Reguly"
    ]
   ],
   "title": "LSP Markov model for reducing the complexity of vector quantization",
   "original": "e95_1041",
   "page_count": 4,
   "order": 269,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "In this paper the vector quantization (VQ) of the LSP (Line Spectrum Pairs) coefficients of the CELP (Code Excited Linear Predictive) coder is addressed. Methods to reduce the complexity of VQ are proposed. First a method, which determines the search zone for the input vector's nearest neighbour in the codebook by comparing the norm of the vectors, is presented. This is followed by the description of a method based on the correlation of consecutive quantized LSP vectors. Here the transition matrix of the n states Markov model is used to reduce the searching zone for the nearest neighbour. The experimental results show that the combination of these two methods is an effective way of reducing the complexity of the vector quantization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-269"
  },
  "mohammadi95_eurospeech": {
   "authors": [
    [
     "H. R. Sadegh",
     "Mohammadi"
    ],
    [
     "W. H.",
     "Holmes"
    ]
   ],
   "title": "Predictive delta adaptive scalar quantization: an efficient method for coding the short-term speech spectrum",
   "original": "e95_1045",
   "page_count": 4,
   "order": 270,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "Line Spectral Frequencies (LSFs) are often used as parameters to represent the vocal tract filter in speech coders using linear prediction. We propose a new method for the quantization of the LSFs, namely Predictive Delta Adaptive Scalar Quantization (PDASQ). This method is a scalar quantization scheme based on a non-linear two-dimensional prediction in the index domain. It is shown that it can be implemented efficiently with negligible computational overhead and memory requirements compared to the simple scalar quantization method. Despite its lower bit rates the quantization distortion resulting from PDASQ is of the same order as with conventional scalar quantization. Satisfactory performance of the new method is verified through experimental tests using computer simulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-270"
  },
  "chang95b_eurospeech": {
   "authors": [
    [
     "Dong-Il",
     "Chang"
    ],
    [
     "Kyungmin",
     "Na"
    ],
    [
     "Souguil",
     "Ann"
    ]
   ],
   "title": "Conditional split vector quantization of LSP parameterswith multiple search",
   "original": "e95_1049",
   "page_count": 4,
   "order": 271,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "In this paper, we propose an efficient vector quantization scheme of line spectral pair parameters which combines conditional split VQ with a multiple search. At first, we show the efficiency of mutiple search for two VQ schemes of LSP, split VQ and conditional split VQ. In spite of improved performance, the unavoidable increase in computational complexity makes the SVQ with multiple search unattractive. However, the reduced complexity of conditional split VQ make the combination with a multiple search realizable and very attractive. Experimental results are presented to show the efficiency of the proposed scheme, the multiple search conditional split VQ (MS-CONSVQ).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-271"
  },
  "bruhn95_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Bruhn"
    ]
   ],
   "title": "Matrix Product Quantization For Very-low-rate Mobile Speech Communications",
   "original": "e95_1053",
   "page_count": 4,
   "order": 272,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "Efficient block coding methods for LPC information play an essential role in very-low-rate speech coding systems. The subject of this contribution is a suboptimal matrix quantization scheme for LPC parameters, called matrix product quantization (MPQ), operating at bit rates between 300 and 750 b/s. In fixed-rate coding systems for mobile communication, MPQ achieves a very high coding efficiency at a low coding delay. Moreover, this paper shows that MPQ is an excellent candidate for mobile speech communication systems in terms of complexity and bit error sensitivity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-272"
  },
  "balss95_eurospeech": {
   "authors": [
    [
     "U.",
     "Balss"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "H.",
     "Schalk"
    ],
    [
     "Dietrich",
     "Wolf"
    ]
   ],
   "title": "Robust vector quantization for low bit rate speech coding",
   "original": "e95_1057",
   "page_count": 4,
   "order": 273,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "Speech coding systems for mobile communication have to cope with noisy channels. In particular, vector quantization as central data reduction scheme is highly sensitive to transmission errors due to the low redundancy in the encoded data. Here we present three methods for the design of a vector quantizer with enhanced robustness against transmission errors. First the optimization of the index assignment of a LBG vector quantizer via simulated annealing is investigated. Second a neighborhood conserving vector quantizer is designed by using a topology conserving feature map. Third we discuss a method in which the error characteristic of the channel is used in the optimization of a vector quantizer as well as in the quantization of a data vector. Simulation experiments and results for a binary symmetric channel with bit error probabilities up to 10% are presented for vector quantization of speech signals and predictor parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-273"
  },
  "ng95c_eurospeech": {
   "authors": [
    [
     "H. C.",
     "Ng"
    ],
    [
     "S. H.",
     "Leung"
    ]
   ],
   "title": "Scalar quantization of LSF parameters at 28 bits/frame",
   "original": "e95_1061",
   "page_count": 4,
   "order": 274,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "Line Spectral Frequencies (LSF) are widely used to quantize spectral information in most of low-bit-rate linear predictive speech coders because of their desirable features with regard to ordering property and spectral sensitivity. Although LSF vector quantizer uses fewer bits [1][2], scalar quantization of LSF [3] is still adopted because codebook storage and computational complexity are the major considerations for real-time implementation. In this paper, a new set of LSF-based coefficients is proposed which can be coded efficiently by increasing the utilization of quantization table. With the novel table search technique based on vector measure, 10th-order LPC parameters can be coded at 28 bits/frame with transparent quantization quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-274"
  },
  "arriola95_eurospeech": {
   "authors": [
    [
     "J. M. Gutierrez",
     "Arriola"
    ],
    [
     "F. M. Gimenez de los",
     "Galanes"
    ],
    [
     "M. H.",
     "Savoji"
    ]
   ],
   "title": "Improvement of the quality of speech synthesis by analysis using segmentation and modeling of the excitation signal",
   "original": "e95_1097",
   "page_count": 4,
   "order": 275,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "The aim of the presented work was twofold: 1) To study whether the effect of the stochastic part of the glottal waveform can be replaced by a postprocessing of the synthesized signal when a mathematical excitation source is used and 2) To find a new way of representing the excitation signal for high quality synthesized speech with the properties of mathematical models which permits avoiding the problems related to prosody control by overlap add methods as used in PSOLA[1].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-275"
  },
  "dymarski95_eurospeech": {
   "authors": [
    [
     "Przemyslaw",
     "Dymarski"
    ],
    [
     "Slawomir",
     "Kuklinski"
    ],
    [
     "Siawomir",
     "Kula"
    ]
   ],
   "title": "A text-to-speech synthesizer for the Polish language",
   "original": "e95_1101",
   "page_count": 4,
   "order": 276,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "Text-to-speech synthesizer is described, based on the the concatenation of the Polish diphones. The text-to-phoneme conversion is based on the neural network. Diphones are extracted and stored pitch- synchronously, using the variable rate linear predictive coder with mixed excitation. The pitch period modification is based on the time- domain interpolation of the excitation signal. Duration is controlled by insertion of the pitch periods and interpolation of the excitation signal. Preliminary results are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-276"
  },
  "jurgens95_eurospeech": {
   "authors": [
    [
     "C.",
     "Jürgens"
    ],
    [
     "M.",
     "Wunderlich"
    ]
   ],
   "title": "A comparison of different speech units for the German TTS-system tubsy",
   "original": "e95_1105",
   "page_count": 4,
   "order": 277,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "The quality of every speech synthesis system depends strongly on the quality of the speech inventory. In this paper we discuss the advantages and disadvantages of different speech units that are mostly used in speech synthesis systems. Therefore we have produced three speech inventories for our TTS-system TUBSY, one containing phonemes, another based on phoneme-clusters and a third one with diphones. The resulting speech quality was evaluated by a cluster-identification test. Based on the results of this test we propose a new kind of speech units that combine the advantages of phoneme-clusters and diphones. Informal listening tests show that these elements provide a significant improvement of speech quality over phoneme-clusters and that they achieve a similar speech quality as diphones but with a much smaller inventory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-277"
  },
  "delogu95_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Andrea",
     "Paoloni"
    ],
    [
     "Paola",
     "Ridolfi"
    ]
   ],
   "title": "Confusions among Italian consonants in good and in telephone conditions: differences between text-to-speech systems and natural speech with noise",
   "original": "e95_1109",
   "page_count": 4,
   "order": 278,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "Natural and synthetic voice differ from various points of view, as shown by the results of many experiments. This difference can be due to possible differences in the acoustic-phonetic structure of the two signals. In order to investigate this hypothesis, we run a consonant confusion test for 19 Italian consonants produced by a natural voice with noise (3 S/N ratios) and 6 TTS systems presented through good and telephone channels. The results showed that the distributions of consonant confutions for natural and synthetic speech (both formant-based and diphone-based synthesis) were often quite different, suggesting some contraddiction in the acoustic cues and in the coarticulation model of the synthetic signals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-278"
  },
  "williams95_eurospeech": {
   "authors": [
    [
     "Briony",
     "Williams"
    ]
   ],
   "title": "Text-to-speech synthesis for welsh and welsh English",
   "original": "e95_1113",
   "page_count": 4,
   "order": 279,
   "p1": "1113",
   "pn": "1117",
   "abstract": [
    "This work represents the first known attempt to develop a text-to-speech synthesiser for Welsh. A list of pseudo-Welsh nonsense words was generated, allowing for certain difficulties particular to Welsh. Diphones were derived semi-manually from the recorded nonsense words. The first known phonemic lexicon for Welsh was derived from an electronic corpus. Letter-to-sound rules for Welsh were written, differentiating between the vocalic and consonantal realisations of two graphemes, and assigning lexical stress. An existing English text-to-speech synthesis system was adapted for Welsh. Some simple duration and F0 rules were written that gave pleasing results with the minimum of rules. The resulting system can be used for Welsh or for English spoken with a recognisable Welsh accent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-279"
  },
  "andersen95_eurospeech": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Multi-lingual testing of a self-learning approach to phonemic transcription of orthography",
   "original": "e95_1117",
   "page_count": 4,
   "order": 280,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "A Self-Learning system for Grapheme to Phoneme conversion is described and tested. The system acquires the knowledge needed for grapheme-to-phoneme conversion from a training session in which a large number of pairs of grapheme strings and their corresponding (manually verified) phonemic transcription strings are presented to the system. The result from the training is a stochastic decision tree in which statistics - as given in the training material - about corresponding graphemes and phonemes are stored for later retrieval. The system is tested on a number of European languages and results from three tests are reported. In the first test, which concerns proper names, only the most probable phoneme candidate at each leaf of the tree is utilised. The second and the third test, both using a database of ordinary words, aims at analysing phoneme and word accuracies resulting from using N-Best phonemes at each leaf and from introducing phonotactic information, respectively. Using N-Best candidates in combination with phonotactic information show a phoneme and word accuracy of up to 88.5% and 46.6%, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-280"
  },
  "beattie95_eurospeech": {
   "authors": [
    [
     "V.",
     "Beattie"
    ],
    [
     "S.",
     "Edmondson"
    ],
    [
     "D.",
     "Miller"
    ],
    [
     "Y.",
     "Patel"
    ],
    [
     "G.",
     "Talvola"
    ]
   ],
   "title": "An integrated multi-dialect speech recognition system with optional speaker adaptation",
   "original": "e95_1123",
   "page_count": 4,
   "order": 281,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "Improved performance for speaker independent speech recognitions systems requires better modelling of different dialects of the target language. Work on this topic [1, 2], as well as our own results, suggest that separate modelling of dialects is needed to capture accurately the many pronunciation differences that occur. No matter how much dialect data is included in training, however, some speakers will not be covered by the resulting model - for instance, non-native speakers of the language and speakers whose speech patterns have been affected by surgery. We describe a speech recognition system which incorporates both dialect modelling to provide coverage for a high percentage of the target population, and optional speaker adaptation for speakers who are not adequately represented by the model. This comprehensive approach does not require detailed phonetic knowledge of dialectical variations, dramatically increased memory usage, or significantly increased complexity during recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-281"
  },
  "neumeyer95_eurospeech": {
   "authors": [
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Ananth",
     "Sankar"
    ],
    [
     "Vassilios",
     "Digalakis"
    ]
   ],
   "title": "A comparative study of speaker adaptation techniques",
   "original": "e95_1127",
   "page_count": 4,
   "order": 282,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "In previous work, we showed how to constrain the estimation of continuous mixture-density hidden Markov models (HMMs) when the amount of adaptation data is small. We used maximum-likelihood (ML) transformation-based approaches and Bayesian techniques to achieve near native performance when testing nonnative speakers of the recognizer language. In this paper, we study various ML-based techniques and compare experimental results on data sets with recordings from nonnative and native speakers of American English. We divide the transformation-based techniques into two groups. In feature-space techniques, we hypothesize an underlying transformation in the feature-space that results in a transformation of the HMM parameters. In model-space techniques, we hypothesize a direct transformation of the HMM parameters. In the experimental section we show how the combination of the best ML and Bayesian adaptation techniques result in significant improvements in recognition accuracy. All the experiments were carried out with SRTs DECIPHER(TM) speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-282"
  },
  "zavaliagkos95_eurospeech": {
   "authors": [
    [
     "G.",
     "Zavaliagkos"
    ],
    [
     "R.",
     "Schwartz"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Adaptation algorithms for large scale HMM recognizers",
   "original": "e95_1131",
   "page_count": 4,
   "order": 283,
   "p1": "1131",
   "pn": "1135",
   "abstract": [
    "We present a framework for Maximum a Posteriori (MAP) adaptation of large scale HMM recognizers. First we review the standard MAP adaptation for Gaussian mixtures. We then show how MAP can be used to estimated transformations which are shared across many parameters. Finally, we combine both techniques: each of the HMM models is adapted based on an interpolation of MAP estimates obtained under varying degrees of sharing. We evaluate this algorithm for adaptation of a continuous density HMM with 96K Gaussians and show that very satisfactory improvements can be achieved, especially for adaptation of non-native speakers of American English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-283"
  },
  "matsunaga95_eurospeech": {
   "authors": [
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Tohru",
     "Shimizu"
    ]
   ],
   "title": "Speaking-style and speaker adaptation for the recognition of spontaneous dialogue speech",
   "original": "e95_1135",
   "page_count": 4,
   "order": 284,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "This paper discusses speaking-style and speaker adaptation techniques in a new ATR spontaneous speech database as a step toward more useful spoken dialogue translation. The task of this data concerns conversations in travel situations consisting of many forms of communications between a receptionist and a guest. Speaker-independent recognition using the data shows that the recognition performance of spontaneous speech is not very low in comparison with that of read speech when adequate speaker- and speaking-style adaptation is applied to a speaker-independent phoneme model. Furthermore, a speech recognition strategy using a context-free grammar and this speaking-style adapted acoustic model shows the effectiveness of this grammar even for spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-284"
  },
  "dobler95_eurospeech": {
   "authors": [
    [
     "S.",
     "Dobler"
    ],
    [
     "Hans-Wilhelm",
     "Rühl"
    ]
   ],
   "title": "Speaker adaptation for telephone based speech dialogue systems",
   "original": "e95_1139",
   "page_count": 4,
   "order": 285,
   "p1": "1139",
   "pn": "1143",
   "abstract": [
    "Speaker adaptation is an important method to increase the recognition rate of speaker independent speech recognizers. For telecommunication applications speaker adaptation must operate unsupervised, without an explicit adaptation phase and must be effective after only a few seconds of input speech. Three algorithms for the adaptation of the parameters of Hidden Markov Models (HMM) are presented. Adaptation of emission probabilities in supervised mode decreases string error rate for a connected-digits-string recognition task by 40 % after two adaptations for each digit. An unsupervised time-synchronous algorithm for adaptation of the emission probabilities decreases errors by 51 %, and unsupervised adaptation of the transition probabilities by 21 %. The simultaneous adaptation both of the emission and the transition probabilities reduces errors by 66 %.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-285"
  },
  "shinoda95_eurospeech": {
   "authors": [
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Speaker adaptation with autonomous control using tree structure",
   "original": "e95_1143",
   "page_count": 4,
   "order": 286,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "In practical use of speaker adaptation, it is important to provide a framework that operates well for any amount of adaptation data since the amount of data available is often changed. We propose one such framework in which the number of free parameters for estimation is autonomously controlled according to the amount of data for adaptation. It has been applied to a speaker-independent speech recognition system using continuous density mixture Gaussian HMMs, and has proven to be effective through 5,000-word recognition experiments. For example, it achieved a 40% reduction in error rate over the speaker-independent recognition system when 50 words were used for adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-286"
  },
  "tonomura95_eurospeech": {
   "authors": [
    [
     "Masahiro",
     "Tonomura"
    ],
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Akito",
     "Monden"
    ]
   ],
   "title": "Speaker adaptation fitting training data size and contents",
   "original": "e95_1147",
   "page_count": 4,
   "order": 287,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "This paper proposes a speaker adaptation algorithm that covers a wide range of adaptation data. The parameter smoothing technique improves adaptation performance for a small amount of adaptation data; however, this smoothing usually reduces adaptation efficiency for a large amount of adaptation data. Our method dynamically controls smoothing strength by using information on the amount of adaptation data to achieve good adaptation performance over a wide range of adaptation data. The proposed method is combined with the maximum a posteriori (MAP) estimation technique, and its effectiveness is shown on a Japanese 26 phoneme recognition test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-287"
  },
  "choi95b_eurospeech": {
   "authors": [
    [
     "H. C.",
     "Choi"
    ],
    [
     "R. W.",
     "King"
    ]
   ],
   "title": "Direct and joint-space approaches to the use of spectral transformation for speaker adaptation in continuous speech recognition",
   "original": "e95_1151",
   "page_count": 4,
   "order": 288,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "This paper describes three different approaches to the use of spectral transformation for supervised speaker adaptation in continuous speech recognition. Each approach may involve transforming feature vectors of the speech of a speaker, transforming mean vectors of the HMMs of a reference recognition system, or transforming both feature vectors and HMMs. A comparison of these approaches is investigated using the ARPA 1000-word Resource Management (RM1) continuous speech corpus. Using the average speaker-independent (SI) test result as a reference point, it is found that the best adaptation approach can achieve an error reduction of 23.5% by using 10 sentences as adaptation speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-288"
  },
  "leggetter95_eurospeech": {
   "authors": [
    [
     "C. J.",
     "Leggetter"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "Flexible speaker adaptation for large vocabulary speech recognition",
   "original": "e95_1155",
   "page_count": 4,
   "order": 289,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "The maximum likelihood linear regression (MLLR) approach for speaker adaptation of continuous density mixture Gaussian HMMs is presented and its application to static and incremental adaptation for both supervised and unsupervised modes described. The approach involves computing a transformation for the mixture component means using linear regression. The transformations are shared between a number of mixture components so that adaptation can be effective with large vocabulary systems which employ a very large number of parameters, using only modest amounts of adaptation data. Results are given for unsupervised incremental adaptation of native speaker Wall Street Journal (WSJ) data, and static-supervised adaptation for non-native speakers. Both show the effectiveness and flexibility of the MLLR approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-289"
  },
  "kaspar95_eurospeech": {
   "authors": [
    [
     "B.",
     "Kaspar"
    ],
    [
     "G.",
     "Fries"
    ],
    [
     "K.",
     "Schuhmacher"
    ],
    [
     "Antje",
     "Wirth"
    ]
   ],
   "title": "Faust - a directory assistance demonstrator",
   "original": "e95_1161",
   "page_count": 4,
   "order": 290,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "In this paper we describe a speech dialogue system for the field of directory assistance. The demonstration task is a directory with 5000 entries, currently. The speech technology employed, however, is feasible for databases of realistic size, too. The system uses keyword spotting, alphabet recognition, and speech synthesis. Recognizer and syntheziser are based on a German pronunciation lexicon. The combination of keyword spotting and alphabet recognition allows, to enlarge the vocabulary to be recognized significantly. The overall system relies on a general architecture for voice dialogues, which was designed to be open for other base components and further development in speech technology. The dialogue software exhibits some general design rules for speech dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-290"
  },
  "watanuki95_eurospeech": {
   "authors": [
    [
     "Keiko",
     "Watanuki"
    ],
    [
     "Fumio",
     "Togawa"
    ]
   ],
   "title": "Some signals of emotional arousal: analysis of conversations using a multimodal interaction database",
   "original": "e95_1165",
   "page_count": 4,
   "order": 291,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "We are developing multimodal man-machine interfaces through which users can communicate by integrating speech, gaze, facial expressions, and gestures such as nodding and finger pointing. In this paper, we study two discrete emotions, interest and disinterest. When communicating with computers, users may display a variety of emotions in faces and voices. Thus, in realizing more flexible and natural communications between humans and computers, we consider that computers need to know about their user's emotional state: whether the user is interested or not. Here we present work performed in our lab towards the analyses on modalities which would encode the user emotion of interest and disinterest in conversation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-291"
  },
  "bruce95_eurospeech": {
   "authors": [
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "M.",
     "Filipsson"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "Merle",
     "Horne"
    ],
    [
     "David",
     "House"
    ],
    [
     "B.",
     "Lastow"
    ],
    [
     "Paul",
     "Touati"
    ]
   ],
   "title": "Speech synthesis in spoken dialogue research",
   "original": "e95_1169",
   "page_count": 4,
   "order": 292,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "This paper concerns the use of speech synthesis as a tool in investigating dialogue structure and in modelling prosody for use in dialogue-based text-to-speech systems. We report on two different approaches used in our efforts to investigate how prosody is used as a communicative tool in dialogue situations. Our work has relevance for practical applications of both text-to-speech and speech recognition technology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-292"
  },
  "zajicek95_eurospeech": {
   "authors": [
    [
     "Mary",
     "Zajicek"
    ],
    [
     "Ken",
     "Brownsey"
    ],
    [
     "Simon",
     "Lippmann"
    ],
    [
     "Patrice",
     "Palau"
    ],
    [
     "Phyl",
     "Greenhead"
    ]
   ],
   "title": "Dynamically created dialogues for automated telephone answering using uncertain reasoning and linguistic theory",
   "original": "e95_1173",
   "page_count": 4,
   "order": 293,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "This paper discusses research into the creation of dynamic dialogues for automated telephone answering systems. In order to create a dialogue a semantic net representing factual or organisational information is traversed by the prototype dialogue creation engine. This assembles questions appropriate to the current user's previous responses and their user model with an aim to ascertain their intention or goal in making the call. A major area of study has been methods of determining fuzzy goals (when the user is not aware of what their specific goal might be) using uncertain reasoning and fuzzy set theory. This research is coupled with the linguistic demands of natural language processing for dynamic sentence creation and use of speech as the medium of interaction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-293"
  },
  "niimi95_eurospeech": {
   "authors": [
    [
     "Y.",
     "Niimi"
    ],
    [
     "Y.",
     "Kobayashi"
    ]
   ],
   "title": "Modeling dialogue control strategies to relieve speech recognition errors",
   "original": "e95_1177",
   "page_count": 4,
   "order": 294,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "This paper considers three dialogue control strategies to relieve speech recognition errors. These are the prompt to speak again (basic strategy), the direct confirmation and the indirect confirmation. The purpose of modeling the dialog control strategies is to estimate two quantities Pac and N, given the performance of the speech recognizer used in a dialogue system. Pac is the probability that information included in user's utterance is conveyed to the system correctly, and N is the average number of turns taken between the user and the system until terminating subdialogue on user's first utterance. The analysis has proven that the direct confirmation can increase Pac and the indirect confirmation can reduce N in comparison with the basic strategy. Since the mathematical analysis described in this paper can easily be extended to more sophisticated strategies, it will contribute to the quantitative design of a dialogue control strategy, give the performance of a speech recognizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-294"
  },
  "baekgaard95_eurospeech": {
   "authors": [
    [
     "Anders",
     "Baekgaard"
    ]
   ],
   "title": "Constraining of input media in a spoken dialogue system",
   "original": "e95_1181",
   "page_count": 4,
   "order": 295,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "A platform for spoken dialogue systems and a dialogue description formalism on which the platform is based is briefly described. The paper gives details about how input devices can automatically be constrained by the dialogue manager on the basis of the dialogue context. The method is formalised and generalised and apply for all input media in a dialogue system, especially the speech recogniser and graphical input devices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-295"
  },
  "kleckova95_eurospeech": {
   "authors": [
    [
     "Jana",
     "Kleckova"
    ],
    [
     "Vaclav",
     "Matousek"
    ],
    [
     "Jana",
     "Netrvalova"
    ]
   ],
   "title": "AN Automatic Creation Of The Language Model For The Spontaneous Czech Speech Recognizer",
   "original": "e95_1185",
   "page_count": 4,
   "order": 296,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "An automatic creation of the language model for the recognition of the spontaneous pronounced sentences requires a creation of a large corpus of training sentences where word and phrase boundaries are labeled in the sentences automatically during sentence generation. The corpus of the training sentences contains about 10,000 domain dependent sentences where each sentence is unique and it has been generated using sentence templates (prototypes) and a stochastic context-free grammar. The syntactic structure of the Czech sentences used for a derivation of sentence prototypes has been described by special sentence formulas.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-296"
  },
  "okane95_eurospeech": {
   "authors": [
    [
     "M.",
     "O'Kane"
    ],
    [
     "P. E.",
     "Kenne"
    ],
    [
     "H. G.",
     "Pearcy"
    ]
   ],
   "title": "A grammar of conversational English",
   "original": "e95_1189",
   "page_count": 4,
   "order": 297,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "We present an approach to the derivation of a formal statistical grammar of the phrase structure of conversational English. This grammar is derived automatically from court transcripts. We use this grammar as an aid in rescoring speech recogniser output and compare the results for similar interaction styles i.e. those within a court, and those found in parliamentary dialogue.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-297"
  },
  "brison95_eurospeech": {
   "authors": [
    [
     "Eric",
     "Brison"
    ],
    [
     "Nadine",
     "Vigouroux"
    ]
   ],
   "title": "Robust comprehension in a spoken dialog system",
   "original": "e95_1193",
   "page_count": 4,
   "order": 298,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "This paper describes the semantic interpretation of user utterance in the IMAR project (multimodal spoken dialogue system). In a human-computer dialogue, a good sentence understanding is a prime necessity to have a good progression in the exchange. The heaviness of an interaction is due to the misunderstanding and the faltering speak of the user. The aim of this paper is to propose a robust spoken understanding system capable to cover effects of spontaneous speech (filled pauses, non grammatical structure), the misrecognition (unknow words) and the ambiguity of the language. The method is based on the use of several linguistic knowledges (syntactic, semantic and pragmatic) and also the context of the interaction (user and task model).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-298"
  },
  "lee95b_eurospeech": {
   "authors": [
    [
     "Youngjik",
     "Lee"
    ],
    [
     "Young-Sum",
     "Kim"
    ],
    [
     "Jung-Chul",
     "Lee"
    ],
    [
     "Joon-Hyung",
     "Ryoo"
    ],
    [
     "Jae-Woo",
     "Yang"
    ]
   ],
   "title": "Korean-Japanese speech translation system for hotel reservation - Korean front desk side",
   "original": "e95_1197",
   "page_count": 4,
   "order": 299,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "Recently, ETRI had developed a Korean-Japanese speech translation system for a Korean front desk side in hotel reservation task. The system consists of three subsystems each of which is responsible for speech recognition, machine translation, and speech synthesis. This paper introduces the outline of the system development and describes the functions of the subsystems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-299"
  },
  "lin95b_eurospeech": {
   "authors": [
    [
     "Sung-Chien",
     "Lin"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Keh-Jiann",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Unconstrained speech retrieval for Chinese document databases with very large vocabulary and unlimited domains",
   "original": "e95_1203",
   "page_count": 4,
   "order": 300,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "This paper presents a new approach for Chinese document database retrieval with unconstrained speech-input queries. This approach is based on a successful integration of both speech recognition and information retrieval technologies with special considerations of the characteristics of the Chinese language. Such an approach is especially important for Chinese language, because Chinese language is not alphabetic and the input of Chinese characters into computers is still a very difficult and unsolved problem. The nice features of the syllable-based approach include proper use of the knowledge acquired from database to provide grammatical constraints for speech recognition, and the tolerance of speech recognition errors by approximate text matching. Based on this approach, a prototype system is implemented and encouraging experimental results are demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-300"
  },
  "delogu95b_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Andrea Di",
     "Carlo"
    ],
    [
     "Rino",
     "Falcone"
    ]
   ],
   "title": "Integrating partial syntactical analysis and plan recognition for understanding DB natural language queries",
   "original": "e95_1207",
   "page_count": 4,
   "order": 301,
   "p1": "1207",
   "pn": "1211",
   "abstract": [
    "In this work we propose a framework for speech understanding in Natural Language DB accessing based on the cooperative behaviour of the system which identifies its goals with user's goals and executes its own plans for satisfying these goals. The purpose of this work is to verify that in this particular human-machine interaction, by starting from some few recognized keywords, it is possible to reconstruct the user's goal and then to satisfy it The process of user's goal recognition is realized by a chart-based mechanism for plan recognition. This approach allows us to use the same formalism to represent both the low level (i.e. the syntactical analysis) and the high level (i.e. plan recognition) analyses.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-301"
  },
  "kosarev95_eurospeech": {
   "authors": [
    [
     "Yuri A.",
     "Kosarev"
    ]
   ],
   "title": "Communicative language model: structure and functioning",
   "original": "e95_1211",
   "page_count": 4,
   "order": 302,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "Many attempts to design systems of human-machine interaction based on natural spoken or written language doesn't bear an expected effect. The main cause of fails is the lack of enough adequate models of natural language considering knowledge from closely-related branches of science. This paper considers conceptual approaches to spoken language simulation. The code model of language and language understanding process, which reconciles with the information theory and the speech psychology, is suggested. The results of simulation are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-302"
  },
  "stevenur95_eurospeech": {
   "authors": [
    [
     "Adelaide",
     "Stevenur"
    ],
    [
     "Patrick",
     "Gallinari"
    ]
   ],
   "title": "Integrating heuristic preferences into a neural understanding system",
   "original": "e95_1215",
   "page_count": 4,
   "order": 303,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "We have been developing a neural natural language understanding system oriented towards the extraction of information relevant to a task. We have implemented a version of this system for an air travel reservation task. We have studied systems that map input sequences to output sequences, and conceived a recurrent neural system for conceptual decoding. Robustness, learnability and flexible integration of different information sources are some of the attractive features of our model. Training Neural Networks to capture long-term dependencies remains an important challenge. We present results of our attempts to capture global constraints between noncontiguous semantic patterns. We propose a new system that combines our connectionist model and an N-Best like module for higher-level post segmental treatment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-303"
  },
  "gamback95_eurospeech": {
   "authors": [
    [
     "Björn",
     "Gamback"
    ],
    [
     "Martin",
     "Eineborg"
    ],
    [
     "Mikael",
     "Eriksson"
    ],
    [
     "Barbro",
     "Ekholm"
    ],
    [
     "Bertil",
     "Lyberg"
    ],
    [
     "Tomas",
     "Svensson"
    ]
   ],
   "title": "A language interface to a polyphone-based speech synthesizer",
   "original": "e95_1219",
   "page_count": 4,
   "order": 304,
   "p1": "1219",
   "pn": "1223",
   "abstract": [
    "The paper describes how the Swedish Core Language Engine, a general-purpose natural-language processing system was used as a preprocessing system for a polyphone-based speech synthesizer. The interface between the two systems utilizes the grammatical information obtained by the NLP system to improve the quality of the speech produced by the synthesizer. The interface is built in a modular fashion, with the first modules using the NLP information in order to assign sentence accent properly on phrase and word level, while the final modules decide on which entries from a polyphone library should be used and calculate the pitch pulse information used by the synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-304"
  },
  "niklfeld95_eurospeech": {
   "authors": [
    [
     "Georg",
     "Niklfeld"
    ],
    [
     "Hannes",
     "Pirker"
    ],
    [
     "Harald",
     "Trost"
    ]
   ],
   "title": "Using two-level morphology as a generator-synthesizer interface for concept-to-speech",
   "original": "e95_1223",
   "page_count": 4,
   "order": 305,
   "p1": "1223",
   "pn": "1226",
   "abstract": [
    "In a project for the development of a concept-to-speech system for German, we apply extended two-level-morphology [8] to provide a unified solution for the tasks of morphotactics, segmental (morpho)phonology, syllabification and assignment of stress. Starting from a lexeme-based lexicon, we show that a declarative two-level-implementation of a single rule-corpus complemented with feature filters is sufficient for a comprehensive account of the various mutual influences holding between separate phonological dimensions in the phonology of German. Information from higher levels of linguistic structure, up to textual representation, can be exploited in our system by performing a look-up of relevant feature-values through the filter conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-305"
  },
  "koo95_eurospeech": {
   "authors": [
    [
     "Myoung-Wan",
     "Koo"
    ],
    [
     "Il-Hyun",
     "Sohn"
    ],
    [
     "Woo-Sung",
     "Kim"
    ],
    [
     "Du-Seong",
     "Chang"
    ]
   ],
   "title": "KT-STS: a speech translation system for hotel reservation and a continuous speech recognition system for speech translation",
   "original": "e95_1227",
   "page_count": 4,
   "order": 306,
   "p1": "1227",
   "pn": "1230",
   "abstract": [
    "In this paper, we present KT-STS(Korea Telecom Speech Translation  System) and a continuous speech recognition system for speech translation. KT-STS is an experimental speech-to-speech translation system which translates a spoken utterance in Korean into one in Japanese. The system has been designed around the task of hotel reservation (dialogues between a Korean customer and a hotel reservation desk in Japan). It consists of Korean speech recognition, Korean-to-Japanese machine translation and Korean speech synthesis. The Korean speech recognition system is an HMM(hidden Markov model) --based speaker-independent, continuous speech recognizer which can recognize about 300 word vocabularies. We have achieved the word recognition rate of 94.5% and the sentence recognition rate of 81.2%. For machine translation, we use dependency grammar and direct transfer method. And Korean speech synthesis is also implemented. KT-STS runs in nearly real time on the SPARC20 workstation with one TMS320C30 DSP board. We had an international joint experiment in which our system was connected with another system developed by KDD in Japan using the leased line.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-306"
  },
  "vilar95_eurospeech": {
   "authors": [
    [
     "J. M.",
     "Vilar"
    ],
    [
     "A.",
     "Marzal"
    ],
    [
     "Enrique",
     "Vidal"
    ]
   ],
   "title": "Learning language translation in limited domains using finite-state models: some extensions and improvements",
   "original": "e95_1231",
   "page_count": 4,
   "order": 307,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "The Onward Subsequential Transducer Inference Algorithm (OSTIA) has been used for learning Language Translations in limited domain tasks. Although it is known to converge to the correct model when presented with enough training examples, the amount of training data can be prohibitive for large vocabularies. We address this problem by using appropriate clustering of words in both the input and output languages. Experimental results are presented which show that this approach effectively avoids dependency on the size of the vocabulary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-307"
  },
  "novick95_eurospeech": {
   "authors": [
    [
     "David G.",
     "Novick"
    ],
    [
     "Karen",
     "Ward"
    ],
    [
     "Benjamin",
     "Corliss"
    ]
   ],
   "title": "The effect of context on the intelligibility of dialogue",
   "original": "e95_1235",
   "page_count": 4,
   "order": 308,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "We measured the effects of task context on the intelligibility of utterances drawn from a corpus of air traffic control dialogue. Subjects understood more words when the utterances were presented in the form of a dialogue, with the original utterance order preserved. Subjects presented with the same utterances in randomized order understood significantly fewer words. This effect was seen with both domain experts (pilots and air traffic controllers) and domain novices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-308"
  },
  "kilian95_eurospeech": {
   "authors": [
    [
     "Ute",
     "Kilian"
    ],
    [
     "Fritz",
     "Class"
    ],
    [
     "Alfred",
     "Kaltenmeier"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Representation of a finite state grammar as bigram language model for continuous speech recognition",
   "original": "e95_1241",
   "page_count": 4,
   "order": 309,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "Based on a speaker-independent continuous speech recognizer working with a conventional bi-gram language model our approach integrates a finite state grammar into the recognition process. For syntactically structured tasks this grammar is represented as bigram language model. Thus the implementation of the recognition process remains unchanged. We report the results of an evaluation on a German database consisting of 1500 strongly structured commands. Test set perplexity is nearly halved and a significant higher recognition rate is reached without a notable delay in the recognition process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-309"
  },
  "generet95_eurospeech": {
   "authors": [
    [
     "M.",
     "Generet"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "F.",
     "Wessel"
    ]
   ],
   "title": "Extensions of absolute discounting for language modeling",
   "original": "e95_1245",
   "page_count": 4,
   "order": 310,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "In this paper, we extend the absolute discounting technique along various directions. To estimate the backing-off distribution, we use ra-gram singletons, i.e. ra-grams that were seen exactly once in the training data. This method is applied in addition to the usual estimation of discounting parameters. The improvement in perplexity is typically between 8% and 12%. We also investigate a cache model. In experimental tests on a large text corpus, the cache model improved the perplexity by up to 28%. The experimental evaluations were carried out on a set of 38 million words from the Wall Street Journal task. We compare our results with the results reported by CMU.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-310"
  },
  "moisa95_eurospeech": {
   "authors": [
    [
     "Loreia",
     "Moisa"
    ],
    [
     "Egidio",
     "Giachin"
    ]
   ],
   "title": "Automatic clustering of words for probabilistic language models",
   "original": "e95_1249",
   "page_count": 4,
   "order": 311,
   "p1": "1249",
   "pn": "1253",
   "abstract": [
    "In this work we compare different methods for clustering words into equivalence classes within a bigram language model, for a specific-domain recognition task (train timetable enquiry). Though the perplexity values obtained by the various methods differ, the word error rates eventually achieved are very similar. We examine this behavior in the light of the word usage peculiarities present in these types of tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-311"
  },
  "martin95_eurospeech": {
   "authors": [
    [
     "Sven",
     "Martin"
    ],
    [
     "Jörg",
     "Liermann"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Algorithms for bigram and trigram word clustering",
   "original": "e95_1253",
   "page_count": 4,
   "order": 312,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "This paper presents and analyzes improved algorithms for clustering bigram and trigram word equivalence classes, and their respective results: 1) We give a detailed time complexity analysis of bigram clustering algorithms. 2) We present an improved implementation of bigram clustering so that large corpora (38 million words and more) can be clustered within a small number of days or even hours. 3) We extend the clustering approach from bigrams to trigrams. 4) We present experimental results on a 38 million word training corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-312"
  },
  "ueberla95_eurospeech": {
   "authors": [
    [
     "Joerg P.",
     "Ueberla"
    ]
   ],
   "title": "More efficient clustering of n-grams for statistical language modeling",
   "original": "e95_1257",
   "page_count": 4,
   "order": 313,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "Clustered language models have the advantage of requiring less training data than n-grams, but they may perform worse if the training corpus is \"sufficiently\" large to train the n-gram well. How do the performance of a clustered language model and a n-gram compare on the Wall Street Journal corpus? While trying to address this question, we develop the following two ideas. First an existing algorithm is extended to deal with higher order n-grams. Second, a heuristic to speed up the algorithm is presented. The resulting algorithm is used to cluster bi- and trigrams on the Wall Street Journal corpus and the language models it produces can compete with existing backoff models. Especially when there is only little training data available, the clustered models outperform the back-off models. This is important for practical recognition systems, where it is not always possible to obtain several million words for a given domain.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-313"
  },
  "naeve95_eurospeech": {
   "authors": [
    [
     "Uta",
     "Naeve"
    ],
    [
     "Gudrun",
     "Socher"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Generation of language models using the results of image analysis",
   "original": "e95_1739",
   "page_count": 4,
   "order": 314,
   "p1": "1739",
   "pn": "1742",
   "abstract": [
    "We present a new approach towards using contextual information to enhance speech recognition and understanding. Dynamically inferred knowledge about the context is used in addition to the static linguistic and domain specific knowledge. Based on the results of image analysis of a given scene language models for constituents of possible utterances concerning that scene are generated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-314"
  },
  "flach95_eurospeech": {
   "authors": [
    [
     "Gudrun",
     "Flach"
    ]
   ],
   "title": "Modelling pronunciation variability for special domains",
   "original": "e95_1743",
   "page_count": 4,
   "order": 315,
   "p1": "1743",
   "pn": "1746",
   "abstract": [
    "For the recognition of words by a speech recognition system a description of particular words as a part of the reference knowledge is ncessary, which describes the real appearence of the words as real as possible. Depending on the concrete recognition task, that is the recognition of single words, of controlled fluent spoken word strings (read speech) or of unreglemented spontaneous speech, there are different problems of modelling. In the case of single word recognition we have mostly complete and clear articulation, while by the realisation of word strings or spontaneous speech we find an increasing variation of articulation, implied by speech economy. Here we find changes at the word level and for spontaneous speech there are changes over word boundaries too. The present paper deals with the investigation of read and spontaneous spoken speech and with the description of the observed phenomena. We derive rules for the description of pronunciation variants and we propose structures for pronunciation dictionaries as a part of the reference knowledge, which represent a powerfull modelling. The proposals are verified by recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-315"
  },
  "cremelie95_eurospeech": {
   "authors": [
    [
     "Nick",
     "Cremelie"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "On the use of pronunciation rules for improved word recognition",
   "original": "e95_1747",
   "page_count": 4,
   "order": 316,
   "p1": "1747",
   "pn": "1750",
   "abstract": [
    "Word models representing single pronunciations are often too simple for continuous speech recognition tasks. Multiple pronunciation word models are likely to improve the recognition performance. In this paper a method is introduced to deal with pronunciation variations originating from articulatory interactions between words. These variations are described by rewrite rules which are inferred automatically from a training set. Test results show that a significant reduction of the word error rate can be obtained with the described method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-316"
  },
  "murakami95_eurospeech": {
   "authors": [
    [
     "Jin'ichi",
     "Murakami"
    ]
   ],
   "title": "Reducing memory requirements and computational costs for the baum-welch algorithm and application to automatic stochastic network grammar acquisition",
   "original": "e95_1751",
   "page_count": 4,
   "order": 317,
   "p1": "1751",
   "pn": "1754",
   "abstract": [
    "This paper describes new techniques for language modeling in speech recognition based on the use of a discrete density Ergodic Hidden Markov Model (HMM). A discrete-output Ergodic HMM has a structure similar to that of a stochastic network language model (SNLM), so it can automatically function as an SNLM from a large amount of text data through the Baum-Welch algorithm. However, when the number of states in this Ergodic HMM is large, a large amount of memory is required and the computational cost is high. Therefore, past studies have limited the number of states. Consequently, the resulting perplexity of the Ergodic HMM has been high, and results as good as those obtained for word bigram models have not been obtained. This paper proposes new techniques to reduce the memory requirements and computational costs associated with the Baum-Welch algorithm. These techniques were evaluated for their ability to automatically give an SNLM for an international conference registration task. Based on both the perplexity obtained and the results of continuous speech recognition, this Ergodic HMM was found to outperform word bigram models or trigram models. This implies that the proposed techniques are effective.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-317"
  },
  "besling95_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Besling"
    ],
    [
     "Hans-Günter",
     "Meier"
    ]
   ],
   "title": "Language model speaker adaptation",
   "original": "e95_1755",
   "page_count": 4,
   "order": 318,
   "p1": "1755",
   "pn": "1758",
   "abstract": [
    "In this paper we address the problem of using background information in order to improve models obtained on insufficient amounts of training material, without removing the characteristics of the original material. We introduce the method of language model fill-up that appears to be better suited for this purpose than classical linear interpolation. The specific task we chose to perform tests on is that of adapting a language model to a speaker, which is particularly important for speaker-dependent dictation systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-318"
  },
  "gros95_eurospeech": {
   "authors": [
    [
     "Jerneja",
     "Gros"
    ],
    [
     "R.",
     "Mihelic"
    ],
    [
     "N.",
     "Pavesic"
    ]
   ],
   "title": "Sentence hypothesisation using NG-gram models",
   "original": "e95_1759",
   "page_count": 4,
   "order": 319,
   "p1": "1759",
   "pn": "1762",
   "abstract": [
    "Sentence hypothesisation in a speech recognition and understanding dialogue system for the Slovenian language is presented. A statistical approach using the 3g-gram model based on Jelinek's trigram model was applied to the task of linking word hypotheses together into grammatically well-formed sentences. The equivalence classes are given by 72 different parts of speech, extended by various morpho-syntactic features. A simple smoothing method is presented and compared to a standard one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-319"
  },
  "rosenfeld95_eurospeech": {
   "authors": [
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Optimizing lexical and N-gram coverage via judicious use of linguistic data",
   "original": "e95_1763",
   "page_count": 4,
   "order": 320,
   "p1": "1763",
   "pn": "1766",
   "abstract": [
    "I study the effect of various types and amounts of North American Business language data on the quality of the derived vocabulary, and use my findings to derive an improved ranking of the words, using only 19% of the NAB corpus. I then study the conflicting effects of increased vocabulary size on a speech recognizer's accuracy, and use the result to pick an optimal vocabulary size. A similar analysis of ngram coverage yields a very different outcome, with the best system being the one based on the most data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-320"
  },
  "spies95_eurospeech": {
   "authors": [
    [
     "Marcus",
     "Spies"
    ]
   ],
   "title": "A language model for compound words in speech recognition",
   "original": "e95_1767",
   "page_count": 4,
   "order": 321,
   "p1": "1767",
   "pn": "1770",
   "abstract": [
    "In several languages, words can be aggregated into compound words. In present speech recognition systems, compound words are treated as as additional single words. This creates redundancies in the phonetic word models that have to be stored and searched during recognition. Moreover, it leads to weaknesses in word or n-gram frequency estimates in language models. - This paper describes a novel approach to speech recognition with vocabularies that contain only the composing words of compounds. The recognition of a compound word is performed via a dedicated accessory language model that evaluates compound word hypotheses only. In this way, very large vocabularies (> 100,000 words) can be handled efficiently. In preliminary recognition tests, the model performed well.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-321"
  },
  "muller95_eurospeech": {
   "authors": [
    [
     "Johannes",
     "Müller"
    ],
    [
     "Holger",
     "Stahl"
    ]
   ],
   "title": "Collecting and analyzing spoken utterances for a speech controlled application",
   "original": "e95_1437",
   "page_count": 4,
   "order": 322,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "To estimate the parameters of stochastic knowledge bases for a speech understanding system, many utterances spoken by many people have to be examined. For the regarded domain of a 'graphic editor', two different manners of collecting training data are discussed. An analysis of spoken commands recorded by a 'Wizard of Oz'-simulation shows that the way to talk to a computer usually depends on how familiar a subject is with a computer. Keywords: speech understanding, training of stochastic models, 'Wizard of Oz'-simulation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-322"
  },
  "nagabuchi95_eurospeech": {
   "authors": [
    [
     "Hiromi",
     "Nagabuchi"
    ],
    [
     "Akira",
     "Takahashi"
    ],
    [
     "Mineyoshi",
     "Ogawa"
    ]
   ],
   "title": "Speech intelligibility and loudness assessment in a wireless personal communication",
   "original": "e95_1441",
   "page_count": 4,
   "order": 323,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "This paper investigates the effects of two important speech quality factors, intelligibility and loudness, on the speech quality in a wireless personal telecommunication system (referred to as PHS: the personal handy-phone system). First, using a reference system for simulating near worst-case quality conditions in the PHS, the effects of fading frequency and electric field intensity on the speech intelligibility are measured. Next, an apparatus for measuring loudness characteristics of PHS terminals is developed and using this apparatus the effects of ambient noise and the distance between a speaker's mouth and the transmitter on optimal loudness are measured and a method is presented for determining the optimal loudness rating (LR) in the PHS terminals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-323"
  },
  "hone95_eurospeech": {
   "authors": [
    [
     "K. S.",
     "Hone"
    ],
    [
     "R. W.",
     "Series"
    ],
    [
     "C.",
     "Baber"
    ]
   ],
   "title": "An experimental investigation of the input and error correction strategies used by subjects entering digits with the AURIX speech recogniser",
   "original": "e95_1445",
   "page_count": 4,
   "order": 324,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "An experiment was performed which investigated the input and error correction strategies used by subjects when entering digit strings with the Aurix1 recogniser. Subjects were allowed to control the point at which they received feedback and corrected misrecognitions throughout the trial. It was predicted that they would alter their input strategies as a result of an entry being misrecognised. Specifically it was hypothesised that following a misrecognition, further attempts at entry of that data would use smaller digit input chunks, which should increase the likelihood of success. Comparison of the mean chunk sizes used in error-full compared to error-free interactions showed this to be the case. Some preliminary modelling work was also undertaken based on the experimental results. This suggests that the choice of input chunk size used by subjects approaches the optimal value based on the recognition accuracy obtained with the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-324"
  },
  "belhoula95_eurospeech": {
   "authors": [
    [
     "Karim",
     "Belhoula"
    ]
   ],
   "title": "Goal-directed generation of intelligibility test vocabularies in the framework of names synthesis",
   "original": "e95_1449",
   "page_count": 4,
   "order": 325,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "In this paper a method for the generation of test vocabularies is introduced, basically aimed for the assessment of the intelligibility of speech output systems. The approach is to generate application-oriented vocabularies which mirror the characteristics of the vocabulary typical of the application field. Mostly mono-and poly-syllabic nonwords are generated that follow the phonemotactic and morphotactic rules of the target language, here of the German language. Statistic data that have an impact on the assessment procedure - and thus on the test results - are taken into account. In the context of this paper the generation procedure of a test vocabulary which is typical of an address-optimized synthesis system is given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-325"
  },
  "haebumbach95_eurospeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Stephan",
     "Gamm"
    ]
   ],
   "title": "Human factors of a voice-controlled car stereo",
   "original": "e95_1453",
   "page_count": 4,
   "order": 326,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "Today speech recognition with small vocabulary can be realized so cost effectively that the technology can penetrate into consumer electronics. This paper presents some user interface guidelines that are adapted to or specific to voice-control, given the current state-of-the-art recognition technology. The design of a voice-controlled car stereo shows how said guidelines are turned into practice.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-326"
  },
  "bernsen95_eurospeech": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Hans",
     "Dybkjaer"
    ],
    [
     "Laila",
     "Dybkjaer"
    ]
   ],
   "title": "Exploring the limits of system-directed dialogue, dialogue evaluation of the danish dialogue system",
   "original": "e95_1457",
   "page_count": 4,
   "order": 327,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "Spoken language dialogue systems technologies are beginning to master the design and implementation of applied systems for complex well-structured tasks. Partly for this reason, there is a need for evaluation metrics which include general concepts of task and dialogue types. The paper reports on the scenario-based user test of the dialogue management of an airline ticket reservation system. The test data are compared to the data from the last Wizard of Oz iteration before the system was implemented. Detailed analysis of user dialogue behaviour reveals a series of principled limitations of system-directed dialogue for complex well-structured tasks. The discussion weighs those limitations against the demonstrated potential of system-directed dialogue for a broad class of tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-327"
  },
  "leeuwen95_eurospeech": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Leo-Geert van den",
     "Berg"
    ],
    [
     "Herman J. M.",
     "Steeneken"
    ]
   ],
   "title": "Human benchmarks for speaker independent large vocabulary recognition performance",
   "original": "e95_1461",
   "page_count": 4,
   "order": 328,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "In order to evaluate and compare the recognition performance of automatic speech recognizers and of humans, sentences were selected from the Wall Street Journal database (wsjO and WSJCAMO). Eighty sentences, spoken by native British and American speakers, were presented to three automatic speech recognizers (trained under strict conditions) and thirty human listeners for recognition. A comparison of the performance of human and machine recognition was made, resulting in average total word error rates of 2.6 % for humans (native listeners) and 12.6% for machines. The ASR systems had great difficulty with sentences with a high perplexity. Listeners tended to be more sensitive to sentence length: long sentences were more difficult to recognize than short sentences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-328"
  },
  "simons95_eurospeech": {
   "authors": [
    [
     "Alison",
     "Simons"
    ]
   ],
   "title": "Predictive assesment for speaker independent isolated word recognisers",
   "original": "e95_1465",
   "page_count": 3,
   "order": 329,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "This paper describes a method for assessing the likely accuracy of a speaker independent isolated word recogniser on a new vocabulary, without the use of field trials or database collections. An algorithm is introduced which attempts to predict vocabulary difficulty using a word confusability measure. This is based on the likely pronunciation of words, coupled with information about likely phone confusions. The power of this technique to predict recogniser accuracy is tested through comparison with the measured performance of an actual recogniser on the 'Macrophone' telephony speech corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-329"
  },
  "satoshi95_eurospeech": {
   "authors": [
    [
     "Kobayashi",
     "Satoshi"
    ],
    [
     "Kitazawa",
     "Shigeyoshi"
    ]
   ],
   "title": "Consistency of inter-transcribers' transcription",
   "original": "e95_1263",
   "page_count": 4,
   "order": 330,
   "p1": "1263",
   "pn": "1266",
   "abstract": [
    "Paralinguistic features have many aspects and play important roles in human dialogue. Transcription of dialogue is a useful technique for analysis of dialogues. However, paralinguistic features are classified by transcribers' subjective. Inconsistencies, such as omissions and misleading, between transcribers are unavoidable. Such inconsistencies between transcribers arise the problem about accuracy and objectivity of analysis. In this paper, we evaluated those errors and inconsistencies based on the crosscheck between transcribers on conditions such as transcribers' skill, using graphical interface and verifying transcription with synthetic sounds. The results show training and graphic interface had positive effects on consistency of inter-transcribers' transcription.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-330"
  },
  "klaus95_eurospeech": {
   "authors": [
    [
     "H.",
     "Klaus"
    ],
    [
     "A.",
     "Niebank"
    ]
   ],
   "title": "Comparison of reference system approaches for the quality assessment of synthesized speech",
   "original": "e95_1267",
   "page_count": 4,
   "order": 331,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "At the Institute of Telecommunications of the Technical University of Berlin, three speech assessment experiments were performed to compare the Modulated Noise Reference Unit (MNRU) [1] and the time and frequency warping (TFW) reference system [2]. To perform this evaluation, a category rating test procedure similar to ITU-Rec. P.85 [3] was implemented in an evaluation system developed at the Institute of Telecommunications [4]. This paper explains in the first part details of the reference systems and summarises the evaluation results of both systems individually (sections 1 to 3). Section 4 compares of the assessment results by a number of synthec speech samples that were commonly used in all experiments. Keywords: synthetic speech, category rating test, reference systems, modulated noise reference unit, time frequency warping\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-331"
  },
  "steeneken95_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Multi-lingual assessment of speaker independent large vocabulary speech-recognition systems: THE SQALE-PROJECT",
   "original": "e95_1271",
   "page_count": 4,
   "order": 332,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "In this paper we present the first results of the CEC-sponsored project Sqale (Speech recognition Quality Assessment for Linguistic Engineering). In this project the recognition performance of four different systems in four different languages is assessed. The official word-error scores are presented, and the significant differences between the systems are indicated. Finally, the first results on the analysis of the various word error rates are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-332"
  },
  "bartkova95_eurospeech": {
   "authors": [
    [
     "K.",
     "Bartkova"
    ],
    [
     "D.",
     "Dubois"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "J.",
     "Monné"
    ]
   ],
   "title": "Error analysis on field data and improved garbage HMM modelling",
   "original": "e95_1275",
   "page_count": 4,
   "order": 333,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "The aim of the this paper is to examine the extent to which field data can improve speech recognition performance when included into the training procedure of the model parameters. The approach used herein was twofold. First of all, a thorough error analysis of the misrecognized words was carried out before and after the introduction of field data into the training procedure. Such an analysis enabled the detection of major perturbation effects on word recognition, as well as the possibility of modelling them using Hidden Markov Models. Secondly, garbage models were trained using non-speech signals and out-of-vocabulary words from the field data, together with garbage models trained on laboratory data. These garbage models showed improved rejection performance when compared to garbage models trained exclusively with laboratory data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-333"
  },
  "verheijen95_eurospeech": {
   "authors": [
    [
     "E. J. A.",
     "Verheijen"
    ],
    [
     "F. L. van",
     "Nes"
    ],
    [
     "L. M. de",
     "Bruyn"
    ],
    [
     "A.",
     "Hasman"
    ],
    [
     "J. W.",
     "Arends"
    ]
   ],
   "title": "Interference of speech recognition feedback during diagnostic tasks",
   "original": "e95_1279",
   "page_count": 4,
   "order": 334,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "The difficulty of detecting errors in one's own reports may be the bottleneck in enabling a widespread application of automatic speech recognition (ASR). Although quite a lot of research concerning correction procedures has been conducted, detection strategies have received little attention. As ASR systems still produce errors a lot depends on the detectability of errors especially in the medical environment, as reports have to be error-free. This paper presents the results of an experiment which investigated error detection performance during a diagnostic task. No differences in detection performance could be found between auditory or visual feedback except for the detection of substituted words. For this category of errors, visual feedback proved to be better.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-334"
  },
  "vaxelaire95_eurospeech": {
   "authors": [
    [
     "Beatrice",
     "Vaxelaire"
    ]
   ],
   "title": "Geometric and temporal constraints in the production of French consonant sequences x-ray and acoustic data for French",
   "original": "e95_1285",
   "page_count": 4,
   "order": 335,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "The aim of this investigation is to analyze, for French, the behaviour of single as opposed to double (abutted) consonants, and also to examine the coordination of articulators in the production of heterorganic consonant sequences in French. X-ray and acoustic data are obtained, for two speakers (one female and one male) at two speaking rates. If configurational constraints have been proposed for articulatory modelling of vowels [1]; [2]; [3], those related to consonant production are lacking in the literature [4].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-335"
  },
  "laboissiere95_eurospeech": {
   "authors": [
    [
     "Rafael",
     "Laboissiere"
    ],
    [
     "Vitpoxio",
     "Sanguineti"
    ],
    [
     "Yohan",
     "Payan"
    ]
   ],
   "title": "On the biomechanical control variables of the tongue during speech movements",
   "original": "e95_1289",
   "page_count": 4,
   "order": 336,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "A model of the tongue musculature is introduced which is based on the Equilibrium Point Hypothesis (A model). The model is fitted to mid-sagittal cinera-diographic data in order to extract muscles synergies through a Principal Component Analysis. An assessment of the effects of this commands on tongue shape is done, showing how the degrees of freedom of tongue profile relate to the biomechanical substrate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-336"
  },
  "bouabana95_eurospeech": {
   "authors": [
    [
     "Soumya",
     "Bouabana"
    ],
    [
     "Shinji",
     "Maeda"
    ]
   ],
   "title": "Multipulse LPC modeling of articulatory movements: analysis and interpretation",
   "original": "e95_1293",
   "page_count": 4,
   "order": 337,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "The temporal variation of the tongue profiles, derived from X-ray film data, are described in terms of those of four articulatory parameters. A filter model of movement for each tongue parameter is developed in which the impulse response is described by a damped, linear second-order system. The filter describing the system are assumed to be time invariant, and the excitation model is represented by a train of pulses. This model, excited by a limited but large number of pulses, produced a faithful synthesized movement. In this paper, our objective is not the faithful reproduction of the movement. Rather, our interest is to find out something about the nature of inter-articulator coordination during speech by analyzing the organization of pulses. Because of this objective, we determine the minimum number of pulses necessary to describe movements using the difference limen of for-mant frequencies as an acoustic criterion. The preliminary analysis suggests that this number depends on the phonetics features in the utterance, of which their realization is related to particular articulatory parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-337"
  },
  "richard95_eurospeech": {
   "authors": [
    [
     "G.",
     "Richard"
    ],
    [
     "M.",
     "Liu"
    ],
    [
     "D.",
     "Snider"
    ],
    [
     "H.",
     "Duncan"
    ],
    [
     "Qiguang",
     "Lin"
    ],
    [
     "James L.",
     "Flanagan"
    ],
    [
     "Stephen",
     "Levinson"
    ],
    [
     "D.",
     "Davis"
    ],
    [
     "S.",
     "Slimon"
    ]
   ],
   "title": "Numerical simulations of fluid flow in the vocal tract",
   "original": "e95_1297",
   "page_count": 4,
   "order": 338,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "An alternate approach to speech synthesis based on numerical solution of Navier-Stokes (NS) and Reynolds-Averaged-Navier-Stokes (RANS) equations is described. Unlike the traditional methods based on linear acoustic theory, the NS and RANS formulations are not limited by the assumptions of linearity, negligible viscous effects, and plane wave propagation. In the present formulation, the Navier-Stokes equations are discretized and solved using a finite difference method. Initial applications involve 2-D simulations of flow through ideal channels (straight or curved tubes). In another application, the formulation is applied to the geometry of the three cardinal vowels. Synthetic speech sounds of encouraging quality are obtained for the three vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-338"
  },
  "suzuki95_eurospeech": {
   "authors": [
    [
     "Hisayoshi",
     "Suzuki"
    ],
    [
     "Takayoshi",
     "Nakai"
    ],
    [
     "Hiroshi",
     "Sakakibara"
    ]
   ],
   "title": "3-d fem analysis of sound propagation in the nasal tract",
   "original": "e95_1301",
   "page_count": 4,
   "order": 339,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "To examine the acoustical effects of complicated cross section shape and bent form of the nasal tract, sound propagation in the nasal tract is investigated. Finite Element Method Analysis (FEM) has been applied for the acoustic models of the nasal tract. The acoustic models are so constructed that each model has different degree of simplification of actual morphological measurements obtained by MRI (Magnetic Resonance Imaging Technology). Comparing the resulting acoustical characteristics of those models, it is found that how the maxillary sinuses and the right-left asymmetry affect the pole-zero configuration of sound spectra of nasal speech, and how the complicated cross section shape of nasal tract affects the sound flow in the tract.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-339"
  },
  "prieto95_eurospeech": {
   "authors": [
    [
     "Pilar",
     "Prieto"
    ],
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Effects of tonal clash on downstepped h* accents in Spanish",
   "original": "e95_1307",
   "page_count": 4,
   "order": 340,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "In this paper we discuss the effects of tonal clash (or strict adjacency between two accents) on the phonetic realization of H* accents in Spanish. A preliminary analysis of the data shows that adjacency of two H* accents triggers a drastic temporal reorganization of the F0 gestures involved, resulting in anticipation of the first gesture and delay of the second. Specifically, in crowded tonal contexts we find that the start of the rise of the first H* accent occurs significantly earlier with respect to the syllable onset, as well as later in the second accent. Similarly, the two adjacent accented syllables are lengthened and their H* rise times and peak delays increase. Thus, the data reveals that F$ gestures are roughly timed to accented syllables, keeping a more or less floating relationship with the segmentals: both right and left-hand prosodic contexts (including clash situations) determine the alignment patterns of the valley and the peak. Finally, no significant differences were found on the F0 scaling in clash vs. non-clash environments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-340"
  },
  "kondo95b_eurospeech": {
   "authors": [
    [
     "Mariko",
     "Kondo"
    ]
   ],
   "title": "The effect of two factors related to speaking tempo on vowel devoicing in Japanese",
   "original": "e95_1311",
   "page_count": 4,
   "order": 341,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "Vowel devoicing rates in Japanese were examined at three different speaking rates. The results showed that effect of speaking rate was generally minimal, and that high vowel devoicing occurred at all tempi, but the effect of tempo varied depending on the devoicing environment: (1) type of preceding consonants and (2) devoicing condition, single or consecutive devoicing environments. There was no tempo effect in single devoicing condition, but more devoicing occurred at faster tempi in the consecutive condition. Unlike similar phenomena in other languages, Japanese vowel devoicing does not appear to be an optional process in fast speech, but more positive process partly controlled by its syllable structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-341"
  },
  "goedemans95_eurospeech": {
   "authors": [
    [
     "Rob",
     "Goedemans"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Duration perception in subsyllabic constituents",
   "original": "e95_1315",
   "page_count": 4,
   "order": 342,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "The central aim of our experiments is to find a phonetic explanation for the phonological irrelevance of the onset (i.e. prevocalic consonants) for syllable weight. From a set of pilot experiments (cf. [1]) we deduced that the cause of this irrelevance might be found in speech perception rather than speech production. Subjects proved to be less sensitive to duration changes in the onset than to such changes in either nucleus or coda. Assuming that perceived duration is the primary phonetic correlate of syllable weight, we may have found a clue as to why the onset does not count in the determination of syllable weight. This answer immediately yields another question. We do not know why subjects are unable to reliably perceive onset durations. So, we must go one step deeper into the matter, and find out what it is that prevents us from correctly determining onset duration. This study focuses on two experiments. The first is an experiment that is designed to check whether listeners remain insensitive to onset duration under different circumstances. In the other experiment we explore one line of reasoning that may provide an answer to the follow-up question.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-342"
  },
  "bergem95_eurospeech": {
   "authors": [
    [
     "Dick R. van",
     "Bergem"
    ]
   ],
   "title": "Experimental evidence for a comprehensive theory of vowel reduction",
   "original": "e95_1319",
   "page_count": 4,
   "order": 343,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "In this paper a comprehensive theory of vowel reduction is presented based on a large amount of experimental data gathered by Van Bergem (1995b). The schwa, playing a major role in the process of vowel reduction, is often thought to be a vowel that is produced with a 'neutral' vocal tract. This view is shown to be wrong. The schwa appears to be a vowel that is completely assimilated with its phonemic context; its vowel colour can vary widely. As a consequence, vowel reduction should not be interpreted as centralization, but as contextual assimilation. Two types of vowel reduction are discussed: acoustic reduction and lexical reduction. These can very well be interpreted as two intermediate stages in the process of the sound change 'full vowel -> schwa'.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-343"
  },
  "ohala95_eurospeech": {
   "authors": [
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Clear speech does not exaggerate phonemic contrast",
   "original": "e95_1323",
   "page_count": 4,
   "order": 344,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "Structural linguistics teaches that one of the principal properties of speech sounds is being different from each other: the essence of a phoneme is that it is not any other phoneme. In addition, many structuralist theories of sound change are based on notions of preservation of contrast between phonemes. From this one might expect that it should be possible to observe speakers' efforts at maintaining contrasts in speech. One situation where this would be expected is in repeated speech, i.e., where a speaker repeats a word after receiving feedback that it has been misapprehended as another similar word. This paper reports results of an analysis of such repeated speech samples of 10 English speakers when they produced one of 12 near-minimal syllables (bayed, paid, bed, ped, bet, pet, bid, bit, etc) under two conditions: control and repetition (in response to feedback that their initial production had been misunderstood as one of the other syllables). Contrary to expectations, there was no significant contrastive exaggerations in VOT or vowel duration as a function of the word presented in feedback.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-344"
  },
  "fitt95_eurospeech": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ]
   ],
   "title": "The pronunciation of unfamiliar native and non-native town names",
   "original": "e95_2227",
   "page_count": 4,
   "order": 345,
   "p1": "2227",
   "pn": "2230",
   "abstract": [
    "This paper will discuss pronunciations of unfamiliar names, both British and foreign, by native speakers of English. Most studies which look at peoples' pronunciations of unfamiliar or pseudowords are based on English word-patterns, rather than a cross-language selection, while algorithms for determining the pronunciation of names from a variety of languages do not necessarily tell us how real people behave in such a situation. This paper shows that subjects may use different systems or sub-systems of rules to pronounce unknown names which they perceive to be non-native. If we wish to model human behaviour in novel word pronunciation, we need to take into account the fact that, while native speakers are not experts in all foreign languages, neither are they linguistically naive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-345"
  },
  "gustafson95_eurospeech": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Using two-level morphology to transcribe Swedish names",
   "original": "e95_2231",
   "page_count": 4,
   "order": 346,
   "p1": "2231",
   "pn": "2234",
   "abstract": [
    "Names are difficult to handle for normal letter-to-sound rules, since these usually are designed for ordinary words. The structure of Swedish names differ from ordinary words - but their multi-morphemic structure make them suitable to analyse with a morphological analyser. The paper presents the work on names from the Swedish telephone directory, as part of the ONOMASTICA project [7], including a brief study of the structure of Swedish names. The speech communication group at KTH have developed a system where a morphology analyser is used together with a set of rules to transcribe ordinary Swedish words. This paper will describe the work done to extend this system to cope with names as well. The paper shows that the approach of transcribing Swedish names with the Two-level Morphology analyser (TWOL) is appropriate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-346"
  },
  "demolin95_eurospeech": {
   "authors": [
    [
     "Didier",
     "Demolin"
    ],
    [
     "Jean-Marie",
     "Hombert"
    ],
    [
     "Véronique",
     "Lecuit"
    ],
    [
     "Christoph",
     "Segebarth"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "An MRI study of French vowels",
   "original": "e95_2235",
   "page_count": 4,
   "order": 347,
   "p1": "2235",
   "pn": "2238",
   "abstract": [
    "MRI techniques have been used to obtain mid-sagittal cuts of vowels uttered by several French speakers. These data have been compared with data coming from X-ray studies made by Bothorel et al. [7]. Area functions and derived formants have been evaluated by a comparison with formant frequencies extracted from vowels in a session different from the MRI recording sessions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-347"
  },
  "sole95_eurospeech": {
   "authors": [
    [
     "Marie-Josep",
     "Solé"
    ],
    [
     "E.",
     "Estebas"
    ]
   ],
   "title": "Connected speech processes: a cross-linguistic study",
   "original": "e95_2239",
   "page_count": 4,
   "order": 348,
   "p1": "2239",
   "pn": "2242",
   "abstract": [
    "Assimilatory and blending processes in connected speech in English and Catalan in a variety of conditions (slow vs fast speech, functional vs lexical words and oral vs nasal segments) were studied. The intergestural adaptation of alveolar+dental/alveolopalatal clusters were analyzed with simultaneous EPG, acoustic and EGG data. The results show a categorical articulatory shift of Cl to the constriction location of C2 in clusters involving the same articulator (tongue tip), which is argued to reflect a reorganization of the alveolar articulation at the gestural level. Alveolar+alveolopalatal clusters, involving two independent articulators (tongue tip and tongue body) show gradient blending processes reflecting overlap of articulatory trajectories at the vocal tract level. Blending of constriction degree in clusters tends to be preservatory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-348"
  },
  "deligne95_eurospeech": {
   "authors": [
    [
     "Sabine",
     "Deligne"
    ],
    [
     "Francois",
     "Yvon"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Variable-length sequence matching for phonetic transcription using joint multigrams",
   "original": "e95_2243",
   "page_count": 4,
   "order": 349,
   "p1": "2243",
   "pn": "2246",
   "abstract": [
    "The joint multigram model is a statistical model, which achieves a many-to-many mapping between two, or more, streams of symbols. It relies on a fully unsupervised training phase, during which variable-length sequences of symbols from each stream are matched together according to a maximum likelihood criterion. Then the model can be used to perform sequence-by-sequence decoding. It is evaluated for a task of automatic orthographic-phonetic transcription, and results in a 95 % phoneme accuracy on the French corpus BDLEX. Preliminary experiments to test the ability of the model for a task of continuous speech recognition are also reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-349"
  },
  "tajchman95_eurospeech": {
   "authors": [
    [
     "Gary",
     "Tajchman"
    ],
    [
     "Eric",
     "Foster"
    ],
    [
     "Daniel",
     "Jurafsky"
    ]
   ],
   "title": "Building multiple pronunciation models for novel words using exploratory computational phonology",
   "original": "e95_2247",
   "page_count": 4,
   "order": 350,
   "p1": "2247",
   "pn": "2250",
   "abstract": [
    "In this paper we describe a completely automatic algorithm that builds multiple pronunciation word models by expanding baseform pronunciations with a set of candidate phonological rules. We show how to train the probabilities of these phonological rules, and how to use these probabilities to assign pronunciation probabilities to words not seen in the training corpus. The algorithm wg propose is an instance of the class of techniques we call Exploratory Computational Phonology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-350"
  },
  "aguilar95_eurospeech": {
   "authors": [
    [
     "Lourdes",
     "Aguilar"
    ],
    [
     "Maria",
     "Machuca"
    ]
   ],
   "title": "Pragmatic factors affecting the phonetic properties of diphthongs",
   "original": "e95_2251",
   "page_count": 4,
   "order": 351,
   "p1": "2251",
   "pn": "2254",
   "abstract": [
    "In this study, the main aim is to investigate the interaction of two pragmatic factors - familiarity between the speakers and intentionality of the speech act- by means of the study of the phonetic realization of diphthongs in Spanish. The experiment compares the acoustic properties of diphthongs found in four speech situations: map task with a familiar addressee, map task with an unfamiliar addressee, interview with a familiar interviewer and interview with an unfamiliar interviewer. Only in the case of the map task is an overt intentionality assumed. It is inferred from the results that a set of phonetic processes can be described in informal unread speech, and that it is the frequency of occurrence of each process which characterises different speech situations. Moreover, it has been shown that the existence of familiarity between the speakers triggers the preference of speakers towards a particular phonetic process when they are performing a specific task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-351"
  },
  "farnetani95_eurospeech": {
   "authors": [
    [
     "Edda",
     "Farnetani"
    ]
   ],
   "title": "The spatial and the temporal dimensions of consonant reduction in conversational Italian",
   "original": "e95_2255",
   "page_count": 4,
   "order": 352,
   "p1": "2255",
   "pn": "2258",
   "abstract": [
    "This study is an electropalatographic investigation of the spatiotemporal characteristics of consonants /t/,/d/,/n/,/l/, /s/ in Italian, produced by two subjects in spontaneous, conversational speech. The data show that consonant reduction is a continuous process, inversely related to duration, in agreement with Lindblom's model [1,2]. Duration-dependent target undershoot is manifested as a progressive decrease in tongue tip/blade contact and a retraction of the constriction location, to extreme cases where no sign of. front contact is detectable (and the consonant is no longer perceived). An important finding is that the degree and the frequency of undershoot is consonant-specific and that the reduction patterns lawfully mirror the tongue-body coarticulation patterns observed in previous research on isolated words. This indicates the consonant-specific production constraints are preserved across speaking styles.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-352"
  },
  "gillis95_eurospeech": {
   "authors": [
    [
     "S.",
     "Gillis"
    ],
    [
     "G. De",
     "Schutter"
    ],
    [
     "J.",
     "Verhoeven"
    ]
   ],
   "title": "Neutralization of consonant length: the case of dutch intervocalic stops",
   "original": "e95_2259",
   "page_count": 4,
   "order": 353,
   "p1": "2259",
   "pn": "2262",
   "abstract": [
    "In a phonological perspective, single intervocalic consonants in Dutch are generally considered to be either autosyllabic or ambisyllabic depending on the nature of the preceding vowel. On the basis of a review of the relevant literature and recently obtained experimental results, it is argued that this phonological distinction does not have any clearly defined phonetic correlates. In this perspective, the concept of neutralization is discussed theoretically with respect to the degemination rule in Dutch. This discussion reveals a number of serious problems regarding the precise nature of neutralization in the case of the degemination rule in Dutch, i.e. whether it is contextual or absolute.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-353"
  },
  "kotani95_eurospeech": {
   "authors": [
    [
     "Manabu",
     "Kotani"
    ],
    [
     "Haruya",
     "Matsumoto"
    ]
   ],
   "title": "Sound perception between two languages based on analyses of onomatopoeic expression",
   "original": "e95_2263",
   "page_count": 4,
   "order": 354,
   "p1": "2263",
   "pn": "2266",
   "abstract": [
    "This paper describes the study on the difference of sound perception between language speakers through the comparison of onomatopoeic utterances on the analytical basis. Onomatopoeic expressions are the words whose sounds relate to the meaning. They, therefore, may form the key by which the relationship between the sound perception and the sound production is clarified. We pay attention to three kinds of psychological attributes of the sound perception and compare them for the Japanese and Chinese languages for original sounds. The result shows that the manner of the perception of acoustic features differs in some extent between Japanese and Chinese speakers. The investigation and the comparison of the onomatopoeic expressions will be effective for the study of the nature of sound perception of each language speaker.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-354"
  },
  "yan95_eurospeech": {
   "authors": [
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "An approach to language identification with enhanced language model",
   "original": "e95_1351",
   "page_count": 4,
   "order": 355,
   "p1": "1351",
   "pn": "1354",
   "abstract": [
    "An approach to Language Identification (LID) based on language-dependent phone recognition is presented. This LID system is designed to exploit varying phonotactic constraints of different languages. Based on the output of language-dependent phone recognizers, various LID features are extracted. Two methods are proposed to enhance the language modeling accuracy, (1) language models based on forward and backward bigrams, and (2) back-propagation based language model optimization. The system was evaluated on a standard 11-language task and a standard nine-language task. The results (correct rate) reached 87.6% for 45-second long utterances and 73.6% for 10-second long utterances for the 11-language task, and reached 87.8% and 74.0% respectively on the nine-language task. By adding channel normalization, the performance of our best systems was further improved to 90.8% and 77.1% for the 11-language task, and 91.1% and 77.5% on the nine-language task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-355"
  },
  "nowell95_eurospeech": {
   "authors": [
    [
     "P.",
     "Nowell"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "The application of dynamic programming techniques to non-word based topic spotting",
   "original": "e95_1355",
   "page_count": 4,
   "order": 356,
   "p1": "1355",
   "pn": "1358",
   "abstract": [
    "This paper describes the application of dynamic programming (DP) techniques to the problems of building and testing non-word based topic spotters. We use a DP algorithm to find sets of similar phoneme sequence fragments, which we call DP-n-grams, and to detect their occurrences in the training and test data. The ability to use partial matches means that that the fragments are longer and more meaningful than the phoneme n-grams that have been tried previously. Detection probabilities of over 90% with less than 10% probability of a false alarm are achieved for seven target categories. Reports about bridges and pontoons are detected with 90% probability at a probability of false alarm of less than 1%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-356"
  },
  "shuichi95_eurospeech": {
   "authors": [
    [
     "Itahashi",
     "Shuichi"
    ],
    [
     "Du",
     "Liang"
    ]
   ],
   "title": "Language identification based on speech fundamental frequency",
   "original": "e95_1359",
   "page_count": 4,
   "order": 357,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "This paper describes a spoken language identification method based on speech fundamental frequency (F0). The procedure is subdivided into three main stages: 1) F0 extraction and segmentation; 2) polygonal line approximation of F0 pattern; 3) discriminant analysis. The stage of F0 extraction uses the Average Magnitude Difference Function(AMDF) and speech energy to estimate the fundamental frequency period of voiced speech sounds. In order to find better features from F0 pattern, polygonal lines are used to approximate the F0 contour of voiced intervals. After previous two stages, the complete parameter set is available for discrimination. The principal component analysis and discriminant analysis are performed at the last stage. The system is trained and tested using a CD-ROM. The Multi-language Speech Database for Telephonometry 1994\", which is produced by NTT and NATC, and the OGI Multi-language Telephone Speech Corpus. Keywords: F0 contour, Principal Component analysis, Discriminant analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-357"
  },
  "lund95_eurospeech": {
   "authors": [
    [
     "Michael A.",
     "Lund"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "Two novel language model estimation techniques for statistical language identification",
   "original": "e95_1363",
   "page_count": 4,
   "order": 358,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "This paper presents two novel language identification algorithms for use with minimal training data. No transcriptions or dictionaries are required for training, as the acoustic models are based on English speech only and the language models are derived from phonetic sequences generated by an HMM recognizer. In the first approach, pseudo-words are generated from the output of a phoneme recognizer by a sub-string alignment algorithm followed by agglomerative clustering of the aligned sub-strings. A bigram language model incorporating phonemes and pseudo-words is built for each language, and HMM likelihood scores (including contributions from both acoustic models and language models) are used in language discrimination. In the second approach, an iterative language model estimation algorithm is used. Language-pair discrimination experiments on the OGI multi-language telephone speech corpus show that both new methods provide an effective characterization of the languages to be identified.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-358"
  },
  "kwan95_eurospeech": {
   "authors": [
    [
     "HingKeung",
     "Kwan"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Recognized phoneme-based N-gram modeling in automatic language identification",
   "original": "e95_1367",
   "page_count": 4,
   "order": 359,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "Since it is by no means easy to achieve good phoneme recognition rate for noisy telephone speech, N-gram built upon recognized phoneme labels was evaluated and was found to be more effective than the N-gram built upon original attached phoneme labels for language identification. The performance of mixed phoneme recognizer, in which both language-dependent and language-independent phonemes were included, was also evaluated. Results showed the performance was better than that using parallel language-dependent phoneme recognizers in which bias existed due to different numbers of phonemes among languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-359"
  },
  "varona95_eurospeech": {
   "authors": [
    [
     "A.",
     "Varona"
    ],
    [
     "I.",
     "Torres"
    ],
    [
     "F.",
     "Casacuberta"
    ]
   ],
   "title": "Discriminative-transitional/steady units for Spanish continuous speech recognition",
   "original": "e95_1471",
   "page_count": 4,
   "order": 360,
   "p1": "1471",
   "pn": "1474",
   "abstract": [
    "The design of current acoustic-phonetic decoders for a specific language involves the selection of an adequate set of sub-lexical units. In previous works [1] a set of context independent units was presented for Spanish Continuous Speech Recognition. The aim of this work was to extend this basic set representing context variability under a discriminative-transitional/steady criterion. Our main goal was to obtain a good trade-off between discriminative ability, context variability and number of units. Finally, and within the framework of Hidden Markov Modelling, different series of acoustic-phonetic decoding experiments were earned out over a Spanish Continuous Speech corpus. The phone recognition rates obtained through these experiments showed that the proposed criterion seems to be an interesting approach to model context v ariability in Spanish.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-360"
  },
  "mingy95_eurospeech": {
   "authors": [
    [
     "Ji",
     "Mingy"
    ],
    [
     "Peter",
     "O'Boyle"
    ],
    [
     "Jack",
     "Smith"
    ]
   ],
   "title": "An HMM with optimized segment-dependent observations for speech recognition",
   "original": "e95_1475",
   "page_count": 4,
   "order": 361,
   "p1": "1475",
   "pn": "1478",
   "abstract": [
    "A hidden Markov model (HMM) with variable-length-segment dependent observations is presented. This model is a form of n -gram constrained HMM, with the segment-length n determined by maximizing the likelihood of the observations in both training and recognition. The objective is to better capture the temporal correlation structure, which is assumed to be time-varying due to the nonstationary and utterance-specific characteristics of speech, as opposed to some constant temporal structures in many available HMM approaches. In the construction of the model, a variable-to-fixed length conversion is introduced; an information-theoretic criterion based segment-length weighting scheme is proposed to control the search for the optimal segment-lengths as well as to compensate for the effect of the segment-length-conversion. The model is estimated by using a modified Viterbi algorithm which performs joint state-sequence decoding and segmentation. Both speaker-dependent and speaker-independent recognition experiments show clearly the advantage of this optimized segment-dependent structure over the fixed-length segment based model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-361"
  },
  "moudenc95_eurospeech": {
   "authors": [
    [
     "T.",
     "Moudenc"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "J.",
     "Monné"
    ]
   ],
   "title": "Improving recognition performances on field data with an a-priori segmentation of the speech signal",
   "original": "e95_1479",
   "page_count": 4,
   "order": 362,
   "p1": "1479",
   "pn": "1482",
   "abstract": [
    "This paper investigates the modelling of speech signal stationarity changes in order to improve both recognitions and rejection performances of an HMM based speech recognition system on field data, which also implies the rejection of noises and out-of-vocabulary utterances. Implemented in an N-best solution post-processing, the use of discrete probability distributions of the number of speech signal stationarity changes inside each phonetic segment enables the computation of a segmental postprocessing score. The experiments conducted on afield recording database resulted in a 6 % reduction in the substitution error rate on the correct and truncated utterances, and a 35 % reduction in the false alarm error rate on noises and out-of-vocabulary utterances. This yielded a 20 % global error rate reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-362"
  },
  "welling95_eurospeech": {
   "authors": [
    [
     "L.",
     "Welling"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "A.",
     "Eiden"
    ],
    [
     "C.",
     "Forbrig"
    ]
   ],
   "title": "Connected digit recognition using statistical template matching",
   "original": "e95_1483",
   "page_count": 4,
   "order": 363,
   "p1": "1483",
   "pn": "1486",
   "abstract": [
    "In this paper we describe the optimization of 'conventional template matching techniques for connected digit recognition (TI/NIST connected digit corpus). In particular we carried out a series of experiments in which we studied various aspects of signal processing, acoustic modeling, mixture densities and linear transforms of the acoustic vector. After all optimization steps, our best string error rate on the TI/NIST connected digit corpus was 1.71% for single densities and 0.74% for mixture densities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-363"
  },
  "falkhausen95_eurospeech": {
   "authors": [
    [
     "Markus",
     "Falkhausen"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "Dietrich",
     "Wolf"
    ]
   ],
   "title": "Calculation of distance measures between hidden Markov models",
   "original": "e95_1487",
   "page_count": 4,
   "order": 364,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "This paper investigates two methods to define a distance measure between any pair of Hidden Markov Models (HMM). The first one is the geometricaly motivated euclidean distance which solely incorporates the feature probabilities. The second mesures is the Kulback-Liebler distance which is based on the discriminating power of the probability measure on the space of feature sequences induced by the HMMs. A method is shown, to compute the proposed measures reasonable fast and the distance measures are compared in a series of simulations involving HMMs from a real world speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-364"
  },
  "shen95_eurospeech": {
   "authors": [
    [
     "Jia-lin",
     "Shen"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "A chernoff distance based segmental probability model (CD-SPM) approach for Mandarin syllable recognition",
   "original": "e95_1491",
   "page_count": 4,
   "order": 365,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "A new model matching approach for Mandarin syllable recognition based on the Chernoff distance is proposed in this paper. The recognition process is evaluated segment by segment instead of frame by frame based on the Chernoff distance such that the recognition time can be reduced significantly. Besides, since the measurement criterion, i.e. Chernoff distance, is also derived from the Bayes theorem, competitive recognition rates are expected to be achieved. Experimental results show that compared with the phone-based CHMM's, more than 25 times of recognition speed can be obtained with a 12.43% error rate reduction using less than | mixture numbers in the proposed Chernoff distance based segmental probability models (CD-SPM).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-365"
  },
  "castro95_eurospeech": {
   "authors": [
    [
     "M. J.",
     "Castro"
    ],
    [
     "F.",
     "Prat"
    ],
    [
     "P.",
     "Aibar"
    ],
    [
     "F.",
     "Casacuberta"
    ]
   ],
   "title": "Geometric pattern recognition techniques for acoustic-phonetic decoding of Spanish continuous speech",
   "original": "e95_1495",
   "page_count": 4,
   "order": 366,
   "p1": "1495",
   "pn": "1498",
   "abstract": [
    "Three different geometric pattern recognition techniques for acoustic modeling of phone units (or other sub-word units) for continuous speech recognition are presented for their application to acoustic-phonetic decoding tasks. In the first methodology, each phone unit is modeled as a collection of templates in order to capture the variability of the acoustic events that characterize it, and the acoustic-phonetic decoding is performed by an improved version of the classical Two-Level algorithm. The other two methodologies are used integrated with hidden Markov models. The learning algorithms of the geometric components of such hybrid models allow to improve the discriminative skills of hidden Markov models, while the temporal deformation of the patterns is dealt with the Markov chain. Different speaker-independent and task-independent experiments with one Spanish database are performed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-366"
  },
  "bocchieri95_eurospeech": {
   "authors": [
    [
     "Enrico",
     "Bocchieri"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "State tying of triphone HMM's for the 1994 AT&t ARPA ATIS recognizer",
   "original": "e95_1499",
   "page_count": 4,
   "order": 367,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "This paper is concerned with a new ensemble merging algorithm for triphone Hidden Markov Model (HMM) state tying, that has been used in the AT&T spoken language recognizer for the official 1994 ARPA ATIS evaluation. In controlled experiments, we show that ensemble merging provides both more robust estimates of the triphone HMM's, and increased HMM resolution. In general, distribution tying is performed by sharing (merging) the data of the available state ensembles, to provide larger ensembles useful for likelihood function estimation. As an objective criterion for state tying, we define a total distortion function. Merging two or more ensembles never decreases the total distortion. To automatically tie acoustically similar states, we merge the ensembles that provide the smallest distortion increase. This straightforward technique has provided a relative word error rate reduction up to.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-367"
  },
  "farhat95_eurospeech": {
   "authors": [
    [
     "Azarshid",
     "Farhat"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A shared-distribution approach in a hidden Markov model-based continuous speech recognition system",
   "original": "e95_1503",
   "page_count": 4,
   "order": 368,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "At the present time, one of the most important problem in large vocabulary continuous speech recognition is to achieve an optimum trade-off between acoustic models complexity and their trainability. In order to do so, we have defined a shared-distribution approach in our HMM-based continuous speech recognizer. In this clustering algorithm the distortion measure between two distributions is only based on the weights of gaussian mixture rather than all parameters of the distributions. Experimental results on the ATIS task show that our shared-distribution approach increased by 6% the word accuracy rate in comparison with our baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-368"
  },
  "ferreiros95_eurospeech": {
   "authors": [
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Preliminary experimentation of different methods for continuous speech recognition in Spanish",
   "original": "e95_1507",
   "page_count": 4,
   "order": 369,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "In this paper we make first a comparative study of the recognition performance of different HMM training algorithms in Spanish: Discrete context-independent phone models, Discrete context-dependent (aglomerative-clustered generalized triphone) models and Semieontinuous context-independent and context-dependent models. We also propose two alternatives to improve the performance of the systems, the first one by using phone-class dependent modelling. The second one by preprocessing the training sentences in order to separate interword pauses and consequently train better contextual models. Preliminary experiments on a speaker dependent database, 1000 words vocabulary show good improvement of both systems compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-369"
  },
  "doblinger95_eurospeech": {
   "authors": [
    [
     "Gerhard",
     "Doblinger"
    ]
   ],
   "title": "Computationally efficient speech enhancement by spectral minima tracking in subbands",
   "original": "e95_1513",
   "page_count": 4,
   "order": 370,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "We present an efficient algorithm for the enhancement of speech signals which are heavily corrupted by short-time stationary, acoustically or electrically added disturbances. The algorithm is based on spectral amplitude estimation using an overlap-add FFT filter bank system. Compared to other systems, the improved performance of our speech enhancement system is achieved by the combination of the best known spectral amplitude estimators of the noisy speech signal and a new efficient and reliable noise spectrum tracker. As a result, our speech enhancement system requires no speech pause detection for noise estimation and needs only 14% - 23% of the resources of a commercially available digital signal pro.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-370"
  },
  "salavedra95_eurospeech": {
   "authors": [
    [
     "Josep M.",
     "Salavedra"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Asunción",
     "Moreno"
    ]
   ],
   "title": "Robust hos-based techniques applied to speech recognition and enhancement",
   "original": "e95_1517",
   "page_count": 4,
   "order": 371,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "We study some speech enhancement algorithms based on the iterative Wiener filtering method due to Lim-Oppenheim [2], where the AR spectral estimation of the speech is carried out using a second-order analysis. But in our algorithms we consider an AR estimation by means of cumulant analysis. This work extends some preceding papers due to the authors, where information of previous speech frames is taken to initiate speech AR modelling of the current frame. Two parameters are introduced to dessign Wiener filter at first iteration of this iterative algorithm. These parameters are the Interframe Factor (IF) and the Previous Frame Iteration (PFI). A detailed study of them shows they allow a very important noise suppression after processing only first iteration of this algorithm, without any appreciable increase of distortion. Finally, the simplest cumulant-based algorithm is applied to Speech Recognition and some preliminary results are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-371"
  },
  "paliwal95c_eurospeech": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "A maximum likelihood equalization technique for robust speech recognition in adverse environments",
   "original": "e95_1521",
   "page_count": 4,
   "order": 372,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "In this paper, we study the problem of robust speech recognition in adverse environments. We focus our attention to the following two types of distortions: 1) the additive noise distortion, 2) the channel mismatch distortion. The maximum likelihood (ML) equalization technique is used to compensate for these distortions. Performance of the ML technique is compared with the following channel equalization techniques: the global mean subtraction (GMS) technique, the local mean subtraction (LMS) technique, the finite impulse response (FIR) highpass filtering technique, the infinite impulse response (IIR) highpass filtering technique, the RASTA (bandpass) filtering technique, and the masking-based filtering technique. These techniques have been recently proposed in the literature and are computationally much simpler than the ML equalization technique. It is shown that the ML equalization technique does not offer any significant advantage over the other channel equalization techniques in terms of recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-372"
  },
  "faucon95_eurospeech": {
   "authors": [
    [
     "G.",
     "Faucon"
    ],
    [
     "R. Le",
     "Bouquin Jeannes"
    ]
   ],
   "title": "Joint system for acoustic echo cancellation and noise reduction",
   "original": "e95_1525",
   "page_count": 4,
   "order": 373,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "Our concern is the investigation of speech enhancement techniques for hands-free audio terminals, including two major problems: noise reduction and acoustic echo cancellation. The objective is to find a joint structure to get a near-end speech signal with a minimum distortion and low levels of echo and noise. To solve the problems addressed by the basic structures, a new technique is investigated and tested in real conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-373"
  },
  "chang95c_eurospeech": {
   "authors": [
    [
     "Jane",
     "Chang"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "A study of speech recognition system robustness to microphone variations",
   "original": "e95_1529",
   "page_count": 4,
   "order": 374,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "This study seeks to improve our understanding of the effects of microphone variations on speech recognition systems. The TIMIT corpus provides data recorded on close talking and far field microphones and over telephone lines. The SUMMIT system is configured for phonetic classification and recognition. At the last ICSLP, we presented an analysis of the data and experiments in phonetic classification using a baseline system and various preprocessing techniques. In this paper, we present experiments in phonetic recognition using an improved baseline system and compensation techniques that require varying amounts of microphone specific data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-374"
  },
  "potamianos95_eurospeech": {
   "authors": [
    [
     "Altxandros",
     "Potamianos"
    ],
    [
     "Li",
     "Lee"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "A feature-space transformation for telephone based speech recognition",
   "original": "e95_1533",
   "page_count": 4,
   "order": 375,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "An experimental study describing the effects of carbon and electret telephone transducers on automatic speech recognition (ASR) performance is presented. It is shown that telephone based ASR performance on a connected digit task actually improves when speech is spoken through the carbon transducer. This surprising result is explained by a study of the differences in acoustic characteristics between carbon and electret telephone handsets. An initial attempt is made to devise a simple procedure for obtaining a parametric transformation which emulates the properties of the carbon transducer. The parameters of this transformation are trained automatically from speech spoken simultaneously through carbon and electret telephone handsets. When telephone speech data is transformed according to this procedure, a significant improvement in ASR performance is obtained. These results are interpreted and future research directions are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-375"
  },
  "cordoba95_eurospeech": {
   "authors": [
    [
     "Ricardo de",
     "Cordoba"
    ],
    [
     "Xavier",
     "Menendez-Pidal"
    ],
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Ascension",
     "Gallardo"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Development and improvement of a real-time ASR system for isolated digits in Spanish over the telephone line",
   "original": "e95_1537",
   "page_count": 4,
   "order": 376,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "We present the development and characteristics of a basic ASR system for isolated digits in Spanish, used over the telephone line. Initially we will introduce our first idea, a basic discrete system, and then we will see the improvements we made to increase the recognition rate at a low CPU cost (always considering its practical implementation as a real time system). The most remarkable advances were obtained with: 1) Semicontinuous modelling. It is a more precise modelling, although more time consuming. 2) End-pointing with a Neural network. 3) One pass decoding with noise models. The intention of both 2 and 3 is to alleviate the effects of a wrong end-pointing. 4) Parametrization using perceptual filters in frequency and filtering in the time domain (RASTA-PLP). We wanted to decrease the effect of telephonic noise in our system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-376"
  },
  "chien95_eurospeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Lee-Min",
     "Lee"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Channel estimation for reference model adaptation in telephone speech recognition",
   "original": "e95_1541",
   "page_count": 4,
   "order": 377,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "This paper deals with a channel estimation problem in speech recognition over telephone networks. In this study, we propose a channel estimation method to adapt the reference models so that the reference models can match the testing environment. For a telephone speech, its channel cepstral vector is estimated by the maximum a posteriori criterion. The reference models are then adapted to the testing environment by merging the estimated channel cepstral vectors. The telephone speech is recognized by using the adapted reference models. Experiments show that the proposed method can well estimate the telephone channel spectra and is successfully applied for telephone speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-377"
  },
  "yasukawa95_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Yasukawa"
    ]
   ],
   "title": "Enhancement of telephone speech quality by simple spectrum extrapolation method",
   "original": "e95_1545",
   "page_count": 4,
   "order": 378,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "This paper describes a quality enhancement system for band limited speech signals. In speech transmission, the quality of the received speech signals is degraded by severe band limitation. We have proposed a spectrum widening method that utilizes aliasing effects in sampling rate conversion and digital filtering for spectrum shaping. In this paper, a new method is proposed that offers improved performance in terms of the spectrum distortion characteristics. Implementation procedures are clarified, and its performance is discussed. The proposed method can effectively enhance speech quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-378"
  },
  "handel95_eurospeech": {
   "authors": [
    [
     "Peter",
     "Handel"
    ]
   ],
   "title": "Low-distortion spectral subtraction for speech enhancement",
   "original": "e95_1549",
   "page_count": 4,
   "order": 379,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "A performance analysis technique for spectral subtraction methods is introduced. From the outcome of the analysis it is, among other things, possible to rank different methods, tune the inherent design variables, derive novel methods with improved performance, and explain audible artifacts such as musical noise. In particular, it is argued that in the hands free mobile telephony scenario low-distortion spectral subtraction gives approximately lOdB noise level suppression.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-379"
  },
  "hu95_eurospeech": {
   "authors": [
    [
     "Zhihong",
     "Hu"
    ],
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Transition-based feature extraction within frame-based recognition",
   "original": "e95_1555",
   "page_count": 4,
   "order": 380,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "Current frame-based speech recognition systems sample speech at a fixed set of locations relative to each frame. Modeling the temporal dynamic behavior of speech is thereby complicated. This work shows that by explicitly using transitional information when extracting features, one can better model the acoustic phonetic structure, resulting in higher word level recognition performance. In this proposed approach, features representing local transitional information are used (a constant number of features are selected at each time frame, but the features are sampled near areas of greatest spectrum change within a relatively long window.) By explicitly modeling transitions in this way, we can also model local contextual information. Using this technique, the word level error rate decreased up to 30% on the databases we tested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-380"
  },
  "girin95_eurospeech": {
   "authors": [
    [
     "L.",
     "Girin"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Noisy speech enhancement with filters estimated from the speaker's lips",
   "original": "e95_1559",
   "page_count": 4,
   "order": 381,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "Since speech is both auditory and visual, visual cues could compensate to a certain extent the deficiency of auditory ones, in order to improve man-machine communication and telecommunication tools. This paper deals with a noise reduction technique based on speech enhancement with adaptive filters estimated from the speaker's lip pattern. We first present the selected filtering techniques, and then the tool we used to predict the filter pattern from the lip shape. The whole noise reduction system is implemented in the context of stationary vowels including a first kick into the problem of non-visible gestures. The results of perceptual tests are presented in order to quantify the performances of the system. These results are quite promising.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-381"
  },
  "adjoudani95_eurospeech": {
   "authors": [
    [
     "A.",
     "Adjoudani"
    ],
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "Audio-visual speech recognition compared across two architectures",
   "original": "e95_1563",
   "page_count": 4,
   "order": 382,
   "p1": "1563",
   "pn": "1566",
   "abstract": [
    "In this paper, we describe two architectures for combining automatic lip-reading and acoustic speech recognition. We propose a model which can improve the performances of an audio-visual speech recognizer in an isolated word and speaker dependent situation. This is achieved by using a hybrid system based on two HMMs trained respectively with auditory and visual data. Both architectures have been tested on degraded audio over a wide range of S/N ratios. The results of these experiments are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-382"
  },
  "liu95_eurospeech": {
   "authors": [
    [
     "Sharlene A.",
     "Liu"
    ]
   ],
   "title": "Noise effects on landmark detection in a speech recognition system",
   "original": "e95_1567",
   "page_count": 4,
   "order": 383,
   "p1": "1567",
   "pn": "1570",
   "abstract": [
    "In a knowledge-based speech recognition system, landmarks are key points in time in the speech waveform. They guide the search for the underlying distinctive features. The effect of background noise on the automatic detection of landmarks is studied. Two landmark detection algorithms are tested. The full algorithm is hierarchical and uses many criteria to look for speech cues. The minimalist algorithm uses a reduced set of these criteria. Experiments show that the full algorithm outperforms the minimalist algorithm in clean speech, while the minimalist algorithm outperforms the full algorithm in very noisy speech. These results suggest that, in the presence of background noise, a speech recognition system should customize its algorithm and parameter values to the characteristics of the noise in order to perform effectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-383"
  },
  "charonnat95_eurospeech": {
   "authors": [
    [
     "Laure",
     "Charonnat"
    ],
    [
     "Joel",
     "Crestel"
    ],
    [
     "Michel",
     "Glutton"
    ],
    [
     "Herve",
     "Chuberre"
    ]
   ],
   "title": "AR identification of the vocal filter from noisy hyperbaric speech signals",
   "original": "e95_1571",
   "page_count": 4,
   "order": 384,
   "p1": "1571",
   "pn": "1574",
   "abstract": [
    "When dealing with hyperbaric speech, the enhancement of the intelligibility is an analysis-synthesis problem in which the deconvolution between the excitation and the vocal filter is known to be crucial. However, the hyperbaric voiced speech signal is characterized by specific properties which are proved to be of interest in the AR identification of the vocal filter especially in case of noisy signals. As it is, the work which is presented brings out two major results. It is shown that, within a pitch period of a noisy speech signal (white noise), a significant segment can be used to compute an/estimate of the noise variance. Then this estimate can be taken into account in the vocal filter AR identification procedure, tnus giving better accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-384"
  },
  "sovka95_eurospeech": {
   "authors": [
    [
     "Pavel",
     "Sovka"
    ],
    [
     "Petr",
     "Pollak"
    ]
   ],
   "title": "The study of speech/pause detectors for speech enhancement methods",
   "original": "e95_1575",
   "page_count": 4,
   "order": 385,
   "p1": "1575",
   "pn": "1578",
   "abstract": [
    "Basic principles of various adaptive algorithms for speech detection in a noise and their behaviour under real car noise conditions are described. Energy, spectral, cepstral, and coherence detectors are compared. All these algorithms are suitable for real time implementation with one or two microphones. High probability of correct speech/pause detection can be obtained even if signal to noise ratio is low and noises are highly nonstationary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-385"
  },
  "sokol95_eurospeech": {
   "authors": [
    [
     "Robert",
     "Sokol"
    ],
    [
     "Guy",
     "Mercier"
    ]
   ],
   "title": "Neural-fuzzy network for phonetic features recognition",
   "original": "e95_1579",
   "page_count": 4,
   "order": 386,
   "p1": "1579",
   "pn": "1582",
   "abstract": [
    "In this article we describe an application of neural-fuzzy networks to phonetic features recognition over the telephone. We first give a short description of our neural-fuzzy system and talk about knowledge included in it. The advantages of such a combination and its use for helping deaf and hard of hearing people in lip reading are then shortly described. We then present the cases processed for the recognition of the following phonetic features : consonant/vowel, voiced/unvoiced, nasality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-386"
  },
  "shamsoddini95_eurospeech": {
   "authors": [
    [
     "A.",
     "Shamsoddini"
    ],
    [
     "P. N.",
     "Denbigh"
    ]
   ],
   "title": "A system for speech separation",
   "original": "e95_1583",
   "page_count": 4,
   "order": 387,
   "p1": "1583",
   "pn": "1586",
   "abstract": [
    "A system has been developed that successfully separates a target voice from an overlapping voice. The algorithm is based mainly on monaural processing, exploiting the harmonicity of voiced speech, but also utilises some binaural information to initially allocate each separated sound to the appropriate speaker. Emphasis has been put on monaural processing because of the sensitivity of binaural cues to room reverberation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-387"
  },
  "zhao95_eurospeech": {
   "authors": [
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Hierarchical mixture models and phonological rules in open-vocabulary speech recognition",
   "original": "e95_1587",
   "page_count": 4,
   "order": 388,
   "p1": "1587",
   "pn": "1590",
   "abstract": [
    "A new acoustic modeling technique of hierarchical mixture densities (HMDs) is presented for handling phone substitutions in pronunciation variations. The lower level of an HMD is comprised of the continuous Gaussian mixture densities (CGMDs) of phone-unit HMMs; the higher level of the HMD is a linear combination of the CGMDs based on phone-substitution probabilities. The HMD technique has been evaluated in an open-vocabulary speaker-independent continuous speech recognition task, with a vocabulary size of 1793 and 52% new vocabulary words. The HMDs were found most effective for modeling substitutions between vowels. By combining the HMDs with several phonological rules of phone deletions, the recognition word accuracy was improved by 10.7% over that of the baseline CGMD HMMs (38.5% error reduction), and it outperformed the performance of using 33% more acoustic-phonetic transcriptions of words generated by phonological rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-388"
  },
  "bonafonte95b_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Rafael",
     "Estany"
    ],
    [
     "Eugenio",
     "Vives"
    ]
   ],
   "title": "Study of subword units for Spanish speech recognition",
   "original": "e95_1607",
   "page_count": 4,
   "order": 389,
   "p1": "1607",
   "pn": "1610",
   "abstract": [
    "This paper studies different sets of subword speech units to be used for recognizing Spanish. In particular it compares context dependent phones, syllables and demisyllables. It shows how context dependent units can effectively reduce the error in a 15% with respect to context independent phones. The benefit of merging similar contexts when there are not enough training data is also validated. On the other hand the paper study the behavior of syllables based units: first, the study reveals that syllables give a similar performance than triphones whereas demisyllables give a similar performance than right (or left) context dependent phones. However, when different types of units are used, context dependent phones give the best results. Results achieved with these sets of units exceed 70% in acoustic-phonetic decoding of Spanish speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-389"
  },
  "holmes95_eurospeech": {
   "authors": [
    [
     "Wendy J.",
     "Holmes"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "Speech recognition using a linear dynamic segmental HMM",
   "original": "e95_1611",
   "page_count": 4,
   "order": 390,
   "p1": "1611",
   "pn": "1614",
   "abstract": [
    "This paper describes research into linear-trajectory dynamic segmental hidden Markov models (HMMs). The main advantage of these models over conventional HMMs is that they allow explicit modelling of speech segment dynamics. In general terms, a trajectory-based segmental HMM provides a parametric representation of the range of possible underlying trajectories for a speech sound. Acoustic feature vectors are regarded as noisy observations of a particular trajectory. This model represents an extension and generalization of the previously-developed static segmental HMM [1], which can be viewed as a constant-trajectory model. In the present paper, a linear-trajectory segmental HMM is described and Baum-Welch-type re-estimation formulae are presented. Preliminary recognition experiments on a connected-digit recognition task have demonstrated performance improvements over the previous results with static segmental HMMs, which in turn outperformed conventional HMMs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-390"
  },
  "yamamoto95_eurospeech": {
   "authors": [
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Comparative evaluation of segmental unit input HMM and conditional density HMM",
   "original": "e95_1615",
   "page_count": 4,
   "order": 391,
   "p1": "1615",
   "pn": "1618",
   "abstract": [
    "The standard HMM cannot express the time variant features during staying at the same state. We tried to capture the dynamic changes by using segmental statistics. We propose a new speech recognition method by the combination of HMM and segmental statistics. Using segmental statistics, since the dimension of parameters increases, it results in a lesser precision in the estimation of covariance matrix. Therefore we used methods for compressing dimension and reducing computation, such as K-L expansion and Modified Quadratic Discriminant Function(MQDF). This method outperformed traditional methods such as a conditional density HMM with the correlation between two frames and an HMM using regression coefficients as dynamic features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-391"
  },
  "matsunaga95b_eurospeech": {
   "authors": [
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Takeshi",
     "Matsumura"
    ],
    [
     "Harald",
     "Singer"
    ]
   ],
   "title": "Continuous speech recognition using non-uniform unit based acoustic and language models",
   "original": "e95_1619",
   "page_count": 4,
   "order": 392,
   "p1": "1619",
   "pn": "1622",
   "abstract": [
    "This paper proposes a continuous speech recognition strategy that uses acoustic non-uniform unit based Hidden Markov models and stochastic language models. The non-uniform units consist of phoneme units and long-units which cover contiguous phonemes. The long-unit is introduced to cope with more fluent acoustic characteristics which frequently occur in target speech, and this unit is also used as a non-uniform unit n-gram model for transparency in the integration of acoustic and linguistic processing. Phrase recognition experiments have shown the non-uniform units to be effective in achieving an error reduction rate of 11% compared with conventional context-dependent phone models, and by adding a non-uniform trigram the error reduction rate becomes 27%, showing the effectiveness of this strategy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-392"
  },
  "blackburn95_eurospeech": {
   "authors": [
    [
     "C. S.",
     "Blackburn"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Towards improved speech recognition using a speech production model",
   "original": "e95_1623",
   "page_count": 4,
   "order": 393,
   "p1": "1623",
   "pn": "1626",
   "abstract": [
    "Considerable improvement in the performance of continuous speech recognition systems, particularly those based on Hidden Markov Models (HMMs), has been shown in recent years. Nevertheless a number of unsolved problems remain which limit this progress, including the successful modelling of co-articulation and the identification of out of vocabulary utterances. One possible solution is to re-synthesise speech from the N-best time-aligned phonemic transcriptions produced by an HMM, and re-score this list based on a spectral comparison between the original and re-synthesised speech frames. In this paper a novel speech production model (SPM) suitable for use in such a system is introduced, and preliminary re-scoring results are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-393"
  },
  "sukkar95_eurospeech": {
   "authors": [
    [
     "Rafid A.",
     "Sukkar"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Bling-Hwang",
     "Juang"
    ]
   ],
   "title": "A vocabulary independent discriminatively trained method for rejection of non-keywords in sub word based speech recognition",
   "original": "e95_1629",
   "page_count": 4,
   "order": 394,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "Two important features of any deployable speech recognition system are the capability to detect if the input speech does not contain any of the recognizer keywords, and spot a keyword embedded in other speech or extraneous sounds. As a result, utterance verification (or non-keyword rejection) is becoming increasingly important as speech recognition systems continue to migrate from the laboratory to actual applications. In this paper we present a framework and a method for vocabulary independent utterance verification in subword based speech recognition. The verification process is cast as a statistical hypothesis test, where vocabulary independence is accomplished through a two stage verification process: subword level verification followed by string level verification. Experimental results show that this vocabulary independent discriminative utterance verification method significantly outperforms a baseline method commonly used in wordspotting tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-394"
  },
  "torrecilla95_eurospeech": {
   "authors": [
    [
     "Juan Carlos",
     "Torrecilla"
    ],
    [
     "Daniel",
     "Tapias"
    ],
    [
     "F. Javier",
     "Caminero-Gil"
    ],
    [
     "Luis",
     "Villarrubia"
    ]
   ],
   "title": "Rejection techniques based on context independent subword units",
   "original": "e95_1633",
   "page_count": 4,
   "order": 395,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "This paper addresses the problem of how to identify and reject Out-Of-Vocabulary (00V) words and background noises using an isolated word speaker independent Hidden Markov Model (HMM) recognizer. The proposed method builds a garbage model as a combination of context independent subword HMM units to represent all possible 00V words and background noises. The rejection rate is adjusted by an appropriate penalty that depends on the utterance duration and then is applied to the garbage model acoustic likelihood. This approach makes the behavior of the system independent of the utterance duration. Additionally, we provide linguistic information to the garbage model so that illegal phone sequences are not allowed. This approach reduces the average branching factor of the garbage model and increases the rejection of the OOV words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-395"
  },
  "fetter95_eurospeech": {
   "authors": [
    [
     "Pablo",
     "Fetter"
    ],
    [
     "Fritz",
     "Class"
    ],
    [
     "Udo",
     "Haiber"
    ],
    [
     "Alfred",
     "Kaltenmeier"
    ],
    [
     "Ute",
     "Kilian"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Detection of unknown words in spontaneous speech",
   "original": "e95_1637",
   "page_count": 4,
   "order": 396,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "This paper presents an analysis of the unknown-word problem and results of experiments in acoustic and language modeling of unknown words. In particular, we introduce the method of iterative substitution for correcting distortions caused by unknown words in the language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-396"
  },
  "nakamura95_eurospeech": {
   "authors": [
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "A minimum error training of garbage model for keyword spotter with artificially generated training data",
   "original": "e95_1641",
   "page_count": 4,
   "order": 397,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "This paper discusses a method for training a garbage model for a keyword spotter. Conventionally, minimum error training of garbage models, which have been shown to be effective, required a large amount of keyword speech samples as training data. So, on changing a keyword set, a process of collecting a large amount of speech samples has been required. In order to solve this problem, this paper proposes a training method which needs no keyword speech samples, so a keyword set can be changed quickly and economically. Experimental results show the spotting performance can be improved by a training process without raw keyword speech samples.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-397"
  },
  "hetherington95_eurospeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "New words: effect on recognition performance and incorporation issues",
   "original": "e95_1645",
   "page_count": 4,
   "order": 398,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "Previously, we demonstrated that new, out-of-vocabulary words occur in a wide variety of tasks no matter how large a system vocabulary is used, and we quantified the new-word rate for a number of tasks/corpora, showing that it is dependent on the task characteristics and vocabulary size [1]. In this paper we quantify the effects of new words on our system's accuracy and computation using carefully controlled experiments. We find that we encounter about 1.5 word errors per new word, some of which occurring in neighboring in-vocabulary words. By examining the computation required to generate a word graph, we find that even the occurrence of a single new word increases computation by nearly a factor of four on average. Finally, we examine some of the issues related to the eventual automatic incorporation of new words. For our system, we examine the relative importance of training the acoustic models, pronunciation models, and the language model on exemplars of new words in order to optimize performance. We were able to assemble a system, completely untrained on the new words, that achieved an error rate measured over a set of new words that was only 1.6 times higher than that of a fully trained system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-398"
  },
  "baldwin95_eurospeech": {
   "authors": [
    [
     "David O.",
     "Baldwin"
    ],
    [
     "Georg F.",
     "Meyer"
    ]
   ],
   "title": "Improving speech recognition using speaker classification",
   "original": "e95_1651",
   "page_count": 3,
   "order": 399,
   "p1": "1651",
   "pn": "1654",
   "abstract": [
    "This work examines the use of speaker classification as a method of improving speech recognition. Basic speech recognisers based upon hidden Markov models and neural networks, are modified by the use of selective training. Speakers are clustered into speaker types and separate recognisers are trained for each type. Performance is shown not to improve significantly. A more individualistic system is proposed. The speaker space is mapped by the use of non-linear interpolation between speaker dependent recognisers. Performance using an abstract 'perfect' speaker classifier is shown to be significantly better than speaker independent recognition. A multi-layer perceptron based speaker classifier is introduced, but is shown to be unable to learn the mapping from speakers to recognisers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-399"
  },
  "glaeser95_eurospeech": {
   "authors": [
    [
     "Axel",
     "Glaeser"
    ]
   ],
   "title": "Modular neural networks with task-specific input parameters for speakerindependent speech recognition",
   "original": "e95_1655",
   "page_count": 4,
   "order": 400,
   "p1": "1655",
   "pn": "1658",
   "abstract": [
    "In recent years, the computational effort for novel speech recognition systems has increased much more than the resulting recognition rates. Therefore, we present an approach for overcoming this drawback by using a modular phoneme recognition system. It combines advantages of neural networks with those of modular architectures. Important system features are module-specific selection of input parameters according to the decision tasks and the utilization of time delay neural networks (TDNNs) as well as static neural networks without time processing. The intention is to improve the recognition rate for speaker-independent phoneme recognition and, at the same time, to reduce the necessary effort for simulating the system after the initial learning phase.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-400"
  },
  "rigoll95_eurospeech": {
   "authors": [
    [
     "Gerhard",
     "Rigoll"
    ],
    [
     "Ch.",
     "Neukirchen"
    ],
    [
     "J.",
     "Rottland"
    ]
   ],
   "title": "Large vocabulary speaker-independent continuous speech recognition with a new hybrid system based on MMI-neural networks",
   "original": "e95_1659",
   "page_count": 4,
   "order": 401,
   "p1": "1659",
   "pn": "1662",
   "abstract": [
    "This paper presents a new hybrid system for speaker independent continuous speech recognition in a large vocabulary task. The hybrid system is a combination of context dependent discrete Hidden Markov Models and artificial neural networks that are trained by an information theory based algorithm. This algorithm maximizes the Mutual Information (MMI) between the network output and the phone descriptions by applying a self-organizing learning approach instead of forcing constrained network outputs. Recognition results have shown that the new hybrid system outperforms a classical k-means-VQ-based HMM-system. For the speaker independent DARPA Resource Management (RM) task (perplexity 60) we report a decrease in word recognition error rate up to 35% (close to the best continuous pdf systems).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-401"
  },
  "bourlard95b_eurospeech": {
   "authors": [
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Yochai",
     "Konig"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "REMAP: recursive estimation and maximization of a posteriori probabilities in connectionist speech recognition",
   "original": "e95_1663",
   "page_count": 4,
   "order": 402,
   "p1": "1663",
   "pn": "1666",
   "abstract": [
    "In this paper, we briefly describe REMAP, an approach for the training and estimation of posterior probabilities, and report its application to speech recognition. REMAP is a recursive algorithm that is reminiscent of the Expectation Maximization (EM) [5] algorithm for the estimation of data likelihoods. Although very general, the method is developed in the context of a statistical model for transition-based speech recognition using ARTIFICIAL Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences. As with earlier hybrid HMM/ANN systems we have developed, ANNs are used to estimate posterior probabilities. In the new approach, however, the network is trained with targets that are themselves estimates of local posterior probabilities. Initial experimental results support the theory by showing an increase in the estimates of posterior probabilities of the correct sentences after REMAP iterations, and a decrease in error rate for an independent test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-402"
  },
  "lee95c_eurospeech": {
   "authors": [
    [
     "Tan",
     "Lee"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "L. W.",
     "Chan"
    ]
   ],
   "title": "An RNN based speech recognition system with discriminative training",
   "original": "e95_1667",
   "page_count": 4,
   "order": 403,
   "p1": "1667",
   "pn": "1670",
   "abstract": [
    "In our previous work [1], a novel method of utilizing a set of fully connected recurrent neural networks (RNNs) for speech modeling has been proposed. Despite the effectiveness of the RNN model in characterizing individual speech units, the system performs less satisfactorily for speech recognition due to poor discrimination between models. In this paper, an efficient discriminative training procedure is developed for the RNN based recognition system. By using discriminative training, each RNN speech model is adjusted to reduce its distance from the designated speech unit while increase distances from the others. In addition, a duration-screening process is introduced to enhance the discriminating power of the recognition system. Speaker-dependent recognition experiments have been carried out for 1) 11 isolated Cantonese digits, 2) 58 very confusing Cantonese CV syllables, and 3) 20 English isolated words. The recognition rates attained are 90.9%, 86.7% and 93.5% respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-403"
  },
  "castano95_eurospeech": {
   "authors": [
    [
     "M. A.",
     "Castano"
    ],
    [
     "Enrique",
     "Vidal"
    ],
    [
     "F.",
     "Casacuberta"
    ]
   ],
   "title": "Preliminary experiments for automatic speech understanding through simple recurrent networks",
   "original": "e95_1673",
   "page_count": 4,
   "order": 404,
   "p1": "1673",
   "pn": "1676",
   "abstract": [
    "Experiments with Automatic Speech Understanding (ASU) problems were presented in [1], in which the underlying acoustic and syntactic-semantic structure of the task was tried to be automatically learned into a global model. Results indicated that, while a Simple Recurrent Network (SRN) was actually able to capture most of the acoustic features of the ASU task, the syntactic-semantic structure was difficult to learn. We propose here appropriate modifications to this SRN which significantly improve the understanding rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-404"
  },
  "yu95d_eurospeech": {
   "authors": [
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "A neural network using non-uniform units for continuous speech recognition",
   "original": "e95_1677",
   "page_count": 4,
   "order": 405,
   "p1": "1677",
   "pn": "1680",
   "abstract": [
    "A new network model, U-net, is proposed to recognize continuous speech, based on the non-uniform unit which is a kind of acoustic sub-word unit. In this model, input speech can be segmented into units by using a part of the network before classification. The unit has steady states at the boundaries and a transient state in the middle. The network structure is designed according to the structure of the unit. The steady states and transient state are recognized by separate networks and different feature parameters are used. For the transient part a delta parameter is used. The segmentation net is trained to reduce the number of unit classes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-405"
  },
  "franco95_eurospeech": {
   "authors": [
    [
     "Horatio",
     "Franco"
    ],
    [
     "Vassilios",
     "Digalakis"
    ]
   ],
   "title": "Temporal correlation modeling in a hybrid neural network/hidden Markov model speech recognizer",
   "original": "e95_1681",
   "page_count": 4,
   "order": 406,
   "p1": "1681",
   "pn": "1684",
   "abstract": [
    "A new scheme to overcome the independence assumption in standard hidden Markov modeling (HMM) formulations is presented within the framework of a hybrid system that uses a discriminatively trained multilayer perceptron (MLP) to compute a correlated emission probability. The scheme takes advantage of the MLP's ability to model correlations across multiple frames allowing the use of multiframe long vector history to condition the emission probability. The required number of parameters is the same as in the standard hybrid HMM/MLP formulation. Results presented in a large vocabulary continuous speech recognition task show that even though performance so far has not improved over the standard approach, the acoustic and language model probabilities are better balanced with this new scheme as compared to the standard one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-406"
  },
  "buniet95_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Buniet"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Continuous speech segmentation with the gamma memory model",
   "original": "e95_1685",
   "page_count": 4,
   "order": 407,
   "p1": "1685",
   "pn": "1688",
   "abstract": [
    "This paper presents the application of a recurrent neural network model, the Gamma Memory Model (GMM), to the problem of speech segmentation. The Gamma Memory Model is composed of a local and constrained feedback. We first present the tests made with gamma units in the input plane and standard neurons in the hidden and output layers. Then, we present the tests made with gamma neurons in the hidden layers : the architecture of the neurons in the hidden layer is modified, a gamma neuron being composed of a standard neuron followed by a gamma unit.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-407"
  },
  "menendezpidal95_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Menendez-Pidal"
    ],
    [
     "Ricardo de",
     "Cordoba"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Incorporating fuzzy modelling in a hybrid HMM-ANNs system for CSR tasks",
   "original": "e95_1689",
   "page_count": 4,
   "order": 408,
   "p1": "1689",
   "pn": "1692",
   "abstract": [
    "In this work, we present a new strategy to combine neural networks with HMMs which tend to take advantage of the modelling abilities of two independent modules, the ANN and the HMM, to make the design of a hybrid system less complicated. This approach incorporates fuzzy probabilistic information in the HMM to decompose the training task of a hybrid system. Using this strategy the training system is optimized about 7 times without significant loss of information. Also, we describe different techniques to improve the performances of the system which reduce the word error rate by 40%. Using this methodology, the hybrid system is trained much faster and can now benefit from two distinct sources of improvements such as neural modelling and classical HMM modelling which is less costly to perform.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-408"
  },
  "reichl95b_eurospeech": {
   "authors": [
    [
     "Wolfgang",
     "Reichl"
    ],
    [
     "S.",
     "Harengel"
    ],
    [
     "F.",
     "Wolfertstetter"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Neural networks for nonlinear discriminant analysis in continuous speech recognition",
   "original": "e95_2163",
   "page_count": 4,
   "order": 409,
   "p1": "2163",
   "pn": "2166",
   "abstract": [
    "In this paper neural networks for Nonlinear Discriminant Analysis in continuous speech recognition are presented. Multilayer Perceptrons are used to estimate a-posteriori probabilities for Hidden-Markov Model states, which are the optimal discriminant features for the separation of the HMM states. The a-posteriori probabilities are transformed by a principal component analysis to calculate the new features for semicontinuous HMMs, which are trained by the known Maximum-Likelihood training. The nonlinear discriminant transformation is used in speaker-independent phoneme recognition experiments and compared to the standard Linear Discriminant Analysis technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-409"
  },
  "rigoll95b_eurospeech": {
   "authors": [
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Speech recognition experiments with a new multilayer LVQ network (MLVQ)",
   "original": "e95_2167",
   "page_count": 4,
   "order": 410,
   "p1": "2167",
   "pn": "2170",
   "abstract": [
    "In this paper, a new neural network paradigm and its application to recognition of speech patterns is presented. The novel NN paradigm is a multilayer version of the well-known LVQ algorithm from Kohonen. The approach includes the following innovations and improvements compared to other popular neural network paradigms: 1) It is - according to the knowledge of the author - the first multilayer version of the classical LVQ algorithm, which is usually based on a one-layer neural network architecture. 2) It presents a new NN architecture, since it uses a perceptron-like propagation function for the hidden layers, and an Euclidean-like propagation function for the output layer. 3) Its architecture can be considered as an optimal compromise between the multilayer principle of an MLP and the principle of representing one class by several neurons adopted from classical LVQ. 4) Compared to MLP, the training of an MLVQ network with the same number of neurons is more effective, since only the weights of the winning neuron are updated, as in classical LVQ. 5) It outperforms both, the classical LVQ and the MLP algorithm in most experiments carried out. 6) In the initialization phase, the layers are trained hierarchically, making use of unsupervised information theory-based training algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-410"
  },
  "neto95_eurospeech": {
   "authors": [
    [
     "Joao",
     "Neto"
    ],
    [
     "Luis",
     "Almeida"
    ],
    [
     "Mike",
     "Hochberg"
    ],
    [
     "Ciro",
     "Martins"
    ],
    [
     "Luis",
     "Nunes"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system",
   "original": "e95_2171",
   "page_count": 4,
   "order": 411,
   "p1": "2171",
   "pn": "2174",
   "abstract": [
    "It is well known that recognition performance degrades significantly when moving from a speaker-dependent to a speaker-independent system. Traditional hidden Markov model (HMM) systems have successfully applied speaker-adaptation approaches to reduce this degradation. In this paper we present and evaluate some techniques for speaker-adaptation of a hybrid HMM-artificial neural network (ANN) continuous speech recognition system. These techniques are applied to a well trained, speaker-independent, hybrid HMM-ANN system and the recognizer parameters are adapted to a new speaker through off-line procedures. The techniques are evaluated on the DARPA RM corpus using varying amounts of adaptation material and different ANN architectures. The results show that speaker-adaptation within the hybrid framework can substantially improve system performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-411"
  },
  "puzrla95_eurospeech": {
   "authors": [
    [
     "Premysl",
     "Puzrla"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Christoph",
     "Windheuser"
    ]
   ],
   "title": "Distributed binary representations for word recognition by TDNN-DTW hybrid systems",
   "original": "e95_2175",
   "page_count": 4,
   "order": 412,
   "p1": "2175",
   "pn": "2178",
   "abstract": [
    "In this paper, we report experiments on the use of distributed binary representations in an architecture based on several TDNN-DTW hybrid systems in parallel, for word recognition. Each TDNN is trained on a different set of binary distinctive features chosen in an arbitrary manner and yield different confusion matrices on the training set. These confusion matrices are used to obtain a global recognition decision by combining all system outputs. We investigate two possibilities which we call multiplicative mode and fusion mode. We show that the proposed architecture is able to provide better word recognition rate than a simple TDNN-DTW hybrid system trained on the 1-out-of-N representation and that it is comparable with a system trained on binary phonetic features. The fusion mode turns out to be an efficient strategy for handling word recognition in our parallel architecture.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-412"
  },
  "anglade95_eurospeech": {
   "authors": [
    [
     "Yolande",
     "Anglade"
    ]
   ],
   "title": "A robust discrimination method based on selectively trained neural networks for confusable words in noisy conditions",
   "original": "e95_2179",
   "page_count": 4,
   "order": 413,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "This work aims at improving the automatic speech recognition of confusable words like letters. To deal with that problem, we have proposed a new discrimination method based on acoustic knowledge and artificial neural networks. In this paper, we present a validation of this method for speaker-independent tests, on a big database, with several additive noises at different Signal to Noise Ratio. The influence of the training conditions is taken into account and the results obtained show a good robustness of this method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-413"
  },
  "abrash95_eurospeech": {
   "authors": [
    [
     "Victor",
     "Abrash"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Ananth",
     "Sankar"
    ],
    [
     "Michael",
     "Cohen"
    ]
   ],
   "title": "Connectionist speaker normalization and adaptation",
   "original": "e95_2183",
   "page_count": 4,
   "order": 414,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "In a speaker-independent, large-vocabulary continuous speech recognition systems, recognition accuracy varies considerably from speaker to speaker, and performance may be significantly degraded for outlier speakers such as nonnative talkers. In this paper, we explore supervised speaker adaptation and normalization in the MLP component of a hybrid hidden Markov model/ multilayer perception version of SRI's DECIPHER(TM) speech recognition system. Normalization is implemented through an additional transformation network that pre-processes the cepstral input to the MLP. Adaptation is accomplished through incremental retraining of the MLP weights on adaptation data. Our approach combines both adaptation and normalization in a single, consistent manner, works with limited adaptation data, and is text-independent. We show significant improvement in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-414"
  },
  "galindo95_eurospeech": {
   "authors": [
    [
     "Pedro L.",
     "Galindo"
    ]
   ],
   "title": "A competitive algorithm for training HMM for speech recognition",
   "original": "e95_2187",
   "page_count": 4,
   "order": 415,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "In this paper we describe an alternative estimation procedure for training Hidden Markov Models (HMM) called Competitive Forward-Backward (CFB) algorithm, which is aimed at minimizing the number of recognition errors. CFB integrates the LVQ3 neural network classification technique into the Baum-Welch training algorithm, and improves significantly the performance of HMM systems over previously reported procedures, such as Baum-Welch and Corrective Training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-415"
  },
  "paping95_eurospeech": {
   "authors": [
    [
     "Martin",
     "Paping"
    ],
    [
     "Hans",
     "Marti"
    ],
    [
     "Mark",
     "Renfer"
    ]
   ],
   "title": "Predictive connectionist speech recognition with a new discriminant learning algorithm",
   "original": "e95_2193",
   "page_count": 4,
   "order": 416,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "A new discriminative learning algorithm for predictive speech recognition is proposed. Based on the hidden control neural network (HCNN) [2] we use the patterns of a certain word class to train both the correct word model in the classical positive way and competing word models in a negative direction. Hence the training is performed in a discriminative way. The influence of the proposed algorithms was tested in several experiments using isolated digits. When using the new untraining algorithm the recognition rates could be increased significantly (from 87.9% to 93.4%). We compared the results of the advanced HCNN algorithm to other connectionist recognizers. With the pure MLP [7] we reached 91.1%, with the TDNN [9] we achieved recognition rates up to 94.3%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-416"
  },
  "rubio95_eurospeech": {
   "authors": [
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "Ronan G.",
     "Reilly"
    ]
   ],
   "title": "Preliminary results on speech signal segmentation with recurrent neural networks",
   "original": "e95_2197",
   "page_count": 4,
   "order": 417,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "A continuous speech recognition system has to explore the search space with the only information given by the probabilities provided by a set of hidden Markov models representing the different sounds of the language and the restrictions imposed by the language model. The objective of this work is to include additional information, related to the estimated location of the boundary between two consecutive phonemes. This information is provided by an artificial recurrent neural network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-417"
  },
  "vicsi95_eurospeech": {
   "authors": [
    [
     "Klara",
     "Vicsi"
    ],
    [
     "Attila",
     "Vig"
    ]
   ],
   "title": "Text independent neural network/rule based hybrid, continuous speech recognition",
   "original": "e95_2201",
   "page_count": 4,
   "order": 418,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "This paper describes a text independent continuous speech recognition system, which is built up from many steps. The method of the process is different in all steps, adapted to the particular problems of that level. The whole system is a combination of the application of neural nets and rule based processes, using segment based approach. The segmentation technics is language independent, and all the other processes can be adapted easily to other languages. The recognition works sentence by sentence. At the input the sound pressure is measured, and at the output the half-syllable series (first and second candidates) of the sentences are given. The recognition time of the sentences is about the pronunciation time of them, running the program on a PC 486.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-418"
  },
  "ng95d_eurospeech": {
   "authors": [
    [
     "Ying Pang",
     "Ng"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "L. W.",
     "Chan"
    ]
   ],
   "title": "Automatic recognition of Cantonese lexical tones in connected speech by multi-layer perceptron",
   "original": "e95_2205",
   "page_count": 4,
   "order": 419,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "In automatic tone language recognition, lexical tone identification forms an integral part of the recognition process. In this paper, automatic recognition of Cantonese lexical tone in connected speech is tackled. Cantonese is a commonly used Chinese dialect and is very rich in tones. Data for this study are from a male native speaker of Cantonese. The study proceeds in 3 stages. Stage 1 studies the feasibility of lexical tone recognition using manually segmented F0 contours. Stage 2 is concerned with the design of a novel approach in automatic segmentation of F0 contours. Stage 3 develops a lexical tone recognition algorithm using automatically extracted F0 contours. Preliminary results show that for a fully automatic system, a tone recognition rate of 72.1 % (N=725) is achieved for connected speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-419"
  },
  "abberley95_eurospeech": {
   "authors": [
    [
     "Dave",
     "Abberley"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Combining HMM processing and formant measurements in automatic speech recognition",
   "original": "e95_2209",
   "page_count": 4,
   "order": 420,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "HMM-based continuous speech recognition (CSR) systems have proved very successful despite the well-known limitations of HMMs as models of speech production. Recent work in pattern classification has demonstrated the performance advantages of combining sets of classifiers operating on complementary sources of input data. We have investigated this approach on a CSR task by combining conventional HMM processing with formant measurements. Our results show that this can significantly improve phone-level classification performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-420"
  },
  "na95b_eurospeech": {
   "authors": [
    [
     "Kyungmin",
     "Na"
    ],
    [
     "Jekwan",
     "Ryu"
    ],
    [
     "Dong-Il",
     "Chang"
    ],
    [
     "Soo-Ik",
     "Chae"
    ],
    [
     "Souguil",
     "Ann"
    ]
   ],
   "title": "Recurrent neural prediction models for speech recognition",
   "original": "e95_2213",
   "page_count": 4,
   "order": 421,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "This paper proposes recurrent neural prediction models (RNPM) for speech recognition which are recurrent neural networks trained as a nonlinear predictor of speech signals. Among various recurrent architectures, two well-known recurrent neural networks are tested here. The RNPM does not require any time alignment algorithm, which allows considerable reduction of computation time in recognition phase. Experiments on Korean digit recognition have shown that the performance of RNPM is a little better than that of other predictive neural networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-421"
  },
  "djezzar95b_eurospeech": {
   "authors": [
    [
     "Linda",
     "Djezzar"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Exploiting acoustic-phonetic knowledge and neural networks for stop recognition",
   "original": "e95_2217",
   "page_count": 4,
   "order": 422,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "A hybrid acoustic-phonetic decoder was developed to recognize intervocalic stop consonants. This recognizer combines the advantages of both explicit acoustic-phonetic knowledge and neural networks. It has been trained and tested on three corpora of continuous speech multispeakers. The statistical tests indicated that the recognition order was: %k (93%) > %t (87%) > %p (84%), and that the three stops were better identified in back and rounded front contexts. To evaluate the efficiency of the acoustic-phonetic knowledge, we compared the hybrid recognizer results to those of a neural recognizer which uses cepstrum coefficients (MFCC). The statistical tests indicated that the former were generally better than the latter.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-422"
  },
  "gomez95_eurospeech": {
   "authors": [
    [
     "P.",
     "Gomez"
    ],
    [
     "V.",
     "Rodellar"
    ],
    [
     "A.",
     "Alvarez"
    ],
    [
     "J.",
     "Bobadilla"
    ],
    [
     "J.",
     "Bernal"
    ],
    [
     "V.",
     "Nieto"
    ],
    [
     "M.",
     "Perez"
    ]
   ],
   "title": "Estimation of speech formant-dynamics using neural networks",
   "original": "e95_2221",
   "page_count": 4,
   "order": 423,
   "p1": "2221",
   "pn": "2224",
   "abstract": [
    "Throughout the present paper, the possibility of using Neural Networks to produce x-y representations from speech in real time, such in vowel and vowel-like sounds, is theoretically shown and practically documented. A certain kind of Time-Delay Neural Network, is shown to be the most efficient operator to extract formant-dynamic information for these plottings. This opens the possibility for constructing Visual User Interfaces for Language Learning Systems using relatively simple hardware.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-423"
  },
  "jongenburger95_eurospeech": {
   "authors": [
    [
     "Willy",
     "Jongenburger"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "The role of linguistic stress in the time course of word recognition in stress-accent languages",
   "original": "e95_1695",
   "page_count": 4,
   "order": 424,
   "p1": "1695",
   "pn": "1698",
   "abstract": [
    "In stress-accent languages minimal stress pairs occur, i.e. identical segment sequences that are lexically distinct only by virtue of a difference in stress position. In these languages stress information might contribute to the human word recognition process. In this study we investigate the exact locus of stress effects in the human on-line word recognition process by means of a cross-modal priming study. The results are inconclusive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-424"
  },
  "gelder95_eurospeech": {
   "authors": [
    [
     "Beatrice de",
     "Gelder"
    ],
    [
     "Paul",
     "Bertelson"
    ],
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Hsuan Chin",
     "Chen"
    ]
   ],
   "title": "Inter-language differences in the mcgurk effect for dutch and Cantonese listeners",
   "original": "e95_1699",
   "page_count": 4,
   "order": 425,
   "p1": "1699",
   "pn": "1702",
   "abstract": [
    "A group of Dutch and Cantonese listeners were compared on a audio-visual speech perception task. Using video techniques, lipmovements of syllables were dubbed on a speech signal such that the heard and seen place of articulation did not match [4]. The Cantonese participants were more influenced by vision than the Dutch. We suggest that the phonological repertoire has an influence on audio-visual speech perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-425"
  },
  "otake95_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Sally M.",
     "Davis"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Listeners representations of within-word structure: a cross-linguistic and cross-dialectal investigation",
   "original": "e95_1703",
   "page_count": 4,
   "order": 426,
   "p1": "1703",
   "pn": "1706",
   "abstract": [
    "Japanese, British English and American English listeners were presented with spoken words in their native language, and asked to mark on a written transcript of each word the first natural division point in the word. The results showed clear and strong patterns of consensus, indicating that listeners have available to them conscious representations of within-word structure. Orthography did not play a strongly deciding role in the results. The patterns of response were at variance with results from on-line studies of speech segmentation, suggesting that the present task taps not those representations used in on-line listening, but levels of representation which may involve much richer knowledge of word-internal structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-426"
  },
  "mcqueen95_eurospeech": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ],
    [
     "Ethan",
     "Cox"
    ]
   ],
   "title": "The use of phonotactic constraints in the segmentation of dutch",
   "original": "e95_1707",
   "page_count": 4,
   "order": 427,
   "p1": "1707",
   "pn": "1710",
   "abstract": [
    "Several proposals have been made as to how human listeners segment continuous speech into discrete words during spoken word recognition. These include segmentation through lexical competition and segmentation based on metrical structure. An experiment is presented which shows that another way listeners solve the segmentation problem is through the use of phonotactic constraints: a word boundary is more likely where the sequence of segments requires there to be a syllable boundary. Monosyllabic Dutch words were easier to detect in bisyllabic nonsense strings when they were aligned with a syllable boundary determined by phonotactics than when they were misaligned with such a boundary. Listeners appear to use several sources of information, phonotactic, metrical and lexical, in their segmentation of continuous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-427"
  },
  "vroomen95_eurospeech": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "Lexical inhibition in spoken word recognition",
   "original": "e95_1711",
   "page_count": 4,
   "order": 428,
   "p1": "1711",
   "pn": "1714",
   "abstract": [
    "Recent evidence suggests that the relations between lexical items influence the ease with which words can be discriminated and subsequently be recognized. Lexical items may influence each other via lateral inhibition during the activation process of lexical candidates [1] or via competition from neighbours at a decision stage [2]. The present study tried to distinguish between these alternatives by employing a cross-modal repetition priming paradigm. The results show that the number of competitors had an effect on low-, but not on high-frequency targets. This result is congruent with a lateral inhibition account and it underscores the relevance of lateral inhibition as a mechanism for continuous speech segmentation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-428"
  },
  "vieira95_eurospeech": {
   "authors": [
    [
     "Maurilio Nunes",
     "Vieira"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "Arnold",
     "Maran"
    ],
    [
     "Colin",
     "Watson"
    ],
    [
     "Moira",
     "Little"
    ]
   ],
   "title": "Methodological aspects in a multimedia database of vocal fold pathologies",
   "original": "e95_1867",
   "page_count": 4,
   "order": 429,
   "p1": "1867",
   "pn": "1870",
   "abstract": [
    "Despite time constraints, a voice clinic is a unique environment to collect rich and vast information related to vocal fold pathologies. In this paper, the instruments and protocols in use at the Royal Infirmary of Edinburgh are described. Biographical, aetiological, acoustic, electroglottographic, and endoscopic data from about 180 patients are already recorded and partially stored in personal computer multimedia file formats (.wav, .bmp, and .avi) integrated in an MS-ACCESS® database. This information is used for medical diagnosis and research purposes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-429"
  },
  "udayashankara95_eurospeech": {
   "authors": [
    [
     "V.",
     "Udayashankara"
    ],
    [
     "A. P.",
     "Shivaprasad"
    ]
   ],
   "title": "The application of volterra LMS adaptive filtering to speech enhancement for the hearing impaired",
   "original": "e95_1871",
   "page_count": 4,
   "order": 430,
   "p1": "1871",
   "pn": "1874",
   "abstract": [
    "It is well known that the communication in a noisy environment is a difficult problem to solve for both normal and hearing impaired listeners. In this work, the signals obtained from the omnidirectional microphone which is mounted just above one ear on the head of the KEMAR mannikan and a hypercardioid microphone which is placed at the center of the vertebral column on the back were used as input to the two channel adaptive noise canceller. The intelligibility tests conducted by the authors are presented in this paper to illustrate the performance of both Linear and Non Linear model. Results show that the Volterra LMS scheme is very effective in improving intelligibility and SNR for different environmental noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-430"
  },
  "rosso95_eurospeech": {
   "authors": [
    [
     "P.",
     "Rosso"
    ],
    [
     "J. H.",
     "Wright"
    ],
    [
     "M.",
     "Smith"
    ]
   ],
   "title": "CAPDA: managing intelligibility in children and young adults with down's syndrome or speech disorders",
   "original": "e95_1875",
   "page_count": 4,
   "order": 431,
   "p1": "1875",
   "pn": "1878",
   "abstract": [
    "The School of Clinical Speech and Language Studies at Trinity College Dublin conducted a speech therapy study on a group of children and young adults who suffer from Down's Syndrome. The study focused on the intelligibility of the speech. The rationale for this focus is.due to the concern voiced by parents that their children's speech be understood. Specific language difficulties, unintelligible speech and fluency difficulties are all often associated with Down's Syndrome. Research has indicated that patients with Down's Syndrome can make progress in developing communication skills through therapy. The Computer Assisted Phonological Data Analysis (CAPDA) software tool was developed in order to assist speech therapists in their assessment of children and young adults who suffer from Down's Syndrome (or speech disorders in general).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-431"
  },
  "wrench95_eurospeech": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "Mary S.",
     "Jackson"
    ],
    [
     "David S.",
     "Soutar"
    ],
    [
     "A. Gerry",
     "Robertson"
    ],
    [
     "Janet MacKenzie",
     "Beck"
    ]
   ],
   "title": "Evaluation of a system for segmental speech quality assessment: voiceless fricatives",
   "original": "e95_1879",
   "page_count": 4,
   "order": 432,
   "p1": "1879",
   "pn": "1882",
   "abstract": [
    "In this paper, an automatic assessment procedure for monitoring the speech progress of patients who have undergone intra-oral surgery is evaluated. The metric is currently limited to fricative segments and is based on centroid analysis of the speech spectra. The scores provided by this means are correlated with an articulatory assessment provided by a panel of 3 phoneticians. Preliminary results show a degree of correlation between mean panel scores for perceived quality and scores relating to this measure obtained by computer analysis but lateral and nasal co-articulation are not well represented by the metric.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-432"
  },
  "teston95_eurospeech": {
   "authors": [
    [
     "B.",
     "Teston"
    ],
    [
     "B.",
     "Galindo"
    ]
   ],
   "title": "A diagnostic and rehabilitation aid workstation for speech and voice pathologies",
   "original": "e95_1883",
   "page_count": 4,
   "order": 433,
   "p1": "1883",
   "pn": "1886",
   "abstract": [
    "We describe a workstation for diagnostic aid and rehabilitation in speech and voice pathologies. It is the adaptation of speech research experimental devices for a medical application. The workstation consists of a PC computer, equipped with acoustic and aerodynamic sensors. Different programs are designed for each clinical investigation procedure under Microsoft WINDOWS operating system. This apparatus allows to set the objective state of normal and pathological speech production, and for the evaluation of functional results of surgical, pharmacological, logopedic and prosthetic treatments of voice and speech disorders.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-433"
  },
  "nemeth95_eurospeech": {
   "authors": [
    [
     "Geza",
     "Nemeth"
    ],
    [
     "Gabor",
     "Olaszy"
    ],
    [
     "Laszlo",
     "Pataki"
    ],
    [
     "Luis",
     "Hernandez Gomez"
    ],
    [
     "Diamantino",
     "Freitas"
    ]
   ],
   "title": "Improvement, evaluation and testing of a low cost multilingual portable speaking aid for the speech impaired",
   "original": "e95_1887",
   "page_count": 4,
   "order": 434,
   "p1": "1887",
   "pn": "1890",
   "abstract": [
    "A PC compatible notebook based multilingual speaking aid (Hungarian, Spanish, Portuguese) was improved and tested in the three respective countries. Within this framework the first Portuguese PC based TTS system was also developed. The project was partly completed within the COST219 European program. Hungarian participation was supported by the ACCORD programme (contract H 9112-0507).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-434"
  },
  "michaelis95_eurospeech": {
   "authors": [
    [
     "Dirk",
     "Michaelis"
    ],
    [
     "Hans Werner",
     "Strube"
    ]
   ],
   "title": "Empirical study to test the independence of different acoustic voice parameters on a large voice database",
   "original": "e95_1891",
   "page_count": 4,
   "order": 435,
   "p1": "1891",
   "pn": "1894",
   "abstract": [
    "A new acoustic parameter for the description of hoarseness has been developed: The Glottal-to-Noise Excitation Factor (GNR-Factor). It shows to which extend a voice is excited by turbulent noise on a vocal-tract constriction or by vocal-fold vibration. The new parameter is introduced briefly. Then the independence of this parameter from other well-known roughness parameters is demonstrated by multidimensional data analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-435"
  },
  "ortiz95_eurospeech": {
   "authors": [
    [
     "Sergio D. Cano",
     "Ortiz"
    ],
    [
     "Daniel Escobedo",
     "Beceiro"
    ],
    [
     "Manuel Socarras",
     "Reyes"
    ]
   ],
   "title": "The spectral analysis of infant cry: an initial approximation",
   "original": "e95_1895",
   "page_count": 4,
   "order": 436,
   "p1": "1895",
   "pn": "1898",
   "abstract": [
    "This paper holds the first experiences in the infant cry analysis oriented to diagnosis, developed by the Group of Speech Processing Faculty of Electrical Engineering, Universidad de Oriente in close collaboration with physicians, linguists and logopedians. It has been postulated that the infant cry is a reflection of complex neurophysiologic functions and that analysis of the infant cry can be utilized to asses the infants status. In order to test this hypothesis a first experience was carried out using a computer-based signal processing system that enabled the observer to relate the acoustic properties of the cry to the anatomic and physiologic characteristics of the infant producing the cry.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-436"
  },
  "seiyama95_eurospeech": {
   "authors": [
    [
     "N.",
     "Seiyama"
    ],
    [
     "A.",
     "Nakamura"
    ],
    [
     "A.",
     "Imai"
    ],
    [
     "T.",
     "Takagi"
    ],
    [
     "E.",
     "Miyasaka"
    ]
   ],
   "title": "Portable speech rate conversion system",
   "original": "e95_1717",
   "page_count": 4,
   "order": 437,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "A new Portable Speech Rate Conversion System is developed in order to assist elderly listeners in comprehension of rapid speech by slowing the input speech rate. Compared with the former experimental system [1], this system has the following features; (1) Holding of all features in the former system which enables a user to convert a speech rate as desired by him/her-self on real time, with invariance in pitch as well as small impairments in quality. (2) Downsizing. (3) Introduction of new methods for changing speech rate quickly and for absorbing temporal enlargement of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-437"
  },
  "zhang95_eurospeech": {
   "authors": [
    [
     "Jialu",
     "Zhang"
    ]
   ],
   "title": "A field test of sivo aid in China",
   "original": "e95_1721",
   "page_count": 4,
   "order": 438,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "A field test of the SiVo aid with a profoundly hearing-impaired listener who has a left comer audiogram had been carried out in China. The testing materials were Chinese intelligibility test lists including syllable lists and word lists which are all phonetically balanced. The speech intelligibility tests were conducted under three different conditions:!, without the SiVo aid; 2. With the SiVo aid; 3. with the SiVo aid but no lip-reading. The testing results show that: L The SiVo aid is good for transmitting the pitch contours and for increasing the tone intelligibility of Chinese; 2. Lip-reading can contribute to lexical tone perception and the duration information of tonemes may be used by the listener; 3. Polysyllabic words gain higher intelligibility increment than monosyllabic words by using ISiVo aid. 4. Some interference between the SiVo aid channel and the lip-reading channel at word level was observed, and the training of continuous speech is' emphasized for Chinese.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-438"
  },
  "arai95_eurospeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Keiko",
     "Okazaki"
    ],
    [
     "Setsuko",
     "Imatomi"
    ],
    [
     "Yuichi",
     "Yoshida"
    ]
   ],
   "title": "Analysis for palatalized articulation of [s] sounds using synthetic speech",
   "original": "e95_1725",
   "page_count": 4,
   "order": 439,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "Palatalized articulation (PA) is frequently observed in speech uttered by postoperative cleft palate patients. We analyzed the PA of [s] sounds and tested human perception of certain synthetic sounds to verify the characteristics of the PA of [s] sounds in Japanese. After analyzing the PA of [s] with linear predictive (LP) analysis, the mono-syllable /sa/, /su/ and /se/ were synthesized by an all-pole model. To synthesize the fricatives, we shifted the frequency of a complex-conjugate pole pair of a filter from 1000 to 3400 Hz. A perceptual experiment involving three speech therapists was carried out to analyze perception of the three syllables. From the results we concluded that fricatives having a peak around 1800 Hz tend to be identified as the PA of [s].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-439"
  },
  "marasek95_eurospeech": {
   "authors": [
    [
     "Krzysztof",
     "Marasek"
    ]
   ],
   "title": "An attempt to classify LX signals",
   "original": "e95_1729",
   "page_count": 4,
   "order": 440,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "Electroglottography [1] provides a noninvasive perspective on vocal fold behaviour without influences from speech articulation. The Lx signal (the changes of the laryngeal conductance) contains not only reliable information about fundamental frequency, but also more detailed information about the voice quality, which may be used to differentiate modes of vocal fold activity. An attempt to use the Lx signals in classification of various phonation types is described, using not only the temporal structure of signal, but also description of the Lx signal shape. The Lx signal was represented using a wide set of parameters and then classified for four phonation types using the rough sets approach [7] and statistical discriminant analysis. The classification rules and subsets of the most important parameters were also determined, showing the importance of both, time and shape, parameters for the Lx signal classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-440"
  },
  "schoentgen95b_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Raoul de",
     "Guchteneere"
    ]
   ],
   "title": "Time series analysis of glottal cycle lengths of healthy and dysphonic speakers",
   "original": "e95_1733",
   "page_count": 4,
   "order": 441,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "We present results of a time series analysis of glottal cycle lengths of healthy and dysphonic speakers. Even during the emission of sustained vowels by healthy speakers, glottis cycle lengths fluctuate slightly. Conventionally, the amount of jitter, as these fluctuations are commonly termed, is estimated by means of measures of dispersion of cycle lengths with respect to the average fundamental period. The problem is that measures of dispersion only are valid descriptors of jitter when perturbations of neighbouring cycles are statistically independent. This hypothesis appears not to be borne out. Therefore, we examined jitter by means of time series analysis which was able to take into account perturbation interdependencies statistically. Discriminant analysis showed that an interdependencies-related feature was indeed the most important factor among those able to distinguish between a set of healthy and dysphonic speakers. The second-most important factor was a measure of dispersion of cycle lengths from which all cycle-to-cycle interdependences had been removed by statistical processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-441"
  },
  "schukattalamazzini95_eurospeech": {
   "authors": [
    [
     "Ernst-Günter",
     "Schukat-Talamazzini"
    ],
    [
     "R.",
     "Hendrych"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Permugram language models",
   "original": "e95_1773",
   "page_count": 4,
   "order": 442,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "In natural languages, the words within an utterance are often correlated over large distances. Long-spanning contextual effects of this type cannot be efficiently and robustly captured by the traditional JV-gram approaches of stochastic language modelling. We present a new kind of stochastic grammar - the permugram model. A permugram model is obtained by linear interpolation of a large number of conventional bigram, trigram, or polygram models which operate on different permutations of the input word sequence under consideration. This way, stochastic dependences between word pairs or word triples lying adjacent as well as remote in the input text can be captured simultaneously without the requirement of very large iNT-grams. Using the permugram model, we achieved test set perplexity reductions of 5-10% compared with interpolated JV-gram models, depending on the application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-442"
  },
  "staab95_eurospeech": {
   "authors": [
    [
     "Steffen",
     "Staab"
    ]
   ],
   "title": "GLR-parsing of word lattices using a beam search method",
   "original": "e95_1777",
   "page_count": 4,
   "order": 443,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "The process of understanding spoken language requires the efficient processing of ambiguities that arise by the nature of speech. This paper presents an approach that allows the efficient incremental integration of speech recognition and language understanding using Tomita's generalized LR-parsing algorithm. For this purpose the GLR-lattice-parsing-algorithm [11] is revised so that an agenda mechanism can be used to control the flow of computation of the parsing process. Subsequently the HMM-evaluations of the word models are combined with a stochastical language model to do a beam search similar to [2, 1, 12], where chartparsers are used to do the job.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-443"
  },
  "seneff95_eurospeech": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Michael",
     "McCandless"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Integrating natural language into the word graph search for simultaneous speech recognition and understanding",
   "original": "e95_1781",
   "page_count": 4,
   "order": 444,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "This paper describes work aimed towards replacing traditional N-gram language models in a recognizer with a more linguistically motivated language model. We report on experiments involving an A* search through a large word graph of candidate hypotheses, within the ARPA ATIS domain. We show that the TINA natural language system, when properly trained, can compete favorably with a traditional word class 4-gram language model, both in terms of raw recognition performance and overall understanding ability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-444"
  },
  "koppelman95_eurospeech": {
   "authors": [
    [
     "Joshua",
     "Koppelman"
    ],
    [
     "Stephen Delia",
     "Pietra"
    ],
    [
     "Mark",
     "Epstein"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "Todd",
     "Ward"
    ]
   ],
   "title": "A statistical approach to language modelling for the ATIS task",
   "original": "e95_1785",
   "page_count": 4,
   "order": 445,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "The goal of this research is to develop an effective natural language component for IBM's spoken language understanding system for the ATIS domain. We use training data to assign a probability distribution to the reference interpretation, the NLParse, which minimizes the observed perplexity of the test data. We limit our scope to deal only with those ATIS2 sentences which can be understood unambiguously out of context (the so-called \"Class A\" queries). The decoder component of the finished system will use the natural language probabilities to select the most probable NLParse translations for a given English input. The NLParse translation can then be deterministically converted to SQL to query the ATIS database for the correct answer. We use a number of different deleted interpolation and maximum entropy techniques to improve on the standard trigram model, and we achieve a reduction in test perplexity from 15.9 to 14.1 bits per item.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-445"
  },
  "jones95b_eurospeech": {
   "authors": [
    [
     "G. J. F.",
     "Jones"
    ],
    [
     "H.",
     "Lloyd-Thomas"
    ],
    [
     "J. H.",
     "Wright"
    ]
   ],
   "title": "Lattice parsing and application of integrated language models for speech recognition",
   "original": "e95_1789",
   "page_count": 4,
   "order": 446,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "This paper analyses the incorporation of rule-based language models based on probabilistic context-free grammars into speech recognition systems. The important issue of efficiency is addressed and the parsing of word lattices is shown to be both effective and fast. It is shown that parse failure can occur when the lattice is incomplete. The application of a phrase level integrated grammar/bigram language model is investigated with particular relevance to handling these parse failures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-446"
  },
  "rayner95_eurospeech": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Peter",
     "Wyard"
    ]
   ],
   "title": "Robust parsing of n-best speech hypothesis lists using a general grammar-based language model",
   "original": "e95_1793",
   "page_count": 4,
   "order": 447,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "We describe a series of experiments designed to investigate the feasibility of using a general linguistically motivated grammar of English to improve the language model of a speech recognizer. A largely automatic corpus-based method was used to convert the general grammar into a specialised version tuned to the domain. This was then used to parse N-best speech hypothesis lists produced by a recognizer, using an algorithm which optionally allowed deletions or substitutions at the beginings and ends of utterances. Competing robust analyses were scored using a weighted combination of several corpus-based preference functions. The sentence accuracy of the recognizer improved from 34.5% to 39%, on a metric which regarded close variants of the reference sentence as successes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-447"
  },
  "brugnara95_eurospeech": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Mauro",
     "Cettolo"
    ]
   ],
   "title": "Improvements in tree-based language model representation",
   "original": "e95_1797",
   "page_count": 4,
   "order": 448,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "This paper describes an efficient way of representing a bigram language model with a finite state network used by a beam-search based and continuous speech HMM recognizer. In a previous paper [1], a compact tree-based organization of the search space was presented, that could be further reduced through an optimization algorithm. There, it was pointed out that for a 10,000-word newspaper dictation task the minimization step could have taken a lot of time and space on a standard workstation. In this paper, a new compilation technique that takes into account the particular tree-based topology is described. Results show that without additional time and space costs, the new technique produces networks equivalent to the tree-based ones but almost as small as the optimized one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-448"
  },
  "kenne95_eurospeech": {
   "authors": [
    [
     "P. E.",
     "Kenne"
    ],
    [
     "M.",
     "O'Kane"
    ],
    [
     "H. G.",
     "Pearcy"
    ]
   ],
   "title": "Language modeling of spontaneous speech in a court context",
   "original": "e95_1801",
   "page_count": 4,
   "order": 449,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "We model the language of the courts by using a number of statistical techniques, and compare the models. In the case of word-phrase bigram and word-phrase trigram models, an issue which arises is the choice of tokens to form the word phrase. We compare the model obtained by choosing the pair which has maximal mutual information, and the model obtained by assuming a binomial ditribution of words and using a likelihood ratio test to choose pairs. The latter model gives a greater reduction in perplexity. We also compare the two choice methods on a corpus which is not based on spoken material, and find similar results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-449"
  },
  "shih95_eurospeech": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Study of vowel variations for a Mandarin speech synthesizer",
   "original": "e95_1807",
   "page_count": 4,
   "order": 450,
   "p1": "1807",
   "pn": "1810",
   "abstract": [
    "This paper reports the results of an acoustic study of Mandarin Chinese that was carried out for the AT&T Mandarin text-to-speech system. We present the optimal classification of vowels for the purpose of the synthesizer, and discuss some coarticulation effects and their implications for the collection of acoustic inventory elements. We are able to achieve excellent speech quality with a diphone-based concatenative system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-450"
  },
  "sanders95_eurospeech": {
   "authors": [
    [
     "Eric",
     "Sanders"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Using statistical models to predict phrase boundaries for speech synthesis",
   "original": "e95_1811",
   "page_count": 4,
   "order": 451,
   "p1": "1811",
   "pn": "1814",
   "abstract": [
    "This paper describes a variety of methods for inserting phrase boundaries in text. The methods work by examining the likelihood of a phrase break occurring in a sequence of three part-of-speech tags. The paper explains this basic technique and desribes more sophisticaed variations using distance probabilities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-451"
  },
  "tatham95_eurospeech": {
   "authors": [
    [
     "Mark",
     "Tatham"
    ],
    [
     "Eric",
     "Lewis"
    ]
   ],
   "title": "Naturalness in a high-level synthetic speech system",
   "original": "e95_1815",
   "page_count": 4,
   "order": 452,
   "p1": "1815",
   "pn": "1818",
   "abstract": [
    "Naturalness in synthetic speech is to a large extent determined by how well the system models the variability found in human speech. Good models of variability are now emerging, and this paper describes how variability of several different types is incorporated into SPRUCE - a high-level text-to-speech synthesis system. The synthesiser is carefully engineered according to the requirements of a recent computational model of speech production. The resulting voice output illustrates the usefulness of well motivated theory in speech synthesiser design.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-452"
  },
  "morton95_eurospeech": {
   "authors": [
    [
     "Katherine",
     "Morton"
    ],
    [
     "Mark",
     "Tatham"
    ]
   ],
   "title": "Pragmatic effects in speech synthesis",
   "original": "e95_1819",
   "page_count": 4,
   "order": 453,
   "p1": "1819",
   "pn": "1822",
   "abstract": [
    "Voice output for applications such as dialogue and information systems requires more than a plain or neutral tone of voice if it is to become more acceptable. These systems must be able to reproduce the acoustic characteristics of emotional and attitudinal effects driven by pragmatic procedures embedded within the dialogue controller. Possible approaches to modelling to this aspect of naturalness are discussed, and some sample data presented which illustrates some of the problems to be encountered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-453"
  },
  "mixdorff95_eurospeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A scheme for a model-based synthesis by rule of F0 contours of German utterances",
   "original": "e95_1823",
   "page_count": 4,
   "order": 454,
   "p1": "1823",
   "pn": "1826",
   "abstract": [
    "The present study deals with the development of a scheme for synthesizing near-to-natural F0 contours of German utterances by rule. It is based on the results of analysis of F0 contours using a quantitative model of the production process of the F0 contour. The analysis is conducted with respect to earlier works on German intonation which are expanded by qualitative and quantitative formulations for intonation patterns on the accent and phrase levels in terms of accent and phrase command parameter values. We discuss the way prosodic rules are produced by analysis of natural F0 contours and construction of a regression tree for every model parameter and explain how the F0 contour for a given utterance can be generated, taking only the text of the utterance itself as input.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-454"
  },
  "ishikawa95_eurospeech": {
   "authors": [
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Speech synthesis by rule based on synthesis units considering prosodic features",
   "original": "e95_1827",
   "page_count": 4,
   "order": 455,
   "p1": "1827",
   "pn": "1830",
   "abstract": [
    "This paper describes on synthesis units for text-to-speech synthesis. Since in concatenative synthesis spectral control is performed by concatenation of synthesis units, a unit is very important problem. Our basic idea is introducing prosodic features into control of spectral features in order to realize natural sounded synthetic speech. In this paper, we report results of basic analytic experiments. Using classification methods that is under consideration of prosodic feature, we have carried out clustering of CV syllables that were detected from sentence utterances. The results show that there exists an obvious relation between prosodic features and spectral features. We also reports quality of synthesis speech which was evaluated by distortion natural speech and CV syllable units obtained by proposed clustering method. These results strongly suggest that spectral control method considered not only phonetic context but also prosodic feature is able to improve quality of synthetic speech in text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-455"
  },
  "kerkhoff95_eurospeech": {
   "authors": [
    [
     "J.",
     "Kerkhoff"
    ],
    [
     "T.",
     "Rietveld"
    ]
   ],
   "title": "The generation of prosody in the nijmegen rule oriented speech synthesis system",
   "original": "e95_1831",
   "page_count": 4,
   "order": 456,
   "p1": "1831",
   "pn": "1834",
   "abstract": [
    "In this contribution a description is given of the modules developed for implementing Dutch prosody, and more in particular an autosegmental description of Dutch intonation in the rule-based Text-to-Speech system NIROS: Nijmegen Interactive Rule Oriented Speech synthesis. Our TTS systeem is completely syntax directed; the synthesizer part is based on a serial pole-zero (ARMA) structure [9]. In NIROS a multi-layer datastructure [6,12] is used reflecting the intuitive notion that speech is organized at serveral levels both segmental and suprasegmental. By letting rules work on the different layers (datastre-ams) of this datastructure the input of the synthesizer is prepared. Emphasis is given both to the multi-layered datastructure, and to the two compilers used in NIROS to manipulate data: the FONPARS rule compiler for handling monolayered datastructures, and the compiler ALF-EIOS for multilayered data. Keywords: text-to-speech, prosody\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-456"
  },
  "vincent95_eurospeech": {
   "authors": [
    [
     "Pean",
     "Vincent"
    ],
    [
     "Lacheret-Dujour",
     "Anne"
    ]
   ],
   "title": "Phonological rules modelling style variations of 'e' caduc in French parisian spontaneous speech for text-to-speech synthesis",
   "original": "e95_1835",
   "page_count": 4,
   "order": 457,
   "p1": "1835",
   "pn": "1838",
   "abstract": [
    "Keywords : phonological variability, speaking styles, spontaneous speech, speech synthesis. The study presented in this paper has been carried out in the framework of phonological variability in French, with applications to automatic speech processing in mind. The specific aim of this study is to both characterize and model intra-speaker schwa variants in two speaking styles. Data have been collected from casual and careful speech corpus. Examples of phonological rules are given here.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-457"
  },
  "wei95_eurospeech": {
   "authors": [
    [
     "Dongbing",
     "Wei"
    ],
    [
     "J. W.",
     "Devaney"
    ],
    [
     "C. C.",
     "Goodyear"
    ]
   ],
   "title": "Voiced diphone synthesis using a parametric model and formant based mapping",
   "original": "e95_1841",
   "page_count": 4,
   "order": 458,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "The vocal tract shapes used by a particular speaker for nine vowels, have been measured using both magnetic resonance imaging and an acoustic reflectance method. These data have been used to obtain parameter values for a simple parametric model of the vocal tract from which the areas of a 21-section acoustic tube synthesiser are found. A technique has been developed for interpolating parameter values among the nine vowel points in the f1,f2 plane. This makes it possible to obtain area functions which, when used in the synthesiser, will provide any prescribed values of the first two formants within the speaker's vowel triangle. The method effectively defines a two-dimensional subspace of the parameter space which is accessed by formant frequencies, thereby offering a method with low computational cost for articulatory copy synthesis of voiced speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-458"
  },
  "yarrington95_eurospeech": {
   "authors": [
    [
     "Debra",
     "Yarrington"
    ],
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Gene",
     "Ball"
    ]
   ],
   "title": "Robust automatic extraction of diphones with variable boundaries",
   "original": "e95_1845",
   "page_count": 4,
   "order": 459,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "This paper presents the approach under development at the Applied Science and Engineering Laboratories (ASEL) for automatic extraction of diphones from a speech database. The present system operates on a set of digitized spoken carrier words to (a) assign segment boundaries within the carrier words, (b) select the best instances of each diphone among the carrier words containing that diphone, and (c) assign multiple, context conditioned, boundaries within the selected diphones. Experiments designed to test the intelligibility and naturalness of the automatically extracted diphones indicated that the automatically extracted diphones resulted in synthesized speech that was slightly more natural sounding and slightly less intelligible than speech synthesized from manually extracted diphones of the same talker.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-459"
  },
  "greenwood95_eurospeech": {
   "authors": [
    [
     "A. R.",
     "Greenwood"
    ]
   ],
   "title": "Generation of articulatory synthesiser parameters from formant frequencies using a cubic mapping function",
   "original": "e95_1849",
   "page_count": 4,
   "order": 460,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "An articulatory synthesiser has been developed that uses the Kelly-Lochbaum filter structure. The vocal tract area function is generated from nine 'quasi-articulatory' parameters. Sets of parameters obtained for several vowels by magnetic resonance imaging have been used to mark the ends of vowel-vowel transitions. A codebook has been obtained for several transitions using an inching technique designed to ensure a uniform coverage in formant space, and that smooth changes in area function occur along straight formant tracks. A non-linear regression technique has then be used to obtain an approximate cubic relationship between formant frequencies and synthesiser parameters. Synthesis of several target tracks have produced synthetic tracks that are similar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-460"
  },
  "veldhuis95_eurospeech": {
   "authors": [
    [
     "R. N. J.",
     "Veldhuis"
    ],
    [
     "I. J. M.",
     "Bogaert"
    ],
    [
     "N. J. C.",
     "Lous"
    ]
   ],
   "title": "Two-mass models for speech synthesis",
   "original": "e95_1853",
   "page_count": 4,
   "order": 461,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "It is shown how two-mass models for the vocal cords can be implemented efficiently with digital signal-processing techniques. This facilitates their use in speech-synthesis hardware. An example of such an implementation and the glottal pulses obtained with it are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-461"
  },
  "bavegard95_eurospeech": {
   "authors": [
    [
     "Mats",
     "Bavegard"
    ]
   ],
   "title": "Introducing a parametric consonantal model to the articulatory speech synthesiser",
   "original": "e95_1857",
   "page_count": 4,
   "order": 462,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "A consonantal extension to our parameterised VT area function model for vowels [1], is proposed. It adds three new parameters to the minimum of the three basic vocalic parameters. Applications to retroflex, lateral and nasal sounds are discussed, in part with reference to VT sweep-tone data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-462"
  },
  "katae95_eurospeech": {
   "authors": [
    [
     "Nobuyuki",
     "Katae"
    ],
    [
     "Tatsuro",
     "Matsumoto"
    ],
    [
     "Shinta",
     "Kimura"
    ],
    [
     "Mitsuko",
     "Kaseda"
    ],
    [
     "Takayuki",
     "Ohyama"
    ]
   ],
   "title": "High-quality Japanese text-to-speech system: NARSYS",
   "original": "e95_1861",
   "page_count": 4,
   "order": 463,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "This paper describes a high-quality Japanese text-to-speech system called NARSYS (NARration SYS-tem). Former systems have two problems, misreadings and unarticulate synthesized speech. For the first problem, we introduce a high accuracy word detection algorithm based on a DP matching method that uses bigram and unigram language models. For the second problem, we introduce a wave-packet concatenating method that uses a tri-phoneme context dependent wave-packet database. The wave packets of 23,000 are manually extracted from natural speech. Accuracies of word-to-phoneme conversion and estimation of bunsetsu accent are 99.8% and 95.9% for sentences in several fields. Syllable articulation score for male and female voices are 88.9% and 73.4%. (* A bunsetsu is a Japanese small phrase which consists of a content word or a content word with some function words.)\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-463"
  },
  "yeou95_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Yeou"
    ]
   ],
   "title": "An investigation of locus equations as a source of information for consonantal place",
   "original": "e95_1901",
   "page_count": 4,
   "order": 464,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "This paper investigates to what extent locus equations can distinguish between different consonants varying in place and manner of articulation. The findings are of two kinds. On the one hand, locus equations do not reflect place-of-articulation distinctions when many consonants varying both in place and manner of articulation are considered. On the other hand, locus equations are successful in distinguising place between pharyngealized and non-pharyngealized consonants. Pharyngealized consonants (/Ss, dS, ss, tV) emerge as a totally distinct class, having the flattest locus equations slopes of all consonants. By virtue of their double articulation, these are highly resistant to vowel coarticulation and induce considerable coarticulatory effects on the adjacent vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-464"
  },
  "parlangeau95_eurospeech": {
   "authors": [
    [
     "Nathalie",
     "Parlangeau"
    ],
    [
     "Regine",
     "Andre-Obrecht"
    ],
    [
     "Alain",
     "Marchal"
    ]
   ],
   "title": "Automatic labelling of multi-sensor speech database: issues and perspectives",
   "original": "e95_1905",
   "page_count": 4,
   "order": 465,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "Speech production is a complex process relying on coordinated gestures, but the acoustic signal does not depict its underlaying organization. Accepting that articulatory gestures are directly recognized through the coarticulation process, our proposal is to investigate the correlations between acoustic and articulatory informations and to assess gestural phonetic theory. We present here the framework for this investigation, the automatic labelling of the multi-sensor speech database ACCOR on French and English sentences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-465"
  },
  "son95b_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "What does consonant reduction look like, if it exists?",
   "original": "e95_1909",
   "page_count": 4,
   "order": 466,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "Vowel reduction has been studied for years. It is a universal phenomenon that reduces the distinction of vowels in informal speech and unstressed syllables. How consonants behave in situations where vowels are reduced is not known. In this paper we compare global durational and spectral data for both intervocalic consonants and vowels uttered in read speech with otherwise identical segments from spontaneous speech. On this global level, it shows that consonants behave like vowels when the speaking style is changed. However, all differences are less pronounced for consonants than for vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-466"
  },
  "bailly95_eurospeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Louls-Jean",
     "Boe"
    ],
    [
     "N.",
     "Vallee"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Articulatori-acoustic vowel prototypes for speech production",
   "original": "e95_1913",
   "page_count": 4,
   "order": 467,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "Articulatory prototypes for the ten French vowels have been obtained via acoustic-to-articulatory inversion. We show that these prototypes are coherent with experimental data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-467"
  },
  "pitermann95_eurospeech": {
   "authors": [
    [
     "M.",
     "Pitermann"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Experimental study of the target theory of vowel production",
   "original": "e95_1917",
   "page_count": 4,
   "order": 468,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "The target undershoot model of vowel production predicts that vowels are characterized by formant values called targets that are not reached under all circumstances. It is assumed that these targets are fixed with respect to speaking rate and stress pattern. In the work described here we attempted to investigate this issue. We dynamically and cinematically modelled formant time series to estimate unreached targets; then we compared the results of a speaking-rate and stress-pattern variance analysis of the first two formants of vowels [a] and [c] to the results of a two-factor variance analysis of the corresponding formant target estimates. The two vowels were produced by two speakers in a [iVi] context at several speaking rate and stress pattern combinations. The corpus was perceptually evaluated to spot uncertain data. Part of our results gave support to the target undershoot model, but they were not sufficiently convincing to back it unambiguously.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-468"
  },
  "sock95_eurospeech": {
   "authors": [
    [
     "R.",
     "Sock"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Anders",
     "Löfqvist"
    ]
   ],
   "title": "Comparing tongue kinematic and acoustic phasing patterns for vowel quantity contrasts in WOLOF",
   "original": "e95_1921",
   "page_count": 4,
   "order": 469,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "The major question addressed in this investigation is: can one pinpoint relationships between acoustic phasing patterns and tongue body kinematic phasing patterns in the production of vocalic quantity contrasts? If so, to what extent could kinematic phasing patterns be inferred from acoustic phasing patterns? Regularities, that may unveil articulatory-acoustic relations, represent crucial information in trying to recover articulatory gestures from speech sounds. This investigation is thus, a contribution, hopefully, to understanding the complex ill-posed problem of inverse mapping.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-469"
  },
  "perrier95_eurospeech": {
   "authors": [
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Lian",
     "Apostol"
    ],
    [
     "Yohan",
     "Payan"
    ]
   ],
   "title": "Evaluation of a vowel normalisation procedure based on speech production knowledge",
   "original": "e95_1925",
   "page_count": 4,
   "order": 470,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "In this paper a vowel normalisation procedure, proposed in 1993 by Pay an & Perrier [1], and based on the notion of affiliation between formants and vocal tract cavities, is quantitatively evaluated on five different speakers. For that, we compare its performance to those of 4 normalisation procedures, currently quoted in the literature, and of a modified version of one of them. Our procedure appears to be the best one for the normalisation of formant F2. It is, however, fairly weak for Fl. This failure seems to be explained by the acoustic origin of formant Fl, which is a Helmholtz resonance of the back cavity, and suggests that the size of the constriction should also be taken in consideration in the procedure. Keywords: speaker normalisation; speaker adaptation;  vowel  production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-470"
  },
  "sampath95_eurospeech": {
   "authors": [
    [
     "Santha",
     "Sampath"
    ]
   ],
   "title": "Dialect specific features of australian English diphthongs in spontaneous speech",
   "original": "e95_1929",
   "page_count": 4,
   "order": 471,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "In this paper we describe similarities and differences in the formant trajectories of Australian English diphthongs /OX/, /ai/ and /IB/ in \"boy\", \"buy\" and \"here\" in two dialects, one spoken by native speakers of Australian English and another spoken by Tamil migrants living in Australia. This contrastive analysis conducted in f1-f2 plane lend support to the view ([5],[6]) that non-native speakers tend to approach native speaker's certain sound with longer exposure to the target dialect.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-471"
  },
  "vescovi95_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Vescovi"
    ],
    [
     "Eric",
     "Castelli"
    ],
    [
     "Xavier",
     "Pelorson"
    ]
   ],
   "title": "Adaptation of a two-mass model of the vocal cords to a particular speaker",
   "original": "e95_1933",
   "page_count": 4,
   "order": 472,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "In this paper, a two-mass model is used in order to reproduce a particular speaker's voice. One of the aim of this work is to reduce the number of parameters of the model, in order to make its control easier. In the classical two-mass model for example, this was done using the mass-tension factor Q which gives relations between the masses and the tensions [4]. Five characterisation parameters are measurered on a small corpus of vowels as to determine the distance between two glottal flow wave shapes. Then, a genetic algorithm is used as to tune the two-mass model commands for different pitches, in order to extract relations between those commands.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-472"
  },
  "springer95_eurospeech": {
   "authors": [
    [
     "Stephen",
     "Springer"
    ],
    [
     "Sara",
     "Basson"
    ],
    [
     "Ashok",
     "Kalyanswamy"
    ],
    [
     "Edward",
     "Man"
    ],
    [
     "Dina",
     "Yashchin"
    ]
   ],
   "title": "The money talks interactive speech technology assessment: a report from the field",
   "original": "e95_1939",
   "page_count": 4,
   "order": 473,
   "p1": "1939",
   "pn": "1942",
   "abstract": [
    "In order to understand how speech technologies may best be configured to provide value to customers in the field, NYNEX Science & Technology conducted a \"Wizard-of-Oz\" field trial in which unsolicited callers to an existing financial information system were presented with a spectrum of speech-driven interfaces. Statistics were gathered to assess the success of each interface, and compared against an existing Touch-Tone-interface. Overall, 85.5% of callers who engaged the speech systems at all successfully completed at least one service. Adding word-spotting capability increased service completion by approximately 3% over systems without it. Allowing callers to barge-in was identified as a strong requirement. Information on optimum wording of prompts was also measured. However, a speech interface did not provide a clear advantage over Touch-Tone in this application, and simpler keyword technology appeared preferable to natural language systems. The results are significant for any field deployment of ASR, particularly where the goal is to replace an existing Touch-Tone interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-473"
  },
  "siroux95_eurospeech": {
   "authors": [
    [
     "J.",
     "Siroux"
    ],
    [
     "M.",
     "Guyomard"
    ],
    [
     "Y.",
     "Jolly"
    ],
    [
     "F.",
     "Multon"
    ],
    [
     "C.",
     "Remondeau"
    ]
   ],
   "title": "Speech and tactile-based georal system",
   "original": "e95_1943",
   "page_count": 4,
   "order": 474,
   "p1": "1943",
   "pn": "1946",
   "abstract": [
    "This paper describes the principal elements of the oral and tactile GEORAL system, as well as the results of a primary evaluation of the system. Firstly, we present the main dialogue functionalities and the problems due to the multimodal activities. These problems are mainly concerned with the relationships between the linguistic syntagms of oral utterances and the user's activity on the screen (pointing, drawing a zone,...). Then we describe the technical choices we made and the architecture of the system. We decided to give priority to the speech input, and to merge the results of the two activities at the latest possible. Some details both on the data structures (dialogue acts, communicative acts) we use and the processing ways are provided. Finally, we present the first results of an evaluation of the system with unexperimented users. The results show the oral-tactile synergy is well accepted and improve slightly the system understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-474"
  },
  "fraser95_eurospeech": {
   "authors": [
    [
     "Norman M.",
     "Fraser"
    ],
    [
     "J. H. Simon",
     "Thornton"
    ]
   ],
   "title": "Vocalist: a robust, portable spoken language dialogue system for telephone applications",
   "original": "e95_1947",
   "page_count": 4,
   "order": 475,
   "p1": "1947",
   "pn": "1950",
   "abstract": [
    "This paper describes Vocalist, a robust, portable spoken language dialogue system for telephony applications. It is robust in that it has been demonstrated to perform well with untrained users. It is portable in that its components are designed in a fashion which is application independent. To port to a new application it is necessary only to define new application knowledge bases. The dialogue engine has been equipped with very general knowledge about information-seeking dialogues such that it could, in principle, operate in any application domain with only minimal customisation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-475"
  },
  "suzuki95b_eurospeech": {
   "authors": [
    [
     "Masami",
     "Suzuki"
    ],
    [
     "Naomi",
     "Inoue"
    ],
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "A prototype of a Japanese-Korean realtime speech translation system",
   "original": "e95_1951",
   "page_count": 4,
   "order": 476,
   "p1": "1951",
   "pn": "1954",
   "abstract": [
    "In this article we report the design principle and the outline of our spoken dialogue translation system from Japanese into Korean. Compared with existing speech translation systems, our prototype system has two major features: 1) Semi-realtime system based on a compact hardware configuration, and 2) Improving dialogue speech recognition rate using a light-weight discourse monitor. Additionally we implemented a user-friendly interface, considering the hotel reservation task environment. The experimental results with evaluation suggested us the feasibility of the system for limited task domains. Based on this experiences, crucial issues for interpreting communications technique will be discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-476"
  },
  "zeigler95_eurospeech": {
   "authors": [
    [
     "B. L.",
     "Zeigler"
    ],
    [
     "B.",
     "Mazor"
    ]
   ],
   "title": "Query-response relationships in the oasis speech-recognition system",
   "original": "e95_1955",
   "page_count": 4,
   "order": 477,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "We have recently developed a speech-recognition system for automation of telephone-service orders, and tested the system through four months of regular use by GTE customers. In this paper, we use quantitative data (response content) to assess the effectiveness of our structured transaction model, and evaluate the extent to which natural queries yielded responses in the predicted linguistic form. Our results showed the structured model to be a successful approach to transaction automation. The results also show that customer responses to our designed queries were both predictable in form and quite limited in variety.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-477"
  },
  "lamel95b_eurospeech": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "S.",
     "Rosset"
    ],
    [
     "S.",
     "Bennacef"
    ],
    [
     "H.",
     "Bonneau-Maynard"
    ],
    [
     "L.",
     "Devillers"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Development of spoken language corpora for travel information",
   "original": "e95_1961",
   "page_count": 4,
   "order": 478,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "In this paper we report on our ongoing work in developing spoken language corpora in the context of information access in two travel domain tasks, l'Atis and Mask. The collection of spoken language corpora remains an important research area and represents a significant portion of work in the development of spoken language systems. The use of additional acoustic and language model training data has been shown to almost systematically improve performance in continuous speech recognition. Similarly, progress in spoken language understanding is closely linked to the availability of spoken language corpora. We record subjects on a regular basis using development versions of the spoken language systems for both tasks, obtaining over 1000 queries/month from 20 subjects. To help assess our progress in system development, each subject since March'95 completes a questionnaire addressing the user-friendliness, reliability, ease-of-use of the Mask data collection system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-478"
  },
  "flammia95_eurospeech": {
   "authors": [
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Empirical evaluation of human performance and agreement in parsing discourse constituents in spoken dialogue",
   "original": "e95_1965",
   "page_count": 4,
   "order": 479,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "This paper presents an empirical study on the annotation of discourse units in spoken dialogues. The goal of this research is to examine whether task-oriented human-human dialogues can be structured as sequences of a small number of individual discourse segments that can be reliably end-pointed. The data used for this study is a corpus of 18 orthographic, transcriptions of actual telephone conversations between customers and travel agents or Yellow Pages operators. We propose the use of a general agreement metric derived from the kappa coefficient and we apply it to measuring the level of agreement among human coders in bracketing discourse segments. Despite the apparent difficulty of this annotation task, we show that a level of agreement around 60% can be reached among at least three out of five coders with variable levels of expertise, using a minimal and theory-neutral set of annotation instructions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-479"
  },
  "nitta95_eurospeech": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Mika",
     "Amamiya"
    ],
    [
     "Hiroyuki",
     "Kamio"
    ],
    [
     "Hiroshi",
     "Matsu'ura"
    ],
    [
     "Arisa",
     "Uchiyama"
    ],
    [
     "Masafumi",
     "Tamura"
    ]
   ],
   "title": "Multimodal spoken dialogue systems and rapid-prototyping",
   "original": "e95_1969",
   "page_count": 4,
   "order": 480,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "A multimodal dialogue platform (MultiksDial) and rapid-prototyping environment of multimodal user-interface (MUI) are described. MUI for a task is designed by using a MUI design support tool (Muse) and translated into UIScript along which the multi modal dialogue platform is operated and tested. MU I designers can set various types of properties of UI-object (input-objects: button, speech, hand-writing, sensor; output-object: text, image, audio, text-to-speech) with dialogue boxes easily. The preparation of the rapid-prototyping environment has reduced the constructing & testing hours for multimodal dialogue systems by less than one-sixth.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-480"
  },
  "larsen95_eurospeech": {
   "authors": [
    [
     "Lars Bo",
     "Larsen"
    ]
   ],
   "title": "Development and evaluation of a spoken dialogue for a telephone based transaction system",
   "original": "e95_1973",
   "page_count": 4,
   "order": 481,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "This paper presents the development of a spoken dialogue system for a Book Club Member Service [1]. The dialogue has been designed and implemented utilising a Generic Dialogue System (GDS) platform, developed at CPK [2]. The goal of this work has been to evaluate whether it would be feasible to design and implement a Spoken language Dialogue System (SDS) based on the functionality of an existing Voice Response (VR) system, and using the GDS. Following the introduction and a description of the application, the paper briefly presents the GDS and the hardware architecture. The dialogue design, implementation and evaluation procedures are then described and discussed. A preliminary field test has been carried out in order to evaluate the system, and to compare it to the original VR system. Finally, some conclusions are proposed, as to whether this principle of creating SDS from VR is applicable, and some future tasks are briefly discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-481"
  },
  "hild95_eurospeech": {
   "authors": [
    [
     "Hermann",
     "Hild"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Integrating spelling into spoken dialogue recognition",
   "original": "e95_1977",
   "page_count": 3,
   "order": 482,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "Recognition of spelled letter sequences is essential for many real-world applications which involve arbitrary names or addresses. Often the letter sequences carry the sentence's crucial information; therefore, it is important to correctly localize and recognize the spelled string. However, large vocabulary speech recognizers tend to perform poorly on spelled letters, especially if they have to deal with spontaneous speech. The research presented here aims at improving the recognition accuracy of spontaneous speech with embedded spelled-letter sequences. We propose methods to localize spelled-letter segments and reclassify them with a specialized letter recognizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-482"
  },
  "gales95_eurospeech": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "The application of parallel model combination to a large vocabulary dictation task",
   "original": "e95_1983",
   "page_count": 4,
   "order": 483,
   "p1": "1983",
   "pn": "1986",
   "abstract": [
    "The aim of the work described in this paper is to make a state of the art HMM based speech recognition system robust to the effects of additive noise. It describes the application of Parallel Model Combination to a large vocabulary speech recognition in noise task, the ARPA 1994 CSRNAB Spoke 10. Two variants of PMC have been examined; the Log-Add approximation and Data-driven Parallel Model Combination (DPMC). In the latter two schemes were investigated, one compensating both the means and variances, the other just the means. All the schemes were found to improve the noise robustness of the system. Comparable performance to the DPMC mean compensated system was achieved using the Log-Add approximation. However, little gain in performance was found when the variances were compensated, indeed on the evaluation data the performance was degraded. This was surprising, since experiments on the Resource Management database with additive noise showed that variance compensation consistently improved performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-483"
  },
  "mokbel95_eurospeech": {
   "authors": [
    [
     "C.",
     "Mokbel"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "J.",
     "Monné"
    ]
   ],
   "title": "Blind equalization using adaptive filtering for improving speech recognition over telephone",
   "original": "e95_1987",
   "page_count": 4,
   "order": 484,
   "p1": "1987",
   "pn": "1990",
   "abstract": [
    "It is well known that the telephone channel introduces a convolved disturbing component in the observed signal which slowly varies with time. Several techniques have been proposed in the literature to reduce the telephone line effects, namely cepstral subtraction and highpass filtering of cepstral trajectories. In a previous work [8] we introduced a theoretical framework to clarify the link between the channel effects and the low frequencies in the cepstral trajectories and to interpret the encouraging results obtained by cepstral subtraction (an off-line approach) and highpass filtering of the cepstral trajectories. This paper proposes a new approach to reduce the telephone line effects. This approach implements an adaptive filter in a blind equalization scheme. On the basis of recognition results obtained on different speaker independent telephone databases, the blind equalization seems to outperform the classical on-line technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-484"
  },
  "cao95_eurospeech": {
   "authors": [
    [
     "Yuchang",
     "Cao"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Miles",
     "Moody"
    ]
   ],
   "title": "Speech-seeking microphone array with multi-stage processing",
   "original": "e95_1991",
   "page_count": 4,
   "order": 485,
   "p1": "1991",
   "pn": "1994",
   "abstract": [
    "This paper describes an auto-seeking microphone array consisting of novel multiple processing stages for speech acquisition. The array automatically detects the speech in the presence of noise and tracks the speech sources. A combination of dual-beamforming and post-beamformer enhancement significantly improves the quality and intelligibility of desired speech at the output of the system. Applications for such a system include hands-free telephony, teleconferencing and special situations where speech signals must be picked up in a noisy acoustic environment and the microphones are hidden (as, e.g., in a forensic covert recording system).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-485"
  },
  "vereecken95_eurospeech": {
   "authors": [
    [
     "Halewijn",
     "Vereecken"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Recognition of noisy speech using an auditory model",
   "original": "e95_1995",
   "page_count": 4,
   "order": 486,
   "p1": "1995",
   "pn": "1998",
   "abstract": [
    "This contribution addresses the problem of speaker-independent continuous speech recognition in the presence of uncorrelated additive noise. Most experiments reported thus far dealt with small vocabulary isolated word recognition. We describe how the acoustic feature extraction by an auditory model can be improved by introducing noise magnitude subtraction inside the auditory model. A computationally attractive algorithm is presented and evaluated. It clearly outperforms the standard power spectral subtraction postprocessing technique applied on the outputs of the original auditory model. The experiments show that no retraining of the speech models and no tuning of the proposed algorithm to specific noise conditions is required.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-486"
  },
  "womack95_eurospeech": {
   "authors": [
    [
     "Brian D.",
     "Womack"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Stress independent robust HMM speech recognition using neural network stress classification",
   "original": "e95_1999",
   "page_count": 4,
   "order": 487,
   "p1": "1999",
   "pn": "2002",
   "abstract": [
    "It is well known that the variability in speech production due to task induced stress contributes significantly to loss in speech recognition performance [6, 8]. If an algorithm could be formulated which estimates the speech stress condition, then such knowledge could be integrated to improve robustness of speech recognizers in adverse conditions. In this paper, the problem of automatic stressed speech recognition is addressed. The primary goal is to formulate a tandem HMM and neural network based algorithm for stress independent recognition. To motivate an effective stress .classifier, an analysis is performed of speech produced across eleven stress conditions (e.g. Angry, Clear, Fast, Lombard, Loud, Slow, Soft, etc.). Features that differentiate stress using a previously established stressed speech database (SUSAS) are employed. A neural network algorithm is formulated to estimate a speech stress condition probability vector (with classification rates on the order of 59-100%). The stress classification output probability vector is used to weight the outputs of a codebook of stress dependent HMM recognizers to generate an improved overall recognition score (for a 6-11% improvement over neutral or multi-style trained recognition systems). It is suggested that this approach will accommodate the intra-speaker variability due to task induced stress in adverse conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-487"
  },
  "linhard95_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Linhard"
    ]
   ],
   "title": "Speech enhancement using two versions of the noisy speech signal",
   "original": "e95_2005",
   "page_count": 4,
   "order": 488,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "In this paper we describe a two-channel speech enhancement system. Two noisy versions of a speech signal are picked up with closely spaced microphones. The system systematically exploits temporal and spatial characteristics of the sound field and fully operates in the frequency domain. The system is based on the spectral subtraction technique and adaptive beamforming. It may be used to improve speech quality for hands-free telephone as well as to preprocess speech for speech operated systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-488"
  },
  "martin95b_eurospeech": {
   "authors": [
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Design and optimization of a two microphone speech enhancement system",
   "original": "e95_2009",
   "page_count": 4,
   "order": 489,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "This contribution presents a novel two microphone speech enhancement system and its optimization using objective measures of speech quality. The speech enhancement system was designed for the voice communication system of a Computed Tomography system (CT scanner) which is used for the observation of the patient during CT examinations. The purpose of the system is to reduce noise and thus to enhance the speech signal of the patient which is transmitted to the control desk. The enhancement is based on the spatial coherence of noise and speech signals and is accomplished in two separate frequency bands by means of adaptive filters, scalar adaptive weighting and a highpass filter. For the optimization of the algorithms we used objective criteria which include the distortion of the clean speech signal and the reduction of noise during speech activity and during speech pause. The real time version of the speech enhancement system is implemented on a single Motorola DSP 56001.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-489"
  },
  "drews95_eurospeech": {
   "authors": [
    [
     "Martin",
     "Drews"
    ]
   ],
   "title": "Time delay estimation for microphone array speech enhancement systems",
   "original": "e95_2013",
   "page_count": 4,
   "order": 490,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "Adaptive microphone array systems have achieved widespread use because of their efficiency at enhancing speech signals which are degraded by additional background noise. However, these multichannel systems suffer significant performance degradation in the presence of time differences of arrival (TDOA's) between the received speech signals. Therefore beamsteering or beamforming techniques are applied in order to estimate and compensate the TDOA's. A new method for estimating TDOA's in noisy speech signals is presented which includes a generalized cross-correlator, an improved peak detector, and a plausibility check of the estimated TDOA's. This time delay estimator shows a significant improvement upon time delay estimators proposed in literature.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-490"
  },
  "cao95b_eurospeech": {
   "authors": [
    [
     "Yuchang",
     "Cao"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Miles",
     "Moody"
    ]
   ],
   "title": "Speech enhancement by eigen decomposition with two-channel observations",
   "original": "e95_2017",
   "page_count": 4,
   "order": 491,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "The concept of eigen decomposition for two-channel signal separation, a novel method of enhancing the desired signal corrupted by interference, is described. The method uses two observations, which come from a pair of single sensors (or beamformers) both of which contain the desired signal and the undesired signal(s). The method assumes that the desired signal and the undesired signal(s) are uncorrelated and the signal-to-noise ratios (the ratio of the desired signal to the undesired signal(s)) of each observation are different. A new algorithm based on eigen analysis has been developed to eliminate the noise components sequentially. The technique has been successfully used to separate a speech signal corrupted heavily by ambient noise, co-talker interference and background music.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-491"
  },
  "giuliani95_eurospeech": {
   "authors": [
    [
     "D.",
     "Giuliani"
    ],
    [
     "M.",
     "Matassoni"
    ],
    [
     "M.",
     "Omologo"
    ],
    [
     "P.",
     "Svaizer"
    ]
   ],
   "title": "Robust continuous speech recognition using a microphone array",
   "original": "e95_2021",
   "page_count": 4,
   "order": 492,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "This paper focuses on the use of microphone arrays for speaker independent continuous speech recognition in real environment. An array of four omnidirectional microphones was placed at 1.5 m distance from the talker; given the array signals, a Time Delay Compensation (TDC) technique was applied to provide a beamformed signal, that is shown effective as input to a Hidden Markov Model (HMM) based recognizer. The paper also refers to three enhancement/normalization techniques that are employed to obtain a better alignment between the new noisy conditions and the clean ones, under which the training was carried out. In particular, a phone HMM adaptation technique seems to be the most promising for developing a scenario of rapid speaker/environment/ channel adaptation of a realtime hands-free recognizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-492"
  },
  "llisterri95_eurospeech": {
   "authors": [
    [
     "Joaquim",
     "Llisterri"
    ],
    [
     "Rafael",
     "Mann"
    ],
    [
     "Carme de la",
     "Mota"
    ],
    [
     "Antonio",
     "Rios"
    ]
   ],
   "title": "Factors affecting F0 peak displacement in Spanish",
   "original": "e95_2061",
   "page_count": 4,
   "order": 493,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "The aim of the paper is to study the displacement of fundamental frequency (Fq) peaks phonologically associated with lexically stressed syllables in Peninsular Spanish in order to determine some of the factors that can influence this phenomenon. The results show that in paroxyton words in non prepausal position F0 peaks are shifted at least one syllable to the right of the one bearing the lexical stress. An important effect of the strength of different types of syntactic boundaries and also an effect of pauses have been observed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-493"
  },
  "auberge95_eurospeech": {
   "authors": [
    [
     "Véronique",
     "Aubergé"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Generation of intonation: a global approach",
   "original": "e95_2065",
   "page_count": 4,
   "order": 494,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "Intonation in governed by independent units. However the coherence of communication processing involve the existence of rendezvous between intonation and other linguistic structures. These rendez-vous are used in a top-down modelization of intonation. Phonetic prototypes of intonation (stored in an hierarchical lexicon) are associated to linguistic and para-linguistic attributes. This global approach was applied on different languages. Some general results of application of this method are presented where prototypes clearly appear.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-494"
  },
  "czigler95_eurospeech": {
   "authors": [
    [
     "Peter E.",
     "Czigler"
    ],
    [
     "Dawn M.",
     "Behne"
    ]
   ],
   "title": "Prevocalic consonant duration in Swedish: effects of vowel quality and postvocalic place of articulation",
   "original": "e95_2069",
   "page_count": 4,
   "order": 495,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "This research examines the effects of vowel quality (high vs. low) and postvocalic place of articulation (labial, dental, velar) on the relative timing of syllable-internal components, concentrating on the duration of prevocalic consonants. Results indicate that the duration of a prevocalic consonant tends to approximate compensate for the inherent duration of a following vowel. And although postvocalic place of articulation was not found to affect the duration of a prevocalic consonant in a CVC syllable, a relatively an inverse relationship was observed between the duration of a vowel and postvocalic consonant. These effects lead to durational patterns for the rhyme which result in a relatively stable overall syllable duration.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-495"
  },
  "vitez95_eurospeech": {
   "authors": [
    [
     "P.",
     "Vitez"
    ],
    [
     "Véronique",
     "Aubergé"
    ]
   ],
   "title": "Intonation gesture of slovene: first indications",
   "original": "e95_2073",
   "page_count": 3,
   "order": 496,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "Intonation of Slovene was studied in few works [1, 5, 7, 8] and rarely in the acoustic domain. This paper proposes some first results issued from corpus-based analysis of Slovene intonation. It is shown that some aspects of Slovene prosody, like realization of lexical stress, were bad known. Some first indices of the existence of intonation prototypes [9] are given in a top-down global approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-496"
  },
  "heuft95_eurospeech": {
   "authors": [
    [
     "Barbara",
     "Heuft"
    ],
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Intrinsic prosodic values and segmental context",
   "original": "e95_2077",
   "page_count": 4,
   "order": 497,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "This investigation deals with two types of microprosodic variations: the intrinsic values for F0 and duration in German and the influence of adjacent sounds on these values. The values are computed using an inventory for concatenative speech synthesis. The results of the measurement of intrinsic F0 and duration for vowels correspond to those of other studies. The F0 values for voiced consonants and durations for all consonants are studied as well. The influence of voicing, nasality and place of articulation of a preceding consonant on vowel F0 and duration is described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-497"
  },
  "tams95_eurospeech": {
   "authors": [
    [
     "Andy",
     "Tams"
    ],
    [
     "Mark",
     "Tatham"
    ],
    [
     "Julian H.",
     "Page"
    ]
   ],
   "title": "Describing speech styles using prosody: a pilot study",
   "original": "e95_2081",
   "page_count": 4,
   "order": 498,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "In Text-to-Speech (hence forth) synthesis and related fields, attention is now being given to the research problem of replicating specific speaking styles. This has been facilitated by recent technological improvements in the capabilities of TTS systems, and the incorporation of the technology into real life applications and commercial products. This paper is concerned with describing and modelling the style of a radio news broadcaster for eventual inclusion in a speech synthesis system. A methodology for this task is specified. Pilot experimental work comparing a selected corpus of radio news broadcasts with read aloud versions of their edited transcriptions is reported. The pilot work described compares prosodic parameters of tone units, stresses, and speaking rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-498"
  },
  "cabrera95_eurospeech": {
   "authors": [
    [
     "C. Franchon",
     "Cabrera"
    ]
   ],
   "title": "Stress and intonation in Spanish for affirmative and interrogative sentences",
   "original": "e95_2085",
   "page_count": 4,
   "order": 499,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "The aim of this research is to study prosodic structure in Spanish. We present an experimental approach to the complex relationship that exists between intonation and lexical stress, and relate it to syntactic structure variations. Results indicate the preeminence of the accentual unit as minimal segment in the prosodic structuration of Spanish. This work has enabled us to elaborate prosodic prediction rules for speech syntesis, taking into account the variability of the three main acoustic parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-499"
  },
  "lehning95_eurospeech": {
   "authors": [
    [
     "Michael",
     "Lehning"
    ]
   ],
   "title": "Statistical methods for the automatic labelling of German prosody",
   "original": "e95_2089",
   "page_count": 4,
   "order": 500,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "Manual prosodic labelling is tedious and time-consuming. To overcome this problem, we propose a semi-automatic method. To support the human transriber we generate a prototypical prosodic description by statistical methods. The prosodic description includes the word boundaries in the speech signal, the accents, and the phrase boundaries. We detect the phone boundaries (including the word boundaries) in the speech signal by the use of an HMM speech recognizer and predict accents and phrase boundaries from the text using a categorical language model (trained with prosodically labelled text data). We combine the results of the segmentation and the prediction to locate the predicted accents and boundaries in the speech signal. This prototypical information about the prosody can be integrated into the human labelling strategy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-500"
  },
  "kai95_eurospeech": {
   "authors": [
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Investigation on unknown word processing and strategies for spontaneous speech understanding",
   "original": "e95_2095",
   "page_count": 4,
   "order": 501,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "In this paper, an unknown word processing method is investigated as an approach to achieve the robust spontaneous speech understanding. The unknown words are detected by a subword-unit based decoder and the process is incorporated into the One-Pass search algorithm. The preliminary experiments showed that the method is also effective to the utterance including interjections (or filled pauses). In addition, the phrase-spotting based approaches were compared with the One-Pass search method in which the unknown word processing was incorporated. The experiments showed that the One-Pass method attained the best performance on spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-501"
  },
  "caminerogil95b_eurospeech": {
   "authors": [
    [
     "F. Javier",
     "Caminero-Gil"
    ],
    [
     "Celinda de la",
     "Torre-Munilla"
    ],
    [
     "Lúis",
     "Hernandez-Gomez"
    ],
    [
     "Cesar",
     "Martin del Álamo"
    ]
   ],
   "title": "New n-best based rejection techniques for improving a real-time telephonic connected word recognition system",
   "original": "e95_2099",
   "page_count": 4,
   "order": 502,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "Most existing rejection techniques for connected speech recognition are based on the likelihood score of different recognition hypotheses. The rejection procedure described in this paper attempts to improve the ability to reject out-of-vocabulary utterances combing likelihood measures with application-dependent knowledge. This knowledge is integrated in the recognition system using the N-best paradigm. The paper describes the adaptation of an N-best algorithm to our recognition task of telephone numbers. Two simple utterance rejection techniques are presented: one considering only the N-best recognition results and other introducing external knowledge. Experimental results using these techniques show that simple methods for utterance verification provide moderate performance using garbage models and the results from the N-best hypotheses. When external knowledge sources are included for our recognition task we obtain a 20% increase of performance in terms of recognition error with an excellent trade-off between false rejection and false alarms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-502"
  },
  "sakamoto95_eurospeech": {
   "authors": [
    [
     "Hiroyuki",
     "Sakamoto"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Detection of unknown words using garbage cluster models for continuous speech recognition",
   "original": "e95_2103",
   "page_count": 4,
   "order": 503,
   "p1": "2103",
   "pn": "2106",
   "abstract": [
    "This paper proposes a speech recognition strategy to deal with utterances having unknown words. The strategy integrates unknown-word detection using garbage (phoneme) cluster Hidden Markov Models (HMMs) and registered-word recognition using phoneme HMMs. These garbage cluster models are designed to minimize both the increase in processing for unknown words and the decrease in the recognition performance for registered words. A further goal is to maximize the detection performance of unknown words. Two important issues are studied: 1) the balance between scores of registered words and those of unknown words, and 2) use of the unknown word penalty derived from the stochastic language model. In sentence recognition experiments using this unknown-word processing, the proposed cluster models that take into account the Japanese syllabic construction achieved the best word accuracy of 71.8%, compared with 56.6% for sentence recognition without this processing. We confirmed the effectiveness of this strategy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-503"
  },
  "jusek95_eurospeech": {
   "authors": [
    [
     "A.",
     "Jusek"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "H.",
     "Rautenstrauch"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Detection of unknown words and its evaluation",
   "original": "e95_2107",
   "page_count": 4,
   "order": 504,
   "p1": "2107",
   "pn": "2110",
   "abstract": [
    "Especially in recognition of spontaneous speech it is necessary to be able to cope with the occurance of unknown words. We present an approach to unknown word detection integrated with speech recognition. Though several schemes exist for the evaluation of detection algorithms all suffer from some deficiencies. Therefore, we also present a new evaluation measure called detection accuracy which is similar to the widely accepted word accuracy. We applied this measure to the evaluation of our approach on a large spontaneous speech recognition task from the German Verbmobil project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-504"
  },
  "lee95d_eurospeech": {
   "authors": [
    [
     "Seung-Bae",
     "Lee"
    ],
    [
     "Lag-Yong",
     "Kim"
    ],
    [
     "Min-Seong",
     "Kim"
    ],
    [
     "Jong-Seok",
     "Lee"
    ],
    [
     "Shin-Wook",
     "Kang"
    ]
   ],
   "title": "Minimum duration constrained non-keyword modeling and rejection for word spotting",
   "original": "e95_2111",
   "page_count": 4,
   "order": 505,
   "p1": "2111",
   "pn": "2114",
   "abstract": [
    "This paper describes the techniques for non-keyword modeling and rejection in a Korean keyword spotting system based on continuous hidden Markov models (CHMM). For non-keyword modeling, we propose a filler model which remains in the optimum path for minimum duration. It consists of several states which share an identical output probability distribution function and the number of states is the same as that of the minimum duration. Also, we investigate the performance of the proposed filler model focused on both reducing the computational complexity and improving the detection rate. In order to reject false alarms, we introduce false models similar to keyword models and employ them to determine the existence of the keyword embedded in the utterance in post processing stage. The experimental results showed that our new filler requires lower computational complexity while it provides slightly better performance than monophone fillers. Also, the false models rejected 75 false alarms out of 100 at 5.9 false alarms per keyword per hour resulting in 79.1 % detection rate. Keywords: rejection, continuous hidden Markov model, filler, false alarms, detection rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-505"
  },
  "accaino95_eurospeech": {
   "authors": [
    [
     "Sari",
     "Accaino"
    ],
    [
     "Bart",
     "D'hoore"
    ],
    [
     "Johan",
     "Vantieghem"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Rejection capabilities for HMM-based speech recognizers",
   "original": "e95_2115",
   "page_count": 4,
   "order": 506,
   "p1": "2115",
   "pn": "2118",
   "abstract": [
    "We present a rejection method of out-of-vocabulary words assessed on two representations of extraneous speech inputs. First, we consider a garbage template to match the non keywords, named the ergodic 7-garbages model, which is directly built from the context independent (CI) phoneme models used to represent the keywords. In a second step, we do not attempt to explicitly model the extraneous inputs : this method refers to the ergodic best-fit model. The main motivation of using these two garbage models is that they do not require any specific training. We report results on rejection performance for speaker independent isolated words recognition task (perplexity 54) with standard discrete density HMM and hybrid HMM/MLP based recognizers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-506"
  },
  "salavedra95b_eurospeech": {
   "authors": [
    [
     "Joan",
     "Salavedra"
    ],
    [
     "Claus",
     "Jacobsen"
    ],
    [
     "Mazin G.",
     "Rahim"
    ],
    [
     "Ilija",
     "Zeljkovic"
    ],
    [
     "Jay G.",
     "Wilpon"
    ]
   ],
   "title": "Multi-lingual connected digits recognition",
   "original": "e95_2119",
   "page_count": 4,
   "order": 507,
   "p1": "2119",
   "pn": "2122",
   "abstract": [
    "This paper addresses the problem of robust multilingual speaker-independent connected digit recognition over telephone lines. We test and compare various techniques for signal conditioning, improved model representation and model training across three different languages, namely Danish, English and Spanish, and evaluate their language dependency. Word errors rates between 0.5% and 1.0% have been achieved for all languages on various speech databases collected over the public telephone network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-507"
  },
  "torremunilla95b_eurospeech": {
   "authors": [
    [
     "Celinda de la",
     "Torre-Munilla"
    ],
    [
     "Lúis",
     "Hernandez-Gomez"
    ],
    [
     "F. Javier",
     "Caminero-Gil"
    ],
    [
     "Cesar",
     "Martin del Álamo"
    ]
   ],
   "title": "Recognition of spontaneously spoken connected numbers in Spanish over the telephone line",
   "original": "e95_2123",
   "page_count": 4,
   "order": 508,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "In this paper, we describe the results obtained with a pioneer application for spontaneously spoken Connected-Number recognition in Castilian Spanish over the telephone line. The results of applying well known techniques in other recognition tasks to our system showed improvements of nearly 68% (including N-Best techniques) comparing with our baseline SCHMM system, that represents a final Word Error Rate of 1.5%, which convert the system in a feasible one to be portable for commercial applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-508"
  },
  "meliani95_eurospeech": {
   "authors": [
    [
     "R. El",
     "Meliani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Lexical fillers for task-independent-training based keyword spotting and detection of new words",
   "original": "e95_2129",
   "page_count": 4,
   "order": 509,
   "p1": "2129",
   "pn": "2132",
   "abstract": [
    "In this paper we describe a preliminary investigation of the use of fillers at the lexical level rather than the modelling level of a hidden Markov model-based keyword spotter and a detector of new words. In our last system, keywords and out-of-vocabulary speech shared the same context-dependent phoneme models with no explicit modelling of extraneous speech. Thus a task-independent training is performed for all models while the scoring method uses a two-pass Viterbi-type algorithm based on a lexical tree constructed with transcriptions of keywords and fillers using the same set of 40 English phonemes. The distinction between keywords and extraneous speech is performed during the search by using the lexical tree and language models. Thus using a simple method, we perform a faster training and allow easier modifications for the word-spotting task. On the other hand this kind of architecture allows our system to be used for both keyword spotting and new word detection tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-509"
  },
  "carey95b_eurospeech": {
   "authors": [
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Eluned S.",
     "Parris"
    ]
   ],
   "title": "Topic spotting with task independent models",
   "original": "e95_2133",
   "page_count": 4,
   "order": 510,
   "p1": "2133",
   "pn": "2136",
   "abstract": [
    "Topic spotting with whole-word models has been shown to give high detection rate with low false alarms. However the system must be capable of the generation of keyword models without repeated data collection sessions to be flexible in use. Since the required vocabulary is unknown a priori the models must be task independent. This however degrades the system performance which then must be restored. This can be achieved by using linear discriminant analysis in the feature extractor and the generation of context dependent subword models using decision trees. The system uses concatenations of the context dependent models to form the keyword models. Keywords are selected according to their usefulness. Non-keyword speech is modelled by a set of monophone models. During topic spotting the significance of the occurrence of keywords is weighted according to the discrimination they provide between topic and non-topic material. The system was tested on the BBC database spotting two minute weather forecasts. It detected 95% of the weather forecasts at a rate of one false alarm per hour. It was also tested on three other topics where its performance was not as good but still useful. Keywords: topic spotting, word-spotting, context dependent models, linear discriminant analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-510"
  },
  "hanazawa95_eurospeech": {
   "authors": [
    [
     "Toshiyuki",
     "Hanazawa"
    ],
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Phrase spotting using pitch pattern information",
   "original": "e95_2137",
   "page_count": 4,
   "order": 511,
   "p1": "2137",
   "pn": "2140",
   "abstract": [
    "This paper proposes a new method for spotting phrases in continuous speech. The phrase spotting is done based on the weighted sum of an acoustic likelihood and a prosodic phrase boundary likelihood. The prosodic phrase boundary likelihood is calculated statistically using pitch pattern HMMs. Introducing the prosodic phrase boundary likelihood is considered to have an effect to suppress the false alarms, because if the start and end points of the phrase candidate are different from the correct phrase boundaries the spotting score is lowered. The method was evaluated by Japanese phrase spotting experiments. The results show that the phrase boundary likelihood is calculated reasonably, and the phrase detection rate is improved using the prosodic phrase boundary likelihood.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-511"
  },
  "itoh95b_eurospeech": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Jiro",
     "Kiyama"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Speech understanding and speech retrieval for TV news by using connected word spotting",
   "original": "e95_2141",
   "page_count": 4,
   "order": 512,
   "p1": "2141",
   "pn": "2144",
   "abstract": [
    "The spotting-based method is thought to be an effective fundamental techniques for understanding the natural chain of human speech, so we applied it to two real-world tasks. The first is to automatically extract the intent of speakers in various contexts such as meetings and unstructured private conversations. The second task is to retrieve speech data. It is difficult to deal with these two tasks by using simple word spotting. We believe connected word spotting, also called concept spotting, is a more effective method. This paper describes a series of preliminary experiments of keyword spotting using TV news and talk programs as raw material. We explain the necessity and the methodology of connected word spotting. The results of the experiments show the effectiveness of connected word spotting.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-512"
  },
  "foote95_eurospeech": {
   "authors": [
    [
     "J. T.",
     "Foote"
    ],
    [
     "G. J. F.",
     "Jones"
    ],
    [
     "K. Sparck",
     "Jones"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Talker-independent keyword spotting for information retrieval",
   "original": "e95_2145",
   "page_count": 4,
   "order": 513,
   "p1": "2145",
   "pn": "2148",
   "abstract": [
    "The goal of the Video Mail Retrieval (VMR) project is to integrate state-of-the-art information retrieval (IR) methods with high-accuracy word spotting to yield a robust and efficient multimedia retrieval system. This paper concerns op en-talker and arbitrary-keyword retrieval based on talker-independent subword models. Because talker-independent subword models can not be expected to work as well as the talker-dependent whole-keyword models used in previous VMR experiments, speaker adaptation is investigated as a means of improving performance (especially for talkers with non-British accents). Both standard FOM word spotting measures and actual retrieval results are computed. The results show that the FOM is not necessarily a good indicator of retrieval performance, and that talker adaptation can substantially improve both spotting and retrieval results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-513"
  },
  "jeanrenaud95_eurospeech": {
   "authors": [
    [
     "P.",
     "Jeanrenaud"
    ],
    [
     "M.",
     "Siu"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "Large vocabulary word scoring as a basis for transcription generation",
   "original": "e95_2149",
   "page_count": 4,
   "order": 514,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "Large Vocabulary Word Scoring (LVWS) integrates word spotting and Large Vocabulary Continuous Speech Recognition (LVCSR) to generate scores and times for a complete lexicon of several thousand words. We first discuss the main characteristic of LVWS and consider two implementations, one based on posterior probability scoring, the other on the n-best lists. Then we address the issue of how to map the scores to more meaningful confidence scores. We propose two solutions and present experimental results on the Switchboard corpus where we associate confidence scores to the words on the recognition output. Finally we discuss how to generate transcriptions directly from the LVWS output. We propose a simple approach and compare its performance to the performance of the LVCSR system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-514"
  },
  "murakami95b_eurospeech": {
   "authors": [
    [
     "Jin'ichi",
     "Murakami"
    ]
   ],
   "title": "New word spotting algorithm based on forward decoding",
   "original": "e95_2153",
   "page_count": 2,
   "order": 515,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "In this paper, we describe a new word spotting algorithm based on forward decoding. In past studies, continuous dynamic programming[1] had been popular for word spotting. This algorithm which uses dynamic programming is considered to have a free start point and end point, and is controlled by some criterion; determining this criterion is very difficult. The proposed decoding algorithm basically performs comparison for the score of the last state of any word HMM. Therefore, it is expected to obtain a higher performance than continuous dynamic programming. Also we describe the problem of Viterbi decoding and explain that this algorithm may resolve the above problem.\n",
    ""
   ]
  },
  "klemm95_eurospeech": {
   "authors": [
    [
     "H.",
     "Klemm"
    ],
    [
     "Fritz",
     "Class"
    ],
    [
     "Ute",
     "Kilian"
    ]
   ],
   "title": "Word- and phrase spotting with syllable-based garbage modelling",
   "original": "e95_2157",
   "page_count": 4,
   "order": 516,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "Word spotting is an appropriate approach to cover real user behaviour [2], because users often embed target words or phrases in longer utterances than the recognition system expects. In our approach we use a standard Hidden-Markov continuous speech recognition system (CSR) with language model [9] for word spotting. This word spotting approach does not require any additional HMM-Training for garbage models, because all kinds of garbage models are a concatenation of the standard HMM sub-word-units of the system. As we want to use our word spotting system as an open input, we principally do not constrain the algorithm to only one keyword or phrase per utterance. It is also shown that there is a soft transition between word spotting, phrase spotting and a standard CSR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-515"
  },
  "liu95b_eurospeech": {
   "authors": [
    [
     "Li",
     "Liu"
    ],
    [
     "Jialong",
     "He"
    ],
    [
     "Günther",
     "Palm"
    ]
   ],
   "title": "Influence of short-time phase on the perception of stop consonants",
   "original": "e95_2269",
   "page_count": 4,
   "order": 517,
   "p1": "2269",
   "pn": "2272",
   "abstract": [
    "With the help of spectrograms the importance of spectral magnitude in speech signal has been highlighted in many classical studies. The other half of the spectral information, the spectral phase, is ignored following the assumption that phase is relatively less or unimportant in speech perception. In this research, an identification experiment was designed to explore the role of phase in the perception of intervocalic stop consonants in vowel-consonant-vowel (VCV) utterances. We employed an iterative algorithm to manipulate phase spectra in the hope that we can construct phonetically different utterances from the same short-time Fourier magnitude spectrum. It was shown that even for a very small analysis window, say 20 msec in length, nearly 16% of the intervocalic stop consonants were perceived as designated consonants other than the consonants from which the magnitude spectra came.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-516"
  },
  "simpson95_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Simpson"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Enhancing the perceptual salience of information-rich regions of natural intervocalic consonants",
   "original": "e95_2273",
   "page_count": 4,
   "order": 518,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "Two experiments were carried out to evaluate the perceptual effect of various enhancements made to information-rich regions of intervocalic consonants. The first tested the benefits of altering the relative amplitude of such regions, whilst the second additionally employed filtering to produce stylised cues to phonetic contrasts. After such manipulations stimuli were combined with speech-shaped noise at 0 and -5dB SNR, and presented to normally-hearing listeners. All enhanced conditions led to statistically significant increases in intelligibility relative to the unenhanced condition. These results demonstrate the benefits of knowledge-based enhancement techniques which target acoustic cues to phonetic contrasts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-517"
  },
  "son95c_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ]
   ],
   "title": "A method to quantify the error distribution in confusion matrices",
   "original": "e95_2277",
   "page_count": 4,
   "order": 519,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "Currently the error-rate is the most widely used general measure of performance for identification experiments. However, the error-rate gives no information about the distribution of errors over the available response categories. Starting from the definition of perplexity of the observations in a confusion matrix, a new measure is introduced, the error-dispersion, that is normalized with respect to error-rate. The error-dispersion can be interpreted as the effective number of error categories per stimulus or response. Furthermore, a technique is introduced to estimate the difference between confusion matrices as a fraction of unique observations or errors. With examples from the literature, it is shown that error-dispersion can point out similarities in cases where error-rate varies, and can point out differences when error-rates are similar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-518"
  },
  "lopezbascuas95_eurospeech": {
   "authors": [
    [
     "Luis E.",
     "Lopez-Bascuas"
    ]
   ],
   "title": "Speech and nonspeech signal densities for the perception of temporal order",
   "original": "e95_2281",
   "page_count": 3,
   "order": 520,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "Identification ratings were taken from English monolingual subjects on two different acoustic continua: a VOT (voice onset time) speech continuum and a TOT (tone onset time) nonspeech continuum. Both continua were made by varying the temporal asynchrony between two component acoustic events. An improved signal detection model was employed for fitting the results. The model could only fit the nonspeech data. Further analyses suggested that the differences between speech and nonspeech processing in humans could lie on the form of the signal densities they evoke. Nonspeech signal densities seem to be gaussian whereas speech signal densities do not.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-519"
  },
  "samarta95_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Sa Marta"
    ],
    [
     "Fernando",
     "Perdigao"
    ],
    [
     "Luis",
     "Vieira de Sa"
    ]
   ],
   "title": "Researching the processing structures of human phoneme recognition by analysis of natural stop-consonant-vowel utterances that elicit correct recognition through unusual acoustic patterns",
   "original": "e95_2285",
   "page_count": 4,
   "order": 521,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "One of the main ultimate objectives of speech perception research is the description, on a neurophysiological basis, of the mechanisms involved in human phoneme recognition. Direct probing is obviously out of the question, but some novel inspiration for hypotheses may be gleaned from human phonemic productions that elicit correct and robust identification in spite of acoustical patterns that blatantly infringe known (production and/or perception) \"rules\". To find a number of productions as such, the detailed analysis of a multinational database of 64 non-professional speakers was undertaken. Some productions showed to grossly violate the previously known \"approximate rule\" that states that in a stop-high vowel CV, a \"slightly ascending F2 transition cues dental place, whereas a markedly ascending transition cues labial place\". A biologically-plausible model of a processing structure ancillary to formant-transition perception is proposed that is compatible not only with these productions, but also with the more common productions that conform to the \"approximate rule\" mentioned. Furthermore, the model appears to explain why a slightly ascending transition (prevalent in dental-stop, high-F2 vowel CV's) may be perceptually similar to a markedly descending transition (prevalent in dental-stop, low-F2 vowel CV's).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-520"
  },
  "widdison95_eurospeech": {
   "authors": [
    [
     "Kirk A.",
     "Widdison"
    ]
   ],
   "title": "The perception of voicing in Spanish sibilants",
   "original": "e95_2289",
   "page_count": 3,
   "order": 522,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "This study explores the value of fricative duration in the perception of voicing in Spanish sibilants. Natural speech forms containing [s] in a vowel-consonant-vowel (VCV) context were recorded by a native speaker of Argentine Spanish. The middle portion of the frication noise of these samples was abbreviated in three steps over a 120 ms range by digital editing techniques. Results of a listening test in which the voicing feature was to be identified indicate that shortening the duration of fs] produced a significant shift in perception towards [z]. This perceptual effect concurs with acoustic data observed here and reported in the literature that reveal a marked separation in the durational range of these two sounds. The experiment demonstrates a direct method for assessing non-contrastive sounds in a language given listener awareness of salient regional speech habits. The interaction of fricative duration and the perception of voicing in sibilants evidenced here is consistent with the patterning of these sounds in similar historical and synchronic data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-521"
  },
  "slawinski95_eurospeech": {
   "authors": [
    [
     "Elzbieta B.",
     "Slawinski"
    ]
   ],
   "title": "Development of the perception of initial prevocalic [r] and [l] by English children",
   "original": "e95_2293",
   "page_count": 4,
   "order": 523,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "Development of the incorporation of spectral and temporal acoustical features in the perception of the phonemic contrast between [r] and [1] sounds in the initial prevocalic position was examined for 3 to 6 years old English children. The integration of those features was evaluated in oddity discrimination tasks. It was found that the ability to integrate acoustical cues improves significantly with age. Thus, it may be suggested that the attainment of perceptual distinction between phonemic contrasts (at least for [r] and [1]) is a gradual and progressive development in the proficiency of categorical perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-522"
  },
  "cutugno95_eurospeech": {
   "authors": [
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Renata",
     "Savy"
    ]
   ],
   "title": "On phonetic boundaries across categories for synthetic and natural vocalic speech sounds",
   "original": "e95_2297",
   "page_count": 4,
   "order": 524,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "Aim of the present study was to give an optimal definition of the boundary between vowel categories in terms of perceptual and acoustic parameters. To reach this target we performed a comparison between the results of vowel identification tests performed with synthetic speech sounds and tests with vowel stimuli segmented from natural connected speech. Two experiments aiming at the definition of vowel boundaries will be illustrated. In both experiments our attention is directed to that portion of the F1/F2 diagram which defines the area of existence of Italian vowels [a, E, e, i]. The results of this work lead us to wonder on the opportunity of looking for fixed boundaries, weather they are acoustical or perceptual. Our conclusions rather seems to confirm the presence of a great adptivity in the perceptual listener's capacity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-523"
  },
  "protopapas95_eurospeech": {
   "authors": [
    [
     "Athanassios",
     "Protopapas"
    ],
    [
     "Steven",
     "Finney"
    ],
    [
     "Peter D.",
     "Eimas"
    ]
   ],
   "title": "Effects of syllabic position in the perception of spoken English",
   "original": "e95_2301",
   "page_count": 4,
   "order": 525,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "We report a series of experiments that demonstrate an effect of syllabic position on reaction time during phoneme monitoring in English. In our stimuli, the third phoneme of each word was the target phoneme, and it belonged either to the coda of the first syllable (coda-type target) or to the onset of the second syllable (onset-type target). By manipulating the proportions of the target types, reaction times to targets of the expected syllabic structure were significantly affected. This effect was only present when words stressed on their second syllable were used; words with first-syllable stress yielded no effect of expectation. The difference may be attributed to the strongly ambisyllabic nature of consonants following stressed vowels. The results are consistent with the idea that prelexical syllabic segmentation always occurs, but is only evident when the syllabic boundaries of the materials are unambiguous. These and previous findings are discussed from a cross-linguistic perspective.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1995-524"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "stevens95_eurospeech",
    "ainsworth95_eurospeech",
    "bourlard95_eurospeech",
    "furui95_eurospeech"
   ]
  },
  {
   "title": "Prosody Modelling in ASR",
   "papers": [
    "jitsuhiro95_eurospeech",
    "takagi95_eurospeech",
    "kondo95_eurospeech",
    "barras95_eurospeech",
    "hirose95_eurospeech"
   ]
  },
  {
   "title": "Wideband Coding",
   "papers": [
    "murgia95_eurospeech",
    "sasaki95_eurospeech",
    "black95_eurospeech",
    "abreusernandez95_eurospeech",
    "chonavel95_eurospeech"
   ]
  },
  {
   "title": "Hardware and Systems for Speech Processing",
   "papers": [
    "rohwer95_eurospeech",
    "ciria95_eurospeech",
    "ortizbalbuena95_eurospeech",
    "leandro95_eurospeech",
    "cochard95_eurospeech"
   ]
  },
  {
   "title": "Discriminative Training I, II",
   "papers": [
    "matsui95_eurospeech",
    "nogueirasrodriguez95_eurospeech",
    "leprieur95_eurospeech",
    "martindelalamo95_eurospeech",
    "na95_eurospeech",
    "huo95_eurospeech",
    "hernando95_eurospeech",
    "rahim95_eurospeech",
    "peinado95_eurospeech",
    "reichl95_eurospeech",
    "paliwal95_eurospeech"
   ]
  },
  {
   "title": "Auditory Modeling in Speech Recognition",
   "papers": [
    "euler95_eurospeech",
    "morris95_eurospeech",
    "fragniere95_eurospeech",
    "jones95_eurospeech",
    "bodden95_eurospeech",
    "dobrin95_eurospeech"
   ]
  },
  {
   "title": "Applications and Systems",
   "papers": [
    "berthommier95_eurospeech",
    "davidek95_eurospeech",
    "coile95_eurospeech",
    "pacifici95_eurospeech",
    "calonge95_eurospeech",
    "li95_eurospeech",
    "bergmann95_eurospeech",
    "avendano95_eurospeech"
   ]
  },
  {
   "title": "Large Vocabulary",
   "papers": [
    "pye95_eurospeech",
    "lamel95_eurospeech",
    "barnett95_eurospeech",
    "brousseau95_eurospeech",
    "dugast95_eurospeech",
    "lin95_eurospeech",
    "riley95_eurospeech",
    "ho95_eurospeech",
    "wang95_eurospeech",
    "cook95_eurospeech"
   ]
  },
  {
   "title": "Speech Coding I",
   "papers": [
    "eriksson95_eurospeech",
    "festa95_eurospeech",
    "tsoukalas95_eurospeech",
    "piazzo95_eurospeech",
    "atkinson95_eurospeech",
    "stefanoiu95_eurospeech",
    "ancin95_eurospeech",
    "mandridake95_eurospeech",
    "popescu95_eurospeech",
    "parris95_eurospeech",
    "torresguijarro95_eurospeech",
    "yu95_eurospeech",
    "wery95_eurospeech",
    "cheetham95_eurospeech"
   ]
  },
  {
   "title": "Speech Signal Processing / Wavelets",
   "papers": [
    "cohen95_eurospeech",
    "micallef95_eurospeech",
    "drygajlo95_eurospeech",
    "rangoussi95_eurospeech",
    "saleh95_eurospeech"
   ]
  },
  {
   "title": "Applications of Speech Technology",
   "papers": [
    "ortel95_eurospeech",
    "asadi95_eurospeech",
    "ludovik95_eurospeech",
    "lokenkim95_eurospeech",
    "chen95_eurospeech"
   ]
  },
  {
   "title": "Visual Speech",
   "papers": [
    "goff95_eurospeech",
    "fusterduran95_eurospeech",
    "beskow95_eurospeech",
    "lavagetto95_eurospeech",
    "kabre95_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition I-III",
   "papers": [
    "he95_eurospeech",
    "ng95_eurospeech",
    "carey95_eurospeech",
    "ng95b_eurospeech",
    "federico95_eurospeech",
    "altosaar95_eurospeech",
    "magrinchagnolleau95_eurospeech",
    "labulin95_eurospeech",
    "homayounpour95_eurospeech",
    "berkling95_eurospeech",
    "olsen95_eurospeech",
    "bonifas95_eurospeech",
    "li95b_eurospeech",
    "gong95_eurospeech",
    "falavigna95_eurospeech",
    "sheikhzadegan95_eurospeech",
    "floch95_eurospeech",
    "wagner95_eurospeech",
    "li95c_eurospeech",
    "matsui95b_eurospeech",
    "che95_eurospeech",
    "yu95b_eurospeech",
    "artieres95_eurospeech"
   ]
  },
  {
   "title": "Voice Source Analysis and Modelling",
   "papers": [
    "beritelli95_eurospeech",
    "darsinos95_eurospeech",
    "rouat95_eurospeech",
    "janer95_eurospeech",
    "navarromesa95_eurospeech",
    "larreategui95_eurospeech",
    "darsinos95b_eurospeech",
    "pfitzinger95_eurospeech",
    "ohno95_eurospeech"
   ]
  },
  {
   "title": "Voice Personality Characteristics in TTS",
   "papers": [
    "lee95_eurospeech",
    "hashimoto95_eurospeech",
    "higuchi95_eurospeech",
    "akagi95_eurospeech",
    "lam95_eurospeech",
    "stylianou95_eurospeech",
    "stylianou95b_eurospeech",
    "boughazah95_eurospeech"
   ]
  },
  {
   "title": "Robust Speech Recognition in Noise",
   "papers": [
    "yoma95_eurospeech",
    "siohan95_eurospeech",
    "yang95_eurospeech",
    "drygajlo95b_eurospeech",
    "yu95c_eurospeech",
    "moreno95_eurospeech"
   ]
  },
  {
   "title": "Modelling and Training for Robust Recognition",
   "papers": [
    "singer95_eurospeech",
    "mirghafori95_eurospeech",
    "chou95_eurospeech",
    "matsumura95_eurospeech",
    "sankar95_eurospeech",
    "huo95b_eurospeech",
    "beppu95_eurospeech",
    "afify95_eurospeech",
    "milner95_eurospeech",
    "rivlin95_eurospeech"
   ]
  },
  {
   "title": "Semantic Interpretation, ELSNET",
   "papers": [
    "biem95_eurospeech",
    "stahl95_eurospeech",
    "levin95_eurospeech",
    "bonafonte95_eurospeech",
    "federico95b_eurospeech",
    "bauer95_eurospeech"
   ]
  },
  {
   "title": "Data based Text-to-Speech",
   "papers": [
    "donovan95_eurospeech",
    "itoh95_eurospeech",
    "black95b_eurospeech",
    "lopezgonzalo95_eurospeech",
    "mana95_eurospeech"
   ]
  },
  {
   "title": "Prosody in Text-to-Speech",
   "papers": [
    "breen95_eurospeech",
    "riedi95_eurospeech",
    "beaugendre95_eurospeech",
    "tzoukermann95_eurospeech",
    "home95_eurospeech"
   ]
  },
  {
   "title": "Speaker Verification",
   "papers": [
    "setlur95_eurospeech",
    "liou95_eurospeech",
    "gauvain95_eurospeech"
   ]
  },
  {
   "title": "CELP Coding",
   "papers": [
    "morenoperez95_eurospeech",
    "atungsiri95_eurospeech",
    "chui95_eurospeech",
    "kang95_eurospeech",
    "cucchi95_eurospeech",
    "bouraoui95_eurospeech",
    "fingscheidt95_eurospeech",
    "watson95_eurospeech",
    "popescu95b_eurospeech"
   ]
  },
  {
   "title": "Formant Analysis",
   "papers": [
    "schmid95_eurospeech",
    "plante95_eurospeech",
    "schoentgen95_eurospeech",
    "sun95_eurospeech",
    "pan95_eurospeech",
    "tokuda95_eurospeech",
    "richards95_eurospeech"
   ]
  },
  {
   "title": "Acoustic-Phonetic Modelling",
   "papers": [
    "aubert95_eurospeech",
    "morgan95_eurospeech",
    "sitaram95_eurospeech",
    "chang95_eurospeech",
    "svendsen95_eurospeech",
    "alvarezcercadillo95_eurospeech",
    "okawa95_eurospeech",
    "li95d_eurospeech",
    "fissore95_eurospeech",
    "dumouchel95_eurospeech",
    "ghio95_eurospeech"
   ]
  },
  {
   "title": "Spoken Language Resources I",
   "papers": [
    "jan95_eurospeech",
    "lander95_eurospeech",
    "cole95_eurospeech",
    "os95_eurospeech",
    "bertenstam95_eurospeech",
    "plante95b_eurospeech",
    "winski95_eurospeech",
    "torremunilla95_eurospeech",
    "lopezdeipina95_eurospeech",
    "pirrelli95_eurospeech",
    "misheva95_eurospeech",
    "hess95_eurospeech",
    "chan95_eurospeech",
    "fink95_eurospeech",
    "wang95b_eurospeech"
   ]
  },
  {
   "title": "Search Methods I-II",
   "papers": [
    "morin95_eurospeech",
    "ortmanns95_eurospeech",
    "wang95c_eurospeech",
    "waast95_eurospeech",
    "noda95_eurospeech",
    "colton95_eurospeech",
    "jimenez95_eurospeech",
    "takeda95_eurospeech",
    "seide95_eurospeech",
    "beyerlein95_eurospeech",
    "komori95_eurospeech",
    "fritsch95_eurospeech"
   ]
  },
  {
   "title": "Analysis for Speech Recognition I-III",
   "papers": [
    "nouza95_eurospeech",
    "nadeu95_eurospeech",
    "tambakas95_eurospeech",
    "hunt95_eurospeech",
    "hubener95_eurospeech",
    "freitag95_eurospeech",
    "rao95_eurospeech",
    "nadeu95b_eurospeech",
    "junqua95_eurospeech",
    "djezzar95_eurospeech",
    "rao95b_eurospeech",
    "vorstermanst95_eurospeech",
    "ramsay95_eurospeech",
    "sirigos95_eurospeech",
    "bitar95_eurospeech",
    "beulen95_eurospeech",
    "westendorf95_eurospeech",
    "navarromesa95b_eurospeech",
    "sarukkai95_eurospeech",
    "dobrisek95_eurospeech"
   ]
  },
  {
   "title": "Perception/Pitch",
   "papers": [
    "sluijter95_eurospeech",
    "owsianny95_eurospeech",
    "house95_eurospeech",
    "takara95_eurospeech",
    "dalessandro95_eurospeech"
   ]
  },
  {
   "title": "Perception/Context",
   "papers": [
    "wauquiergravelines95_eurospeech",
    "son95_eurospeech",
    "ainsworth95b_eurospeech",
    "bonneau95_eurospeech",
    "donselaar95_eurospeech"
   ]
  },
  {
   "title": "Prosody: Models/Modelling Prosody",
   "papers": [
    "fujisaki95_eurospeech",
    "nicolas95_eurospeech",
    "ross95_eurospeech",
    "hunt95b_eurospeech",
    "caspers95_eurospeech"
   ]
  },
  {
   "title": "Prosody I-III",
   "papers": [
    "sakata95_eurospeech",
    "campbell95_eurospeech",
    "petek95_eurospeech",
    "mohler95_eurospeech",
    "dalykelly95_eurospeech",
    "yamashita95_eurospeech",
    "kompe95_eurospeech",
    "harbeck95_eurospeech",
    "hirai95_eurospeech",
    "taylor95_eurospeech",
    "strik95_eurospeech",
    "abe95_eurospeech",
    "caminerogil95_eurospeech",
    "strom95_eurospeech",
    "morlec95_eurospeech",
    "fach95_eurospeech",
    "hermes95_eurospeech",
    "langlais95_eurospeech"
   ]
  },
  {
   "title": "Quantization of Spectral Parameters",
   "papers": [
    "paliwal95b_eurospeech",
    "choi95_eurospeech",
    "baghairavary95_eurospeech",
    "kovesi95_eurospeech",
    "mohammadi95_eurospeech",
    "chang95b_eurospeech",
    "bruhn95_eurospeech",
    "balss95_eurospeech",
    "ng95c_eurospeech"
   ]
  },
  {
   "title": "Systems and Evaluation",
   "papers": [
    "arriola95_eurospeech",
    "dymarski95_eurospeech",
    "jurgens95_eurospeech",
    "delogu95_eurospeech",
    "williams95_eurospeech",
    "andersen95_eurospeech"
   ]
  },
  {
   "title": "Adaption in Speech Recognition",
   "papers": [
    "beattie95_eurospeech",
    "neumeyer95_eurospeech",
    "zavaliagkos95_eurospeech",
    "matsunaga95_eurospeech",
    "dobler95_eurospeech",
    "shinoda95_eurospeech",
    "tonomura95_eurospeech",
    "choi95b_eurospeech",
    "leggetter95_eurospeech"
   ]
  },
  {
   "title": "Spoken Dialogue",
   "papers": [
    "kaspar95_eurospeech",
    "watanuki95_eurospeech",
    "bruce95_eurospeech",
    "zajicek95_eurospeech",
    "niimi95_eurospeech",
    "baekgaard95_eurospeech",
    "kleckova95_eurospeech",
    "okane95_eurospeech",
    "brison95_eurospeech",
    "lee95b_eurospeech"
   ]
  },
  {
   "title": "Miscellaneous on Language Processing",
   "papers": [
    "lin95b_eurospeech",
    "delogu95b_eurospeech",
    "kosarev95_eurospeech",
    "stevenur95_eurospeech",
    "gamback95_eurospeech",
    "niklfeld95_eurospeech",
    "koo95_eurospeech",
    "vilar95_eurospeech",
    "novick95_eurospeech"
   ]
  },
  {
   "title": "Language Modelling I, II",
   "papers": [
    "kilian95_eurospeech",
    "generet95_eurospeech",
    "moisa95_eurospeech",
    "martin95_eurospeech",
    "ueberla95_eurospeech",
    "naeve95_eurospeech",
    "flach95_eurospeech",
    "cremelie95_eurospeech",
    "murakami95_eurospeech",
    "besling95_eurospeech",
    "gros95_eurospeech",
    "rosenfeld95_eurospeech",
    "spies95_eurospeech"
   ]
  },
  {
   "title": "Benchmarking and Assessment I-II",
   "papers": [
    "muller95_eurospeech",
    "nagabuchi95_eurospeech",
    "hone95_eurospeech",
    "belhoula95_eurospeech",
    "haebumbach95_eurospeech",
    "bernsen95_eurospeech",
    "leeuwen95_eurospeech",
    "simons95_eurospeech",
    "satoshi95_eurospeech",
    "klaus95_eurospeech",
    "steeneken95_eurospeech",
    "bartkova95_eurospeech",
    "verheijen95_eurospeech"
   ]
  },
  {
   "title": "Production/Biomechanics",
   "papers": [
    "vaxelaire95_eurospeech",
    "laboissiere95_eurospeech",
    "bouabana95_eurospeech",
    "richard95_eurospeech",
    "suzuki95_eurospeech"
   ]
  },
  {
   "title": "Phonetics and Phonology I",
   "papers": [
    "prieto95_eurospeech",
    "kondo95b_eurospeech",
    "goedemans95_eurospeech",
    "bergem95_eurospeech",
    "ohala95_eurospeech",
    "fitt95_eurospeech",
    "gustafson95_eurospeech",
    "demolin95_eurospeech",
    "sole95_eurospeech",
    "deligne95_eurospeech",
    "tajchman95_eurospeech",
    "aguilar95_eurospeech",
    "farnetani95_eurospeech",
    "gillis95_eurospeech",
    "kotani95_eurospeech"
   ]
  },
  {
   "title": "Language Identification",
   "papers": [
    "yan95_eurospeech",
    "nowell95_eurospeech",
    "shuichi95_eurospeech",
    "lund95_eurospeech",
    "kwan95_eurospeech"
   ]
  },
  {
   "title": "Pattern Recognition and HMM's",
   "papers": [
    "varona95_eurospeech",
    "mingy95_eurospeech",
    "moudenc95_eurospeech",
    "welling95_eurospeech",
    "falkhausen95_eurospeech",
    "shen95_eurospeech",
    "castro95_eurospeech",
    "bocchieri95_eurospeech",
    "farhat95_eurospeech",
    "ferreiros95_eurospeech"
   ]
  },
  {
   "title": "Compensations for Unknown Channels",
   "papers": [
    "doblinger95_eurospeech",
    "salavedra95_eurospeech",
    "paliwal95c_eurospeech",
    "faucon95_eurospeech",
    "chang95c_eurospeech",
    "potamianos95_eurospeech",
    "cordoba95_eurospeech",
    "chien95_eurospeech",
    "yasukawa95_eurospeech",
    "handel95_eurospeech"
   ]
  },
  {
   "title": "Overcoming the Effects of Noise and Variability",
   "papers": [
    "hu95_eurospeech",
    "girin95_eurospeech",
    "adjoudani95_eurospeech",
    "liu95_eurospeech",
    "charonnat95_eurospeech",
    "sovka95_eurospeech",
    "sokol95_eurospeech",
    "shamsoddini95_eurospeech",
    "zhao95_eurospeech"
   ]
  },
  {
   "title": "Modelling Segments and Units",
   "papers": [
    "bonafonte95b_eurospeech",
    "holmes95_eurospeech",
    "yamamoto95_eurospeech",
    "matsunaga95b_eurospeech",
    "blackburn95_eurospeech"
   ]
  },
  {
   "title": "Out of Vocabulary Recognition",
   "papers": [
    "sukkar95_eurospeech",
    "torrecilla95_eurospeech",
    "fetter95_eurospeech",
    "nakamura95_eurospeech",
    "hetherington95_eurospeech"
   ]
  },
  {
   "title": "Neural Networks I-IV",
   "papers": [
    "baldwin95_eurospeech",
    "glaeser95_eurospeech",
    "rigoll95_eurospeech",
    "bourlard95b_eurospeech",
    "lee95c_eurospeech",
    "castano95_eurospeech",
    "yu95d_eurospeech",
    "franco95_eurospeech",
    "buniet95_eurospeech",
    "menendezpidal95_eurospeech",
    "reichl95b_eurospeech",
    "rigoll95b_eurospeech",
    "neto95_eurospeech",
    "puzrla95_eurospeech",
    "anglade95_eurospeech",
    "abrash95_eurospeech",
    "galindo95_eurospeech",
    "paping95_eurospeech",
    "rubio95_eurospeech",
    "vicsi95_eurospeech",
    "ng95d_eurospeech",
    "abberley95_eurospeech",
    "na95b_eurospeech",
    "djezzar95b_eurospeech",
    "gomez95_eurospeech"
   ]
  },
  {
   "title": "Human Word Recognition",
   "papers": [
    "jongenburger95_eurospeech",
    "gelder95_eurospeech",
    "otake95_eurospeech",
    "mcqueen95_eurospeech",
    "vroomen95_eurospeech"
   ]
  },
  {
   "title": "Pathology and Communications I-II",
   "papers": [
    "vieira95_eurospeech",
    "udayashankara95_eurospeech",
    "rosso95_eurospeech",
    "wrench95_eurospeech",
    "teston95_eurospeech",
    "nemeth95_eurospeech",
    "michaelis95_eurospeech",
    "ortiz95_eurospeech",
    "seiyama95_eurospeech",
    "zhang95_eurospeech",
    "arai95_eurospeech",
    "marasek95_eurospeech",
    "schoentgen95b_eurospeech"
   ]
  },
  {
   "title": "Language Modelling III and Syntactic Analysis",
   "papers": [
    "schukattalamazzini95_eurospeech",
    "staab95_eurospeech",
    "seneff95_eurospeech",
    "koppelman95_eurospeech",
    "jones95b_eurospeech",
    "rayner95_eurospeech",
    "brugnara95_eurospeech",
    "kenne95_eurospeech"
   ]
  },
  {
   "title": "Linguistics Aspects of TTS",
   "papers": [
    "shih95_eurospeech",
    "sanders95_eurospeech",
    "tatham95_eurospeech",
    "morton95_eurospeech",
    "mixdorff95_eurospeech",
    "ishikawa95_eurospeech",
    "kerkhoff95_eurospeech",
    "vincent95_eurospeech"
   ]
  },
  {
   "title": "Articulatory Modelling in Synthesis",
   "papers": [
    "wei95_eurospeech",
    "yarrington95_eurospeech",
    "greenwood95_eurospeech",
    "veldhuis95_eurospeech",
    "bavegard95_eurospeech",
    "katae95_eurospeech"
   ]
  },
  {
   "title": "Production",
   "papers": [
    "yeou95_eurospeech",
    "parlangeau95_eurospeech",
    "son95b_eurospeech",
    "bailly95_eurospeech",
    "pitermann95_eurospeech",
    "sock95_eurospeech",
    "perrier95_eurospeech",
    "sampath95_eurospeech",
    "vescovi95_eurospeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems",
   "papers": [
    "springer95_eurospeech",
    "siroux95_eurospeech",
    "fraser95_eurospeech",
    "suzuki95b_eurospeech",
    "zeigler95_eurospeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems II (Development and Tools)",
   "papers": [
    "lamel95b_eurospeech",
    "flammia95_eurospeech",
    "nitta95_eurospeech",
    "larsen95_eurospeech",
    "hild95_eurospeech"
   ]
  },
  {
   "title": "Overview of Robust Speech Recognition Techniques",
   "papers": [
    "gales95_eurospeech",
    "mokbel95_eurospeech",
    "cao95_eurospeech",
    "vereecken95_eurospeech",
    "womack95_eurospeech"
   ]
  },
  {
   "title": "Speech Processing with Multiple Microphones",
   "papers": [
    "linhard95_eurospeech",
    "martin95b_eurospeech",
    "drews95_eurospeech",
    "cao95b_eurospeech",
    "giuliani95_eurospeech"
   ]
  },
  {
   "title": "Prosody: Models and Data",
   "papers": [
    "llisterri95_eurospeech",
    "auberge95_eurospeech",
    "czigler95_eurospeech",
    "vitez95_eurospeech",
    "heuft95_eurospeech",
    "tams95_eurospeech",
    "cabrera95_eurospeech",
    "lehning95_eurospeech"
   ]
  },
  {
   "title": "Out of Vocabulary and Connected Speech",
   "papers": [
    "kai95_eurospeech",
    "caminerogil95b_eurospeech",
    "sakamoto95_eurospeech",
    "jusek95_eurospeech",
    "lee95d_eurospeech",
    "accaino95_eurospeech",
    "salavedra95b_eurospeech",
    "torremunilla95b_eurospeech"
   ]
  },
  {
   "title": "Word Spotting",
   "papers": [
    "meliani95_eurospeech",
    "carey95b_eurospeech",
    "hanazawa95_eurospeech",
    "itoh95b_eurospeech",
    "foote95_eurospeech",
    "jeanrenaud95_eurospeech",
    "murakami95b_eurospeech",
    "klemm95_eurospeech"
   ]
  },
  {
   "title": "Perception",
   "papers": [
    "liu95b_eurospeech",
    "simpson95_eurospeech",
    "son95c_eurospeech",
    "lopezbascuas95_eurospeech",
    "samarta95_eurospeech",
    "widdison95_eurospeech",
    "slawinski95_eurospeech",
    "cutugno95_eurospeech",
    "protopapas95_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.1995"
}