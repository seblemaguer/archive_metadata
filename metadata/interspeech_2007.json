{
 "title": "Interspeech 2007",
 "location": "Antwerp, Belgium",
 "startDate": "27/8/2007",
 "endDate": "31/8/2007",
 "chair": "General Chairs: Dirk Van Compernolle, Lou Boves",
 "conf": "Interspeech",
 "year": "2007",
 "name": "interspeech_2007",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2007",
 "date": "27-31 August 2007",
 "booklet": "interspeech_2007.pdf",
 "papers": {
  "zue07_interspeech": {
   "authors": [
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "On organic interfaces",
   "original": "i07_0001",
   "page_count": 8,
   "order": 1,
   "p1": "1",
   "pn": "8",
   "abstract": [
    "For over four decades, our research community has taken remarkable strides in advancing human language technologies. This has resulted in the emergence of spoken dialogue interfaces that can communicate with humans on their own terms. For the most part, however, we have assumed that these interfaces are static; it knows what it knows and doesn't know what it doesn't. In my opinion, we are not likely to succeed until we can build interfaces that behave more like organisms that can learn, grow, reconfigure, and repair themselves, much like humans. In this paper, I will argue my case and outline some new research challenges.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-1"
  },
  "scott07_interspeech": {
   "authors": [
    [
     "Sophie K.",
     "Scott"
    ]
   ],
   "title": "The neural basis of speech perception - a view from functional imaging",
   "original": "i07_0009",
   "page_count": 5,
   "order": 2,
   "p1": "9",
   "pn": "13",
   "abstract": [
    "Functional imaging techniques, such as Positron Emission Tomography (PET), functional Magnetic Resonance Imaging (fMRI), have enabled neuroscientists to elaborate how the human brain solves the formidable problem of decoding the speech signal. In this paper I will outline the properties of primate auditory cortex, and use this as an anatomical framework to address the data from functional imaging studies of auditory processing and speech perception. I will outline how at least two different streams of processing can be seen in primary auditory cortex, and that this apparently maps onto two different ways in which the human brain processes speech. I will also address data suggesting that there are considerable hemispheric asymmetries in speech perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-2"
  },
  "waibel07_interspeech": {
   "authors": [
    [
     "Alex",
     "Waibel"
    ],
    [
     "Keni",
     "Bernardin"
    ],
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "Computer-supported human-human multilingual communication",
   "original": "i07_0014",
   "page_count": 8,
   "order": 3,
   "p1": "14",
   "pn": "21",
   "abstract": [
    "Computers have become an essential part of modern life, providing services in a multiplicity of ways. Access to these services, however, comes at a price: human attention is bound and directed toward a technical artifact in a human-machine interaction setting at the expense of time and attention for other humans. This paper explores a new class of computer services that support human- human interaction and communication\n",
    "implicitly and transparently. Computers in the Human Interaction Loop (CHIL), require consideration of all communication modalities, multimodal integration and more robust performance. We review the technologies and several CHIL services providing human-human support. Among them, we specifically highlight advanced computer services for cross-lingual communication.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-3"
  },
  "oudeyer07_interspeech": {
   "authors": [
    [
     "Pierre-Yves",
     "Oudeyer"
    ]
   ],
   "title": "Self-organization in the evolution of shared systems of speech sounds: a computational study",
   "original": "i07_0022",
   "page_count": 8,
   "order": 4,
   "p1": "22",
   "pn": "29",
   "abstract": [
    "How did culturally shared systems of combinatorial speech sounds initially appear in human evolution? This paper proposes the hypothesis that their bootstrapping may have happened rather easily if one assumes an individual capacity for vocal replication, and thanks to self-organization in the neural coupling of vocal modalities and in the coupling of babbling individuals. This hypothesis is embodied in agent-based computational experiments, that allow to show that crucial phenomena, including structural regularities and diversity of sound systems, can only be accounted if speech is considered as a complex adaptive system. Thus, the second objective of this paper is to show that integrative computational approaches, even if speculative in certain respects, might be key in the understanding of speech and its evolution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-4"
  },
  "li07_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Soft margin feature extraction for automatic speech recognition",
   "original": "i07_0030",
   "page_count": 4,
   "order": 5,
   "p1": "30",
   "pn": "33",
   "abstract": [
    "We propose a new discriminative learning framework, called soft margin feature extraction (SMFE), for jointly optimizing the parameters of transformation matrix for feature extraction and of hidden Markov models (HMMs) for acoustic modeling. SMFE extends our previous work of soft margin estimation (SME) to feature extraction. Tested on the TIDIGITS connected digit recognition task, the proposed approach achieves a string accuracy of 99.61%, much better than our previously reported SME results. To our knowledge, this is the first study on applying the margin-based method in joint optimization of feature extraction and acoustic modeling. The excellent performance of SMFE demonstrates the success of soft margin based method, which targets to obtain both high accuracy and good model generalization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-5"
  },
  "yin07_interspeech": {
   "authors": [
    [
     "Yan",
     "Yin"
    ],
    [
     "Hui",
     "Jiang"
    ]
   ],
   "title": "A fast optimization method for large margin estimation of HMMs based on second order cone programming",
   "original": "i07_0034",
   "page_count": 4,
   "order": 6,
   "p1": "34",
   "pn": "37",
   "abstract": [
    "In this paper, we present a new fast optimization method to solve large margin estimation (LME) of continuous density hidden Markov models (CDHMMs) for speech recognition based on second order cone programming (SOCP). SOCP is a class of nonlinear convex optimization problems which can be solved quite efficiently. In this work, we have proposed a new convex relaxation condition under which LME of CDHMMs can be formulated as an SOCP problem. The new LME/SOCP method has been evaluated in a connected digit string recognition task using the TIDIGITS database. Experimental results clearly demonstrate that the LME using SOCP outperforms the previous gradient descent method and can achieve comparable performance as our previously proposed semidefinite programming (SDP) approach. But the SOCP yields much better efficiency in terms of optimization time (about 20-200 times faster) and memory usage when compared with the SDP method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-6"
  },
  "li07b_interspeech": {
   "authors": [
    [
     "Hao-Zheng",
     "Li"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Frame margin probability discriminative training algorithm for noisy speech recognition",
   "original": "i07_0038",
   "page_count": 4,
   "order": 7,
   "p1": "38",
   "pn": "41",
   "abstract": [
    "This paper presents a novel discriminative training technique for noisy speech recognition. First, we define a Frame Margin Probability (FMP) which denotes the difference of score of a frame on its right model and on its competing model. The frames with negative FMP values are regarded as confusable frames and the frames with positive FMP values are regarded as discriminable frames. Second, the confusable frames will be emphasized and the overly discriminable frames will be deweighted by an empirical weighting function. Then the acoustic model parameters are tuned using the weighted frames. By this kind of weighting, the confusable frames, which are often noisy, can contribute more to the acoustic model than those without weighting. We evaluate this technology using the Aurora standard database (TIdigits) and HTK3.3, and obtain a 15.9% WER reduction for noisy speech recognition and a 13.13% WER reduction for clean speech recognition compared with the MLE baseline systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-7"
  },
  "valente07_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Christian",
     "Plahl"
    ],
    [
     "Christian",
     "Gollan"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Ralf",
     "Schlüter"
    ]
   ],
   "title": "Hierarchical neural networks feature extraction for LVCSR system",
   "original": "i07_0042",
   "page_count": 4,
   "order": 8,
   "p1": "42",
   "pn": "45",
   "abstract": [
    "This paper investigates the use of a hierarchy of Neural Networks for performing data driven feature extraction. Two different hierarchical structures based on long and short temporal context are considered. Features are tested on two different LVCSR systems for Meetings data (RT05 evaluation data) and for Arabic Broadcast News (BNAT05 evaluation data). The hierarchical NNs outperforms the single NN features consistently on different type of data and tasks and provides significant improvements w.r.t. respective baselines systems. Best results are obtained when different time resolutions are used at different level of the hierarchy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-8"
  },
  "olsen07_interspeech": {
   "authors": [
    [
     "Peder A.",
     "Olsen"
    ],
    [
     "John R.",
     "Hershey"
    ]
   ],
   "title": "Bhattacharyya error and divergence using variational importance sampling",
   "original": "i07_0046",
   "page_count": 4,
   "order": 9,
   "p1": "46",
   "pn": "49",
   "abstract": [
    "Many applications require the use of divergence measures between probability distributions. Several of these, such as the Kullback Leibler (KL) divergence and the Bhattacharyya divergence, are tractable for single Gaussians, but intractable for complex distributions such as Gaussian mixture models (GMMs) used in speech recognizers. For tasks related to classification error, the Bhattacharyya divergence is of special importance. Here we derive efficient approximations to the Bhattacharyya divergence for GMMs, using novel variational methods and importance sampling. We introduce a combination of the two, variational importance sampling (VISa), which performs importance sampling using a proposal distribution derived from the variational approximation. VISa achieves the same accuracy as naive importance sampling at a fraction of the computation. Finally we apply the Bhattacharyya divergence to compute word confusability and compare the corresponding estimates using the KL divergence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-9"
  },
  "wu07_interspeech": {
   "authors": [
    [
     "Tingyao",
     "Wu"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Dirk",
     "Compernolle"
    ]
   ],
   "title": "Phoneme dependent frame selection preference",
   "original": "i07_0050",
   "page_count": 4,
   "order": 10,
   "p1": "50",
   "pn": "53",
   "abstract": [
    "In previous study we proposed algorithms to select representative frames from a segment for phoneme likelihood evaluation. In this paper we show that this frame selection behavior is phoneme dependent. We observe that some phonemes benefit from frame selection while others do not, and that this separation matches the phonetic categories. For those phonemes sensitive to frame selection, we find that selecting frames at some pre-defined positions in the segment enhances the discrimination between phonemes. These phoneme-dependent positions are explicitly retrieved and used in a phoneme classification task. Experimental results on the TIMIT phonetic database show that the frame selection method significantly outperforms decoding by the classical Viterbi decoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-10"
  },
  "zhou07_interspeech": {
   "authors": [
    [
     "Xinhui",
     "Zhou"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ],
    [
     "Mark",
     "Tiede"
    ],
    [
     "Suzanne",
     "Boyce"
    ]
   ],
   "title": "An articulatory and acoustic study of \"retroflex\" and \"bunched\" american English rhotic sound based on MRI",
   "original": "i07_0054",
   "page_count": 4,
   "order": 11,
   "p1": "54",
   "pn": "57",
   "abstract": [
    "The North American rhotic liquid has two maximally distinct articulatory variants, the classic \"retroflex\" and the classic \"bunched\" tongue postures. The evidence for acoustic differences between these two variants is reexamined using magnetic resonance images of the vocal tract in this study. Two subjects with similar vocal tract dimensions but different tongue postures for sustained /r/ are used. It is shown that these two variants have similar patterns of F1-F3 and zero frequencies. However, the \"retroflex\" variant has a larger difference between F4 and F5 than the \"bunched\" one (around 1400 Hz vs. around 700 Hz). This difference can be explained by the geometry differences between these two variants, in particular, the shorter and more forward palatal constriction of the \"retroflex\" /r/ and the sharper transition between palatal constriction and its anterior and posterior cavities. This formant pattern difference is confirmed by measurement from acoustic data of several additional subjects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-11"
  },
  "martins07_interspeech": {
   "authors": [
    [
     "Paula",
     "Martins"
    ],
    [
     "Inês",
     "Carbone"
    ],
    [
     "Augusto",
     "Silva"
    ],
    [
     "António J. S.",
     "Teixeira"
    ]
   ],
   "title": "An MRI study of european portuguese nasals",
   "original": "i07_0058",
   "page_count": 4,
   "order": 12,
   "p1": "58",
   "pn": "61",
   "abstract": [
    "In this work we present a recently acquired MRI database for European Portuguese. As a first example of possible studies, we present results on 2D and 3D analyses of European Portuguese nasals, particularly nasal vowels. This database will enable the extraction of 2D and/or 3D articulatory parameters as well as some dynamic information to include in articulatory synthesizers. It can also be useful to compare the production of European Portuguese with the production of other languages and have further insight on some of the European Portuguese characteristics, as the nasalization and coarticulation. The MRI database and related studies were made possible by the interdisciplinary nature of the research team, comprised of a radiologist, image processing specialists and a speech scientist.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-12"
  },
  "takano07_interspeech": {
   "authors": [
    [
     "Sayoko",
     "Takano"
    ],
    [
     "Hiroki",
     "Matsuzaki"
    ],
    [
     "Kunitoshi",
     "Motoki"
    ]
   ],
   "title": "A four-cube FEM model of the extrinsic and intrinsic tongue muscles to simulate the production of vowel /i/",
   "original": "i07_0062",
   "page_count": 4,
   "order": 13,
   "p1": "62",
   "pn": "65",
   "abstract": [
    "Roles of the extrinsic and intrinsic tongue muscles in the production of vowel /i/ were examined using a finite element model applied to the tagged cine-MRI data. It has been thought that tongue tissue deformation for /i/ is mainly due to the combined actions of the genioglossus muscle bundles advancing the tongue root to elevate the dorsum with a mid-line grooving. A recent study with the tagging-MRI revealed an independent hydrostat factor of the anterior half of the tongue during /ei/ sequence: elevation of the tongue blade was caused by medial tissue compression with earlier, faster and greater tissue deformation. This result indicates that the contraction of the genioglossus is not a single factor to account for the vowel /i/ and implies that the intrinsic tongue muscles also contribute to tongue deformation to produce the vowel. In this study, a simple four-cube model was build to examine co-contraction effect of the genioglossus and transverse muscles using finite element method (FEM). The simulation result with the anterior transverse muscle (Ta) showed a good agreement with the tagging-MRI data, suggesting that transverse anterior also plays an important role for the production of the vowel /i/.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-13"
  },
  "torres07_interspeech": {
   "authors": [
    [
     "Juan",
     "Torres"
    ],
    [
     "Elliot",
     "Moore"
    ]
   ],
   "title": "Performance evaluation of glottal quality measures from the perspective of vocal tract filter consistency",
   "original": "i07_0066",
   "page_count": 4,
   "order": 14,
   "p1": "66",
   "pn": "69",
   "abstract": [
    "The main difficulty in glottal waveform estimation is the separation of the unknown vocal tract and glottal components of the speech signal. Several glottal quality measures (GQM's) have been proposed to objectively assess the quality of source-tract separation by exploiting known properties of glottal waveforms. In this paper, we present a performance evaluation of 10 GQM's based on the consistency of estimated vocal tract filters (VTF's) on sustained vowel utterances. We compare the results obtained using GQM's to select the optimal estimates to the case where the linear prediction window is aligned exactly with the glottal closure instant (GCI). Although GCI use resulted in the most consistent VTF's, there was a significant benefit from combining several GQM's for selecting optimal estimates. In addition, the GQM-derived estimates were shown to have higher divergence than the GCI estimates across some phoneme-pairs, suggesting higher class-separability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-14"
  },
  "singampalli07_interspeech": {
   "authors": [
    [
     "Veena D.",
     "Singampalli"
    ],
    [
     "Philip J. B.",
     "Jackson"
    ]
   ],
   "title": "Statistical identification of critical, dependent and redundant articulators",
   "original": "i07_0070",
   "page_count": 4,
   "order": 15,
   "p1": "70",
   "pn": "73",
   "abstract": [
    "A compact, data-driven statistical model for identifying roles played by articulators in production of English phones using 1D and 2D articulatory data is presented. Articulators critical in production of each phone were identified and were used to predict the pdfs of dependent articulators based on the strength of articulatory correlations. The performance of the model is evaluated on MOCHA database using proposed and exhaustive search techniques and the results of synthesised trajectories presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-15"
  },
  "qin07_interspeech": {
   "authors": [
    [
     "Chao",
     "Qin"
    ],
    [
     "Miguel Á.",
     "Carreira-Perpiñán"
    ]
   ],
   "title": "An empirical investigation of the nonuniqueness in the acoustic-to-articulatory mapping",
   "original": "i07_0074",
   "page_count": 4,
   "order": 16,
   "p1": "74",
   "pn": "77",
   "abstract": [
    "Articulatory inversion is the problem of recovering the sequence of vocal tract shapes that produce a given acoustic speech signal. Traditionally, its difficulty has been attributed to nonuniqueness of the inverse mapping, where different vocal tract shapes can produce the same acoustics. However, evidence for the nonuniqueness has been restricted to theoretical studies, or to data from atypical speech or very specific sounds. We present a systematic large-scale study using articulatory data for normal speech from the Wisconsin XRDB. We find that nonuniqueness does exist for some sounds, but that the majority of normal speech is produced with a unique vocal tract shape.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-16"
  },
  "dusan07_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ]
   ],
   "title": "Vocal tract length during speech production",
   "original": "i07_1366",
   "page_count": 4,
   "order": 17,
   "p1": "1366",
   "pn": "1369",
   "abstract": [
    "It is known that formant frequencies are inversely proportional with the vocal tract length of the speaker. Although it was observed that vocal tract length of a speaker is variable during speech production, the extent of this variability has not been fully examined in the literature. This paper presents a statistical analysis of the vocal tract length of a female speaker during the production of ten sentences in French. In addition, this paper examines various correlations between vocal tract length, lips protrusion, and larynx height, on one side, and the parameters of Maeda's articulatory model, on the other side. The paper proposes a linear regression model of the vocal tract length as a function of eight articulatory parameters and provides a discussion on the role of the lips and larynx height maneuvers in optimizing the production of speech in terms of achieving high phonetic contrast, high speed, and minimum energy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-17"
  },
  "miki07_interspeech": {
   "authors": [
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Kyohei",
     "Hayashi"
    ]
   ],
   "title": "Approximation method of subglottal system using ARMA filter",
   "original": "i07_1370",
   "page_count": 4,
   "order": 18,
   "p1": "1370",
   "pn": "1373",
   "abstract": [
    "We propose a method of approximation using a rational polynomial of s for the subglottal impedance of the model of Fredberg and Hoenig, and of realization of an ARMA filter model of the subglottal system. We employ the data of the structure and size of branching network of the subglottal system, and adjust the data for Japanese adults using the MRI data of the trachea. Our subglottal model can be adjusted to the circuit model of the vocal tract with the glottal impedance. Using the model with the dummy section, we show the relation between the circuit model and forward/backward waves at the glottis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-18"
  },
  "toutios07_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Konstantinos",
     "Margaritis"
    ]
   ],
   "title": "Enhancing acoustic-to-EPG mapping with lip position information",
   "original": "i07_1374",
   "page_count": 4,
   "order": 19,
   "p1": "1374",
   "pn": "1377",
   "abstract": [
    "This paper investigates the hypothesis that cues involving the positioning of the lips may improve upon a system that performs a mapping from acoustic parameters to electropalatographic (EPG) information; that is, patterns of contact between the tongue and the hard palate. We adopt a multilayer perceptron as a relatively simple model for the acoustic-to-electropalatographic mapping and demonstrate that its performance is improved when parameters describing the positioning of the lips recorded by means of electromagnetic articulography (EMA) are added to the input of the model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-19"
  },
  "kaburagi07_interspeech": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Yosuke",
     "Tanabe"
    ]
   ],
   "title": "A model of glottal flow incorporating viscous-inviscid interaction",
   "original": "i07_1378",
   "page_count": 4,
   "order": 20,
   "p1": "1378",
   "pn": "1381",
   "abstract": [
    "A model of flow passing through the glottis is presented by employing the boundary-layer assumption. A thin boundary layer near the glottal wall influences the flow behavior in terms of the flow separation, jet formation, and pressure distribution along the channel. The integral momentum relation has been developed to analyze the boundary layer accurately, and it can be solved numerically for the given core flow velocity on the basis of the similarity of velocity profiles. On the other hand, boundary layer reduces the effective size of the channel and increases the flow velocity. Therefore, the boundary-layer problem entails viscous-inviscid interaction inherently. To investigate the process of voice production, this paper presents a method to solve the boundary-layer problem including such interaction. Experiments show that the method is useful for predicting the flow rate, pressure distribution, and other properties when the glottal configuration and subglottal pressure are specified as the phonation condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-20"
  },
  "seeber07_interspeech": {
   "authors": [
    [
     "Kilian G.",
     "Seeber"
    ]
   ],
   "title": "Thinking outside the cube: modeling language processing tasks in a multiple resource paradigm",
   "original": "i07_1382",
   "page_count": 4,
   "order": 21,
   "p1": "1382",
   "pn": "1385",
   "abstract": [
    "This paper sets out to find an alternative to Wickens' cube in order to better visually represent the different resource pools recruited by complex language processing tasks. The model's two principal shortcomings, i.e. its inability to visually account for the notion of general resources and the difficulty to visually represent the tasks and their structural proximity, are addressed and compensated for by redrawing the cube and eventually abandoning the three dimensional design in favor of a two dimensional model, the so-called cognitive resource footprint, which we believe to be a more intuitive reflection of the resource involved in these tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-21"
  },
  "cisonni07_interspeech": {
   "authors": [
    [
     "Julien",
     "Cisonni"
    ],
    [
     "Annemie Van",
     "Hirtum"
    ],
    [
     "Jan",
     "Willems"
    ],
    [
     "Xavier",
     "Pelorson"
    ]
   ],
   "title": "Experimental validation of direct and inverse glottal flow models for unsteady flow conditions",
   "original": "i07_1386",
   "page_count": 4,
   "order": 22,
   "p1": "1386",
   "pn": "1389",
   "abstract": [
    "The pressure drop along the glottal constriction drives vocal folds self-sustained oscillations during phonation. Physical modeling of phonation is classically assessed with the glottal geometry and the subglottal pressure as known input parameters. Several studies including\n",
    "in-vitro validation show that simplified one-dimensional flow models allow predictions of the flow characteristics to a fair extent. Application of physical modeling to study phonation abnormalities and pathologies requires input parameters which can be related to in-vivo measurable quantities commonly corresponding to the physical model output parameters. The current paper considers the inversion of some popular simplified flow models in order to estimate the subglottal pressure, the glottal constriction area or the flow model parameters under unsteady flow conditions. The theoretical predictions are tested against in-vitro measurements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-22"
  },
  "nomura07_interspeech": {
   "authors": [
    [
     "Hideyuki",
     "Nomura"
    ],
    [
     "Tetsuo",
     "Funada"
    ]
   ],
   "title": "Effect of unsteady glottal flow on the speech production process",
   "original": "i07_1390",
   "page_count": 4,
   "order": 23,
   "p1": "1390",
   "pn": "1393",
   "abstract": [
    "The purpose of the present study is to clarify the effects of unsteady glottal flow on the phonation. We numerically simulate the speech production process within the larynx and the vocal tract based on our proposed glottal sound source model. The simulation shows amplitude and waveform fluctuations in pressure within the larynx caused by unsteady fluid motion. In order to investigate the unsteady motion effects on the phonation, the coefficient of variation (CV) of amplitude and harmonic-to-noise ratio (HNR) in terms of measures of fluctuations are estimated. The CV and the HNR indicate the greatest fluctuation near the glottis, although the CV and the HNR do not show the fluctuation faraway from the glottis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-23"
  },
  "schneider07_interspeech": {
   "authors": [
    [
     "Katrin",
     "Schneider"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Word stress correlates in spontaneous child-directed speech in German",
   "original": "i07_1394",
   "page_count": 4,
   "order": 24,
   "p1": "1394",
   "pn": "1397",
   "abstract": [
    "In this paper we focus on the use of acoustic as well as voice quality parameters to mark word stress in German. Our aim was to identify the speech parameters parents use to indicate word stress differences to their children. Therefore, mothers and their children were recorded during a period of at least one year while they performed a special playing task using word pairs that differ only in the position of word stress. The recorded target words were analyzed acoustically and with respect to voice quality. The results presented here concern the mothers' productions of contrastive word stress, and we discuss our findings with respect to the results of previous studies investigating word stress. Our results provide further insight into the process of word stress acquisition in German.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-24"
  },
  "aron07_interspeech": {
   "authors": [
    [
     "Michael",
     "Aron"
    ],
    [
     "Nicolas",
     "Ferveur"
    ],
    [
     "Erwan",
     "Kerrien"
    ],
    [
     "Marie-Odile",
     "Berger"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Acquisition and synchronization of multimodal articulatory data",
   "original": "i07_1398",
   "page_count": 4,
   "order": 25,
   "p1": "1398",
   "pn": "1401",
   "abstract": [
    "This paper describes a setup to synchronize data used to track speech articulators during speech production. Our method couples together an ultrasound, an electromagnetic and an audio system to record speech sequences. The coupling requires a precise temporal synchronization, to know exactly the delay between the recording start of each modality, and to know the sampling rate of each modality. A complete setup and methods for automatically synchronizing data are described. The aim is to get a fast, low-cost and easily reproducible acquisition system in order to temporally align data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-25"
  },
  "robert07_interspeech": {
   "authors": [
    [
     "Vincent",
     "Robert"
    ],
    [
     "Yves",
     "Laprie"
    ],
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "A phonetic concatenative approach of labial coarticulation",
   "original": "i07_1402",
   "page_count": 4,
   "order": 26,
   "p1": "1402",
   "pn": "1405",
   "abstract": [
    "Predicting the effects of labial coarticulation is an important aspect with a view to developing an artificial talking head. This paper describes a concatenation approach that uses sigmoids to represent the evolution of labial parameters. Labial parameters considered are lip aperture, protrusion, stretching and jaw aperture. A first formal algorithm determines the relevant transitions, i.e. those corresponding to phonemes imposing constraints on one of the labial parameters. Then relevant transitions are either retrieved or interpolated from a set of reference sigmoids which have been trained on a speaker specific corpus. This labial corpus is made up of isolated vowels, CV, VCV, VCCV and 100 sentences. A final stage consists in improving the overall syntagmatic consistency of the concatenation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-26"
  },
  "turkmani07_interspeech": {
   "authors": [
    [
     "Aseel",
     "Turkmani"
    ],
    [
     "Adrian",
     "Hilton"
    ],
    [
     "Philip J. B.",
     "Jackson"
    ],
    [
     "James",
     "Edge"
    ]
   ],
   "title": "Visual analysis of lip coarticulation in VCV utterances",
   "original": "i07_1406",
   "page_count": 4,
   "order": 27,
   "p1": "1406",
   "pn": "1409",
   "abstract": [
    "This paper presents an investigation of the visual variation on the bilabial plosive consonant /p/ in three coarticulation contexts. The aim is to provide detailed ensemble analysis to assist coarticulation modelling in visual speech synthesis. The underlying dynamics of labeled visual speech units, represented as lip shape, from symmetric VCV utterances, is investigated. Variation in lip dynamics is quantitatively and qualitatively analyzed. This analysis shows that there are statistically significant differences in both the lip shape and trajectory during coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-27"
  },
  "airas07_interspeech": {
   "authors": [
    [
     "Matti",
     "Airas"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Comparison of multiple voice source parameters in different phonation types",
   "original": "i07_1410",
   "page_count": 4,
   "order": 28,
   "p1": "1410",
   "pn": "1413",
   "abstract": [
    "A large sample of vowels produced by male and female speakers were inverse filtered and parameterized using 21 different glottal flow parameters. The performance of the different parameters in expression of the phonation type was then tested using objective statistical methods. The comparison of the results revealed marked differences in the parameters' performance, and therefore, guidelines for parameter use and comparison were established.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-28"
  },
  "knoll07_interspeech": {
   "authors": [
    [
     "Monja",
     "Knoll"
    ],
    [
     "Lisa",
     "Scharrer"
    ]
   ],
   "title": "Acoustic and affective comparisons of natural and imaginary infant-, foreigner- and adult-directed speech",
   "original": "i07_1414",
   "page_count": 4,
   "order": 29,
   "p1": "1414",
   "pn": "1417",
   "abstract": [
    "This study evaluated the use of imagined interactions in speech research, by comparing speech addressed to imaginary speech partners with natural speech addressed to genuine interaction partners. Samples of speech directed to an imaginary infant (IDS), foreigner (FDS) and adult (ADS) produced by ten female students were acoustically analysed and also rated on positive vocal affect. Our results for vocal affect are consistent with previous findings using natural interactions, with IDS rated higher in positive vocal affect than ADS/FDS. However, acoustic analyses of IDS revealed a much smaller vowel space than ADS/FDS, with no difference between those two conditions. Unlike the findings in the natural speech samples, our IDS mean pitch was not significantly higher than ADS/FDS. Since these results are contrary to those from interactions with genuine speech partners, speech obtained from imaginary interactions should be used with caution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-29"
  },
  "araujo07_interspeech": {
   "authors": [
    [
     "André",
     "Araújo"
    ],
    [
     "Luis M. T.",
     "Jesus"
    ],
    [
     "Isabel M.",
     "Costa"
    ]
   ],
   "title": "Vowel production in two occlusal classes",
   "original": "i07_1418",
   "page_count": 4,
   "order": 30,
   "p1": "1418",
   "pn": "1421",
   "abstract": [
    "The influence of occlusal class in speech production has been studied using the X-ray Microbeam Speech Production Database (XRMB-SPD). The objective of the study was to relate the occlusal classes I and II with vowel production adaptations. The \"Modified A-Space\" method was used to select 4 speakers (1 class I male, 1 class I female, 1 class II male and 1 class II female). Articulatory and acoustic features of the vowels [i, , A, u] were studied using different tasks and methods. Results show some structural differences related with occlusal class and variance in class II subjects' structures and articulatory adaptations. The major differences found in the vowels' formants were between male and female groups. Occlusal class also seems to influence acoustical features of vowels produced by female speakers. Structural differences were found, but subjects showed a high adaptation capacity, being able to adjust their articulators to produce all vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-30"
  },
  "khatiwada07_interspeech": {
   "authors": [
    [
     "Rajesh",
     "Khatiwada"
    ]
   ],
   "title": "Nepalese retroflex stops: a static palatography study of inter- and intra-speaker variability",
   "original": "i07_1422",
   "page_count": 4,
   "order": 31,
   "p1": "1422",
   "pn": "1425",
   "abstract": [
    "Retroflex sounds are classically defined as produced with the tongue tip curled backward and often in contact behind the alveolar ridge ([1], [2], [3]). The sounds, however, present a great inter-language, inter- and intra-speaker articulatory variation. Retroflex stops in Nepali, an Indo-Aryan language spoken in Nepal, are produced with the tongue tip with no backward curling movement at the alveolar ridge ([4],[5]). For Pokharel [5], this is not a real type of retroflexion, but rather is apico-alveolar with no backward curling of the tongue tip.\n",
    "The aim of this study is to experimentally verify Pokharel's statement and the originality is to go beyond this claim. We wish to verify whether there is any co-articulation effect while producing the retroflex in different vocalic contexts. We use the direct palatography method to determine the place of articulation. Our articulatory data reveal an important articulatory inter and intra speaker variability. The majority of the retroflex stops realized in our study are subapico- post-alveolar in the case of back vowels and apicoalveolar in the case of the front vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-31"
  },
  "lamoureux07_interspeech": {
   "authors": [
    [
     "Charles A.",
     "Lamoureux"
    ],
    [
     "Victor J.",
     "Boucher"
    ]
   ],
   "title": "Effects of testosterone levels on temporal and intonational aspects of speech: more exploratory data",
   "original": "i07_1426",
   "page_count": 3,
   "order": 32,
   "p1": "1426",
   "pn": "1428",
   "abstract": [
    "There is a growing body of work on the effects of hormonal factors on speech and language behavior. The present research explores the links between speakers' testosterone levels and suprasegmental aspects of speech, namely speaking rate and pitch measures for intonational phrases. Saliva samples were collected from 40 men aged between 20 and 27 years in order to assess testosterone levels. Subjects were recorded reading a standard text. Acoustic analyses of the readings revealed significant correlations where speakers with low testosterone levels tended to use higher and more variable pitch than speakers with high testosterone levels at phrase boundaries. Furthermore, the results also showed significant relationships between salivary testosterone and speaking rate during the readings. These findings reinforce the assumption that some within-sex differences in speech and voice may be based on hormonal factors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-32"
  },
  "karsmakers07_interspeech": {
   "authors": [
    [
     "Peter",
     "Karsmakers"
    ],
    [
     "Kristiaan",
     "Pelckmans"
    ],
    [
     "Johan",
     "Suykens"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Fixed-size kernel logistic regression for phoneme classification",
   "original": "i07_0078",
   "page_count": 4,
   "order": 33,
   "p1": "78",
   "pn": "81",
   "abstract": [
    "Kernel logistic regression (KLR) is a popular non-linear classification technique. Unlike an empirical risk minimization approach such as employed by Support Vector Machines (SVMs), KLR yields probabilistic outcomes based on a maximum likelihood argument which are particularly important in speech recognition. Different from other KLR implementations we use a Nyström approximation to solve large scale problems with estimation in the primal space such as done in fixed-size Least Squares Support Vector Machines (LS-SVMs). In the speech experiments it is investigated how a natural KLR extension to multi-class classification compares to binary KLR models coupled via a one-versus-one coding scheme. Moreover, a comparison to SVMs is made.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-33"
  },
  "park07_interspeech": {
   "authors": [
    [
     "Seung Seop",
     "Park"
    ],
    [
     "Jong Won",
     "Shin"
    ],
    [
     "Jong Kyu",
     "Kim"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "A multiple-model based framework for automatic speech segmentation",
   "original": "i07_0082",
   "page_count": 4,
   "order": 34,
   "p1": "82",
   "pn": "85",
   "abstract": [
    "We propose a new approach to automatic speech segmentation for corpus-based speech synthesis. We utilizes multiple independent automatic segmentation machines (ASMs), instead of using a single ASM, to get final segmentation results: Given multiple independent time-marks from various ASMs, we remove biases of the time-marks, and then compute the weighted sum of the bias-removed time-marks. The bias and weight parameters needed for the proposed method are estimated for each phonetic context through a training procedure where manually-segmented results are used as the references. The bias parameters are obtained by averaging the corresponding errors. The weight parameters are simultaneously optimized through the gradient projection method to overcome a set of constraints in the weight parameter space. A decision tree is employed to deal with the unseen phonetic contexts. Experimental results show that the proposed method remarkably improves the segmentation accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-34"
  },
  "jansen07_interspeech": {
   "authors": [
    [
     "Aren",
     "Jansen"
    ],
    [
     "Partha",
     "Niyogi"
    ]
   ],
   "title": "Semi-supervised learning of speech sounds",
   "original": "i07_0086",
   "page_count": 4,
   "order": 35,
   "p1": "86",
   "pn": "89",
   "abstract": [
    "Recently, there has been much interest in both semi-supervised and manifold learning algorithms, though their applicability has not been explored for all domains. This paper has two goals: (i) to demonstrate semi-supervised approaches based solely on clustering are insufficient for phoneme classification and (ii) to present a new manifold-based semi-supervised algorithm to remedy this shortcoming. The improved performance of our approach over cluster-based methods substantiates the practical relevance of a geometric perspective on speech sounds.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-35"
  },
  "parate07_interspeech": {
   "authors": [
    [
     "Abhinav",
     "Parate"
    ],
    [
     "Ashish",
     "Verma"
    ],
    [
     "Jayanta",
     "Basak"
    ]
   ],
   "title": "Evaluation of syllable stress using single class classifier",
   "original": "i07_0090",
   "page_count": 4,
   "order": 36,
   "p1": "90",
   "pn": "93",
   "abstract": [
    "Evaluation of syllable stress in speech utterances is an important and challenging task in the area of speaker evaluation. In this paper, we propose a method to classify correct utterances of English words based on the evaluation of the lexical syllable stress pattern. Here we use only correctly stressed utterances of the words as training samples since a statistically significant pool of incorrectly stressed utterances is difficult to obtain. The underlying assumption here is that the correct utterances of a word form a compact cluster or a collection of compact clusters (with speaker dependent variations) in a suitably chosen multi-dimensional attribute space. We experimentally demonstrate the effectiveness of the proposed method on several English words and also compare with the standard classifiers where samples from both correct and incorrect utterances were used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-36"
  },
  "huda07_interspeech": {
   "authors": [
    [
     "Mohammad Nurul",
     "Huda"
    ],
    [
     "Ghulam",
     "Muhammad"
    ],
    [
     "Junsei",
     "Horikawa"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Distinctive phonetic feature (DPF) based phone segmentation using hybrid neural networks",
   "original": "i07_0094",
   "page_count": 4,
   "order": 37,
   "p1": "94",
   "pn": "97",
   "abstract": [
    "Segmentation of speech into its corresponding phones has become very important issue in many speech processing areas such as speech recognition, speech analysis, speech synthesis, and speech database. In this paper, for accurate segmentation in speech recognition applications, we introduce Distinctive Phonetic Feature (DPF) based feature extraction using a two-stage NN (Neural Networks) system consists of a RNN (Recurrent Neural Network) in the first stage and an MLN (Multi-Layer Neural Network) in the second stage. The RNN maps continuous acoustic features, Local Feature (LF), onto discrete DPF patterns, while the MLN constraints DPF context or dynamics in an utterance. The experiments are carried out using JNAS (Japanese Newspaper Article Sentences) continuous utterances that contains vowels and consonants. The proposed DPF based feature extractor provides good segmentation and high recognition rate with a reduced mixture-set of HMMs (Hidden Markov Models) by resolving co-articulation effect.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-37"
  },
  "goldman07_interspeech": {
   "authors": [
    [
     "J. -Ph.",
     "Goldman"
    ],
    [
     "M.",
     "Avanzi"
    ],
    [
     "A. -C.",
     "Simon"
    ],
    [
     "Anne",
     "Lacheret"
    ],
    [
     "A.",
     "Auchlin"
    ]
   ],
   "title": "A methodology for the automatic detection of perceived prominent syllables in spoken French",
   "original": "i07_0098",
   "page_count": 4,
   "order": 38,
   "p1": "98",
   "pn": "101",
   "abstract": [
    "Prosodic transcription of spoken corpora relies mainly on the identification of perceived prominence. However, the manual annotation of prominent phenomena is extremely time-consuming, and varies greatly from one expert to another. Automating this procedure would be of great importance. In this study, we present the first results of a methodology aiming at an automatic detection of prominence syllables. It is based on 1. a spontaneous French corpus that has been manually annotated according to a strict methodology and 2. some acoustic prosodic parameters, shown to be corpus-independent, that are used to detect prominent syllables. Some automatic tools, used to handle large corpora, are also described.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-38"
  },
  "niu07_interspeech": {
   "authors": [
    [
     "Xiaochuan",
     "Niu"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Dual-channel acoustic detection of nasalization states",
   "original": "i07_1921",
   "page_count": 4,
   "order": 39,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "Automatic detection of different oral-nasal configurations during speech is useful for understanding normal nasalization and assessing certain speech disorders. We propose an algorithm to extract nasalization features from dual-channel acoustic signals that are acquired by a simple two-microphone setup. The feature is based on a dual-channel acoustic model and the associated analysis method. We successfully test this feature in speaker-dependent and speaker-independent tasks by comparing it with the conventional single-channel MFCC feature. The proposed feature uniformly performs better in both tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-39"
  },
  "pruthi07_interspeech": {
   "authors": [
    [
     "Tarun",
     "Pruthi"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Acoustic parameters for the automatic detection of vowel nasalization",
   "original": "i07_1925",
   "page_count": 4,
   "order": 40,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "The aim of this work was to propose Acoustic Parameters (APs) for the automatic detection of vowel nasalization based on prior knowledge of the acoustics of nasalized vowels. Nine automatically extractable APs were proposed to capture the most important acoustic correlates of vowel nasalization (extra pole-zero pairs, F1 amplitude reduction, F1 bandwidth increase and spectral flattening). The performance of these APs was tested on several databases with different sampling rates and recording conditions. Accuracies of 96.28%, 77.90% and 69.58% were obtained by using these APs on StoryDB, TIMIT and WS96/97 databases, respectively, in a Support Vector Machine classifier framework. To our knowledge these results are the best anyone has achieved on this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-40"
  },
  "hou07_interspeech": {
   "authors": [
    [
     "Jun",
     "Hou"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ],
    [
     "Sorin",
     "Dusan"
    ]
   ],
   "title": "On the use of time-delay neural networks for highly accurate classification of stop consonants",
   "original": "i07_1929",
   "page_count": 4,
   "order": 41,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "Time-Delay Neural Networks (TDNN) have been shown by Waibel et al. [1] to be a good method for the classification of dynamic speech sounds such as voiced stop consonants. In this paper we discuss key issues in the design and training of a TDNN, based on a Multi-Layer Perceptron (MLP), when used for classification of the sets of voiced stop consonants (/b/, /d/, and /g/) and unvoiced stop consonants (/p/, /t/ and /k/) from the TIMIT database. We show that by transforming each input parameter to the TDNN to be a zero mean, unit variance distribution (separately for each phoneme class) we can greatly improve the overall classification performance. The resulting TDNN classification accuracy for voiced or unvoiced stop consonants is around 91%. This performance is achieved without any specific discriminative spectral measurements and can be applied directly to the classification of any of the dynamic phoneme classes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-41"
  },
  "golipour07_interspeech": {
   "authors": [
    [
     "Ladan",
     "Golipour"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A new approach for phoneme segmentation of speech signals",
   "original": "i07_1933",
   "page_count": 4,
   "order": 42,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "In this paper, we present a new method for segmenting speech at the phoneme level. For this purpose, we use the short-time Fourier transform of the speech signal. The goal is to recognize the locations of main energy changes in frequency over time, which can be described as phoneme boundaries. We apply a sub-band analysis and search for energy changes in individual bands as well to obtain further precision. Moreover, we employ the modified group-delay function to achieve a more clear representation of the locations of boundaries, and smooth out the undesired fluctuations of the signal. We also study the use of an auditory spectrogram instead of a regular spectrogram in the segmentation process. Since this method merely utilizes the power spectrum of the signal for segmentation, there is no need for any adaptation of the parameters or training for different speakers in advance. In addition, no transcript information such as the phonemes themselves or voiced/unvoiced decision making is required. The method was tested over the phonetically-diverse part of the Timit database, and the results show that 87% of the boundaries are successfully recognized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-42"
  },
  "stouten07_interspeech": {
   "authors": [
    [
     "Veronique",
     "Stouten"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Automatically learning the units of speech by non-negative matrix factorisation",
   "original": "i07_1937",
   "page_count": 4,
   "order": 43,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "We present an unsupervised technique to discover the (word-sized) speech units in which a corpus of utterances can be decomposed. First, a fixed-length high-dimensional vector representation of the utterances is obtained. Then, the resulting matrix is decomposed in terms of additive units by applying the non-negative matrix factorisation algorithm. On a small vocabulary task, the obtained basis vectors each represent one of the uttered words. We also investigate the amount of speech data that is needed to obtain a correct set of basis vectors. By decreasing the number of occurrences of the words in the corpus, an indication of the learning rate of the system is obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-43"
  },
  "kalinli07_interspeech": {
   "authors": [
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech",
   "original": "i07_1941",
   "page_count": 4,
   "order": 44,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "A bottom-up or saliency driven attention allows the brain to detect nonspecific conspicuous targets in cluttered scenes before fully processing and recognizing the targets. Here, a novel biologically plausible auditory saliency map is presented to model such saliency based auditory attention. Multi-scale auditory features are extracted based on the processing stages in the central auditory system, and they are combined into a single master saliency map. The usefulness of the proposed auditory saliency map in detecting the prominent syllable and word locations in speech is tested in an unsupervised manner. When evaluated with broadcast news-style read speech using the BU Radio News Corpus, the model achieves 75.9 % accuracy at the syllable level, and 78.1 % accuracy at word level. These results compare well to results reported on human performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-44"
  },
  "an07_interspeech": {
   "authors": [
    [
     "Sung Jun",
     "An"
    ],
    [
     "Young-Ik",
     "Kim"
    ],
    [
     "Rhee Man",
     "Kil"
    ]
   ],
   "title": "Zero-crossing-based ratio masking for sound segregation",
   "original": "i07_1945",
   "page_count": 4,
   "order": 45,
   "p1": "1945",
   "pn": "1948",
   "abstract": [
    "This paper presents a new method of zero-crossing based binaural mask estimation for sound segregation under the condition that multiple sound sources are present simultaneously. The masking is determined by the estimated sound source directions using the spatial cues such as inter-aural time differences (ITDs) and inter-aural intensity differences (IIDs). In the suggested method, the estimation of ITDs is utilizing the statistical properties of zero-crossings detected from binaural filter-bank outputs. We also consider the estimation of ITDs with the aid of IID samples to cope with the phase ambiguities of ITD samples in high frequencies. For the masking method, we consider to use the target-to-total power ratio in each segment of the time-frequency domain. We show that this power ratio is optimal from the view point of reconstructing the target speech signal. As a result, the proposed method is able to provide an accurate estimate of sound source directions and also a good masking scheme for speech segregation while offering significantly less computational complexity compared to cross-correlation-based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-45"
  },
  "tanaka07_interspeech": {
   "authors": [
    [
     "Satomi",
     "Tanaka"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Event detection of speech signals based on auditory processing with a dynamic compressive gammachirp filterbank",
   "original": "i07_1949",
   "page_count": 4,
   "order": 46,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "To simulate the perceptual extraction of temporal structures of speech, the authors have been proposing an event-plausibility model that detects the occurrence of subevents in continuous speech signals based on a auditory processing. One of its core components is the filterbank module that simulates the mechanical frequency analysis of the basilar membrane in the cochlea. In this paper, output by the new model using a dynamic compressive gammachirp (dcGC) auditory filterbank was compared with the previous model using a gammatone auditory filterbank. The most important difference between these filters was the nonlinear dynamic level-dependence of the new filter; the previous filterbank was linear. Simulation results revealed that no significant advantage for the new filter (dcGC) was observed for event detection by the event-plausibility model, which suggests that the algorithm for the event-plausibility model has robustness against differences in peripheral auditory processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-46"
  },
  "scharenborg07_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Mirjam",
     "Ernestus"
    ],
    [
     "Vincent",
     "Wan"
    ]
   ],
   "title": "Segmentation of speech: child's play?",
   "original": "i07_1953",
   "page_count": 4,
   "order": 47,
   "p1": "1953",
   "pn": "1956",
   "abstract": [
    "The difficulty of the task of segmenting a speech signal into its words is immediately clear when listening to a foreign language; it is much harder to segment the signal into its words, since the words of the language are unknown. Infants are faced with the same task when learning their first language.\n",
    "This study provides a better understanding of the task that infants face while learning their native language. We employed an automatic algorithm on the task of speech segmentation without prior knowledge of the labels of the phonemes. An analysis of the boundaries erroneously placed inside a phoneme showed that the algorithm consistently placed additional boundaries in phonemes in which acoustic changes occur. These acoustic changes may be as great as the transition from the closure to the burst of a plosive or as subtle as the formant transitions in low or back vowels. Moreover, we found that glottal vibration may attenuate the relevance of acoustic changes within obstruents. An interesting question for further research is how infants learn to overcome the natural tendency to segment these ‘dynamic’ phonemes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-47"
  },
  "errity07_interspeech": {
   "authors": [
    [
     "Andrew",
     "Errity"
    ],
    [
     "John",
     "McKenna"
    ],
    [
     "Barry",
     "Kirkpatrick"
    ]
   ],
   "title": "Dimensionality reduction methods applied to both magnitude and phase derived features",
   "original": "i07_1957",
   "page_count": 4,
   "order": 48,
   "p1": "1957",
   "pn": "1960",
   "abstract": [
    "A number of previous studies have shown that speech sounds may have an intrinsic low dimensional structure. Such studies have focused on magnitude-based features ignoring phase information, as is the convention in many speech processing applications. In this paper dimensionality reduction methods are applied to MFCC and modified group delay function (MODGDF) features derived from the magnitude and phase spectrum, respectively. The low dimensional structure of these representations is examined and a method to combine these features is detailed. Results show that both magnitude and phase derived features have a low dimensional structure. MFCCs are found to offer higher accuracy than MODGDFs in phone classification tasks. Results indicate that combining MFCCs and MODGDFs gives improvements for phone classification. PCA is shown to be capable of efficiently combining MFCCs and MODGDFs for improved classification accuracy without large increases in feature dimensionality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-48"
  },
  "mori07_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Voice source and vocal tract variations as cues to emotional states perceived from expressive conversational speech",
   "original": "i07_0102",
   "page_count": 4,
   "order": 49,
   "p1": "102",
   "pn": "105",
   "abstract": [
    "Speech parameters originating from voice source and vocal tract were analyzed to find acoustic correlates of dimensional descriptions of emotional states. To achieve this goal best, we adopted the Utsunomiya University Spoken Dialogue Database, which was designed for studies on paralinguistic information in expressive conversational speech. Analyses for four female and two male speakers showed: (i) Prosodic parameters were highly correlated especially with the activation dimension, (ii) The aperiodicity-related voice source parameter showed that breathy phonation was mainly used in unpleasant utterances for three females, (iii) Due to smiling facial expression, formant frequencies were higher in pleasant utterances for a female.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-49"
  },
  "yang07_interspeech": {
   "authors": [
    [
     "Fan",
     "Yang"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "Exploring initiative strategies using computer simulation",
   "original": "i07_0106",
   "page_count": 4,
   "order": 50,
   "p1": "106",
   "pn": "109",
   "abstract": [
    "We envision that next-generation spoken dialogue systems will be mixed-initiative. However, it is unclear how exactly a mixed-initiative strategy should be designed; under what circumstances should the system take the initiative, and under what circumstances should it let the user do so. The initiative strategies used in human-human conversation are a good starting point, because they are natural for the user to follow. Studying human-human conversation, however, only gives a descriptive account of human strategies. In this paper, we explore the use of computer simulation to better understand human conventions and give an explanatory account. We have two software agents solve a collaborative task using different initiative strategies, the first derived from analysis of human-human dialogues, and two alternatives based on proposals in the literature. Our simulation results show that the former is more efficient than the others. This helps support the explanation that people use an initiative strategy that minimizes collaborative effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-50"
  },
  "tseng07_interspeech": {
   "authors": [
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Zhao-yu",
     "Su"
    ]
   ],
   "title": "From one base form to multiple output styles - predicting stylistic dynamics of discourse prosody",
   "original": "i07_0110",
   "page_count": 4,
   "order": 51,
   "p1": "110",
   "pn": "113",
   "abstract": [
    "We hypothesize that various prosody output styles can be predicted and simulated from one default base form by accounting for contributions from higher level information to cross-phrase prosodic relationship. Speech materials of four prosody styles were selected: (1.) Han and Tang poetry, (2.) Tang Ballads and Song poetry, (3.) Qin, Tang and Song classic prose and (4.) contemporary TV weather forecast. F0 contours were analyzed using the Fujisaki model, while quantitative analyses of predictions from layered-and-cumulative contribution specified by the HPG (Hierarchical Prosodic phrase Grouping) framework [Tseng et al, 2004; 2005; 2006] were performed across styles and speakers. Results confirmed that higher level contribution is significant across style; contribution distribution patterns and style specific; more regular prosodic formats require more contribution from higher level; stylistic dynamics are predictable; and the HPG base form is indeed default.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-51"
  },
  "crocco07_interspeech": {
   "authors": [
    [
     "Claudia",
     "Crocco"
    ],
    [
     "Renata",
     "Savy"
    ]
   ],
   "title": "Topic in dialogue: prosodic and syntactic features",
   "original": "i07_0114",
   "page_count": 4,
   "order": 52,
   "p1": "114",
   "pn": "117",
   "abstract": [
    "We investigate the relationship between phonetic phrasing, tonal pattern and phrase structure in left peripherical sentence topic. Our corpus consists of three task-oriented Italian dialogues. The results of prosodic analysis show that topics are usually associated to the highest pitch values in the Tone Unit, regardless to their actual syntactic position. Syntactic analysis shows that, while topic phrase structure is rather variable, topic function is quite stable, i.e., topics have mostly circumstantial-locative function, and less frequently subject function. Finally, phonetic phrasing, prominence placement and phrase structure shows clearly regular relationships.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-52"
  },
  "watanabe07_interspeech": {
   "authors": [
    [
     "Michiko",
     "Watanabe"
    ],
    [
     "Yasuharu",
     "Den"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Shusaku",
     "Miwa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Features of pauses and conjunctions at syntactic and discourse boundaries in Japanese monologues",
   "original": "i07_0118",
   "page_count": 4,
   "order": 53,
   "p1": "118",
   "pn": "121",
   "abstract": [
    "Syntactic and discourse boundaries are signalled by prosodic cues as well as linguistic cues in speech. We investigated whether there is a correspondence between prosodic or linguistic cues and the boundary strengths. We measured the rates of filled pauses (FPs) and conjunctions, and the durations of silent and filled pauses and conjunctions at four types of boundaries in casual presentations in Japanese. The results showed that the rates of FPs and conjunctions and the durations of silent pauses correspond to the boundary strengths. However, no significant correspondence was found between the duration of FPs or conjunctions and the boundary strengths. The results suggest that how long the speaker pauses and whether he or she utters a FP or a conjunction is relevant to the boundary strengths. However, the durations of FPs and conjunctions are likely to be affected by the other factors such as planning difficulties of the following parts of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-53"
  },
  "wootton07_interspeech": {
   "authors": [
    [
     "Craig",
     "Wootton"
    ],
    [
     "Michael",
     "McTear"
    ],
    [
     "Terry",
     "Anderson"
    ]
   ],
   "title": "Utilizing online content as domain knowledge in a multi-domain dynamic dialogue system",
   "original": "i07_0122",
   "page_count": 4,
   "order": 54,
   "p1": "122",
   "pn": "125",
   "abstract": [
    "Recent research in dialogue systems has investigated the feasibility of relying on information extracted from the Internet as a source of content and domain knowledge. However, this information needs to be processed and prepared into a form understandable by the dialogue manager. The number of domains and web sites are often restricted to a finite number, with prior knowledge of the site structure itself usually required by the dialogue manager.\n",
    "We present an architecture which demonstrates that multi-domain dialogue, relying on information extracted from online sources, is possible without the need for human intervention or knowledge of the site structure itself.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-54"
  },
  "schooten07_interspeech": {
   "authors": [
    [
     "Boris van",
     "Schooten"
    ],
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Olivier",
     "Galibert"
    ],
    [
     "Aurélien",
     "Max"
    ],
    [
     "Rieks op den",
     "Akker"
    ],
    [
     "Gabriel",
     "Illouz"
    ]
   ],
   "title": "Handling speech input in the ritel QA dialogue system",
   "original": "i07_0126",
   "page_count": 4,
   "order": 55,
   "p1": "126",
   "pn": "129",
   "abstract": [
    "The Ritel system aims to provide open-domain question answering to casual users by means of a telephone dialogue. This implies some unique challenges, such as very fast overall performance and large-vocabulary speech recognition. Speech QA is an error-prone process, but errors or problems that occur may be resolved with help of user feedback, as part of a natural dialogue. This paper reports on a pilot study conducted with the latest version of Ritel, which includes search term confirmation and improved follow-up question handling, as well as various improvements on the other system components. We collected a small dialogue corpus with the new system, which we use to evaluate and discuss it.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-55"
  },
  "kim07_interspeech": {
   "authors": [
    [
     "Woosung",
     "Kim"
    ]
   ],
   "title": "Online call quality monitoring for automating agent-based call centers",
   "original": "i07_0130",
   "page_count": 4,
   "order": 56,
   "p1": "130",
   "pn": "133",
   "abstract": [
    "One of the challenges in automating a call center is the tradeoff between customer satisfaction and the cost of human agents: i.e., most callers prefer human agents to automated systems, but adding human agents substantially increases call center operating costs. One possible compromise is to let callers use automation at the beginning of the call and bring in a human agent if they have problems. The key problem here is, obviously, how to detect the problematic calls promptly before it is too late. This paper proposes a novel method for monitoring call quality, aiming to salvage callers having problems with automation by bringing in a human agent in a timely manner. We propose to use finite state machines to automatically label call data for training and use the log likelihood ratio for monitoring calls to detect bad calls. We demonstrate, by experiments, that it is possible to detect bad calls before callers give up the call, which increases customer satisfaction and minimizes costs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-56"
  },
  "moller07_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Klaus-Peter",
     "Engelbrecht"
    ],
    [
     "Antti",
     "Oulasvirta"
    ]
   ],
   "title": "Analysis of communication failures for spoken dialogue systems",
   "original": "i07_0134",
   "page_count": 4,
   "order": 57,
   "p1": "134",
   "pn": "137",
   "abstract": [
    "Communication failures are typical for interactions with spoken dialogue systems, in particular when dialogues get less structured and less foreseeable. In this paper, we adopt a new classification scheme of communication failures and their consequences and show its usefulness in three respects: (1) For the systematic analysis of data collected in user testing, (2) for the prediction of user-perceived quality and usability, and (3) for the automatic testing of usability in a simulation testbed. Experimental results are presented for two spoken dialogue systems which differ in their dialogue structure and complexity. They show that the failure classification may uncover the causes of interaction problems between user and system, irrespective of system complexity, and that failure consequences can serve as a predictor of user satisfaction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-57"
  },
  "mann07_interspeech": {
   "authors": [
    [
     "Sandra",
     "Mann"
    ],
    [
     "André",
     "Berton"
    ],
    [
     "Ute",
     "Ehrlich"
    ]
   ],
   "title": "How to access audio files of large data bases using in-car speech dialogue systems",
   "original": "i07_0138",
   "page_count": 4,
   "order": 58,
   "p1": "138",
   "pn": "141",
   "abstract": [
    "Today, a number of in-car speech interfaces to handle large vocabulary are available. We propose an approach that allows accessing audio data on different media carriers and in various formats in a uniform way. This uniformity is achieved by providing an audio data retrieval via metadata. Each audio file is enhanced with machine readable information about several categories (e.g. title, artist, genre etc.). Searching for particular audio data the user may pre-select one of these categories, thus restricting the search area. The categories are the same in the metadata of all connected media carriers. The user may directly address the contents of the categories by means of speakable text entries (text enrolments), irrespective of the media carrier or format. Alternatively the user may search globally across all categories by speaking the complete name of a title, album, artist, genre or year - without having to navigate through complex hierarchies and long result lists. Uncertainties that are very likely to occur due to the amount of data that needs to be active are resolved by the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-58"
  },
  "komatani07_interspeech": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Analyzing temporal transition of real user's behaviors in a spoken dialogue system",
   "original": "i07_0142",
   "page_count": 4,
   "order": 59,
   "p1": "142",
   "pn": "145",
   "abstract": [
    "Managing various behaviors of real users is indispensable for spoken dialogue systems to operate adequately in real environments. We have analyzed various users' behaviors using data collected over 34 months from the Kyoto City Bus Information System. We focused on \"barge-in\" and added barge-in rates to our analysis. Temporal transitions of users' behaviors, such as automatic speech recognition (ASR) accuracy, task success rates and barge-in rates, were initially investigated. We then examined the relationship between ASR accuracy and barge-in rates. Analysis revealed that the ASR accuracy of utterances inputted with barge-ins was lower because many novices, who were not accustomed to the timing when to utter, used the system. We also observed that the ASR accuracy of utterances with barge-ins differed based on the barge-in rates of individual users. The results indicate that the barge-in rate can be used as a novel user profile for detecting ASR errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-59"
  },
  "sherwani07_interspeech": {
   "authors": [
    [
     "J.",
     "Sherwani"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Tim",
     "Paek"
    ],
    [
     "Mary",
     "Czerwinski"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Voicepedia: towards speech-based access to unstructured information",
   "original": "i07_0146",
   "page_count": 4,
   "order": 60,
   "p1": "146",
   "pn": "149",
   "abstract": [
    "Currently there are no dialog systems that enable purely voice-based access to the unstructured information on websites such as Wikipedia. Such systems could be revolutionary for non-literate users in the developing world. To investigate interface issues in such a system, we developed VoicePedia, a telephone-based dialog system for searching and browsing Wikipedia. In this paper, we present the system, as well as a user study comparing the use of VoicePedia to SmartPedia, a Smartphone GUI-based alternative. Keyword entry through the voice interface was significantly faster, while search result navigation, and page browsing were significantly slower. Although users preferred the GUI-based interface, task success rates between both systems were comparable - a promising result for regions where Smartphones and data plans are not viable.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-60"
  },
  "rangarajan07_interspeech": {
   "authors": [
    [
     "Vivek",
     "Rangarajan"
    ],
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Exploiting prosodic features for dialog act tagging in a discriminative modeling framework",
   "original": "i07_0150",
   "page_count": 4,
   "order": 61,
   "p1": "150",
   "pn": "153",
   "abstract": [
    "Cue-based automatic dialog act tagging uses lexical, syntactic and prosodic knowledge in the identification of dialog acts. In this paper, we propose a discriminative framework for automatic dialog act tagging using maximum entropy modeling. We propose two schemes for integrating prosody in our modeling framework: (i) Syntax-based categorical prosody prediction from an automatic prosody labeler, (ii) A novel method to model continuous acoustic-prosodic observation sequence as a discrete sequence through the means of quantization. The proposed prosodic feature integration results in a relative improvement of 11.8% over using lexical and syntactic features alone on the Switchboard-DAMSL corpus. The performance of using the lexical, syntactic and prosodic features results in an dialog act tagging accuracy of 84.1%, close to the human agreement of 84%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-61"
  },
  "ai07_interspeech": {
   "authors": [
    [
     "Hua",
     "Ai"
    ],
    [
     "Antonio",
     "Roque"
    ],
    [
     "Anton",
     "Leuski"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "Using information state to improve dialogue move identification in a spoken dialogue system",
   "original": "i07_0154",
   "page_count": 4,
   "order": 62,
   "p1": "154",
   "pn": "157",
   "abstract": [
    "In this paper we investigate how to improve the performance of a dialogue move and parameter tagger for a task-oriented dialogue system using the information-state approach. We use a corpus of utterances and information states from an implemented system to train and evaluate a tagger, and then evaluate the tagger in an on-line system. Use of information state context is shown to improve performance of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-62"
  },
  "chu07_interspeech": {
   "authors": [
    [
     "Shiu-Wah",
     "Chu"
    ],
    [
     "Ian",
     "O'Neill"
    ],
    [
     "Philip",
     "Hanna"
    ]
   ],
   "title": "Using multiple strategies to manage spoken dialogue",
   "original": "i07_0158",
   "page_count": 4,
   "order": 63,
   "p1": "158",
   "pn": "161",
   "abstract": [
    "This paper describes the algorithm used by a multi-strategy dialogue manager (DM) for a speech-based dialogue system and presents the results of an evaluation of the new DM. From its different dialogue strategies the DM is capable of determining the most appropriate strategy based on a set of criteria and the corresponding algorithm. The DM can adopt a number of styles ranging from highly naturalistic mixed-initiative dialogues to rigid system-led dialogues, taking into account factors such as user experience and recognition conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-63"
  },
  "quindere07_interspeech": {
   "authors": [
    [
     "Marcelo",
     "Quinderé"
    ],
    [
     "Luís Seabra",
     "Lopes"
    ],
    [
     "António J. S.",
     "Teixeira"
    ]
   ],
   "title": "An information state based dialogue manager for a mobile robot",
   "original": "i07_0162",
   "page_count": 4,
   "order": 64,
   "p1": "162",
   "pn": "165",
   "abstract": [
    "The paper focuses on an Information State (IS) based dialogue manager developed for Carl, an intelligent mobile robot. It uses a Knowledge Acquisition and Management (KAM) module that integrates information obtained from various interlocutors. This mixed-initiative dialogue manager (DM) handles pronoun resolution, is capable of performing different kinds of clarification/ confirmation questions and generates observations based on the current knowledge acquired. An evaluation of the DM on knowledge acquisition tasks is shown.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-64"
  },
  "yu07_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Automated directory assistance system - from theory to practice",
   "original": "i07_2709",
   "page_count": 4,
   "order": 65,
   "p1": "2709",
   "pn": "2712",
   "abstract": [
    "The automated directory assistance system (ADAS) is traditionally formulated as an automatic speech recognition (ASR) problem. Recently, it has been formulated as a voice search problem, where a spoken utterance is firstly converted into text, which in turn is used to search for the listing. In this paper, we focus on the design and development of the utterance-to-listing component of ADAS. We show that many theoretical and practical issues need to be resolved when applying the basic idea of voice search to the development of ADAS. We share our experiences in addressing these issues, especially in pre-processing the listing database, generating a high performance LM, and developing efficient, accurate, and robust search algorithms. Field tests of our prototype system indicate that an 81% task completion rate can be achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-65"
  },
  "zweig07_interspeech": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "The voice-rate dialog system for consumer ratings",
   "original": "i07_2713",
   "page_count": 4,
   "order": 66,
   "p1": "2713",
   "pn": "2716",
   "abstract": [
    "Voice-Rate is an experimental dialog system that makes product and business ratings available to consumers via a toll-free phone number. By calling Voice-Rate, users can access the ratings of more than one million products, a quarter million local businesses (restaurants), and three thousand national businesses. This paper describes the Voice Rate system, and solutions to three key technical challenges: robust name-matching, efficient disambiguation, and review synthesis for telephone playback. Voice-Rate can be accessed by calling 1-877-456-DATA (toll-free) within the U.S.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-66"
  },
  "winterboer07_interspeech": {
   "authors": [
    [
     "Andi",
     "Winterboer"
    ],
    [
     "Jiang",
     "Hu"
    ],
    [
     "Johanna D.",
     "Moore"
    ],
    [
     "Clifford",
     "Nass"
    ]
   ],
   "title": "The influence of user tailoring and cognitive load on user performance in spoken dialogue systems",
   "original": "i07_2717",
   "page_count": 4,
   "order": 67,
   "p1": "2717",
   "pn": "2720",
   "abstract": [
    "This paper presents results of a Wizard-of-Oz (WoZ) study examining the effect of two different information presentation methods on a secondary task, namely driving. The results demonstrate that the user-model based summarize and refine (UMSR) approach enables more efficient information retrieval in comparison to the data-driven summarize and refine (SR) approach, and does not negatively affect driving performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-67"
  },
  "wang07_interspeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Confidence measures for voice search applications",
   "original": "i07_2721",
   "page_count": 4,
   "order": 68,
   "p1": "2721",
   "pn": "2724",
   "abstract": [
    "Voice search is the technology underlying many spoken dialog applications that enable users to access information using spoken queries. This paper reviews voice search technology, and proposes a new and effective method for computing semantic confidence measures. It explores the use of maximum entropy classifiers as confidence models, and investigates a feature selection algorithm that leads to an effective subset of prominent features for the classifier. The experimental results on a directory assistance application show that the reduced feature set not only makes the model more effective in handling different recognition and search engine combinations, but also results in a very informative confidence measure that is closely correlated with the actual voice search accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-68"
  },
  "higashinaka07_interspeech": {
   "authors": [
    [
     "Ryuichiro",
     "Higashinaka"
    ],
    [
     "Kohji",
     "Dohsaka"
    ],
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Hideki",
     "Isozaki"
    ]
   ],
   "title": "Effects of quiz-style information presentation on user understanding",
   "original": "i07_2725",
   "page_count": 4,
   "order": 69,
   "p1": "2725",
   "pn": "2728",
   "abstract": [
    "This paper proposes quiz-style information presentation for interactive systems as a means to improve user understanding in educational tasks. Since the nature of quizzes can highly motivate users to stay voluntarily engaged in the interaction and keep their attention on receiving information, it is expected that information presented as quizzes can be better understood by users. To verify the effectiveness of the approach, we implemented read-out and quiz systems and performed comparison experiments using human subjects. In the task of memorizing biographical facts, the results showed that user understanding for the quiz system was significantly better than that for the read-out system, and that the subjects were more willing to use the quiz system despite the long duration of the quizzes. This indicates that quiz-style information presentation promotes engagement in the interaction with the system, leading to the improved user understanding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-69"
  },
  "kuo07_interspeech": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "A data visualization and analysis method for natural language call routing system design",
   "original": "i07_2729",
   "page_count": 4,
   "order": 70,
   "p1": "2729",
   "pn": "2732",
   "abstract": [
    "We describe a data visualization tool that allows a natural language call routing system designer to browse the data from high level routing target classes down to individual sentences. For each target class, automatic clustering creates groups that cluster similar requests. Relabeling data is much more efficient because a cluster of many sentences, instead of individual sentences, can be relabeled in one action. The tool also detects and displays potential confusions between sub-clusters across different classes. The confusability may be caused by erroneous labeling, in which case the entire sub-cluster can be relabeled. If the confusability is inherent, the system designer can design a disambiguation dialogue to clarify the caller's intent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-70"
  },
  "bauer07_interspeech": {
   "authors": [
    [
     "Josef G.",
     "Bauer"
    ],
    [
     "Bernt",
     "Andrassy"
    ],
    [
     "Ekaterina",
     "Timoshenko"
    ]
   ],
   "title": "Discriminative optimization of language adapted HMMs for a language identification system based on parallel phoneme recognizers",
   "original": "i07_0166",
   "page_count": 4,
   "order": 71,
   "p1": "166",
   "pn": "169",
   "abstract": [
    "Recently an unsupervised learning scheme for Hidden Markov Models (HMMs) used in acoustical Language Identification (LID) based on Parallel Phoneme Recognizers (PPR) was proposed. This avoids the high costs for orthographically transcribed speech data and phonetic lexica but was found to introduce a considerable increase of classification errors. Also very recently discriminative Minimum Language Identification Error (MLIDE) optimization of HMMs for PPR based LID was introduced that again only requires language tagged speech data and an initial HMM. The described work shows how to combine both approaches to an unsupervised and discriminative learning scheme. Experimental results on large telephone speech databases show that using MLIDE the relative increase in error rate introduced by unsupervised learning can be reduced from 61% to 26%. The absolute difference in LID error rate due to the supervised learning step is reduced from 4.1% to 0.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-71"
  },
  "sim07_interspeech": {
   "authors": [
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Fusion of contrastive acoustic models for parallel phonotactic spoken language identification",
   "original": "i07_0170",
   "page_count": 4,
   "order": 72,
   "p1": "170",
   "pn": "173",
   "abstract": [
    "This paper investigates combining contrastive acoustic models for parallel phonotactic language identification systems. PRLM, a typical phonotactic system, uses a phone recogniser to extract phonotactic information from the speech data. Combining multiple PRLM systems together forms a Parallel PRLM (PPRLM) system. A standard PPRLM system utilises multiple phone recognisers trained on different languages and phone sets to provide diversification. In this paper, a new approach for PPRLM is proposed where phone recognisers with different acoustic models are used for the parallel systems. The STC and SPAM precision matrix modelling schemes as well as the MMI training criterion are used to produce contrastive acoustic models. Preliminary experimental results are reported on the NIST language recognition evaluation sets. With only two training corpora, a 12-way PPRLM system, using different acoustic modelling schemes, outperformed the standard 2-way PPRLM system by 2.0-5.0% absolute EER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-72"
  },
  "wang07b_interspeech": {
   "authors": [
    [
     "Liang",
     "Wang"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Eric H. C.",
     "Choi"
    ]
   ],
   "title": "Multi-layer kohonen self-organizing feature map for language identification",
   "original": "i07_0174",
   "page_count": 4,
   "order": 73,
   "p1": "174",
   "pn": "177",
   "abstract": [
    "In this paper we describe a novel use of a multi-layer Kohonen self-organizing feature map (MLKSFM) for spoken language identification (LID). A normalized, segment-based input feature vector is used in order to maintain the temporal information of speech signal. The LID is performed by using different system configurations of the MLKSFM. Compared with a baseline PPRLM system, our novel system is capable of achieving a similar identification rate, but requires less training time and no phone labeling of training data. The MLKSFM with the sheet-shaped map and the hexagonal-lattice neighborhoods relationship is found to give the best performance for the LID task, and this system is able to achieve a LID rate of 76.4% and 62.4% for the 45-sec and 10-sec OGI speech utterances, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-73"
  },
  "yin07b_interspeech": {
   "authors": [
    [
     "Bo",
     "Yin"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Fang",
     "Chen"
    ]
   ],
   "title": "Hierarchical language identification based on automatic language clustering",
   "original": "i07_0178",
   "page_count": 4,
   "order": 74,
   "p1": "178",
   "pn": "181",
   "abstract": [
    "Due to the limitation of single-level classification, existing fusion techniques experience difficulty in improving the performance of language identification when the number of languages and features are further increased. Given that the similarity of feature distribution between different languages may vary, we propose a novel hierarchical language identification framework with multi-level classification. In this approach, target languages are hierarchically clustered into groups according to the distance between them, models are trained both for individual languages and language groups, and classification is hierarchically done in multi-levels. This framework is implemented and evaluated in this paper, the results showing an relative 15.1% error-rate improvement in 30s case on OGI 10-language database compared to modern GMM fusion system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-74"
  },
  "timoshenko07_interspeech": {
   "authors": [
    [
     "Ekaterina",
     "Timoshenko"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "Using speech rhythm for acoustic language identification",
   "original": "i07_0182",
   "page_count": 4,
   "order": 75,
   "p1": "182",
   "pn": "185",
   "abstract": [
    "This paper presents results on using rhythm for automatic language identification (LID). The idea is to explore the duration of pseudo-syllables as language discriminative feature. The resulting Rhythm system is based on Bigram duration models of neighbouring pseudo-syllables. The Rhythm system is fused with a Spectral system realized by parallel Phoneme Recognition (PPR) approach using MFCC's. The LID systems were evaluated on a 7 languages identification task using the Speech- Dat II databases. Tests were performed with 7 seconds utterances. Whereas the Spectral system acting as a baseline system achieved an error rate of 7.9% the fused system reduced the error rate by 10% relatively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-75"
  },
  "wong07_interspeech": {
   "authors": [
    [
     "Ka-keung",
     "Wong"
    ],
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "A model-based estimation of phonotactic language verification performance",
   "original": "i07_0186",
   "page_count": 4,
   "order": 76,
   "p1": "186",
   "pn": "189",
   "abstract": [
    "One of the most common approaches in language verification (LV) is the phonotactic language verification. Currently, LV performances for different languages under different environments and durations have to be compared experimentally and this can make it difficult to understand LV performances across corpora or durations. LV can be viewed as a special case of hypothesis testing such that Neyman-Pearson theorem and other information theoretic analysis are applicable. In this paper, we introduce a measure of phonotactic confusablity based on the phonotactic distribution, and make it possible to assess the difficulty of the verification problem analytically. We then propose a method of predicting LV performance. The effectiveness of the proposed approach is demonstrated on the NIST 2003 language recognition evaluation test set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-76"
  },
  "rosner07_interspeech": {
   "authors": [
    [
     "Mike",
     "Rosner"
    ],
    [
     "Paulseph-John",
     "Farrugia"
    ]
   ],
   "title": "A tagging algorithm for mixed language identification in a noisy domain",
   "original": "i07_0190",
   "page_count": 4,
   "order": 77,
   "p1": "190",
   "pn": "193",
   "abstract": [
    "The bilingual nature of the Maltese Islands gives rise to frequent occurrences of code switching, both verbally and in writing. In designing a polyglot TTS system capable of handling SMS messages within the local context, it was necessary to come up with a pre-processing mechanism for identifying the language of origin of individual word tokens. Given that certain common words can be interlingually ambiguous and that the domain under consideration is both open and subject to containing various word contractions and spelling mistakes, the task is not as straightforward as it may seem at first. In this paper we discuss a language neutral language identification approach capable of handling the characteristics of the domain in a robust fashion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-77"
  },
  "toledano07_interspeech": {
   "authors": [
    [
     "Doroteo T.",
     "Toledano"
    ],
    [
     "Javier",
     "Gonzalez-Dominguez"
    ],
    [
     "Alejandro",
     "Abejon-Gonzalez"
    ],
    [
     "Danilo",
     "Spada"
    ],
    [
     "Ismael",
     "Mateos-Garcia"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ]
   ],
   "title": "Improved language recognition using better phonetic decoders and fusion with MFCC and SDC features",
   "original": "i07_0194",
   "page_count": 4,
   "order": 78,
   "p1": "194",
   "pn": "197",
   "abstract": [
    "One of the most popular and better performing approaches to language recognition (LR) is Parallel Phonetic Recognition followed by Language Modeling (PPRLM). In this paper we report several improvements in our PPRLM system that allowed us to move from an Equal Error Rate (EER) of over 15% to less than 8% on NIST LR Evaluation 2005 data still using a standard PPRLM system. The most successful improvement was the retraining of the phonetic decoders on larger and more appropriate corpora. We have also developed a new system based on Support Vector Machines (SVMs) that uses as features both Mel Frequency Cepstral Coefficients (MFCCs) and Shifted Delta Cepstra (SDC). This new SVM system alone gives an EER of 10.5% on NIST LRE 2005 data. Fusing our PPRLM system and the new SVM system we achieve an EER of 5.43% on NIST LRE 2005 data, a relative reduction of almost 66% from our baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-78"
  },
  "leeuwen07_interspeech": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Khiet P.",
     "Truong"
    ]
   ],
   "title": "An open-set detection evaluation methodology applied to language and emotion recognition",
   "original": "i07_0338",
   "page_count": 4,
   "order": 79,
   "p1": "338",
   "pn": "341",
   "abstract": [
    "This paper introduces a detection methodology for recognition technologies in speech for which it is difficult to obtain an abundance of non-target classes. An example is language recognition, where we would like to be able to measure the detection capability of a single target language without confounding with the modeling capability of non-target languages. The evaluation framework is based on a cross validation scheme leaving the non-target class out of the allowed training material for the detector. The framework allows us to use Detection Error Tradeoff curves properly. As another application example we apply the evaluation scheme to emotion recognition in order to obtain single-emotion detection performance assessment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-79"
  },
  "yang07b_interspeech": {
   "authors": [
    [
     "Xi",
     "Yang"
    ],
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Boosting with anti-models for automatic language identification",
   "original": "i07_0342",
   "page_count": 4,
   "order": 80,
   "p1": "342",
   "pn": "345",
   "abstract": [
    "In this paper, we adopt the boosting framework to improve the performance of acoustic-based Gaussian mixture model (GMM) Language Identification (LID) systems. We introduce a set of low-complexity, boosted target and anti-models that are estimated from training data to improve class separation, and these models are integrated during the LID backend process. This results in a fast estimation process. Experiments were performed on the 12-language, NIST 2003 language recognition evaluation classification task using a GMM-acoustic-score-only LID system, as well as the one that combines GMM acoustic scores with sequence language model scores from GMM tokenization. Classification errors were reduced from 18.8% to 10.5% on the acoustic-score-only system, and from 11.3% to 7.8% on the combined acoustic and tokenization system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-80"
  },
  "castaldo07_interspeech": {
   "authors": [
    [
     "Fabio",
     "Castaldo"
    ],
    [
     "Daniele",
     "Colibro"
    ],
    [
     "Emanuele",
     "Dalmasso"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Claudio",
     "Vair"
    ]
   ],
   "title": "Acoustic language identification using fast discriminative training",
   "original": "i07_0346",
   "page_count": 4,
   "order": 81,
   "p1": "346",
   "pn": "349",
   "abstract": [
    "Gaussian Mixture Models (GMMs) in combination with Support Vector Machine (SVM) classifiers have been shown to give excellent classification accuracy in speaker recognition.\n",
    "In this work we use this approach for language identification, and we compare its performance with the standard approach based on GMMs.\n",
    "In the GMM-SVM framework, a GMM is trained for each training or test utterance. Since it is difficult to accurately train a model with short utterances, in these conditions the standard GMMs perform better than the GMM-SVM models.\n",
    "To overcome this limitation, we present an extremely fast GMM discriminative training procedure that exploits the information given by the separation hyperplanes estimated by an SVM classifier. We show that our discriminative GMMs provide considerable improvement compared with the standard GMMs and perform better than the GMM-SVM approach for short utterances, achieving state of the art performance for acoustic only systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-81"
  },
  "li07c_interspeech": {
   "authors": [
    [
     "Ming",
     "Li"
    ],
    [
     "Hongbin",
     "Suo"
    ],
    [
     "Xiao",
     "Wu"
    ],
    [
     "Ping",
     "Lu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Spoken language identification using score vector modeling and support vector machine",
   "original": "i07_0350",
   "page_count": 4,
   "order": 82,
   "p1": "350",
   "pn": "353",
   "abstract": [
    "The support vector machine (SVM) framework based on generalized linear discriminate sequence (GLDS) kernel has been shown effective and widely used in language identification tasks. In this paper, in order to compensate the distortions due to inter-speaker variability within the same language and solve the practical limitation of computer memory requested by large database training, multiple speaker group based discriminative classifiers are employed to map the cepstral features of speech utterances into discriminative language characterization score vectors (DLCSV). Furthermore, backend SVM classifiers are used to model the probability distribution of each target language in the DLCSV space and the output scores of backend classifiers are calibrated as the final language recognition scores by a pair-wise posterior probability estimation algorithm. The proposed SVM framework is evaluated on 2003 NIST Language Recognition Evaluation databases, achieving an equal error rate of 4.0% in 30-second tasks, which outperformed the state-of-art SVM system by more than 30% relative error reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-82"
  },
  "cordoba07_interspeech": {
   "authors": [
    [
     "R.",
     "Cordoba"
    ],
    [
     "L. F.",
     "D'Haro"
    ],
    [
     "F.",
     "Fernandez-Martinez"
    ],
    [
     "J.",
     "Macias-Guarasa"
    ],
    [
     "J.",
     "Ferreiros"
    ]
   ],
   "title": "Language identification based on n-gram frequency ranking",
   "original": "i07_0354",
   "page_count": 4,
   "order": 83,
   "p1": "354",
   "pn": "357",
   "abstract": [
    "We present a novel approach for language identification based on a text categorization technique, namely an n-gram frequency ranking. We use a Parallel phone recognizer, the same as in PPRLM, but instead of the language model, we create a ranking with the most frequent n-grams, keeping only a fraction of them. Then we compute the distance between the input sentence ranking and each language ranking, based on the difference in relative positions for each n-gram. The objective of this ranking is to be able to model reliably a longer span than PPRLM, namely 5-gram instead of trigram, because this ranking will need less training data for a reliable estimation. We demonstrate that this approach overcomes PPRLM (6% relative improvement) due to the inclusion of 4-gram and 5-gram in the classifier. We present two alternatives: ranking with absolute values for the number of occurrences and ranking with discriminative values (11% relative improvement).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-83"
  },
  "shen07_interspeech": {
   "authors": [
    [
     "Wade",
     "Shen"
    ],
    [
     "Douglas",
     "Reynolds"
    ]
   ],
   "title": "Improving phonotactic language recognition with acoustic adaptation",
   "original": "i07_0358",
   "page_count": 4,
   "order": 84,
   "p1": "358",
   "pn": "361",
   "abstract": [
    "In recent evaluations of automatic language recognition systems, phonotactic approaches have proven highly effective [1][2]. However, as most of these systems rely on underlying ASR techniques to derive a phonetic tokenization, these techniques are potentially susceptible to acoustic variability from non-language sources (i.e. gender, speaker, channel, etc.). In this paper we apply techniques from ASR research to normalize and adapt HMM-based phonetic models to improve phonotactic language recognition performance. Experiments we conducted with these techniques show an EER reduction of 29% over traditional PRLM-based approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-84"
  },
  "bolanos07_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bolanos"
    ],
    [
     "Wayne",
     "Ward"
    ],
    [
     "Sarel Van",
     "Vuuren"
    ],
    [
     "Javier",
     "Garrido"
    ]
   ],
   "title": "Syllable lattices as a basis for a children's speech reading tracker",
   "original": "i07_0198",
   "page_count": 4,
   "order": 85,
   "p1": "198",
   "pn": "201",
   "abstract": [
    "In this paper we present an algorithm that makes use of information contained in syllable lattices to significantly reduce the classification error rate of a children's speech reading tracker. The task is to verify whether each word in a reference string was actually spoken. A syllable graph is generated from the reference word string to represent acceptable pronunciation alternatives. A syllable based continuous speech recognizer is used to generate a syllable lattice. The best alignment between the reference graph and the syllable lattice is determined using a dynamic programming algorithm. The speech vectors that are aligned with each syllable are used as features for Support Vector Machine classifiers that accept or reject each syllable in the aligned path.\n",
    "Experimental results over three children's speech corpora show that this algorithm can substantially reduce the classification error rate over the standard word based tracker and over a simple best-path syllable based tracker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-85"
  },
  "pan07_interspeech": {
   "authors": [
    [
     "Fuping",
     "Pan"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Mandarin vowel pronunciation quality evaluation by using formant pattern recognition",
   "original": "i07_0202",
   "page_count": 4,
   "order": 86,
   "p1": "202",
   "pn": "205",
   "abstract": [
    "In this paper we propose to apply formant pattern recognition to Mandarin vowel pronunciation assessment. We devise a novel pitch cycle detection method and suggest estimating formant frequencies from observations of the frequency domain by using pitch-synchronous analysis. Statistically based classifiers are trained to discriminate formant patterns for vowel pronunciation assessment. Five confusable Mandarin vowels are selected for experiments. Assessment results show an average human-machine score correlation improvement of 6.10% of the new method over ASR technique, and show an average improvement of 6.37% over traditional LPC analyzing method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-86"
  },
  "black07_interspeech": {
   "authors": [
    [
     "Matthew",
     "Black"
    ],
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Patti",
     "Price"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Automatic detection and classification of disfluent reading miscues in young children's speech for the purpose of assessment",
   "original": "i07_0206",
   "page_count": 4,
   "order": 87,
   "p1": "206",
   "pn": "209",
   "abstract": [
    "This paper explores the importance of disfluent reading miscues (sounding-out, hesitations, whispering, elongated onsets, question intonations) in automating the assessment of children's oral word reading tasks. Analysis showed that a significant portion (21%) of the speech obtained from grades K-2 children from predominantly Spanish-speaking families contained at least one disfluent reading miscue. We discovered human evaluators rated the fluency nearly as important as accuracy when judging the overall reading ability of a child. We devised a lexical method for automatically detecting the sounding-out, hesitation, and whispering disfluencies, which achieved a 14.9% missed detection and 8.9% false alarm rate. We were also able to discriminate 69.4% of the sound-outs from other disfluencies with a 28.5% false alarm rate, a promising and novel result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-87"
  },
  "minematsu07_interspeech": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "K.",
     "Kamata"
    ],
    [
     "Satoshi",
     "Asakawa"
    ],
    [
     "T.",
     "Makino"
    ],
    [
     "T.",
     "Nishimura"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Structural assessment of language learners' pronunciation",
   "original": "i07_0210",
   "page_count": 4,
   "order": 88,
   "p1": "210",
   "pn": "213",
   "abstract": [
    "Speaker-invariant structural representation of speech was proposed [1], where only the phonic contrasts between speech sounds were extracted to form their external structure. The acoustic substances were completely discarded. Considering a mapping function between speaker A's acoustic space and B's space, the speech dynamics was mathematically proven to be invariant between the two irrespective of the form of the function [2]. This structural and dynamic representation was applied to describe the pronunciation of learners [3]. Since the non-linguistic factors were removed effectively, the representation could highlighted the non-nativeness in the individual pronunciations. For vowel learning, it was automatically estimated for each of the learners which vowels to correct by priority [4]. Unlike the conventional approach, the estimation was done without the direct use of sound substances such as spectrums. In this paper, using the vowel charts of the learners plotted by an expert phonetician, the validity of this contrastive or relative approach is examined by comparing it with the conventional absolute approach. Results show the high validity of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-88"
  },
  "samir07_interspeech": {
   "authors": [
    [
     "Abdurrahman",
     "Samir"
    ],
    [
     "Sherif Mahdy",
     "Abdou"
    ],
    [
     "Ahmed Husien",
     "Khalil"
    ],
    [
     "Mohsen",
     "Rashwan"
    ]
   ],
   "title": "Enhancing usability of CAPL system for qur'an recitation learning",
   "original": "i07_0214",
   "page_count": 4,
   "order": 89,
   "p1": "214",
   "pn": "217",
   "abstract": [
    "This paper describes some enhancements for a speech-enabled Computer Aided Pronunciation Learning (CAPL) system HAFSS. This system was developed for teaching Holy Qur'an recitation rules and Arabic pronunciations to non-native speakers. One important point that is critical in any practical language learning system that exploits ASR technology is the user enrolment time. In this paper we introduce the modifications that were done on the baseline system to reduce the amount of the enrolment time while keeping the system accuracy at the same level. Also we introduce results of some experiments that measure the correlation between the judgments of HAFSS system and the judgments of human experts. Also we measured the usefulness of HAFSS system for beginner users by measuring their proficiencies before and after using the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-89"
  },
  "wet07_interspeech": {
   "authors": [
    [
     "Febe de",
     "Wet"
    ],
    [
     "Christa van der",
     "Walt"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "Automatic large-scale oral language proficiency assessment",
   "original": "i07_0218",
   "page_count": 4,
   "order": 90,
   "p1": "218",
   "pn": "221",
   "abstract": [
    "We describe first results obtained during the development of an automatic system for the assessment of spoken English proficiency of university students. The ultimate aim of this system is to allow fast, consistent and objective assessment of oral proficiency for the purpose of placing students in courses appropriate to their language skills. Rate of speech (ROS) was chosen as an indicator of fluency for a number of oral language exercises. In a test involving 106 student subjects, the assessments of 5 human raters are compared with evaluations based on automatically-derived ROS scores. It is found that, although the ROS is estimated accurately, the correlation between human assessments and the ROS scores varies between 0.5 and 0.6. However, the results also indicate that only two of the five human raters were consistent in their appraisals, and that there was only mild inter-rater agreement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-90"
  },
  "denda07_interspeech": {
   "authors": [
    [
     "Yuki",
     "Denda"
    ],
    [
     "Takamasa",
     "Tanaka"
    ],
    [
     "Masato",
     "Nakayama"
    ],
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Yoichi",
     "Yamashita"
    ]
   ],
   "title": "Noise-robust hands-free voice activity detection with adaptive zero crossing detection using talker direction estimation",
   "original": "i07_0222",
   "page_count": 4,
   "order": 91,
   "p1": "222",
   "pn": "225",
   "abstract": [
    "This paper proposes a novel hands-free voice activity detection (VAD) method utilizing not only temporal features but also spatial features, called adaptive zero crossing detection (AZCD), that uses talker direction estimation. It firstly estimates talker direction to extract two spatial features: spatial reliability and spatial variance, based on weighted cross-power spectrum phase analysis and maximum likelihood estimation. Then, the AZCD detects voice activity frames by robustly detecting zero crossing information of speech with adaptively controlled thresholds using the extracted spatial features in noisy environments. The experimental results in an actual office room confirmed that the VAD performance of the proposed method that utilizes both temporal and spatial features is superior to that of the conventional method that utilizes only the temporal or spatial features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-91"
  },
  "alvarez07_interspeech": {
   "authors": [
    [
     "A.",
     "Álvarez"
    ],
    [
     "R.",
     "Martínez"
    ],
    [
     "P.",
     "Gómez"
    ],
    [
     "V.",
     "Nieto"
    ],
    [
     "V.",
     "Rodellar"
    ]
   ],
   "title": "A robust mel-scale subband voice activity detector for a car platform",
   "original": "i07_0226",
   "page_count": 4,
   "order": 92,
   "p1": "226",
   "pn": "229",
   "abstract": [
    "Voice-controlled devices provide a smart solution to operate add-on appliances in a car. Although, speech recognition appears as a key technology to produce useful end-user interfaces, the amount of acoustic disturbances existing in automotive platforms usually prevents satisfactory results. In most of the cases, noise reduction techniques involving a Voice Activity Detector (VAD) are required. Through this paper, a robust method for speech detection under the influence of noise and reverberation in an automobile environment is proposed. This method determines a consistent speech/non-speech discrimination by means of a set of Order-Statistics Filters (OSFs) applied to the log-energies associated to a mel-scale based subband division. The paper also includes an extensive performance evaluation of the algorithm using AURORA3 database recordings. According to our simulation results, the proposed algorithm shows on average a significantly better performance than standard VADs such as ITU-G.729B, GSM-AMR or ETSI-AFE, and other recently reported methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-92"
  },
  "ishizuka07_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Noboru",
     "Miyazaki"
    ]
   ],
   "title": "Noise robust front-end processing with voice activity detection based on periodic to aperiodic component ratio",
   "original": "i07_0230",
   "page_count": 4,
   "order": 93,
   "p1": "230",
   "pn": "233",
   "abstract": [
    "This paper proposes a front-end processing method for automatic speech recognition (ASR) that employs a voice activity detection (VAD) method based on the periodic to aperiodic component ratio (PAR). The proposed VAD method is called PARADE (PAR based Activity DEtection). By considering the powers of the periodic and aperiodic components of the observed signals simultaneously, PARADE can detect speech segments more precisely in the presence of noise than conventional VAD methods. In this paper, PARADE is applied to a front-end processing technique that employs a robust feature extraction method called SPADE (Subband based Periodicity and Aperiodicity DEcomposition). The noisy ASR performance was examined with the CENSREC-1-C database, which includes connected continuous digit speech utterances drawn from CENSREC-1 (Japanese version of AURORA-2). The result shows that the SPADE front-end combined with PARADE achieves average word accuracy of 74.22% at signal to noise ratios of 0 to 20 dB. This accuracy is significantly higher than that achieved by the ETSI ES 202 050 front-end (63.66%) and the SPADE front-end without PARADE (64.28%). This result also confirmed that PARADE can improve the performance of front-end processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-93"
  },
  "toh07_interspeech": {
   "authors": [
    [
     "A. M.",
     "Toh"
    ],
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Sven",
     "Nordholm"
    ]
   ],
   "title": "Feature and distribution normalization schemes for statistical mismatch reduction in reverberant speech recognition",
   "original": "i07_0234",
   "page_count": 4,
   "order": 94,
   "p1": "234",
   "pn": "237",
   "abstract": [
    "Reverberant noise has been a major concern in speech recognition systems. Many speech recognition systems, even with state-of-art features, fail to respond to reverberant effects and the recognition rate deteriorates. This paper explores the significance of normalization strategies in reducing statistical mismatches for robust speech recognition in reverberant environment. Most normalization works focused only on ambient noise and have yet been experimented on reverberant noise. In addition, we propose a new approach for the odd order cepstral moment normalization which is computationally more efficient and reduces the convergence rate in the algorithm. The proposed method is experimentally justified and corroborated by the performance of other normalization schemes. The results emphasize the significance of reducing statistical mismatches in feature space for reverberant speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-94"
  },
  "gibson07_interspeech": {
   "authors": [
    [
     "Matthew",
     "Gibson"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Temporal masking for unsupervised minimum Bayes risk speaker adaptation",
   "original": "i07_0238",
   "page_count": 4,
   "order": 95,
   "p1": "238",
   "pn": "241",
   "abstract": [
    "The minimum Bayes risk (MBR) criterion has previously been applied to the task of speaker adaptation in large vocabulary continuous speech recognition. The success of unsupervised MBR speaker adaptation, however, has been limited by the accuracy of the estimated transcription of the acoustic data. This paper addresses this issue not by improving the accuracy of the estimated transcription but via temporal masking of its erroneous regions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-95"
  },
  "hsieh07_interspeech": {
   "authors": [
    [
     "Tsung-hsueh",
     "Hsieh"
    ],
    [
     "Jeih-weih",
     "Hung"
    ]
   ],
   "title": "Speech feature compensation based on pseudo stereo codebooks for robust speech recognition in additive noise environments",
   "original": "i07_0242",
   "page_count": 4,
   "order": 96,
   "p1": "242",
   "pn": "245",
   "abstract": [
    "In this paper, we propose several compensation approaches to alleviate the effect of additive noise on speech features for speech recognition. These approaches are simple yet efficient noise reduction techniques that use online constructed pseudo stereo codebooks to evaluate the statistics in both clean and noisy environments. The process yields transforms for noise-corrupted speech features to make them closer to their clean counterparts. We apply these compensation approaches on various well-known speech features, including mel-frequency cepstral coefficients (MFCC), autocorrelation mel-frequency cepstral coefficients (AMFCC) and perceptual linear prediction cepstral coefficients (PLPCC). Experimental results conducted on the Aurora-2 database show that the proposed approaches provide all types of the features with a significant performance gain when compared to the baseline results and those obtained by using the conventional utterance-based cepstral mean and variance normalization (CMVN).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-96"
  },
  "dimitriadis07_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Dimitriadis"
    ],
    [
     "Petros",
     "Maragos"
    ],
    [
     "Stamatios",
     "Lefkimmiatis"
    ]
   ],
   "title": "Multiband, multisensor robust features for noisy speech recognition",
   "original": "i07_0246",
   "page_count": 4,
   "order": 97,
   "p1": "246",
   "pn": "249",
   "abstract": [
    "This paper presents a novel feature extraction scheme taking advantage of both the nonlinear modulation speech model and the spatial diversity of speech and noise signals in a multisensor environment. Herein, we propose applying robust features to speech signals captured by a multisensor array minimizing a noise energy criterion over multiple frequency bands. We show that we can achieve improved recognition performance by minimizing the Teager-Kaiser energy of the noise-corrupted signals in different frequency bands. These Multiband, Multisensor Cepstral (MBSC) features are inspired by similar ones already been applied to single-microphone noisy Speech Recognition tasks with significantly improved results. The recognition results show that the proposed features can perform better than the widely-used MFCC features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-97"
  },
  "sasou07_interspeech": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ],
    [
     "Hiroaki",
     "Kojima"
    ]
   ],
   "title": "Noise robust speech recognition for voice driven wheelchair",
   "original": "i07_0250",
   "page_count": 4,
   "order": 98,
   "p1": "250",
   "pn": "253",
   "abstract": [
    "In this paper, we introduce a noise robust speech recognition system for a voice-driven wheelchair. Our system has adopted a microphone array system in order for the user not to need to wear a microphone. By mounting the microphone array system on the wheelchair, our system can easily distinguish the user's utterances from other voices without using a speaker identification technique. We have also adopted a feature compensation technique. By combining the microphone array system and the feature compensation technique, our system can be applied to various noise environments. This is because the microphone array system can provide reliable information about voice activity detection to the feature compensation method, and the feature compensation method can compensate for the weak point of the microphone array system, which is that the microphone array system tends to be less effective for omni-directional noises.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-98"
  },
  "hu07_interspeech": {
   "authors": [
    [
     "Yu",
     "Hu"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "Irrelevant variability normalization based HMM training using VTS approximation of an explicit model of environmental distortions",
   "original": "i07_1042",
   "page_count": 4,
   "order": 99,
   "p1": "1042",
   "pn": "1045",
   "abstract": [
    "In a traditional HMM compensation approach to robust speech recognition that uses Vector Taylor Series (VTS) approximation of an explicit model of environmental distortions, the set of generic HMMs are typically trained from \"clean\" speech only. In this paper, we present a maximum likelihood approach to training generic HMMs from both \"clean\" and \"corrupted\" speech based on the concept of irrelevant variability normalization. Evaluation results on Aurora2 connected digits database demonstrate that the proposed approach achieves significant improvements in recognition accuracy compared to the traditional VTS-based HMM compensation approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-99"
  },
  "buera07_interspeech": {
   "authors": [
    [
     "Luis",
     "Buera"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Óscar",
     "Saz"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "On the jointly unsupervised feature vector normalization and acoustic model compensation for robust speech recognition",
   "original": "i07_1046",
   "page_count": 4,
   "order": 100,
   "p1": "1046",
   "pn": "1049",
   "abstract": [
    "To compensate the mismatch between training and testing conditions, an unsupervised hybrid compensation technique is proposed. It combines Multi-Environment Model based LInear Normalization (MEMLIN) with a novel acoustic model adaptation method based on rotation transformations. A set of rotation transformations is estimated between clean and MEMLIN-normalized data by linear regression in a training process. Thus, each MEMLIN-normalized frame is decoded using the expanded acoustic models, which are obtained from the reference ones and the set of rotation transformations. During the search algorithm, one of the rotation transformations is on-line selected for each frame according to the ML criterion in a modified Viterbi algorithm. Some experiments with Spanish SpeechDat Car database were carried out. MEMLIN over standard ETSI front-end parameters reaches 75.53% of mean improvement in WER, while the introduced hybrid solution goes up to 90.54%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-100"
  },
  "tsao07_interspeech": {
   "authors": [
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "An ensemble modeling approach to joint characterization of speaker and speaking environments",
   "original": "i07_1050",
   "page_count": 4,
   "order": 101,
   "p1": "1050",
   "pn": "1053",
   "abstract": [
    "We propose an ensemble modeling framework to jointly characterize speaker and speaking environments for robust speech recognition. We represent a particular environment by a super-vector formed by concatenating the entire set of mean vectors of the Gaussian mixture components in its corresponding hidden Markov model set. In the training phase we generate an ensemble speaker and speaking environment super-vector by concatenating all the super-vectors trained on data from many real or simulated environments. In the recognition phase the ensemble speaker and speaking environment super-vector is converted to the super-vector for the testing environment with an affine transformation that is estimated online with a maximum likelihood (ML) algorithm. We used a simplified formulation for the proposed approach and evaluated its performance on the Aurora 2 database. In an unsupervised adaptation mode, the proposed approach achieves 7.27% and 13.68% WER reductions, respectively, when tested in clean and averaged noisy conditions (from 0dB to 20dB) over the baseline performance on a gender dependent system. The results suggest that the proposed approach can well characterize environments under the presence of either single or multiple distortion sources.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-101"
  },
  "lin07_interspeech": {
   "authors": [
    [
     "Shih-Hsiang",
     "Lin"
    ],
    [
     "Yao-Ming",
     "Yeh"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Cluster-based polynomial-fit histogram equalization (CPHEQ) for robust speech recognition",
   "original": "i07_1054",
   "page_count": 4,
   "order": 102,
   "p1": "1054",
   "pn": "1057",
   "abstract": [
    "Noise robustness is one of the primary challenges facing most automatic speech recognition (ASR) systems. A vast amount of research efforts on preventing the degradation of ASR performance under various noisy environments have been made during the past several years. In this paper, we consider the use of histogram equalization (HEQ) for robust ASR. In contrast to conventional methods, a novel data fitting method based on polynomial regression was presented to efficiently approximate the inverse of the cumulative density functions of speech feature vectors for HEQ. Moreover, a more elaborate attempt of using such polynomial regression models to directly characterizing the relationship between the speech feature vectors and their corresponding probability distributions, under various noise conditions, was proposed as well. All experiments were carried out on the Aurora-2 database and task. The performance of the presented methods were extensively tested and verified by comparison with the other methods. Experimental results shown that for clean-condition training, our method achieved a considerable word error rate reduction over the baseline system, and also significantly outperformed the other methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-102"
  },
  "martinez07_interspeech": {
   "authors": [
    [
     "Pedro M.",
     "Martinez"
    ],
    [
     "Jose C.",
     "Segura"
    ],
    [
     "Luz",
     "Garcia"
    ]
   ],
   "title": "Robust distributed speech recognition using histogram equalization and correlation information",
   "original": "i07_1058",
   "page_count": 4,
   "order": 103,
   "p1": "1058",
   "pn": "1061",
   "abstract": [
    "In this paper, we propose a noise compensation method for robust speech recognition in DSR (Distributed Speech Recognition) systems based on histogram equalization and correlation information. The objective of this method is to exploit the correlation between components of the feature vector and the temporal correlation between consecutive frames of each component. The recognition experiments, including results in the Aurora 2, Aurora 3-Spanish and Aurora 3-Italian databases, demonstrate that the use of this correlation information increases the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-103"
  },
  "chien07_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Predictive minimum Bayes risk classification for robust speech recognition",
   "original": "i07_1062",
   "page_count": 4,
   "order": 104,
   "p1": "1062",
   "pn": "1065",
   "abstract": [
    "This paper presents a new Bayes classification rule towards minimizing the predictive Bayes risk for robust speech recognition. Conventionally, the plug-in maximum a posteriori (MAP) classification is constructed by adopting nonparametric loss function and deterministic model parameters. Speech recognition performance is limited due to the environmental mismatch and the ill-posed model. Concerning these issues, we develop the predictive minimum Bayes risk (PMBR) classification where the predictive distributions are inherent in Bayes risk. More specifically, we exploit the Bayes loss function and the predictive word posterior probability for Bayes classification. Model mismatch and randomness are compensated to improve generalization capability in speech recognition. In the experiments on car speech recognition, we estimate the prior densities of hidden Markov model parameters from adaptation data. With the prior knowledge of new environment and model uncertainty, PMBR classification is realized and evaluated to be better than MAP, MBR and Bayesian predictive classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-104"
  },
  "ma07_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Applying word duration constraints by using unrolled HMMs",
   "original": "i07_1066",
   "page_count": 4,
   "order": 105,
   "p1": "1066",
   "pn": "1069",
   "abstract": [
    "Conventional HMMs have weak duration constraints. In noisy conditions, the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce word matches with unrealistic durations. This paper presents a simple way to incorporate word duration constraints by unrolling HMMs to form a lattice where word duration probabilities can be applied directly to state transitions. The expanded HMMs are compatible with conventional Viterbi decoding. Experiments on connected-digit recognition show that when using explicit duration constraints the decoder generates word matches with more reasonable durations, and word error rates are significantly reduced across a broad range of noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-105"
  },
  "xiao07_interspeech": {
   "authors": [
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Evaluating the temporal structure normalisation technique on the Aurora-4 task",
   "original": "i07_1070",
   "page_count": 4,
   "order": 106,
   "p1": "1070",
   "pn": "1073",
   "abstract": [
    "We evaluate the temporal structure normalisation (TSN), a feature normalisation technique for robust speech recognition, on the large vocabulary Aurora-4 task. The TSN technique operates by normalising the trend of the feature's power spectral density (PSD) function to a reference function using finite impulse response (FIR) filters. The features are the cepstral coefficients and the normalisation procedure is performed on every cepstral channel of each utterance. Experimental results show that the TSN reduces the average word error rate (WER) by 7.20% and 8.16% relatively over the mean-variance normalisation (MVN) and the histogram equalisation (HEQ) baselines respectively. We further evaluate two other state-of-the-art temporal filters. Experimental results show that among the three evaluated temporal filters, the TSN filter performs the best. Lastly, our results also demonstrates that fixed smoothing filters are less effective on Aurora-4 task than on Aurora-2 task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-106"
  },
  "boril07_interspeech": {
   "authors": [
    [
     "Hynek",
     "Bořil"
    ],
    [
     "Petr",
     "Fousek"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "Two-stage system for robust neutral/lombard speech recognition",
   "original": "i07_1074",
   "page_count": 4,
   "order": 107,
   "p1": "1074",
   "pn": "1077",
   "abstract": [
    "Performance of current speech recognition systems is significantly deteriorated when exposed to strongly noisy environment. It can be attributed to background noise and Lombard effect (LE). Attempts for LE-robust systems often display a tradeoff between LE-specific improvements and the portability to neutral speech. Therefore, towards LE-robust recognition, it seems effective to use a set of conditions-dedicated subsystems driven by a condition classifier, rather than attempting for one universal recognizer.\n",
    "Presented paper focuses on a design of a two-stage recognition system (TSR) comprising talking style classifier (neutral/LE) followed by two style-dedicated recognizers differing in input features. First, the binary neutral/LE classifier is built, with a particular interest in developing suitable features for the classification. Second, performance of common speech features (MFCC, PLP), LE-robust features (Expolog) and newly proposed features is compared in neutral/LE digit recognition tasks. In addition, robustness to the changes of average speech pitch and various noise backgrounds is evaluated. Third, the TSR is built, employing two recognizers, each using style-specific features. Comparison of the proposed system with either neutral-specific or LE-specific recognizer on a joint neutral/LE speech shows an improvement 6.5→4.2 % WER on neutral and 48.1→28.4 % WER on LE Czech utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-107"
  },
  "jitsuhiro07_interspeech": {
   "authors": [
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Tomoji",
     "Toriyama"
    ],
    [
     "Kiyoshi",
     "Kogure"
    ]
   ],
   "title": "Noise suppression using search strategy with multi-model compositions",
   "original": "i07_1078",
   "page_count": 4,
   "order": 108,
   "p1": "1078",
   "pn": "1081",
   "abstract": [
    "We introduce a new noise suppression method by using a search strategy with multi-model compositions that includes the following models: speech, noise, and their composites. Before noise suppression, a beam search is performed to find the best sequences of these models using noise acoustic models, noise-label n-gram models, and a noise-label lexicon. Noise suppression is frame-synchronously performed by the multiple models selected by the search. We evaluated this method using the E-Nightingale task, which contains voice memoranda spoken by nurses during actual work at hospitals. For this difficult task, the proposed method obtained a 21.6% error reduction rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-108"
  },
  "nishiura07_interspeech": {
   "authors": [
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Yoshiki",
     "Hirano"
    ],
    [
     "Yuki",
     "Denda"
    ],
    [
     "Masato",
     "Nakayama"
    ]
   ],
   "title": "Investigations into early and late reflections on distant-talking speech recognition toward suitable reverberation criteria",
   "original": "i07_1082",
   "page_count": 4,
   "order": 109,
   "p1": "1082",
   "pn": "1085",
   "abstract": [
    "Reverberation-robust speech recognition has become very important in the recognition of distant-talking speech. However, as no common reverberation criteria for the recognition of reverberant-speech have been proposed, it has been difficult to estimate this. We have thus focused on a reverberation criterion for the recognition of distant-talking speech. The reverberation time is generally currently used as a reverberation criterion for the recognition of distant-talking speech. This is unique and does not depend on the position of the source in a room. However, distant-talking speech recognition greatly depends on the location of the talker relative to that of the microphone and the distance between them. We investigated a suitable reverberation criterion with the ISO3382 acoustic parameters for distant-talking speech recognition to overcome this problem. We first calculated distant-talking speech recognition with early and late reflections based on the impulse response between the talker and microphone. As a result, we found that early reflections within about 12.5 ms from the duration of direct sound contributed slightly to distant-talking speech recognition in non-noisy environments. We then evaluated it based on ISO3382 acoustic parameters. We consequently confirmed that the ISO3382 acoustic parameters are strong candidates for the new reverberation criteria for distant-talking speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-109"
  },
  "windmann07_interspeech": {
   "authors": [
    [
     "Stefan",
     "Windmann"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "An approach to iterative speech feature enhancement and recognition",
   "original": "i07_1086",
   "page_count": 4,
   "order": 110,
   "p1": "1086",
   "pn": "1089",
   "abstract": [
    "In this paper we propose a novel iterative speech feature enhancement and recognition architecture for noisy speech recognition. It consists of model-based feature enhancement employing Switching Linear Dynamical Models (SLDM), a hidden Markov Model (HMM) decoder and a state mapper, which maps HMM to SLDM states. To consistently adhere to a Bayesian paradigm, posteriors are exchanged between these processing blocks. By introducing the feedback from the recognizer to the enhancement stage, enhancement can exploit both the SLDMs ability to model short-term dependencies and the HMMs ability to model long-term dependencies present in the speech data. Experiments have been conducted on the Aurora II database, which demonstrate that significant word accuracy improvements are obtained at low signal-to-noise ratios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-110"
  },
  "hung07_interspeech": {
   "authors": [
    [
     "Jeih-weih",
     "Hung"
    ]
   ],
   "title": "Optimization of temporal filters in the modulation frequency domain for constructing robust features in speech recognition",
   "original": "i07_1090",
   "page_count": 4,
   "order": 111,
   "p1": "1090",
   "pn": "1093",
   "abstract": [
    "In this paper, we derive new data-driven temporal filters that employ the statistics of the modulation spectra of the speech features. The new temporal filtering approaches are based on the constrained version of Principal Component Analysis (C-PCA) and Maximum Class Distance (C-MCD), respectively. It is shown that the proposed C-PCA and C-MCD temporal filters can effectively improve the speech recognition accuracy in various noise corrupted environments. In experiments conducted on Test Set A of the Aurora-2 noisy digits database, these new temporal filters, together with cepstral mean and variance normalization (CMVN), provides average relative error reduction rates of over 40% and 27%, when compared with the baseline MFCC processing and CMVN alone, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-111"
  },
  "petrick07_interspeech": {
   "authors": [
    [
     "Rico",
     "Petrick"
    ],
    [
     "Kevin",
     "Lohde"
    ],
    [
     "Matthias",
     "Wolff"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "The harming part of room acoustics in automatic speech recognition",
   "original": "i07_1094",
   "page_count": 4,
   "order": 112,
   "p1": "1094",
   "pn": "1097",
   "abstract": [
    "Automatic speech recognition (ASR) systems used in real indoor scenarios suffer from different noise and reverberation conditions compared to the training conditions. This article describes a study which aims to find out what are the most harming parts of reverberation to speech recognition. Noise influences are left out. Therefore different real room impulse responses in different rooms and different speaker to microphone distances are measured and modified. The results of the recognition experiments with the related convoluted impulse responses clearly show the dependency of early and late as well as high and low frequency reflections. Conclusions concerning the design of a dereverberation method are made.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-112"
  },
  "liao07_interspeech": {
   "authors": [
    [
     "Yuan Fu",
     "Liao"
    ],
    [
     "Yh-Her",
     "Yang"
    ],
    [
     "Chi-Hui",
     "Hsu"
    ],
    [
     "Cheng-Chang",
     "Lee"
    ],
    [
     "Jing-Teng",
     "Zeng"
    ]
   ],
   "title": "A reference model weighting-based method for robust speech recognition",
   "original": "i07_1098",
   "page_count": 4,
   "order": 113,
   "p1": "1098",
   "pn": "1101",
   "abstract": [
    "In this paper a reference model weighting (RMW) method is proposed for fast hidden Markov model (HMM) adaptation which aims to use only one input test utterance to online estimate the characteristic of the unknown test noisy environment. The idea of RMW is to first collect a set of reference HMMs in the training phase to represent the space of noisy environments, and then synthesize a suitable HMM for the unknown test noisy environment by interpolating the set of reference HMMs. Noisy environment mismatch can hence be efficiently compensated. The proposed method was evaluated on the multi-condition training task of Aurora2 corpus. Experimental results showed that the proposed RMW approach outperformed both the histogram equalization (HEQ) method and the distributed speech recognition (DSR) standard ES 202 212 proposed by European Telecommunications Standards Institute (ETSI).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-113"
  },
  "nasersharif07_interspeech": {
   "authors": [
    [
     "Babak",
     "Nasersharif"
    ],
    [
     "Ahmad",
     "Akbari"
    ],
    [
     "Mohammad Mehdi",
     "Homayounpour"
    ]
   ],
   "title": "Mel sub-band filtering and compression for robust speech recognition",
   "original": "i07_1102",
   "page_count": 4,
   "order": 114,
   "p1": "1102",
   "pn": "1105",
   "abstract": [
    "The Mel-frequency cepstral coefficients (MFCC) are commonly used in speech recognition systems. But, they are high sensitive to presence of external noise. In this paper, we propose a noise compensation method for Mel filter bank energies and so MFCC features. This compensation method is performed in two stages: Mel sub-band filtering and then compression of Mel-sub-band energies. In the compression step, we propose a sub-band SNR-dependent compression function. We use this function in place of logarithm function in conventional MFCC feature extraction in presence of additive noise. Results show that the proposed method significantly improves MFCC features performance in noisy conditions where it decreases average word error rate up to 30% for isolated word recognition on three test sets of Aurora 2 database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-114"
  },
  "tang07_interspeech": {
   "authors": [
    [
     "Yun",
     "Tang"
    ],
    [
     "Richard",
     "Rose"
    ]
   ],
   "title": "Clustered maximum likelihood linear basis for rapid speaker adaptation",
   "original": "i07_0254",
   "page_count": 4,
   "order": 115,
   "p1": "254",
   "pn": "257",
   "abstract": [
    "Speaker space based adaptation methods for automatic speech recognition have been shown to provide significant performance improvements for tasks where only a few seconds of adaptation speech is available. This paper proposes a robust, low complexity technique within this general class that has been shown to reduce word error rate, reduce the large storage requirements associated with speaker space approaches, and eliminate the need for large numbers of utterances per speaker in training. The technique is based on representing speakers as a linear combination of clustered linear basis vectors and a procedure is presented for ML estimation these vectors from training data. Significant word error rate reduction was obtained relative to speaker independent performance for the Resource Management and Wall Street Journal task domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-115"
  },
  "teng07_interspeech": {
   "authors": [
    [
     "Wenxuan",
     "Teng"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Frédéric",
     "Soufflet"
    ]
   ],
   "title": "Rapid speaker adaptation by reference model interpolation",
   "original": "i07_0258",
   "page_count": 4,
   "order": 116,
   "p1": "258",
   "pn": "261",
   "abstract": [
    "We present in this work a novel algorithm for fast speaker adaptation using only small amounts of adaptation data. It is motivated by the fact that a set of representative speakers can provide a priori knowledge to guide the estimation of a new speaker in the speaker-space. The proposed algorithm enables an a posteriori selection of reference models in the speaker-space as opposed to the a priori selection of reference speaker-space commonly used in techniques such as Eigenvoices. We compare the proposed algorithm with the common rapid adaptation techniques within the context of phoneme recognition task. Experimental results on the IDIOLOGOS and PAIDIALOGOS corpus [1] show that the proposed algorithm achieves slightly better improvement than classic Eigenvoices in phoneme accuracy rate, especially for atypical speakers such as children.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-116"
  },
  "gomez07_interspeech": {
   "authors": [
    [
     "Randy",
     "Gomez"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Rapid unsupervised speaker adaptation using single utterance based on MLLR and speaker selection",
   "original": "i07_0262",
   "page_count": 4,
   "order": 117,
   "p1": "262",
   "pn": "265",
   "abstract": [
    "In this paper, we employ the concept of HMM-Sufficient Statistics (HMM-Suff Stat) and N-best speakers selection to realize a rapid implementation of Baum-Welch and MLLR. Only a single arbitrary utterance is required which is used to select the N-best speakers HMM-Suff Stat from the training database as adaptation data. Since HMM-Suff Stat are pre-computed offline, computation load is minimized. Moreover, adaptation data from the target speaker is not needed. An absolute improvement of 1.8% WA is achieved when using the rapid Baum-Welch as opposed to using SI model and an improvement of 1.1% WA is achieved when the rapid MLLR is used compared to rapid Baum-Welch adaptation using HMM-Suff Stat. Adaptation time is as fast as 6 sec and 7 sec respectively. Evaluation is done in noisy environment conditions where the adaptation algorithm is integrated in a speech dialogue system. Additional experiments with VTLN, MAP, and the conventional MLLR are performed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-117"
  },
  "mak07_interspeech": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Roger",
     "Hsiao"
    ]
   ],
   "title": "Robustness of several kernel-based fast adaptation methods on noisy LVCSR",
   "original": "i07_0266",
   "page_count": 4,
   "order": 118,
   "p1": "266",
   "pn": "269",
   "abstract": [
    "We have been investigating the use of kernel methods to improve conventional linear adaptation algorithms for fast adaptation, when there are less than 10s of adaptation speech. On clean speech, we had shown that our new kernel-based adaptation methods, namely, embedded kernel eigenvoice (eKEV) and kernel eigenspace-based MLLR (KEMLLR) outperformed their linear counterparts. In this paper, we study their unsupervised adaptation performance under additive and convoluted noises using the Aurora4 Corpus, with no assumption or prior knowledge of the noise type and its level. It is found that both eKEV and KEMLLR adaptation continue to outperform MAP and MLLR, and the simple reference speaker weighting (RSW) algorithm continues to perform favorably with KEMLLR. Furthermore, KEMLLR adaptation gives the greatest overall improvement over the speaker-independent model by about 19%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-118"
  },
  "pylkkonen07_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkönen"
    ]
   ],
   "title": "Estimating VTLN warping factors by distribution matching",
   "original": "i07_0270",
   "page_count": 4,
   "order": 119,
   "p1": "270",
   "pn": "273",
   "abstract": [
    "Several methods exist for estimating the warping factors for vocal tract length normalization (VTLN), most of which rely on an exhaustive search over the warping factors to maximize the likelihood of the adaptation data. This paper presents a method for warping factor estimation that is based on matching Gaussian distributions by Kullback-Leibler divergence. It is computationally more efficient than most maximum likelihood methods, but above all it can be used to incorporate the speaker normalization very early in the training process. This can greatly simplify and speed up the training. The estimation method is compared to the baseline maximum likelihood method in three large vocabulary continuous speech recognition tasks. The results confirm that the method performs well in a variety of tasks and configurations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-119"
  },
  "liu07_interspeech": {
   "authors": [
    [
     "Ming",
     "Liu"
    ],
    [
     "Xi",
     "Zhou"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Thomas S.",
     "Huang"
    ],
    [
     "Zhengyou",
     "Zhang"
    ]
   ],
   "title": "Frequency domain correspondence for speaker normalization",
   "original": "i07_0274",
   "page_count": 4,
   "order": 120,
   "p1": "274",
   "pn": "277",
   "abstract": [
    "Due to physiology and linguistic difference between speakers, the spectrum pattern for the same phoneme of two speakers can be quite dissimilar. Without appropriate alignment on the frequency axis, the inter-speaker variation will reduce the modeling efficiency and result in performance degradation. In this paper, a novel data-driven framework is proposed to build the alignment of the frequency axes of two speakers. This alignment between two frequency axes is essentially a frequency domain correspondence of these two speakers. To establish the frequency domain correspondence, we formulate the task as an optimal matching problem. The local matching is achieved by comparing the local features of the spectrogram along the frequency bins. This local matching is actually capturing the similarity of the local patterns along different frequency bins in the spectrogram. After the local matching, a dynamic programming is then applied to find the global optimal alignment between two frequency axes. Experiments on TIDIGITS and TIMIT clearly show the effectiveness of this method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-120"
  },
  "nishida07_interspeech": {
   "authors": [
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Unsupervised training of adaptation rate using q-learning in large vocabulary continuous speech recognition",
   "original": "i07_0278",
   "page_count": 4,
   "order": 121,
   "p1": "278",
   "pn": "281",
   "abstract": [
    "This paper describes a novel approach based on unsupervised training of the MAP adaptation rate using Q-learning. Q-learning is a reinforcement learning technique and is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. The proposed method defines the likelihood of the adapted model as a reward and learns a weight factor that indicates the relative balance between the initial model and adaptation data without the need for supervised data. We conducted recognition experiments on a lecture using a corpus of spontaneous Japanese. We were able to estimate the optimal weight factor using Q-learning in advance. MAP adaptation using the weight factor estimated with the proposed method acquired recognition accuracy that was equivalent to MAP adaptation using a weight factor determined experimentally.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-121"
  },
  "karafiat07_interspeech": {
   "authors": [
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Application of CMLLR in narrow band wide band adapted systems",
   "original": "i07_0282",
   "page_count": 4,
   "order": 122,
   "p1": "282",
   "pn": "285",
   "abstract": [
    "The amount of training data has a crucial effect on the accuracy of HMM based meeting recognition systems. Conversational telephone speech matches speech in meetings well. However it is naturally recorded with low bandwidth. In this paper we present a scheme that allows to transform wide-band meeting data into the same space for improved model training. The transformation into a joint space allows simpler and more efficient implementation of joint speaker adaptive training (SAT) as well as adaptation of statistics for heteroscedastic discriminant analysis (HLDA). Models are tested on the NIST RT'05 meeting evaluation where a relative reduction in word error rate of 4% was achieved. With the use of HLDA and SAT the improvement was retained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-122"
  },
  "levy07_interspeech": {
   "authors": [
    [
     "Christophe",
     "Lévy"
    ],
    [
     "Georges",
     "Linarès"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Fast adaptation of GMM-based compact models",
   "original": "i07_0286",
   "page_count": 4,
   "order": 123,
   "p1": "286",
   "pn": "289",
   "abstract": [
    "In this paper, a new strategy for a fast adaptation of acoustic models is proposed for embedded speech recognition. It relies on a general GMM, which represents the whole acoustic space, associated with a set of HMM state-dependent probability functions modeled as transformations of this GMM.\n",
    "The work presented here takes advantage of this architecture to propose a fast and efficient way to adapt the acoustic models. The adaptation is performed only on the general GMM model, using techniques gathered from the speaker recognition domain. It does not require state-dependent adaptation data and it is very efficient in terms of computational cost.\n",
    "We evaluate our approach in the voice-command task, using a car-based corpus. This adaptation method achieved a relative error-rate decrease of about 10% even if few adaptation data are available. The complete system allows a total relative gain of more than 20% compared to a basic HMM-based system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-123"
  },
  "loof07_interspeech": {
   "authors": [
    [
     "Jonas",
     "Lööf"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Efficient estimation of speaker-specific projecting feature transforms",
   "original": "i07_1557",
   "page_count": 4,
   "order": 124,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "This paper introduces a new, efficient approach for estimating projecting feature transforms for speech recognition. It is based on the MMI' criterion, a likelihood ratio criterion motivated by a simplification of the MMI criterion, and is shown to be closely related to HLDA. In comparison to current methods, the new method is faster, making it more suitable for speaker adaptive training, where the number of speakers, and therefore the number of transforms are substantial.\n",
    "The proposed method was integrated into the RWTH parliamentary speeches transcription system. Experimental results are presented using speaker specific projecting transforms, both when used in recognition only and when used for speaker adaptive training, showing consistent improvements. Furthermore, the observed improvements are shown to be additive to the improvement of MLLR. Comparisons to DLT are presented, and results are presented for a new projecting DLT method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-124"
  },
  "omar07_interspeech": {
   "authors": [
    [
     "Mohamed Kamal",
     "Omar"
    ]
   ],
   "title": "Regularized feature-based maximum likelihood linear regression for speech recognition",
   "original": "i07_1561",
   "page_count": 4,
   "order": 125,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "In many automatic speech recognition (ASR) applications, maximum likelihood linear regression (MLLR), and feature-based maximum likelihood linear regression (FMLLR) are used for speaker adaptation. This paper investigates a possible generalization of FMLLR which addresses the degradation in the performance of ASR systems due to small - possibly time-varying - perturbations of the training and the testing data. We formulate the problem as a regularized maximum likelihood linear regression problem. Based on this formulation, we describe a computationally efficient algorithm for estimating the linear regression parameters which maximize the sum of the log likelihood and the negative of a measure of the sensitivity of the estimated likelihood to these perturbations. This approach does not make any assumptions about the noise model during training and testing. We present several large vocabulary speech recognition experiments that show significant recognition accuracy improvement compared to using the speaker-adapted baseline models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-125"
  },
  "morales07_interspeech": {
   "authors": [
    [
     "Omar Caballero",
     "Morales"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Modelling confusion matrices to improve speech recognition accuracy, with an application to dysarthric speech",
   "original": "i07_1565",
   "page_count": 4,
   "order": 126,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "Dysarthria is a motor speech disorder characterized by weakness, paralysis, or poor coordination of the muscles responsible for speech. Although automatic speech recognition (ASR) systems have been developed for disordered speech, factors such as low intelligibility and limited vocabulary decrease speech recognition accuracy. In this paper, we introduce a technique that can increase recognition accuracy in speakers with low intelligibility by incorporating information from an estimate of the speaker's phoneme confusion matrix. The technique performs much better than standard speaker adaptation when the number of sentences available from a speaker for confusion matrix estimation or adaptation is low, and has similar performance for larger numbers of sentences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-126"
  },
  "huo07_interspeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Wei",
     "Li"
    ]
   ],
   "title": "An active approach to speaker and task adaptation based on automatic analysis of vocabulary confusability",
   "original": "i07_1569",
   "page_count": 4,
   "order": 127,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "Speaker and task adaptation can be made more efficient if an automatic speech recognition system can actively elicit particularly useful adaptation data from a new speaker for a given speech recognition task. This paper presents such an active approach based on an automatic analysis of how difficult the given task vocabulary is. Comparative experiments are designed and conducted for a simple application scenario of searching an item from a long list via voice. The experimental results demonstrate that the proposed active adaptation strategy performs much better than traditional passive adaptation strategies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-127"
  },
  "zheng07_interspeech": {
   "authors": [
    [
     "Jing",
     "Zheng"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "fMPE-MAP: improved discriminative adaptation for modeling new domains",
   "original": "i07_1573",
   "page_count": 4,
   "order": 128,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "Maximum a posteriori (MAP) adaptation and its discriminative variants, such as MMI-MAP (maximum mutual information MAP) and MPE-MAP (minimum phone error MAP), have been widely applied to acoustic model adaptation. This paper introduces a new adaptation approach, fMPE-MAP, which is an extension to the original fMPE (feature minimum phone error) algorithm, with the enhanced ability in porting Gaussian models and fMPE transforms to a new domain. We applied this approach to the SRI-ICSI 2007 NIST meeting recognition system, for which we ported our conversational telephone speech (CTS) and broadcast news (BN) models to the meeting domain. Experiments showed that the proposed fMPE-MAP approach has comparable or better performance than simply training the fMPE transform on combined data, in addition to the obvious speed advantage. In combination with MPE-MAP, we obtained about 20% relative word error rate reduction on a lecture meeting evaluation test set, over the models trained with the standard MAP approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-128"
  },
  "hazen07_interspeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Erik",
     "McDermott"
    ]
   ],
   "title": "Discriminative MCE-based speaker adaptation of acoustic models for a spoken lecture processing task",
   "original": "i07_1577",
   "page_count": 4,
   "order": 129,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "This paper investigates the use of minimum classification error (MCE) training in conjunction with speaker adaptation for the large vocabulary speech recognition task of lecture transcription. Emphasis is placed on the case of supervised adaptation, though an examination of the unsupervised case is also conducted. This work builds upon our previous work using MCE training to construct speaker independent acoustic models. In this work we explore strategies for incorporating MCE training into a model interpolation adaptation scheme in the spirit of traditional maximum a posteriori probability (MAP) adaptation. Experiments show relative error rate reductions between 3% and 7% over a baseline system which uses standard ML estimation instead of MCE training during the adaptation phase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-129"
  },
  "karam07_interspeech": {
   "authors": [
    [
     "Zahi N.",
     "Karam"
    ],
    [
     "William M.",
     "Campbell"
    ]
   ],
   "title": "A new kernel for SVM MLLR based speaker recognition",
   "original": "i07_0290",
   "page_count": 4,
   "order": 130,
   "p1": "290",
   "pn": "293",
   "abstract": [
    "Speaker recognition using support vector machines (SVMs) with features derived from generative models has been shown to perform well. Typically, a universal background model (UBM) is adapted to each utterance yielding a set of features that are used in an SVM. We consider the case where the UBM is a Gaussian mixture model (GMM), and maximum likelihood linear regression (MLLR) adaptation is used to adapt the means of the UBM. We examine two possible SVM feature expansions that arise in this context: the first, a GMM supervector is constructed by stacking the means of the adapted GMM, and the second consists of the elements of the MLLR transform. We examine several kernels associated with these expansions. We show that both expansions are equivalent given an appropriate choice of kernels. Experiments performed on the NIST SRE 2006 corpus clearly highlight that our choice of kernels, which are motivated by distance metrics between GMMs, outperform ad-hoc ones. We also apply SVM nuisance attribute projection (NAP) to the kernels as a form of channel compensation and show that, with a proper choice of kernel, we achieve results comparable to existing SVM based recognizers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-130"
  },
  "lee07_interspeech": {
   "authors": [
    [
     "Kong-Aik",
     "Lee"
    ],
    [
     "Changhuai",
     "You"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tomi",
     "Kinnunen"
    ]
   ],
   "title": "A GMM-based probabilistic sequence kernel for speaker verification",
   "original": "i07_0294",
   "page_count": 4,
   "order": 131,
   "p1": "294",
   "pn": "297",
   "abstract": [
    "This paper describes the derivation of a sequence kernel that transforms speech utterances into probabilistic vectors for classification in an expanded feature space. The sequence kernel is built upon a set of Gaussian basis functions, where half of the basis functions contain speaker specific information while the other half implicates the common characteristics of the competing background speakers. The idea is similar to that in the Gaussian mixture model - universal background model (GMM-UBM) system, except that the Gaussian densities are treated individually in our proposed sequence kernel, as opposed to two mixtures of Gaussian densities in the GMM-UBM system. The motivation is to exploit the individual Gaussian components for better speaker discrimination. Experiments on NIST 2001 SRE corpus show convincing results for the probabilistic sequence kernel approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-131"
  },
  "aronowitz07_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Speaker recognition using kernel-PCA and intersession variability modeling",
   "original": "i07_0298",
   "page_count": 4,
   "order": 132,
   "p1": "298",
   "pn": "301",
   "abstract": [
    "This paper presents a new method for text independent speaker recognition. We embed both training and test sessions into a session space. The session space is a direct sum of a common-speaker subspace and a speaker-unique subspace. The common-speaker subspace is Euclidean and is spanned by a set of reference sessions. Kernel-PCA is used to explicitly embed sessions into the common-speaker subspace. The common-speaker subspace typically captures attributes that are common to many speakers. The speaker-unique subspace is the orthogonal complement of the common-speaker subspace and typically captures attributes that are speaker unique. We model intersession variability in the common-speaker subspace, and combine it with the information that exists in the speaker-unique subspace. Our suggested framework leads to a 43.5% reduction in error rate compared to a Gaussian Mixture Model (GMM) baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-132"
  },
  "dehak07_interspeech": {
   "authors": [
    [
     "Réda",
     "Dehak"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Linear and non linear kernel GMM supervector machines for speaker verification",
   "original": "i07_0302",
   "page_count": 4,
   "order": 133,
   "p1": "302",
   "pn": "305",
   "abstract": [
    "This paper presents a comparison between Support Vector Machines (SVM) speaker verification systems based on linear and non linear kernels defined in GMM supervector space. We describe how these kernel functions are related and we show how the nuisance attribute projection (NAP) technique can be used with both of these kernels to deal with the session variability problem. We demonstrate the importance of GMM model normalization (M-Norm) especially for the non linear kernel. All our experiments were performed on the core condition of NIST 2006 speaker recognition evaluation (all trials). Our best results (an equal error rate of 6.3%) were obtained using NAP and GMM model normalization with the non linear kernel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-133"
  },
  "lopezmoreno07_interspeech": {
   "authors": [
    [
     "Ignacio",
     "Lopez-Moreno"
    ],
    [
     "Ismael",
     "Mateos-Garcia"
    ],
    [
     "Daniel",
     "Ramos"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ]
   ],
   "title": "Support vector regression for speaker verification",
   "original": "i07_0306",
   "page_count": 4,
   "order": 134,
   "p1": "306",
   "pn": "309",
   "abstract": [
    "This paper explores Support Vector Regression (SVR) as an alternative to the widely-used Support Vector Classification (SVC) in GLDS (Generalized Linear Discriminative Sequence)-based speaker verification. SVR allows the use of a ε-insensitive loss function which presents many advantages. First, the optimization of the ε parameter adapts the system to the variability of the features extracted from the speech. Second, the approach is robust to outliers when training the speaker models. Finally, SVR training is related to the optimization of the probability of the speaker model given the data. Results are presented using the NIST SRE 2006 protocol, showing that SVR-GLDS yields a relative improvement of 31% in EER compared to SVC-GLDS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-134"
  },
  "longworth07_interspeech": {
   "authors": [
    [
     "C.",
     "Longworth"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Derivative and parametric kernels for speaker verification",
   "original": "i07_0310",
   "page_count": 4,
   "order": 135,
   "p1": "310",
   "pn": "313",
   "abstract": [
    "The use of Support Vector Machines (SVMs) for speaker verification has become increasingly popular. To handle the dynamic nature of the speech utterances, many SVM-based systems use dynamic kernels. Many of these kernels can be placed into two classes, parametric kernels, where the feature-space consists of parameters from the utterance-dependent model, and derivative kernels, where the derivatives of the utterance log-likelihood with respect to parameters of a generative model are used. This paper contrasts the attributes of these two forms of kernel. Furthermore, the conditions under which the two forms of kernel are identical are described. Two forms of dynamic kernel are examined in detail, based on MLLR-adaptation and mean MAP-adapted models. The performance of these kernels is evaluated on the NIST SRE 2002 dataset. Combining the two forms of kernel together gave a 35% relative reduction in Equal Error Rate compared to the best individual kernel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-135"
  },
  "calvo07_interspeech": {
   "authors": [
    [
     "Jose R.",
     "Calvo"
    ],
    [
     "Rafael",
     "Fernández"
    ],
    [
     "Gabriel",
     "Hernández"
    ]
   ],
   "title": "Application of shifted delta cepstral features in speaker verification",
   "original": "i07_0734",
   "page_count": 4,
   "order": 136,
   "p1": "734",
   "pn": "737",
   "abstract": [
    "Recently, Shifted Delta Cepstral (SDC) feature was reported to produce superior performance to the delta and delta-delta features in cepstral feature based language identification (LID) systems [1, 2]. This paper examines the application of SDC features in speaker verification and evaluates its robustness to channel mismatch, manner of speaking and session variability. The result of the experiment reflects superior or at least similar performance of SDC regarding delta and delta-delta features in speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-136"
  },
  "ferrer07_interspeech": {
   "authors": [
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Kemal",
     "Sönmez"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "A smoothing kernel for spatially related features and its application to speaker verification",
   "original": "i07_0738",
   "page_count": 4,
   "order": 137,
   "p1": "738",
   "pn": "741",
   "abstract": [
    "Most commonly used kernels are invariant to permutations of the feature vector components. This characteristic may make machine learning methods that use such kernels suboptimal in cases where the feature vector has an underlying structure. In this paper we will consider one such case, where the features are spatially related. We show a way to modify the objective function of the support vector machine (SVM) optimization problem to account for this structure. The new optimization problem can be implemented as a standard SVM using a particular smoothing kernel. Results are shown on a speaker verification task using prosodic features that are transformed using a particular implementation of the Fisher score. The proposed method leads to improvements of as much as 15% in equal error rate (EER).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-137"
  },
  "charlet07_interspeech": {
   "authors": [
    [
     "D.",
     "Charlet"
    ],
    [
     "M.",
     "Collet"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "VZ-norm: an extension of z-norm to the multivariate case for anchor model based speaker verification",
   "original": "i07_0742",
   "page_count": 4,
   "order": 138,
   "p1": "742",
   "pn": "745",
   "abstract": [
    "This paper proposes a vectorial Z-normalization approach, the VZ-norm, which extends Z-normalisation to the multivariate case. It is applied in the framework of Anchor Model (AM) based speaker verification. It experimentally proves to significantly improve performance of anchor models on NIST and ESTER databases. A comparative study of different strategies for computing the covariance matrix involved in the AM VZ-norm is presented and commented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-138"
  },
  "lei07_interspeech": {
   "authors": [
    [
     "Howard",
     "Lei"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Word-conditioned HMM supervectors for speaker recognition",
   "original": "i07_0746",
   "page_count": 4,
   "order": 139,
   "p1": "746",
   "pn": "749",
   "abstract": [
    "We improve upon the current Hidden Markov Model (HMM) techniques for speaker recognition by using the means of Gaussian mixture components of keyword HMM states in a support vector machine (SVM) classifier. We achieve an 11% improvement over the traditional keyword HMM approach on SRE06 for the 8 conversation task, using the original set of keywords. Using an expanded set of keywords, we achieve a 4.3% EER standalone on SRE06, and a 2.6% EER in combination with a word-conditioned phone N-grams system, a GMM-based system, and the traditional keyword HMM system on SRE05+06. The latter result improves on our previous best.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-139"
  },
  "tsai07_interspeech": {
   "authors": [
    [
     "Wei-Ho",
     "Tsai"
    ]
   ],
   "title": "Speaker clustering using direct maximization of a BIC-based score",
   "original": "i07_0750",
   "page_count": 4,
   "order": 140,
   "p1": "750",
   "pn": "753",
   "abstract": [
    "This paper presents an effective method for clustering unknown speech utterances based on their associated speakers. The proposed method jointly optimizes the generated clusters and the required number of clusters according to a Bayesian information criterion (BIC). The criterion assesses a partitioning of utterances based on how high the level of within-cluster homogeneity can be achieved at the expense of increasing the number of clusters. Unlike the existing methods, in which BIC is used only to determine the optimal number of clusters, the proposed method uses BIC in conjunction with a genetic algorithm to determine the optimal cluster where each utterance should be located. The experimental results show that the proposed speaker-clustering method outperforms the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-140"
  },
  "preti07_interspeech": {
   "authors": [
    [
     "A.",
     "Preti"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "F.",
     "Capman"
    ],
    [
     "B.",
     "Ravera"
    ]
   ],
   "title": "Confidence measure based unsupervised target model adaptation for speaker verification",
   "original": "i07_0754",
   "page_count": 4,
   "order": 141,
   "p1": "754",
   "pn": "757",
   "abstract": [
    "This paper proposes a new method for updating online the client models of a speaker recognition system using the test data. This problem is called unsupervised adaptation. The main idea of the proposed approach is to adapt the client model using the complete set of data gathered from the successive test, without deciding if the test data belongs to the client or to an impostor. The adaptation process includes a weighting scheme of the test data, based on the a posteriori probability that a test belongs to the targeted client model. The proposed approach is evaluated within the framework of the NIST 2005 and 2006 Speaker Recognition Evaluations. The links between the adaptation method and channel mismatch factors is also explored, using both Feature Mapping and Latent Factor Analysis (LFA) methods. The proposed unsupervised adaptation outperforms the baseline system, with a relative DCF improvement of 27% (37% for EER). When the LFA channel compensation technique is used, the proposed approach achieves a reduction in DCF of 20% (12.5% for EER).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-141"
  },
  "bao07_interspeech": {
   "authors": [
    [
     "Huanjun",
     "Bao"
    ],
    [
     "Ming-Xing",
     "Xu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ]
   ],
   "title": "Emotion attribute projection for speaker recognition on emotional speech",
   "original": "i07_0758",
   "page_count": 4,
   "order": 142,
   "p1": "758",
   "pn": "761",
   "abstract": [
    "Emotion is one of the important factors that cause the system performance degradation. By analyzing the similarity between channel effect and emotion effect on speaker recognition, an emotion compensation method called emotion attribute projection (EAP) is proposed to alleviate the intra-speaker emotion variability. The use of this method has achieved an equal error rate (EER) reduction of 11.7% with the EER reduced from 9.81% to 8.66%. When a linear fusion based on a GMM-UBM system with an EER of 9.38% and an SVM-EAP system with an EER of 8.66% is adopted, another EER reduction of 22.5% and 16.1% can be further achieved, respectively, and the final EER can be 7.27%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-142"
  },
  "zhang07_interspeech": {
   "authors": [
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "High-level feature-based speaker verification via articulatory phonetic-class pronunciation modeling",
   "original": "i07_0762",
   "page_count": 4,
   "order": 143,
   "p1": "762",
   "pn": "765",
   "abstract": [
    "Although articulatory feature-based conditional pronunciation models (AFCPMs) can capture the pronunciation characteristics of speakers, they requires one discrete density function for each phoneme, which may lead to inaccurate models when the amount of training data is limited. This paper proposes a phonetic-class based AFCPM in which the density functions in speaker models are conditioned on phonetic classes instead of phonemes. Phonemes are mapped to phonetic classes by (1) vector quantizing the phoneme-dependent universal background models, (2) grouping phonemes according to the classical phoneme tree, and (3) combination of (1) and (2). A new scoring method that uses an SVM to combine the scores of phonetic-class models is also proposed. Evaluations based on 2000 NIST SRE show that the proposed approach can effectively solve the data sparseness problem encountered in conventional AFCPM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-143"
  },
  "yingthawornsuk07_interspeech": {
   "authors": [
    [
     "T.",
     "Yingthawornsuk"
    ],
    [
     "H. Kaymaz",
     "Keskinpala"
    ],
    [
     "D. M.",
     "Wilkes"
    ],
    [
     "R. G.",
     "Shiavi"
    ],
    [
     "R. M.",
     "Salomon"
    ]
   ],
   "title": "Direct acoustic feature using iterative EM algorithm and spectral energy for classifying suicidal speech",
   "original": "i07_0766",
   "page_count": 4,
   "order": 144,
   "p1": "766",
   "pn": "769",
   "abstract": [
    "Research has shown that the voice itself contains important information about immediate psychological state and certain vocal parameters are capable of distinguishing speaking patterns of speech signal affected by emotional disturbances (i.e., clinical depression). In this study, the GMM based feature of the vocal tract system response and spectral energy have been studied and found to be a primary acoustic feature set for separating two groups of female patients carrying a diagnosis of depression and suicidal risk.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-144"
  },
  "garreton07_interspeech": {
   "authors": [
    [
     "Claudio",
     "Garreton"
    ],
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Fernando",
     "Huenupán"
    ],
    [
     "Carlos",
     "Molina"
    ]
   ],
   "title": "On comparing and combining intra-speaker variability compensation and unsupervised model adaptation in speaker verification",
   "original": "i07_0770",
   "page_count": 4,
   "order": 145,
   "p1": "770",
   "pn": "773",
   "abstract": [
    "In this paper an unsupervised intra-speaker variability compensation method, ISVC, and unsupervised model adaptation are tested to address the problem of limited enrolling data in text-dependent speaker verification. In contrast to model adaptation methods, ISVC is memoryless with respect to previous verification attempts. As shown here, unsupervised model adaptation can lead to substantial improvements in EER but is highly dependent on the sequence of client/impostor verification events. In adverse scenarios, unsupervised model adaptation might even provide reductions in verification accuracy when compared with the baseline system. In those cases, ISVC may outperform adaptation schemes. It is worth emphasizing that ISVC and unsupervised model adaptation are compatible and the combination of both methods always improves the performance of model adaptation. The combination of both schemes can lead to improvements in EER as high as 34%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-145"
  },
  "zhao07_interspeech": {
   "authors": [
    [
     "Xianyu",
     "Zhao"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Hao",
     "Yang"
    ],
    [
     "Jian",
     "Zhao"
    ],
    [
     "Liang",
     "Lu"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": "Comparison of two kinds of speaker location representation for SVM-based speaker verification",
   "original": "i07_0774",
   "page_count": 4,
   "order": 146,
   "p1": "774",
   "pn": "777",
   "abstract": [
    "In anchor modeling, each speaker utterance is represented as a fixed-length location vector in the space of reference speakers by scoring against a set of anchor models. SVM-based speaker verification systems using the anchor location representation have been studied in previously reported work with promising results. In this paper, linear combination weights in reference speaker weighting (RSW) adaptation are explored as an alternative kind of speaker location representation. And this kind of RSW location representation is compared with the anchor location representation in various speaker verification tasks on the 2006 NIST Speaker Recognition Evaluation corpus. Experimental results indicate that with long utterances for reliable maximum likelihood estimation in RSW, the RSW location representation leads to better speaker verification performance than the anchor location; while the latter is more effective for verification of short utterances in high-dimensional representation space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-146"
  },
  "farrus07_interspeech": {
   "authors": [
    [
     "Mireia",
     "Farrús"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Pascual",
     "Ejarque"
    ]
   ],
   "title": "Jitter and shimmer measurements for speaker recognition",
   "original": "i07_0778",
   "page_count": 4,
   "order": 147,
   "p1": "778",
   "pn": "781",
   "abstract": [
    "Jitter and shimmer are measures of the cycle-to-cycle variations of fundamental frequency and amplitude, respectively, which have been largely used for the description of pathological voice quality. Since they characterise some aspects concerning particular voices, it is a priori expected to find differences in the values of jitter and shimmer among speakers. In this paper, several types of jitter and shimmer measurements have been analysed. Experiments performed with the Switchboard-I conversational speech database show that jitter and shimmer measurements give excellent results in speaker verification as complementary features of spectral and prosodic parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-147"
  },
  "shan07_interspeech": {
   "authors": [
    [
     "Zhenyu",
     "Shan"
    ],
    [
     "Yingchun",
     "Yang"
    ],
    [
     "Ruizhi",
     "Ye"
    ]
   ],
   "title": "Natural-emotion GMM transformation algorithm for emotional speaker recognition",
   "original": "i07_0782",
   "page_count": 4,
   "order": 148,
   "p1": "782",
   "pn": "785",
   "abstract": [
    "One of the largest challenges in speaker recognition is dealing with speaker-emotion variability problem. Nowadays, compensation techniques are the main solutions to this problem. In these methods, all kinds of speakers' emotion speech should be elicited thus it is not user-friendly in the application. Therefore the basic problem is how to get the distribution of speakers' emotion speech and how to train emotion GMM from their natural speech. This paper presents a natural-emotion GMM transformation algorithm to train users' emotion model to overcome this problem. The algorithm can convert natural GMM to emotion GMM based on an emotion database. It only needs speakers' natural speech and needn't to align the natural utterances with the emotion utterances. The performance evaluation is carried on the MASC database. The promising result is achieved compared to the traditional speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-148"
  },
  "tseng07b_interspeech": {
   "authors": [
    [
     "Ivy H.",
     "Tseng"
    ],
    [
     "Olivier",
     "Verscheure"
    ],
    [
     "Deepak S.",
     "Turaga"
    ],
    [
     "Upendra V.",
     "Chaudhari"
    ]
   ],
   "title": "Optimized one-bit quantization for adapted GMM-based speaker verification",
   "original": "i07_0786",
   "page_count": 4,
   "order": 149,
   "p1": "786",
   "pn": "789",
   "abstract": [
    "We tackle the problem of designing the optimized one-bit quantizer for speech cepstral features (MFCCs) in speaker verification systems that use the likelihood ratio test, with Gaussian Mixture Models for likelihood functions, and a Universal Background Model (UBM) with Bayesian adaptation used to derive individual speaker models from the UBM. Unlike prior work, that designed a Minimum Log-Likelihood Ratio Difference (MLLRD) quantizer, we design a new quantizer that explicitly optimizes the desired tradeoff between the probabilities of false alarm and detection, directly in probability space. We analytically derive the optimal reconstruction levels for a one-bit quantizer, given a classification decision threshold, and evaluate its performance for speaker verification on the Switchboard corpus. The designed quantizer shows minimal impact on equal error rate (with an achieved compression ratio of 32) as compared to the original system, and significantly outperforms the MLLRD strategy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-149"
  },
  "mclaren07_interspeech": {
   "authors": [
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Brendan",
     "Baker"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "A comparison of session variability compensation techniques for SVM-based speaker recognition",
   "original": "i07_0790",
   "page_count": 4,
   "order": 150,
   "p1": "790",
   "pn": "793",
   "abstract": [
    "This paper compares two of the leading techniques for session variability compensation in the context of GMM mean super-vector SVM classifiers for speaker recognition: inter-session variability modelling and nuisance attribute projection. The former is incorporated in the GMM model training while the latter is employed as a modified SVM kernel. Results on both the NIST 2005 and 2006 corpora demonstrate the effectiveness of both techniques for reducing the effects of session variation. Further, system- and score-level fusion experiments show that the combination of the two methods provides improved performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-150"
  },
  "fauve07_interspeech": {
   "authors": [
    [
     "Benoît",
     "Fauve"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Neil",
     "Pearson"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "John",
     "Mason"
    ]
   ],
   "title": "Influence of task duration in text-independent speaker verification",
   "original": "i07_0794",
   "page_count": 4,
   "order": 151,
   "p1": "794",
   "pn": "797",
   "abstract": [
    "Short duration tasks for text-independent speaker verification have received relatively little attention when compared to that directed at tasks involving many minutes of speech. In this paper we investigate verification performance on a range of durations from a few seconds to a few minutes. We begin with a state-of-the-art GMM-based system operating on a few minutes of speech per person and show that the same system is suboptimal on short (10 seconds) speech recordings. In particular we highlight that optimal frame selection exhibits a dependency on overall duration. This work sheds some light on the difficulties of transposing recent and important techniques such as SVM-NAP to the short duration tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-151"
  },
  "shriberg07_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Luciana",
     "Ferrer"
    ]
   ],
   "title": "A text-constrained prosodic system for speaker verification",
   "original": "i07_1226",
   "page_count": 4,
   "order": 152,
   "p1": "1226",
   "pn": "1229",
   "abstract": [
    "We describe four improvements to a prosody SVM system, including a new method based on text- and part-of-speech-constrained prosodic features. The improved system shows remarkably good performance on NIST SRE06 data, reducing the error rate of an MLLR system by as much as 23% after combination. In addition, an N-best system analysis using eight systems reveals that the prosody SVM is the third and second most important system for 1- and 8-side training conditions, respectively - providing more complementary information than other state-of-the-art cepstral systems. We conclude that as cepstral systems continue to improve, it should become only more important to develop systems based on higher-level features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-152"
  },
  "hannani07_interspeech": {
   "authors": [
    [
     "Asmaa El",
     "Hannani"
    ],
    [
     "Dijana",
     "Petrovska-Delacrétaz"
    ]
   ],
   "title": "Fusing acoustic, phonetic and data-driven systems for text-independent speaker verification",
   "original": "i07_1230",
   "page_count": 4,
   "order": 153,
   "p1": "1230",
   "pn": "1233",
   "abstract": [
    "This paper describes our recent efforts in exploring data-driven high-level features and their combination with low-level spectral features for speaker verification. In particular, we compare the phonetic and data-driven approaches and study their complementarity with short-term acoustic approach. Our objective is to show that data-driven units automatically acquired from the speech data, can be used like phonemes to extract high-level features and to bring complementary speaker-specific information that can therefore provide improvements when fused with acoustic systems. Results obtained on the NIST 2006 Speaker Recognition Evaluation data show that the combination of the phonetic, data-driven and Gaussian Mixture Models (GMM) systems brings a 27% relative reduction of the EER in comparison to the baseline GMM system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-153"
  },
  "dehak07b_interspeech": {
   "authors": [
    [
     "Najim",
     "Dehak"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Continuous prosodic features and formant modeling with joint factor analysis for speaker verification",
   "original": "i07_1234",
   "page_count": 4,
   "order": 154,
   "p1": "1234",
   "pn": "1237",
   "abstract": [
    "In this paper, we introduced the use of formants contours with prosodic contours based on pitch and energy for speaker recognition. These contours are modeled on continuous manners by using the Legendre polynomials on basic unit which represents syllables. The parameters extracted from the Legendre polynomials coefficients plus the syllables duration are modeled with Gaussian Mixture Models (GMM). Factor analysis is used to treat the speaker and channel variability. The results obtained on the core condition of NIST 2006 speaker recognition evaluation show that the use of formant with prosodic information gives an absolute improvement of approximately 3% on equal error rate (EER) compared with the results obtained by prosodic informations alone. However when the formants and the prosodic system scores are fused with a state of the art cepstral joint factor analysis system, we obtain equivalent results to the results obtained when we fused system based on prosodic features alone with the same cepstral joint factor analysis system. This fusion gives a relative improvement of 8.0% (all trials) and 12.0% (English only) on EER compared to cepstral system alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-154"
  },
  "vair07_interspeech": {
   "authors": [
    [
     "Claudio",
     "Vair"
    ],
    [
     "Daniele",
     "Colibro"
    ],
    [
     "Fabio",
     "Castaldo"
    ],
    [
     "Emanuele",
     "Dalmasso"
    ],
    [
     "Pietro",
     "Laface"
    ]
   ],
   "title": "Loquendo - Politecnico di torino's 2006 NIST speaker recognition evaluation system",
   "original": "i07_1238",
   "page_count": 4,
   "order": 155,
   "p1": "1238",
   "pn": "1241",
   "abstract": [
    "This paper describes the Loquendo - Politecnico di Torino system evaluated on the 2006 NIST speaker recognition evaluation dataset. This system was among the best participants in this evaluation. It combines the results of two independent GMM systems: a Phonetic GMM and a classical GMM. Both systems rely on an intersession variation compensation approach, performed in the feature domain. It allowed a 30% error rate reduction with respect to our 2005 system. The linear combination of the two GMM engines gives a further 10% error rate reduction.\n",
    "We also report the results of a set of post evaluation experiments, related to the training data for the intersession variation evaluation, both for the telephone and microphone datasets. The approach adopted for the two wire tests is also described, showing the effect of the speaker segmentation component of our system. Finally, we describe how we performed the incremental unsupervised adaptation tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-155"
  },
  "matrouf07_interspeech": {
   "authors": [
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Benoît",
     "Fauve"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "A straightforward and efficient implementation of the factor analysis model for speaker verification",
   "original": "i07_1242",
   "page_count": 4,
   "order": 156,
   "p1": "1242",
   "pn": "1245",
   "abstract": [
    "For a few years, the problem of session variability in text-independent automatic speaker verification is being tackled actively. A new paradigm based on a factor analysis model have successfully been applied for this task. While very efficient, its implementation is demanding. In this paper, the algorithms involved in the eigenchannel MAP model are written down for a straightforward implementation, without referring to previous work or complex mathematics. In addition, a different compensation scheme is proposed where the standard GMM likelihood can be used without any modification to obtain good performance (even without the need of score normalization). The use of the compensated supervectors within a SVM classifier through a distance based kernel is also investigated. Experiments results shows an overall 50% relative gain over the standard GMM-UBM system on NIST SRE 2005 and 2006 protocols (both at the DCFmin and EER).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-156"
  },
  "hazen07b_interspeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Daniel",
     "Schultz"
    ]
   ],
   "title": "Multi-modal user authentication from video for mobile or variable-environment applications",
   "original": "i07_1246",
   "page_count": 4,
   "order": 157,
   "p1": "1246",
   "pn": "1249",
   "abstract": [
    "In this study, we apply a combination of face and speaker identification techniques to the task of multi-modal (i.e., multi-biometric) user authentication for mobile or variable-environment applications. Audio-visual data was collected using a web camera connected to a laptop computer in three different environments: a quiet indoor office, a busy indoor cafe, and near a noisy outdoor street intersection. Experiments demonstrated the benefits that may be obtained from using a multi-modal approach, even when both input modalities suffer from difficult environmental conditions or a poor match between training and testing conditions. Over twelve different training and testing conditions, user authentication equal error rates were reduced an average of 19% from the best individual biometric in each condition, and 36% from an audio-only system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-157"
  },
  "gerber07_interspeech": {
   "authors": [
    [
     "Michael",
     "Gerber"
    ],
    [
     "René",
     "Beutler"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Quasi text-independent speaker-verification based on pattern matching",
   "original": "i07_1993",
   "page_count": 4,
   "order": 158,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "We present a new approach to quasi text-independent speaker verification based on pattern matching. Our method first seeks phonetically matched segments in two speech signals. For all aligned frame pairs of these segments we compute the probability that they were uttered by the same speaker. Based on these frame-level probabilities we take the decision whether the two signals were spoken by the same speaker or not. Our method to find phonetically matched segments does not depend on a speech recognizer. We show that our system performs better than a baseline speaker verification system based on Gaussian mixture models when the signals are long enough. Especially interesting is the fact that a combination of the devised system with the baseline system performs much better than either of the systems alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-158"
  },
  "solewicz07_interspeech": {
   "authors": [
    [
     "Yosef A.",
     "Solewicz"
    ],
    [
     "Moshe",
     "Koppel"
    ]
   ],
   "title": "Virtual fusion for speaker recognition",
   "original": "i07_1997",
   "page_count": 4,
   "order": 159,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "This paper presents a simplified post-classification framework for enhancing the performance of a given speaker recognition classifier by means of other \"auxiliary\" classifiers. We call it Virtual Fusion, since the assisting classifiers are used only for training the post-classifier and are not necessary in operating mode. Experiments performed using Nist'04 and '05 evaluations suggest that the proposed technique is able to consistently improve the EER of a typical GMM-cepstrum classifier by up to 15%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-159"
  },
  "chao07_interspeech": {
   "authors": [
    [
     "Yi-Hsiang",
     "Chao"
    ],
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Shih-Sian",
     "Cheng"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Ruei-Chuan",
     "Chang"
    ]
   ],
   "title": "Evolutionary minimum verification error learning of the alternative hypothesis model for LLR-based speaker verification",
   "original": "i07_2001",
   "page_count": 4,
   "order": 160,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "It is usually difficult to characterize the alternative hypothesis precisely in a log-likelihood ratio (LLR)-based speaker verification system. In a previous work, we proposed using a weighted arithmetic combination (WAC) or a weighted geometric combination (WGC) of the likelihoods of the background models instead of heuristic combinations, such as the arithmetic mean and the geometric mean, to better characterize the alternative hypothesis. In this paper, we further propose learning the parameters associated with WAC or WGC via an evolutionary minimum verification error (MVE) training method, such that both the false acceptance probability and the false rejection probability can be minimized. Our experiment results show that the proposed methods outperform conventional LLR-based approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-160"
  },
  "nakagawa07_interspeech": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Kouhei",
     "Asakawa"
    ],
    [
     "Longbiao",
     "Wang"
    ]
   ],
   "title": "Speaker recognition by combining MFCC and phase information",
   "original": "i07_2005",
   "page_count": 4,
   "order": 161,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "In conventional speaker recognition method based on MFCC, the phase information has been ignored. In this paper, we proposed a method that integrates the phase information on a speaker recognition method. The speaker identification experiments were performed using NTT database which consists of sentences uttered at normal speed mode by 35 Japanese speakers (22 males and 13 females) on five sessions over ten months. Each speaker uttered only 5 training utterances (about 20 seconds in total). Using the phase information, the speaker recognition error rate was reduced by about 44%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-161"
  },
  "manocha07_interspeech": {
   "authors": [
    [
     "Sandeep",
     "Manocha"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "A semi-automatic approach for speaker mining of tapped telephone conversations",
   "original": "i07_2009",
   "page_count": 4,
   "order": 162,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "Speaker mining involves speaker detection in a set of multi-speaker files. In previous work on speaker mining, training data is used for constructing target speaker models. In this study, a new speaker mining scenario was considered, where there is no demarcation between training and testing data and prior target speaker models are absent. Given the ENRON database which consists of tapped telephone conversations between traders and customers, the task is to identify conversations having one or more speakers in common. Since the poor audio quality of this database makes automatic speaker segmentation ineffective, a new technique was developed where a multi-speaker model is trained on the entire conversation and various scoring strategies were tried. A semi-automatic approach was adopted and it reduces the manual effort involved in speaker mining by 68%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-162"
  },
  "yang07c_interspeech": {
   "authors": [
    [
     "Hao",
     "Yang"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Xianyu",
     "Zhao"
    ],
    [
     "Jian",
     "Zhao"
    ],
    [
     "Liang",
     "Lu"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": "Cluster adaptive training weights as features in SVM-based speaker verification",
   "original": "i07_2013",
   "page_count": 4,
   "order": 163,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "In this paper, we propose the use of cluster adaptive training (CAT) weights as features in support vector machine (SVM) based text-independent verification task. The speaker utterance is characterized by a vector of cluster weights, which are extracted during the cluster adaptive training process. The effects of the number of classes, which are obtained by partitioning the components of the model, and the number of clusters on the verification performance are investigated. To remove session variability due to influences of microphone, environment, etc, Nuisance Attribute Projection (NAP) is also evaluated. Experimental results in a NIST SRE 2006 task show that this CAT weights SVM system achieves comparable performance to a state-of-the-art cepstral GMM-UBM verification system, and their fusion can give further performance gains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-163"
  },
  "okamoto07_interspeech": {
   "authors": [
    [
     "Hideki",
     "Okamoto"
    ],
    [
     "Mariko",
     "Kojima"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Study on speaker verification with non-audible murmur segments",
   "original": "i07_2017",
   "page_count": 4,
   "order": 164,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "We investigated a speaker verification method that uses non-audible murmur (NAM) segments using newly collected data and obtained several findings that will be useful when speaker verification systems are made in practice. NAM is recorded using a special microphone placed on the surface of the body, so it includes almost no external noise and is hard for other people to hear. By utilizing these properties, we have already reported a text-dependent method using NAM segments that can use a keyword phrase safely. This paper extends the examination with newly collected data consisting of NAM uttered by 18 male and 9 female imposter speakers and by 18 male and 10 female customer speakers. Experiments with various numbers of training utterances and sessions show that it is effective to use data recorded in multiple sessions. We also investigated the minimum number of training utterances needed in our method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-164"
  },
  "lu07_interspeech": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Dimension reduction for speaker identification based on mutual information",
   "original": "i07_2021",
   "page_count": 4,
   "order": 165,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "Dimension reduction is a necessary step for speech feature extraction in a speaker identification system. Discrete Cosine Transform (DCT) or Principal Component Analysis (PCA) is widely used for dimension reduction. By choosing basis vectors from basis vector pool of DCT or PCA which contribute more to data distribution variance or reconstruction accuracy of speech data set, we can transform the data set by projecting them on to the selected basis vectors. However, keeping the maximum distribution variance or high reconstruction accuracy does not guarantee the optimal keeping of high speaker discriminative information. In this paper, we proposed a basis vector selection method based on mutual information concept which guarantees the keeping of high speaker discriminative information. The mutual information is used to measure the dependency between the features extracted using basis vectors and speaker class labels. The high mutual information related basis vectors are chosen for feature extraction. Considering one speaker feature may be encoded in more than one basis vectors, we proposed to use joint mutual information concept which takes the dependency between feature variables into consideration. Based on the selected basis vectors from DCT or PCA basis vector pool, we extracted features for speaker identification experiments. Experimental results showed that the speaker identification error rate using proposed feature was reduced 11% and 8% on average for DCT and PCA based features respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-165"
  },
  "lindh07_interspeech": {
   "authors": [
    [
     "Jonas",
     "Lindh"
    ],
    [
     "Anders",
     "Eriksson"
    ]
   ],
   "title": "Robustness of long time measures of fundamental frequency",
   "original": "i07_2025",
   "page_count": 4,
   "order": 166,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "In many speech technology based applications as well as in forensic phonetics it is desirable to obtain reliable estimates of a speaker's fundamental frequency. We would like the measures to be accurate and reliable enough in order to be used meaningfully as a parameter in speaker identification or verification. Under optimal conditions such as when high quality studio recordings and normal speech styles are used this is often possible. In real life applications such conditions are the exception rather than the rule. The study presented here reports the result from an investigation where different measures were tested on speech material that varied with respect to speaking style, vocal effort and recording quality. Based on the results from these tests we would like to suggest a measure we call the alternative fundamental frequency baseline as the measure that is most robust with respect to the above-mentioned sources of variation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-166"
  },
  "prakash07_interspeech": {
   "authors": [
    [
     "Vinod",
     "Prakash"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Score distribution scaling for speaker recognition",
   "original": "i07_2029",
   "page_count": 4,
   "order": 167,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "In this study, we transform the verification scores of a speaker recognition system in order to standardize the imposter score distribution, this facilitates setting of a speaker-independent threshold at desired False Alarm (FA) rates. Impostor score distributions are estimated using GMMs, and a univariate Gaussianization [1] transform (which is a monotonically increasing mapping) is applied on the scores. It is shown that if a monotonically increasing mapping is used, the Probability of correct detection for a given setting of the FA is maintained as before. Hence, the proposed technique performs distribution scaling without affecting the False Alarm to False Reject relationship of the original test statistic. The maximum (relative) mismatch between the obtained and desired False Alarm rates is less than 10% for a wide range of False Alarm rates. When compared to modeling the imposter score distributions using a single Gaussian (Z-norm case), the overall relative mismatch is reduced by an average of 30%. While the application focus is on speaker recognition, the proposed technique can be used for other binary speech classification tasks as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-167"
  },
  "morris07_interspeech": {
   "authors": [
    [
     "A. C.",
     "Morris"
    ],
    [
     "J.",
     "Koreman"
    ],
    [
     "B.",
     "Ly-Van"
    ],
    [
     "H.",
     "Sellahewa"
    ],
    [
     "S.",
     "Jassim"
    ],
    [
     "R. Llarena",
     "Gómez"
    ]
   ],
   "title": "Global features for rapid identity verification with dynamic biometric data",
   "original": "i07_2033",
   "page_count": 4,
   "order": 168,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "Some of the biometrics used in identity verification, such as face, iris or fingerprint, have small fixed size pattern vectors which can easily be stored for rapid scoring. Important dynamic biometrics such as voice and signature, however, have much larger and variable sized feature vectors which require far greater verification computation. Such dynamic biometrics cannot therefore normally be used where verification must be performed rapidly on a device with little memory and very low speed. In this paper we compare the verification speed and accuracy obtained using different small fixed size global feature representations with that obtained, for voice and signature, using state of the art techniques with the full dynamic feature matrix. Using the best of these techniques we were able to reduce the multimodal processing time on a PDA SIM card, combining voice, signature and face biometrics, from one hour to ten seconds, while retaining a reduced but useful level of verification accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-168"
  },
  "pham07_interspeech": {
   "authors": [
    [
     "Tuan Van",
     "Pham"
    ],
    [
     "Michael",
     "Neffe"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "Robust voice activity detection for narrow-bandwidth speaker verification under adverse environments",
   "original": "i07_2037",
   "page_count": 4,
   "order": 169,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "We describe a voice activity detection algorithm which leads to significant improvement of a narrow-bandwidth speaker verification system under harsh environments. This algorithm is based on a time-scale feature which is extracted from wavelet subbands. A statistical quantile filtering technique is proposed to estimate an adaptive noise threshold. A hang-over scheme is then applied to bridge short pauses between speech frames. This optimized voice activity detector is embedded in the front-end unit of the narrow-bandwidth speaker verification system. The proposed algorithm is evaluated by objective tests on band-pass filtered utterances from the TIMIT database which was artificially corrupted by different additive noise types. Furthermore, it is tested with band-pass filtered SPEECHDAT-AT and WSJ0 databases in terms of speaker verification rates. This algorithm shows its superiority in performance due to the robust time-scale feature and the adaptive threshold.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-169"
  },
  "huenupan07_interspeech": {
   "authors": [
    [
     "Fernando",
     "Huenupán"
    ],
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Carlos",
     "Molina"
    ],
    [
     "Claudio",
     "Garreton"
    ]
   ],
   "title": "Speaker verification with multiple classifier fusion using Bayes based confidence measure",
   "original": "i07_2041",
   "page_count": 4,
   "order": 170,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "A novel framework based on Bayes-based confidence measure (BBCM) for Multiple Classifier System (MCS) fusion is proposed. As shown here, BBCM based MCS combination scheme corresponds to the ordinary Bayes fusion weighted by the reliability of each individual classifier. BBCM provides a formal model for heuristic weighting functions employed elsewhere. When compared with the ordinary Bayesian fusion, the proposed method leads to reductions as high as 20% and 50% in EER and the area below the ROC curve, respectively, in speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-170"
  },
  "chetty07_interspeech": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Audiovisual speaker identity verification based on lip motion features",
   "original": "i07_2045",
   "page_count": 4,
   "order": 171,
   "p1": "2045",
   "pn": "2048",
   "abstract": [
    "In this paper, we propose the fusion of audio and explicit lip motion features for speaker identity verification applications. Experimental results using GMM-based speaker models indicate that audiovisual fusion with explicit lip motion information provides significant performance improvement for verifying both the speaker identity and the liveness, due to tracking of the closely coupled acoustic labial dynamics. Experiments performed on different gender specific subsets of data from the VidTIMIT and UCBN databases under clean and noisy conditions show that the best performance of 7%-11% EER is achieved for the speaker verification task and 4%-8% EER for the liveness verification scenario.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-171"
  },
  "tur07_interspeech": {
   "authors": [
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Sachin",
     "Kajarekar"
    ]
   ],
   "title": "Duration and pronunciation conditioned lexical modeling for speaker verification",
   "original": "i07_2049",
   "page_count": 4,
   "order": 172,
   "p1": "2049",
   "pn": "2052",
   "abstract": [
    "We propose a method to improve speaker recognition lexical model performance using acoustic-prosodic information. More specifically, the lexical model is trained using duration- and pronunciation-conditioned word N-grams, simultaneously modeling lexical information along with their acoustic and prosodic characteristics. Support vector machines are used for modeling and scoring, with N-gram frequency vectors serving as features. Experimental results using NIST Speaker Recognition Evaluation data sets show that this method outperforms the regular word N-gram-based lexical models. Furthermore, our approach gives additional information when combined with a high-accuracy acoustic speaker model. We believe that this is a promising step toward integrated speaker recognition models that combine multiple types of high-level features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-172"
  },
  "bonastre07_interspeech": {
   "authors": [
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Corinne",
     "Fredouille"
    ]
   ],
   "title": "Artificial impostor voice transformation effects on false acceptance rates",
   "original": "i07_2053",
   "page_count": 4,
   "order": 173,
   "p1": "2053",
   "pn": "2056",
   "abstract": [
    "This paper investigates the effect of a transfer function-based voice transformation on automatic speaker recognition system performance. We focus on increasing the impostor acceptance rate, by modifying the voice of an impostor in order to target a specific speaker. This paper follows previous works where we demonstrate that, if someone has a knowledge on the speaker recognition method used, it is possible to impersonate a given speaker, in the view of this speaker recognition method. In this paper we extend the previous work by relaxing the needed knowledge on the targeted speaker recognition system. The results show that the voice transformation allows a drastic increase of the false acceptance rate, without damaging the natural perception of the voice, and without needing a large knowledge on the targeted speaker recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-173"
  },
  "miller07_interspeech": {
   "authors": [
    [
     "David R. H.",
     "Miller"
    ],
    [
     "Michael",
     "Kleber"
    ],
    [
     "Chia-Lin",
     "Kao"
    ],
    [
     "Owen",
     "Kimball"
    ],
    [
     "Thomas",
     "Colthurst"
    ],
    [
     "Stephen A.",
     "Lowe"
    ],
    [
     "Richard M.",
     "Schwartz"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "Rapid and accurate spoken term detection",
   "original": "i07_0314",
   "page_count": 4,
   "order": 174,
   "p1": "314",
   "pn": "317",
   "abstract": [
    "We present a state-of-the-art system for performing spoken term detection on continuous telephone speech in multiple languages. The system compiles a search index from deep word lattices generated by a large-vocabulary HMM speech recognizer. It estimates word posteriors from the lattices and uses them to compute a detection threshold that minimizes the expected value of a user-specified cost function. The system accommodates search terms outside the vocabulary of the speech-to-text engine by using approximate string matching on induced phonetic transcripts. Its search index occupies less than 1Mb per hour of processed speech and it supports sub-second search times for a corpus of hundreds of hours of audio. This system had the highest reported accuracy on the telephone speech portion of the 2006 NIST Spoken Term Detection evaluation, achieving 83% of the maximum possible accuracy score in English.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-174"
  },
  "pan07b_interspeech": {
   "authors": [
    [
     "Yi-cheng",
     "Pan"
    ],
    [
     "Hung-lin",
     "Chang"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Subword-based position specific posterior lattices (s-PSPL) for indexing speech information",
   "original": "i07_0318",
   "page_count": 4,
   "order": 175,
   "p1": "318",
   "pn": "321",
   "abstract": [
    "Position Specific Posterior Lattices (PSPL) have been recently proposed as very powerful, compact structures for indexing speech. In this paper, we take PSPL one step further to Subword-based Position Specific Posterior Lattices (S-PSPL). As with PSPL, we include posterior probabilities and proximity information, but we base this information on subword units rather than words. The advantages of S-PSPL over PSPL mainly come from rare and/or OOV words, which may be included in S-PSPL but generally are not in PSPL. Experiments on Mandarin Chinese broadcast news showed significant improvements from S-PSPL as compared to PSPL. Such advantages are believed to be language independent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-175"
  },
  "merkel07_interspeech": {
   "authors": [
    [
     "Andreas",
     "Merkel"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Improved methods for language model based question classification",
   "original": "i07_0322",
   "page_count": 4,
   "order": 176,
   "p1": "322",
   "pn": "325",
   "abstract": [
    "In this paper, we propose a language model based approach to classify user questions in the context of question answering systems. As categorization paradigm, a Bayes classifier is used to determine a corresponding semantic class. We present experiments with state-of-the-art smoothing methods as well as with some improved language models. Our results indicate that the techniques proposed here provide performance superior to the standard methods, including support vector machines.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-176"
  },
  "akiba07_interspeech": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Hirofumi",
     "Tsujimura"
    ]
   ],
   "title": "Error-tolerant question answering for spoken documents",
   "original": "i07_0326",
   "page_count": 4,
   "order": 177,
   "p1": "326",
   "pn": "329",
   "abstract": [
    "This paper proposes an error-tolerant question answering method for spoken documents. Though the question answering system for written documents can be directly applied to the transcribed spoken documents by using a LVCSR system, the recognition errors significantly degrade the QA performance. Especially, it is often the case that the answer itself is miss-recognized and in that case it becomes quite difficult to find the answer. To cope with such a problem, instead of conventional NE extraction, the proposed method utilizes named entity detection that decides only whether a section of speech, i.e. an utterance, contains named entities of a specific type. Because the NE detection is much easier task and utilized wider context than the NE extraction, it is expected to work robustly for erroneous transcribed speech data. The experimental results showed that the proposed method outperformed the baseline methods with respect to the spoken document with recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-177"
  },
  "hakkanitur07_interspeech": {
   "authors": [
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Michael",
     "Levit"
    ]
   ],
   "title": "Exploiting information extraction annotations for document retrieval in distillation tasks",
   "original": "i07_0330",
   "page_count": 4,
   "order": 178,
   "p1": "330",
   "pn": "333",
   "abstract": [
    "Information distillation aims to extract relevant pieces of information related to a given query from massive, possibly multilingual, audio and textual document sources. In this paper, we present our approach for using information extraction annotations to augment document retrieval for distillation. We take advantage of the fact that some of the distillation queries can be associated with annotation elements introduced for the NIST Automatic Content Extraction (ACE) task. We experimentally show that using the ACE events to constrain the document set returned by an information retrieval engine significantly improves the precision at various recall rates for two different query templates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-178"
  },
  "thambiratnam07_interspeech": {
   "authors": [
    [
     "K.",
     "Thambiratnam"
    ],
    [
     "F.",
     "Seide"
    ]
   ],
   "title": "Learning spoken document similarity and recommendation using supervised probabilistic latent semantic analysis",
   "original": "i07_0334",
   "page_count": 4,
   "order": 179,
   "p1": "334",
   "pn": "337",
   "abstract": [
    "This paper presents a model-based approach to spoken document similarity called Supervised Probabilistic Latent Semantic Analysis (PLSA). The method differs from traditional spoken document similarity techniques in that it allows similarity to be learned rather than approximated. The ability to learn similarity is desirable in applications such as Internet video recommendation, in which complex relationships like user-preference or speaking style need to be predicted. The proposed method exploits prior knowledge of document relationships to learn similarity. Experiments on broadcast news and Internet video corpora yielded 16.2% and 9.7% absolute mAP gains over traditional PLSA. Additionally, a cascaded Supervised+Discriminative PLSA system achieved a 3.0% absolute mAP gain over a Discriminative PLSA system, demonstrating the complementary nature of Supervised and Discriminative PLSA training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-179"
  },
  "wallace07_interspeech": {
   "authors": [
    [
     "Roy",
     "Wallace"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "A phonetic search approach to the 2006 NIST spoken term detection evaluation",
   "original": "i07_2385",
   "page_count": 4,
   "order": 180,
   "p1": "2385",
   "pn": "2388",
   "abstract": [
    "This paper details the submission from the Speech and Audio Research Lab of Queensland University of Technology (QUT) to the inaugural 2006 NIST Spoken Term Detection Evaluation. The task involved accurately locating the occurrences of a specified list of English terms in a given corpus of broadcast news and conversational telephone speech. The QUT system uses phonetic decoding and Dynamic Match Lattice Spotting to rapidly locate search terms, combined with a neural network-based verification stage. The use of phonetic search means the system is open vocabulary and performs usefully (Actual Term-Weighted Value of 0.23) whilst avoiding the cost of a large vocabulary speech recognition engine.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-180"
  },
  "itoh07_interspeech": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Kohei",
     "Iwata"
    ],
    [
     "Kazunori",
     "Kojima"
    ],
    [
     "Masaaki",
     "Ishigame"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shi-wook",
     "Lee"
    ]
   ],
   "title": "An integration method of retrieval results using plural subword models for vocabulary-free spoken document retrieval",
   "original": "i07_2389",
   "page_count": 4,
   "order": 181,
   "p1": "2389",
   "pn": "2392",
   "abstract": [
    "Spoken document retrieval (SDR) systems must be vocabulary-free in order to deal with arbitrary query words because a user often searches the section where a query word is spoken, and query words are liable to be special terms that are not included in a speech recognizer's dictionary. We have previously proposed new subword models, such as the 1/2 phone model, the 1/3 phone model, and the sub-phonetic segment (SPS) model, and have confirmed the effectiveness of these models for SDR [1]. These models are more sophisticated on the time axis than phoneme models such as the triphone model. The present paper proposes an integration method of plural retrieval results that are obtained from each subword model and demonstrates the performance improvement through experiments using an actual presentation speech corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-181"
  },
  "vergyri07_interspeech": {
   "authors": [
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Ramana R.",
     "Gadde"
    ],
    [
     "Murat",
     "Akbacak"
    ],
    [
     "Brian",
     "Roark"
    ],
    [
     "Wen",
     "Wang"
    ]
   ],
   "title": "The SRI/OGI 2006 spoken term detection system",
   "original": "i07_2393",
   "page_count": 4,
   "order": 182,
   "p1": "2393",
   "pn": "2396",
   "abstract": [
    "This paper describes the system developed jointly at SRI and OGI for participation in the 2006 NIST Spoken Term Detection (STD) evaluation. We participated in the three genres of the English track: Broadcast News (BN), Conversational Telephone Speech (CTS), and Conference Meetings (MTG). The system consists of two phases. First, audio indexing, an offline phase, converts the input speech waveform into a searchable index. Second, term retrieval, possibly an online phase, returns a ranked list of occurrences for each search term. We used a word-based indexing approach, obtained with SRI's large vocabulary Speech-to-Text (STT) system.\n",
    "Apart from describing the submitted system and its performance on the NIST evaluation metric, we study the tradeoffs between performance and system design. We examine performance versus indexing speed, effectiveness of different index ranking schemes on the NIST score, and the utility of approaches to deal with out-of-vocabulary (OOV) terms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-182"
  },
  "goto07_interspeech": {
   "authors": [
    [
     "Masataka",
     "Goto"
    ],
    [
     "Jun",
     "Ogata"
    ],
    [
     "Kouichirou",
     "Eto"
    ]
   ],
   "title": "Podcastle: a web 2.0 approach to speech recognition research",
   "original": "i07_2397",
   "page_count": 4,
   "order": 183,
   "p1": "2397",
   "pn": "2400",
   "abstract": [
    "In this paper, we describe a public web service, \"PodCastle\", that provides full-text searching of Japanese podcasts on the basis of automatic speech recognition. This is an instance of our research approach, \" Speech Recognition Research 2.0\", which is aimed at providing users with a web service based on Web 2.0 so that they can experience state-of-the-art speech recognition performance, and at promoting speech recognition technologies in cooperation with anonymous users. PodCastle enables users to find podcasts that include a search term, read full texts of their recognition results, and easily correct recognition errors. The results of the error correction can then be used to improve the performance of both full-text search and speech recognition. Although we know of no state-of-the-art speech recognizer that can successfully transcribe all of the various kinds of podcasts, the mechanism we propose will gradually increase the usefulness and applicability of PodCastle.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-183"
  },
  "camelin07_interspeech": {
   "authors": [
    [
     "Nathalie",
     "Camelin"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Speech mining in noisy audio message corpus",
   "original": "i07_2401",
   "page_count": 4,
   "order": 184,
   "p1": "2401",
   "pn": "2404",
   "abstract": [
    "Within the framework of automatic analysis of spoken telephone surveys we propose a robust Speech Mining strategy that selects, from a large database of spoken messages, the ones likely to be correctly processed by the Automatic Speech Recognition and Classification processes. The problem considered in this paper is the analysis of messages uttered by the users of a telephone service in response to a recorded message that asks if a problem they had was satisfactorily solved. Very often in these cases, subjective information is combined with factual information. The purpose of this type of analysis is the extraction of the distribution of users opinions. Therefore it is very important to check the representativeness of the subset of messages kept by the rejection strategies. Several measures, based on the Kullback-Leibler divergence, are proposed in order to evaluate the correctness of the information extracted as well as its representativeness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-184"
  },
  "shao07_interspeech": {
   "authors": [
    [
     "Jian",
     "Shao"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Zhaojie",
     "Liu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "A fast fuzzy keyword spotting algorithm based on syllable confusion network",
   "original": "i07_2405",
   "page_count": 4,
   "order": 185,
   "p1": "2405",
   "pn": "2408",
   "abstract": [
    "This paper presents a fast fuzzy search algorithm to extract keyword candidates from syllable confusion networks (SCNs) in Mandarin spontaneous speech. Since the recognition accuracy of spontaneous speech is quite poor, syllable confusion matrix (SCM) is applied to compensate for the recognition errors and to improve recall. For fast retrieval, an efficient vocabulary-independent index structure is designed, which selects individual arcs of syllable confusion network as indexing units. An inverted search algorithm that uses syllable confusion matrix to calculate relevance score and search in this index structure is proposed. In experiments performed on a telephone conversational task, the equal error rate (EER) was reduced by about 33% relative over the baseline where keywords are directly extracted from phoneme lattices. Additionally, it only took computer one or two seconds to search 100 keywords in one hour speech data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-185"
  },
  "kim07b_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Advances in speechfind: transcript reliability estimation employing confidence measure based on discriminative sub-word model for SDR",
   "original": "i07_2409",
   "page_count": 4,
   "order": 186,
   "p1": "2409",
   "pn": "2412",
   "abstract": [
    "This study presents our recent advances in our spoken document retrieval (SDR) system SpeechFind including our partnership with the Collaborative Digitization Program (CDP). A proto-type of SpeechFind for the CDP is currently serving as the search engine for 1,300 hours of the CDP audio content. These audio corpus of spoken document possess a wide range of conditions which make speech recognition challenging for reliable transcripts. In this paper, a reliability estimation method for the ASR-generated transcripts is proposed to provide more effective retrieval information for SpeechFind. The proposed estimator is based on Bayesian classification employing several confidence measures. We also propose a novel confidence measure for reliability estimation employing acoustically discriminative sub-word models. Experimental results on CDP material demonstrate that the proposed confidence measure is effective in improving the reliability estimator. By employing the proposed confidence measure based on discriminative model, 10.5% and 20.9% relative improvements were obtained in accuracy and critical error respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-186"
  },
  "favre07_interspeech": {
   "authors": [
    [
     "Benoit",
     "Favre"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Patrice",
     "Bellot"
    ]
   ],
   "title": "An interactive timeline for speech database browsing",
   "original": "i07_2413",
   "page_count": 4,
   "order": 187,
   "p1": "2413",
   "pn": "2416",
   "abstract": [
    "Speech databases lack efficient interfaces to explore information along time. We introduce an interactive timeline that helps the user in browsing an audio stream on a large time scale and recontextualize targeted information. Time can be explored at different granularities using synchronized scales. We try to take advantage of automatic transcription to generate a conceptual structure of the database. The timeline is annotated with two elements to reflect the information distribution relevant to a user need. Information density is computed using an information retrieval model and displayed as a continuous shade on the timeline whereas anchorage points are expected to provide a stronger structure and to guide the user through his exploration. These points are generated using an extractive summarization algorithm. We present a prototype implementing the interactive timeline to browse broadcast news recordings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-187"
  },
  "yip07_interspeech": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ]
   ],
   "title": "Spoken word recognition of Chinese homophones: a further investigation",
   "original": "i07_0362",
   "page_count": 4,
   "order": 188,
   "p1": "362",
   "pn": "365",
   "abstract": [
    "A cross-modal naming experiment was conducted to examine the effects of context and other lexical information in the processing of Chinese homophones during spoken language comprehension. In this experiment, listeners named aloud a visual probe as fast as they could, at a pre-designated point upon hearing the sentence, which ended with a spoken Chinese homophone. Results further support that prior context has an early effect on the disambiguation of various homophonic meanings, shortly after the acoustic onset of the word. Second, context interacts with frequency of the individual meanings of a homophone during lexical access. Finally, the present results pattern is clearly consistent with the context-dependency hypothesis that selection of the appropriate meaning of an ambiguous word depends on the simultaneous interaction of both sentential and lexical information during lexical access.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-188"
  },
  "wolters07_interspeech": {
   "authors": [
    [
     "Maria",
     "Wolters"
    ],
    [
     "Pauline",
     "Campbell"
    ],
    [
     "Christine",
     "DePlacido"
    ],
    [
     "Amy",
     "Liddell"
    ],
    [
     "David",
     "Owens"
    ]
   ],
   "title": "The role of outer hair cell function in the perception of synthetic versus natural speech",
   "original": "i07_0366",
   "page_count": 4,
   "order": 189,
   "p1": "366",
   "pn": "369",
   "abstract": [
    "Hearing loss as assessed by pure-tone audiometry (PTA) is significantly correlated with the intelligibility of synthetic speech. However, PTA is a subjective audiological measure that assesses the entire auditory pathway and does not discriminate between the different afferent and efferent contributions. In this paper, we focus on one particular aspect of hearing that has been shown to correlate with hearing loss: outer hair cell (OHC) function. One role of OHCs is to increase sensitivity and frequency selectivity. This function of OHCs can be assessed quickly and objectively through otoacoustic emissions (OAE) testing, which is little known outside the field of audiology. We find that OHC function affects the perception of human speech, but not that of synthetic speech. This has important implications not just for audiological and electrophysiological research, but also for adapting speech synthesis to ageing ears.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-189"
  },
  "kusumoto07_interspeech": {
   "authors": [
    [
     "Akiko",
     "Kusumoto"
    ],
    [
     "Alexander B.",
     "Kain"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Hybridizing conversational and clear speech",
   "original": "i07_0370",
   "page_count": 4,
   "order": 190,
   "p1": "370",
   "pn": "373",
   "abstract": [
    "\"Clear\" ( clr) speech is a speaking style that speakers adopt to be understood correctly in a difficult communication environment. Studies have shown that clr speech, as opposed to \"conversational\" ( cnv) speech, has significantly higher intelligibility in various conditions. While many differences in acoustic features have been identified, it is not known which individual feature or combinations of features cause the higher intelligibility of clr speech. The objectives of the current study are to examine whether it is possible to improve speech intelligibility by approximating clr speech features and to determine which acoustic features contribute to intelligibility. Our approach creates speech samples that combine acoustic features of cnv and clr speech, using a hybridization algorithm. Results with normal-hearing listeners showed significant sentence-level intelligibility improvements of 11-23% over cnv speech when replacing certain acoustic features with those from clr speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-190"
  },
  "dufour07_interspeech": {
   "authors": [
    [
     "Sophie",
     "Dufour"
    ],
    [
     "Ulrich Hans",
     "Frauenfelder"
    ]
   ],
   "title": "Neighborhood density and neighborhood frequency effects in French spoken word recognition",
   "original": "i07_0374",
   "page_count": 4,
   "order": 191,
   "p1": "374",
   "pn": "377",
   "abstract": [
    "According to activation-based models of spoken word recognition, words with many and high frequency neighbors are processed more slowly than words with few and low frequency neighbors. Because empirical support for inhibitory neighborhood effects comes mainly from studies conducted in English, the effects of neighborhood density and neighborhood frequency were examined in French. As typically observed in English, we found that words residing in dense neighborhoods are recognized more slowly than words residing in sparse neighborhoods. Moreover, we showed that words with higher frequency neighbors are processed more slowly than words with no higher frequency neighbors. Implications of theses results for spoken word recognition are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-191"
  },
  "irino07_interspeech": {
   "authors": [
    [
     "Toshio",
     "Irino"
    ],
    [
     "Yoshie",
     "Aoki"
    ],
    [
     "Yoshie",
     "Hayashi"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Roy D.",
     "Patterson"
    ]
   ],
   "title": "Discrimination and recognition of scaled word sounds",
   "original": "i07_0378",
   "page_count": 4,
   "order": 192,
   "p1": "378",
   "pn": "381",
   "abstract": [
    "Smith et al. [2] and Ives et al. [3] demonstrated that humans could extract information about the size of a speaker's vocal tract from speech sounds (vowels and syllables, respectively). We have extended their discrimination and recognition experiments to naturally pronounced words. The Just Noticeable Difference (JND) for size discrimination was between 5.5% and 19% depending on the listener. The smallest JND is comparable to that of the syllable experiments; the average JND is comparable to that of the vowel experiments. The word recognition scores remain above 50% for speaker sizes beyond the normal range for humans. The fact that good performance extends over such a large range of acoustic scales supports Irino and Patterson's hypothesis [1] that the auditory system segregates size and shape information at an early stage in the processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-192"
  },
  "toth07_interspeech": {
   "authors": [
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "Benchmarking human performance on the acoustic and linguistic subtasks of ASR systems",
   "original": "i07_0382",
   "page_count": 4,
   "order": 193,
   "p1": "382",
   "pn": "385",
   "abstract": [
    "Many believe that comparisons of machine and human speech recognition could help determine both the room for and the direction of improvement for speech recognizers. Yet, such experiments are made quite rarely or over such complex domains where instructive conclusions are hard to draw. In this paper we attempt to measure human performance on the tasks of the acoustic and language models of ASR systems separately. To simulate the task of acoustic decoding, subjects were instructed to phonetically transcribe short nonsense sentences. Here, besides the well-known superior segment classification, we also observed a good performance in word segmentation. To imitate higher-level processing, the subjects had to correct deliberately corrupted texts. Here we found that humans can achieve a word accuracy of about 80% even when almost one third of the phonemes are incorrect, and that with word boundary position information the word error rate roughly halves.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-193"
  },
  "yang07d_interspeech": {
   "authors": [
    [
     "Lin",
     "Yang"
    ],
    [
     "Jianping",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Contributions of temporal fine structure cues to Chinese speech recognition in cochlear implant simulation",
   "original": "i07_0386",
   "page_count": 4,
   "order": 194,
   "p1": "386",
   "pn": "389",
   "abstract": [
    "This study evaluated the relative contributions of temporal fine structure cues in different frequency bands to Mandarin speech recognition both in quiet and in noise. Chinese tone, vowel, consonant and sentence recognition scores were measured in a 4-channel continuous interleaved sampling (CIS) simulation model with six kinds of carriers: all noise carriers (N1234), all fine structure carriers (F1234) and fine structure carrier in one channel while noise carriers in the others (F1N234, F2N134, F3N124, F4N123). Results showed that low-frequency fine structure below 400 Hz contributed significantly to tone recognition, while mid-frequency fine structure from 400 to 1000 Hz contributed most to vowel and consonant recognition in quiet. But in severe noise it was the common contributions of the temporal fine structure in each band that improved the recognition performance of vowel and consonant significantly. For sentence recognition tones contributed most compared to vowel and consonant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-194"
  },
  "wu07b_interspeech": {
   "authors": [
    [
     "Xihong",
     "Wu"
    ],
    [
     "Jing",
     "Chen"
    ],
    [
     "Zhigang",
     "Yang"
    ],
    [
     "Qiang",
     "Huang"
    ],
    [
     "Mengyuan",
     "Wang"
    ],
    [
     "Liang",
     "Li"
    ]
   ],
   "title": "Effect of number of masking talkers on speech-on-speech masking in Chinese",
   "original": "i07_0390",
   "page_count": 4,
   "order": 195,
   "p1": "390",
   "pn": "393",
   "abstract": [
    "In this study, targets were nonsense sentences spoken by a Chinese female, and maskers were nonsense sentences spoken by other 1, 2, 3, or 4 Chinese females. All stimuli were presented by two spatially separated loudspeakers. Using the precedence effect, manipulation of the delay between the two loudspeakers for the masker determined whether the target and masker were perceived as coming from the same or different locations. The results show that the masking effect remarkably increased with the number of masking talkers increased progressively from 1 to 4, which is also confirmed by the calculation of the speech intelligibility index. However, the perceived spatial separation, which predominantly reduced informational masking, caused the largest improvement in speech identification with the two-talker masker, indicating that two-voice speech had the highest informational masking impact. Some differences between Chinese speech masking and English speech masking were discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-195"
  },
  "bagou07_interspeech": {
   "authors": [
    [
     "Odile",
     "Bagou"
    ],
    [
     "Sophie",
     "Dufour"
    ],
    [
     "Cécile",
     "Fougeron"
    ],
    [
     "Alain",
     "Content"
    ],
    [
     "Ulrich Hans",
     "Frauenfelder"
    ]
   ],
   "title": "Do different boundary types induce subtle acoustic cues to which French listeners are sensitive?",
   "original": "i07_0394",
   "page_count": 4,
   "order": 196,
   "p1": "394",
   "pn": "397",
   "abstract": [
    "This paper examines the production of perception of three types of phonological boundaries. In the first part, we extended our previous acoustic analysis to confirm that French speakers mark word and syllables boundaries differently in enchaînement sequences. The durational properties of vowels and consonants were compared in 3 boundary conditions: (A) enchaînement (V1C#V2), (B) word-initial consonant (V1#CV2), (C) syllable onset consonant (V1.CV2). Results showed that the three boundary conditions are varying in subtle durational differences on V1 and C. In the second part, the sensitivity of French listeners to these acoustic cues was evaluated. Preliminary results showed that participants are sensitive to durational differences, at least for discriminating between syllable and word boundaries. Implications of these results for lexical segmentation are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-196"
  },
  "stadler07_interspeech": {
   "authors": [
    [
     "Svante",
     "Stadler"
    ],
    [
     "Arne",
     "Leijon"
    ],
    [
     "Björn",
     "Hagerman"
    ]
   ],
   "title": "An information theoretic approach to predict speech intelligibility for listeners with normal and impaired hearing",
   "original": "i07_0398",
   "page_count": 4,
   "order": 197,
   "p1": "398",
   "pn": "401",
   "abstract": [
    "A computational method to predict speech intelligibility in noisy environments has been developed. By modeling speech and noise as stochastic signals, the information transmission through a given auditory model can be estimated. Rate-distortion theory is then applied to predict speech recognition performance. Results are compared with subjective tests on normal and hearing impaired listeners. It is found that the method underestimates the supra-threshold deficits of hearing impairment, which is believed to be due to an overly simple auditory model and a small dictionary size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-197"
  },
  "wade07_interspeech": {
   "authors": [
    [
     "Travis",
     "Wade"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Speaking rate effects in a landmark-based phonetic exemplar model",
   "original": "i07_0402",
   "page_count": 4,
   "order": 198,
   "p1": "402",
   "pn": "405",
   "abstract": [
    "In this study we describe a model of speech perception in which neither speaking rate nor lower level temporal cues are considered explicitly. Instead, newly encountered speech signals are encoded as sequences of detailed acoustic events specified in real time at salient landmarks and compared directly with previously heard patterns. When presented with obstruent-vowel sequences occurring in the TIMIT database, the model performs similarly to humans in relying on temporal information for consonant and vowel recognition - and interpreting this information in a rate-dependent manner - when non-temporal cues are ambiguous; and by being adversely affected by local rate variability. These results indicate that compensation for speaking rate in human perception may follow implicitly from even modest knowledge of the robust correlations between temporal and other properties of individual speech events and those of their surrounding contexts, and do not require special normalization processes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-198"
  },
  "maniwa07_interspeech": {
   "authors": [
    [
     "Kazumi",
     "Maniwa"
    ],
    [
     "Allard",
     "Jongman"
    ],
    [
     "Travis",
     "Wade"
    ]
   ],
   "title": "Acoustic correlates of intelligibility enhancements in clearly produced fricatives",
   "original": "i07_0406",
   "page_count": 4,
   "order": 199,
   "p1": "406",
   "pn": "409",
   "abstract": [
    "Two experiments investigated whether and how clear speech production enhances intelligibility of English fricatives for normal-hearing listeners and listeners with simulated hearing impairment. Babble thresholds were measured for minimal pair distinctions. Clear speech benefited both groups overall; however, for impaired listeners, the clear speech effect held only for sibilant pairs. Correlation analyses comparing acoustic and perceptual data indicated that a shift of energy concentration toward higher frequency regions and greater source strength contributed to the clear speech effect for normal-hearing listeners, while listeners with simulated loss seemed to benefit mostly from cues involving lower frequency regions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-199"
  },
  "jurgens07_interspeech": {
   "authors": [
    [
     "Tim",
     "Jürgens"
    ],
    [
     "Thomas",
     "Brand"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Modelling the human-machine gap in speech reception: microscopic speech intelligibility prediction for normal-hearing subjects with an auditory model",
   "original": "i07_0410",
   "page_count": 4,
   "order": 200,
   "p1": "410",
   "pn": "413",
   "abstract": [
    "In this study speech intelligibility in noise for normal-hearing subjects is predicted by a model that consists of an auditory preprocessing and a speech recognizer. Using a highly systematic speech corpus of phoneme combinations (logatomes) allows the analysis of response rates and confusions of single phonemes. The predicted data is validated by listening tests using the same nonsense speech material. If testing utterances that are not identical to those in training material are used, the psychometric function in noise is predicted with an offset of 13 dB to higher signal-to-noise-ratios (SNR). This is consistent with the man-machine performance gap between human speech recognition (HSR) and automatic speech recognition (ASR) [1].\n",
    "However, this offset reduces to 4 dB in a second model design with identical recordings for training and testing. Furthermore predicted confusion matrices are compared to those of normal-hearing subjects with the second model design.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-200"
  },
  "ikeno07_interspeech": {
   "authors": [
    [
     "Ayako",
     "Ikeno"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Lombard speech impact on perceptual speaker recognition",
   "original": "i07_0414",
   "page_count": 4,
   "order": 201,
   "p1": "414",
   "pn": "417",
   "abstract": [
    "It is well known that stress and Lombard effect impact speech production. The goal in this study is to investigate how Lombard effect impacts perceptual speaker recognition. We report results from In-Set/Out-of-Set speaker identification (ID) tasks performed by human subjects with a comparison to automatic algorithms. The main trends show that mismatch in reference and test data causes a significant decrease in speaker ID accuracy. The results also indicate that Lombard speech contributes to higher accuracy for In-Set speaker ID, but interferes with correct detection of Out-of-Set speakers. In addition, it is observed that the mismatched conditions cause a higher false reject rate, and that the matched conditions result in higher false acceptance. We further discuss automated system performance in comparison to human performance. Overall observations suggest that deeper understanding of cognitive factors involved in perceptual speaker ID offers meaningful insights for further development of automatic systems and combined automatic-human based systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-201"
  },
  "goy07_interspeech": {
   "authors": [
    [
     "Huiwen",
     "Goy"
    ],
    [
     "Kathleen",
     "Pichora-Fuller"
    ],
    [
     "Pascal van",
     "Lieshout"
    ],
    [
     "Gurjit",
     "Singh"
    ],
    [
     "Bruce",
     "Schneider"
    ]
   ],
   "title": "Effect of within- and between-talker variability on word identification in noise by younger and older adults",
   "original": "i07_0418",
   "page_count": 4,
   "order": 202,
   "p1": "418",
   "pn": "421",
   "abstract": [
    "Talkers alter their speech in noisy environments yet most speech-in-noise testing uses materials recorded in quiet. Sentences from a common test (SPIN-R) were recorded by a new talker in different talking conditions and the original and new materials were used to test word identification accuracy in younger and older adults. Inter- and intra-talker differences affected performance. Intelligibility was better for materials heard in noise when the materials were spoken in noise, or when the talker was asked to speak loudly, especially for older listeners. The most likely acoustical explanation for the differences seems to be increased intensity and duration in the production of the sentence-final target words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-202"
  },
  "bunnell07_interspeech": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "N. Carolyn",
     "Schanen"
    ],
    [
     "Linda D.",
     "Vallino"
    ],
    [
     "Thierry G.",
     "Morlet"
    ],
    [
     "James B.",
     "Polikoff"
    ],
    [
     "Jennette D.",
     "Driscoll"
    ],
    [
     "James T.",
     "Mantell"
    ]
   ],
   "title": "Speech perception in children with speech sound disorder",
   "original": "i07_0422",
   "page_count": 4,
   "order": 203,
   "p1": "422",
   "pn": "425",
   "abstract": [
    "This paper describes preliminary results from an ongoing study designed to characterize acoustic-phonetic traits in children with speech delay (SD) of unknown origin. Here we present data on 13 SD children and their siblings (26 children in all) from two speech perception tasks: a two-alternative forced choice categorical perception (ID) task, and an error monitoring (EM) task. In the ID task, minimally differing words (e.g., cage - gauge) were used to create 9-step synthetic continua. For the EM task, children heard both correctly and incorrectly articulated words and indicated whether the word was correct or not. Some word tokens in this task were produced by the SD children identified as probands in this study. On both tasks, SD children performed more poorly than their non-SD siblings, showing more gradual slopes in their ID functions, and less accuracy in identifying correct versus error productions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-203"
  },
  "wang07c_interspeech": {
   "authors": [
    [
     "Huan",
     "Wang"
    ],
    [
     "Werner",
     "Hemmert"
    ]
   ],
   "title": "Speech coding and information processing by auditory neurons",
   "original": "i07_0426",
   "page_count": 4,
   "order": 204,
   "p1": "426",
   "pn": "429",
   "abstract": [
    "One fundamental difference between information processing in the auditory pathway and automatic speech recognition (ASR) systems lies in the coding and processing of nerve-action potentials. Spike trains code amplitude information by means of a rate-code but most information is carried by precise spike timing. In this paper we focus on neurons located in the ventral cochlear nucleus (VCN), which get direct input from primary auditory nerve fibers (ANF). We generate spike trains of the ANFs and VCN neurons with our inner ear model and calculate the transmitted information using a vowel as input stimulus. For ANFs, transmitted information is highest in the frequency range of 200-500 Hz, and decreases towards higher frequencies, due to the degrading temporal precision of the spikes. A single stellate neuron is able to transmit a large portion (up to 66%) of information transmitted by five of its innervating ANFs. Due to their slow membrane time constant the information rate decreases even faster with characteristic frequency (CF) compared to ANFs. The spectral information of sound signals is well reflected in the rate-place code of ANFs and VCN neurons, however, the major part of the information (about 90%) is carried by spike timing. We conclude that we should not neglect this fine-grained temporal information for automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-204"
  },
  "gilbert07_interspeech": {
   "authors": [
    [
     "Annie C.",
     "Gilbert"
    ],
    [
     "Victor J.",
     "Boucher"
    ]
   ],
   "title": "What do listeners attend to in hearing prosodic structures? investigating the human speech-parser using short-term recall",
   "original": "i07_0430",
   "page_count": 4,
   "order": 205,
   "p1": "430",
   "pn": "433",
   "abstract": [
    "This study examines how heard prosodic patterns are parsed by reference to a principle of focus of attention [1]. According to this principle, attention holds up to four items at once, and the same upper limit appears to apply to the number of syllables in rhythm groups [2]. On this basis it was predicted that in recalling heard prosodic structures, listeners would attend primarily to rhythm groups. 31 Ss were asked to recall the prosody of heard series of [pa] bearing various intonation groups and repetitive or varying rhythms. Exp. 1 showed how the focus of attention can shift when rhythm patterns are repetitive. However, Exp. 2 showed that listeners focus on rhythm when patterns vary (as in speech). The results bear implications on explaining the role of prosodic groups in speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-205"
  },
  "brungart07_interspeech": {
   "authors": [
    [
     "Douglas S.",
     "Brungart"
    ],
    [
     "Nandini",
     "Iyer"
    ]
   ],
   "title": "Time-compressed speech perception with speech and noise maskers",
   "original": "i07_1581",
   "page_count": 4,
   "order": 206,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "Many researchers have shown that speech signals can be time compressed (TC) by a factor of two or more without a significant loss in intelligibility. However, most previous studies with TC speech have been conducted either in quiet or, in a very small number of cases, with noise maskers. In this experiment, we examine the effect that TC has on the perception of a speech signal in the presence of a speech or noise masker. The results show that normal speech can be accelerated a modest amount (20-30%) without increased susceptibility to masking, but that higher TC ratios can lead to dramatically worse performance in the presence of an interfering sound. The results also indicate that time-expansion can, in some cases, lead to improved performance when a listener is attending to the quieter of two talkers in an auditory mixture. These results suggest that there are some important practical limitations on how TC should be used to enhance communications efficiency in auditory speech displays.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-206"
  },
  "cutler07_interspeech": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Maria Luisa Garcia",
     "Lecumberri"
    ],
    [
     "Dennis",
     "Pasveer"
    ]
   ],
   "title": "L2 consonant identification in noise: cross-language comparisons",
   "original": "i07_1585",
   "page_count": 4,
   "order": 207,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "The difficulty of listening to speech in noise is exacerbated when the speech is in the listener's L2 rather than L1. In this study, Spanish and Dutch users of English as an L2 identified American English consonants in a constant intervocalic context. Their performance was compared with that of L1 (British English) listeners, under quiet conditions and when the speech was masked by speech from another talker or by noise. Masking affected performance more for the Spanish listeners than for the L1 listeners, but not for the Dutch listeners, whose performance was worse than the L1 case to about the same degree in all conditions. There were, however, large differences in the pattern of results across individual consonants, which were consistent with differences in how consonants are identified in the respective L1s.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-207"
  },
  "le07_interspeech": {
   "authors": [
    [
     "Jennifer T.",
     "Le"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Michael D.",
     "Tyler"
    ],
    [
     "Christian",
     "Kroos"
    ]
   ],
   "title": "Effects of non-native dialects on spoken word recognition",
   "original": "i07_1589",
   "page_count": 4,
   "order": 208,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "The present study examined the premise that lexical information (top-down factors) interacts with phonetic detail (bottom-up, episodic traces) by assessing the impact of dialect variation and word frequency on spoken word recognition. Words were either spoken in the listeners' native dialect (Australian English: AU), or in one of two non-native English dialects differing in phonetic similarity to Australian: South African (SA: more similar) and Jamaican Mesolect (JA: less similar). It was predicted that low-frequency English words spoken in non-native dialects, especially the less similar dialect, would require more information to be recognised due to systematic phonological and/or phonetic differences from native-dialect versions. A gating task revealed that more gates were required for JA than SA dialect words, with this effect even more pronounced for low than high-frequency words. This suggests that recognition of words is contingent upon both detailed phonetic properties within the mental lexicon, as evident in the effects of goodness of fit between native and non-native dialect pronunciations, and on lexical information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-208"
  },
  "meyer07_interspeech": {
   "authors": [
    [
     "Julien",
     "Meyer"
    ],
    [
     "Fanny",
     "Meunier"
    ],
    [
     "Laure",
     "Dentel"
    ]
   ],
   "title": "Identification of natural whistled vowels by non-whistlers",
   "original": "i07_1593",
   "page_count": 4,
   "order": 209,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "Whistled speech consists of a phonetic emulation of the sounds produced in spoken voice. This style of speech is the result of the adaptation of the human productive and perceptive intelligence to a language behavior. In the typology of whistled forms of languages, Spanish is among the languages for which the whistled strategy emulates primarily segmental acoustic cues of vowels and consonants. The present study tests the perception of four Spanish whistled vowels by French non-whistlers. The results show that French non-whistlers were able to categorize these vowels without any learning, although not as accurately as native whistlers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-209"
  },
  "jesse07_interspeech": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Prelexical adjustments to speaker idiosyncrasies: are they position-specific?",
   "original": "i07_1597",
   "page_count": 4,
   "order": 210,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "Listeners use lexical knowledge to adjust their prelexical representations of speech sounds in response to the idiosyncratic pronunciations of particular speakers. We used an exposure-test paradigm to investigate whether this type of perceptual learning transfers across syllabic positions. No significant learning effect was found in Experiment 1, where exposure sounds were onsets and test sounds were codas. Experiments 2-4 showed that there was no learning even when both exposure and test sounds were onsets. But a trend was found when exposure sounds were codas and test sounds were onsets (Experiment 5). This trend was smaller than the robust effect previously found for the coda-to-coda case. These findings suggest that knowledge about idiosyncratic pronunciations may be position specific: Knowledge about how a speaker produces sounds in one position, if it can be acquired at all, influences perception of sounds in that position more strongly than of sounds in another position.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-210"
  },
  "mitterer07_interspeech": {
   "authors": [
    [
     "Holger",
     "Mitterer"
    ]
   ],
   "title": "Top-down effects on compensation for coarticulation are not replicable",
   "original": "i07_1601",
   "page_count": 4,
   "order": 211,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "Listeners use lexical knowledge to judge what speech sounds they heard. I investigated whether such lexical influences are truly top-down or just reflect a merging of perceptual and lexical constraints. This is achieved by testing whether the lexically determined identity of a phone exerts the appropriate context effects on surrounding phones. The current investigations focuses on compensation for coarticulation in vowel-fricative sequences, where the presence of a rounded vowel (/y/ rather than /i/) leads fricatives to be perceived as /s/ rather than /esh/. This results was consistently found in all three experiments. A vowel was also more likely to be perceived as rounded /y/ if that lead listeners to be perceive words rather than nonwords (Dutch: meny, English id. vs. meni nonword). This lexical influence on the perception of the vowel had, however, no consistent influence on the perception of following fricative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-211"
  },
  "igarashi07_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Igarashi"
    ]
   ],
   "title": "Pitch pattern alternation in goshogawara Japanese: evidence for a prosodic phrase above the domain for downstep",
   "original": "i07_0434",
   "page_count": 4,
   "order": 212,
   "p1": "434",
   "pn": "437",
   "abstract": [
    "The lexically accented words in Goshogawara Japanese can be realized in either of the two surface pitch patterns. The pitch pattern is said to alternate regularly, depending on the phrasing structure of an utterance. The organization of prosodic phrasing of this dialect, however, has been little investigated and thus it remains unclear what prosodic phrase functions as the domain for the alternation.\n",
    "This work determines the level of the phrase serving as the domain for the pitch pattern alternation. Is it hierarchically higher or lower than the domain for downstep? The experimental results reveal that the alternation does not take place at the prosodic boundary where downstep effect is blocked. The results provide evidence for the phrasing one-level above the downstep domain, whose existence was not evident in the model proposed for Tokyo Japanese.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-212"
  },
  "nesterenko07_interspeech": {
   "authors": [
    [
     "Irina",
     "Nesterenko"
    ],
    [
     "Pavel",
     "Skrelin"
    ]
   ],
   "title": "Some evidence on the phonetics and phonology of prosodic phrasing in Russian",
   "original": "i07_0438",
   "page_count": 4,
   "order": 213,
   "p1": "438",
   "pn": "441",
   "abstract": [
    "This paper treats the issue of prosodic segmentation into phrasing domains in Russian and is framed in the prosodic phonology paradigm. Distributions of prosodic boundaries are obtained in a perception experiment and the results are further explored to advance the hypotheses about the levels of prosodic constituency in Russian. The temporal organisation of the perceived domains and the eurhythmic constraints on phrasing are investigated. Particularly, the empirical data suggest that beyond the level of intonational units, there are two other levels, a level of metrical domain and one of phonological phrase, relevant in the perception of phrasing patterns.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-213"
  },
  "volin07_interspeech": {
   "authors": [
    [
     "Jan",
     "Volín"
    ],
    [
     "Radek",
     "Skarnitzl"
    ]
   ],
   "title": "Temporal downtrends in Czech read speech",
   "original": "i07_0442",
   "page_count": 4,
   "order": 214,
   "p1": "442",
   "pn": "445",
   "abstract": [
    "A possible existence of a regular temporal trend superimposed over the durational pattern of individual segments is explored in read continuous speech in the western Slavonic language of Czech. A short text read by 75 speakers was used to ascertain whether the contextually conditioned temporal variation would allow any phrasal tendencies to manifest. The data were normalized against the speakers' characteristics and against the intrinsic duration of individual phones. The results indicate that while the linear trendlines are regularly declining, the most reliable partial trend is phrase-final deceleration. Three more general non-linear trends are identified.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-214"
  },
  "cho07_interspeech": {
   "authors": [
    [
     "Hyongsil",
     "Cho"
    ],
    [
     "Daniel",
     "Hirst"
    ]
   ],
   "title": "Empirical evidence for prosodic phrasing: pauses as linguistic annotation in Korean read speech",
   "original": "i07_0446",
   "page_count": 4,
   "order": 215,
   "p1": "446",
   "pn": "449",
   "abstract": [
    "This paper looks at the relationship between acoustic cues and judgments of prosodic boundaries. It is argued that in read speech, the presence of a silent pause can generally be taken as an indication of the presence of a prosodic boundary although in spontaneous speech the presence of a silent pause is neither a necessary nor a sufficient condition for a prosodic boundary. Two experiments are described concerning Korean read speech. In the first, subjects were asked to say whether extracts of speech (filtered to make them unintelligible) were taken from the same or from two different sentences. The results confirmed that in the majority of cases the listeners do not rely on the presence of a silent pause since even when the pause has been removed the boundary is correctly predicted almost as well as when it has not been removed. In the second experiment a number of acoustic cues, temporal and frequential, were used to predict the distribution of pauses without reference to the silence itself.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-215"
  },
  "dreyer07_interspeech": {
   "authors": [
    [
     "Markus",
     "Dreyer"
    ],
    [
     "Izhak",
     "Shafran"
    ]
   ],
   "title": "Exploiting prosody for PCFGs with latent annotations",
   "original": "i07_0450",
   "page_count": 4,
   "order": 216,
   "p1": "450",
   "pn": "453",
   "abstract": [
    "We propose novel methods for integrating prosody in syntax using generative models. By adopting a grammar whose constituents have latent annotations, the influence of prosody on syntax can be learned from data. In one method, prosody is utilized to seed the latent annotations of a grammar which is then refined using EM iterations. In an orthogonal approach, we integrate prosody into grammar more explicitly using a model that jointly observes words and associated prosody. We evaluate the two methods by parsing speech data from the Switchboard corpus. The results are compared against baseline results from a model that does not use prosody. The experiments show that prosody improves a grammar in terms of accuracy as well as the parsimonious use of parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-216"
  },
  "shi07_interspeech": {
   "authors": [
    [
     "Qin",
     "Shi"
    ],
    [
     "DanNing",
     "Jiang"
    ],
    [
     "FanPing",
     "Meng"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Combining length distribution model with decision tree in prosodic phrase prediction",
   "original": "i07_0454",
   "page_count": 4,
   "order": 217,
   "p1": "454",
   "pn": "457",
   "abstract": [
    "In Text-to-Speech (TTS) systems, prosody phrase prediction is important for the naturalness and intelligibility of synthesized voice. Statistic methods, such as dynamic programming (DP), decision tree (DT), maximum entropy (ME), etc, have been considered for the task. Features based on syntactic and lexical information are widely used. However, the predicted prosody phrases are often observed to have unrealistic length due to the lack of length distribution modeling. This paper proposes a novel algorithm to incorporate the length distribution model in prosody phrase prediction. Rather than directly use phrase length as a feature of DT or ME, the algorithm exploits the correlation between the length and the possibility given by a decision tree. Experiments show that the recalling rate and precise rate are improved 16.37% and 14.05% relatively by using the proposed algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-217"
  },
  "yang07e_interspeech": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Duration and pauses as boundary-markers in speech: a cross-linguistic study",
   "original": "i07_0458",
   "page_count": 4,
   "order": 218,
   "p1": "458",
   "pn": "461",
   "abstract": [
    "Duration is an important feature both to achieve high-quality synthesis and as a marker of phrasal organization in speech. In this study, we investigate pauses and durational patterns in English and Mandarin spontaneous conversation, as well as how reliably such elements can serve as boundary-marking predictors cross-linguistically. Results show that pause duration is significantly correlated with specific boundary status and that syllable duration is inversely correlated with distance to phrase end in both English and Chinese. Our findings support the view that duration features are highly consistent and that it is useful to integrate such knowledge to enhance performance in interactive systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-218"
  },
  "yu07b_interspeech": {
   "authors": [
    [
     "Jian",
     "Yu"
    ],
    [
     "Lixing",
     "Huang"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Xia",
     "Wang"
    ]
   ],
   "title": "Modeling incompletion phenomenon in Mandarin dialog prosody",
   "original": "i07_0462",
   "page_count": 4,
   "order": 219,
   "p1": "462",
   "pn": "465",
   "abstract": [
    "The paper proposes a prosody generation method for dialog speech synthesis in Mandarin. The method is an extension of a prosody model for read speech and also takes the essential characteristic of dialog speech into account. Besides the faster speaking rate and narrower pitch range in dialog speech, our method concentrates on the more underlying and essential characteristic: the incompletion of pitch contour within a syllable and its impacts on adjacent syllables. To simulate this phenomenon, a CART-based method is constructed to predict whether a syllable is incomplete or not. Based on that, a prosody generation model which focuses on the prosody constraint between adjacent syllables is constructed, and this method can simulate the influence of incomplete syllable on adjacent syllables. Experiments show that the synthesized results based on that prosody model sound much natural and colloquial.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-219"
  },
  "tamm07_interspeech": {
   "authors": [
    [
     "Anne",
     "Tamm"
    ],
    [
     "Kálmán",
     "Abari"
    ],
    [
     "Gábor",
     "Olaszy"
    ]
   ],
   "title": "Accent assignment algorithm in Hungarian, based on syntactic analysis",
   "original": "i07_0466",
   "page_count": 4,
   "order": 220,
   "p1": "466",
   "pn": "469",
   "abstract": [
    "This article presents the results of the research aimed at developing an accent assignment system for Hungarian. Two methods are compared. The shallow method targets local and short-distance factors that determine accent; the deep (syntactic) method targets long-distance influences (such as focus). Neither of the methods alone results in absolutely satisfactory output; frequently, however, mistakes are complementary. The article presents the problems and solutions of both methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-220"
  },
  "lin07b_interspeech": {
   "authors": [
    [
     "Cheng-Yuan",
     "Lin"
    ],
    [
     "Pei-Chi",
     "Jao"
    ],
    [
     "J. -S. Roger",
     "Jang"
    ]
   ],
   "title": "An effective initial/final duration prediction method for corpus-based singing voice synthesis of Mandarin Chinese",
   "original": "i07_0470",
   "page_count": 4,
   "order": 221,
   "p1": "470",
   "pn": "473",
   "abstract": [
    "In this paper, we propose an effective method for predicting initial/final duration for corpus-based singing voice synthesis of Mandarin Chinese. The goal of the method is to improve the naturalness and clarity of the synthesized singing voices. To achieve this goal, we construct an individual initial/final (I/F) duration prediction model for each category of consonants. Support vector machine is used for duration prediction in each model. In order to achieve better accuracy, we use both linguistic/phonetic attributes and music-score information as the input features for the I/F duration prediction model. Experimental results demonstrate that the proposed method is effective in predicting the I/F duration for singing voice synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-221"
  },
  "nemeth07_interspeech": {
   "authors": [
    [
     "Géza",
     "Németh"
    ],
    [
     "Márk",
     "Fék"
    ],
    [
     "Tamás Gábor",
     "Csapó"
    ]
   ],
   "title": "Increasing prosodic variability of text-to-speech synthesizers",
   "original": "i07_0474",
   "page_count": 4,
   "order": 222,
   "p1": "474",
   "pn": "477",
   "abstract": [
    "The lack of prosody variation in text-to-speech systems contributes to their perceived unnaturalness when synthesizing extended passages. In this paper, we present a method to improve prosody generation in this direction. A database of natural sample sentences is searched for sentences having similar word and syllable structure to the input. One sentence is selected randomly from the similar sentences found. The prosody of the randomly selected natural sentence is used as a target to generate the prosody of the synthetic one. An experiment was conducted to determine the potential of the proposed method. The rule-based pitch contour generation of a Hungarian concatenative synthesizer was replaced by a semi-automatic implementation of the proposed method. A listening test showed that subjects preferred sentences synthesized by the proposed method over a rule-based solution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-222"
  },
  "lolive07_interspeech": {
   "authors": [
    [
     "Damien",
     "Lolive"
    ],
    [
     "Nelly",
     "Barbot"
    ],
    [
     "Olivier",
     "Boeffard"
    ]
   ],
   "title": "Unsupervised HMM classification of F0 curves",
   "original": "i07_0478",
   "page_count": 4,
   "order": 223,
   "p1": "478",
   "pn": "481",
   "abstract": [
    "This article describes a new unsupervised methodology to learn F0 classes using HMM models on a syllable basis. A F0 class is represented by a HMM with three emitting states. The clustering algorithm relies on an iterative gaussian splitting and EM retraining process. First, a single class is learnt on a training corpus (8000 syllables) and it is then divided by perturbing gaussian means of successive levels. At each step, the mean RMS error is evaluated on a validation corpus (3000 syllables). The algorithm stops automatically when the error becomes stable or increases. The syllabic structure of a sentence is the reference level we have taken for F0 modelling even if the methodology can be applied to other structures. Clustering quality is evaluated in terms of cross-validation using a mean of RMS errors between F0 contours on a test corpus and the estimated HMM trajectories. The results show a pretty good quality of the classes (mean RMS error around 4Hz).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-223"
  },
  "read07_interspeech": {
   "authors": [
    [
     "Ian",
     "Read"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Automatic pitch accent prediction for text-to-speech synthesis",
   "original": "i07_0482",
   "page_count": 4,
   "order": 224,
   "p1": "482",
   "pn": "485",
   "abstract": [
    "Determining pitch accents in a sentence is a key task for a text-to-speech (TTS) system. We describe some methods for pitch accent assignment which make use of features that contain information about a complete phrase or sentence, in contrast to most previous work which has focused on using features local to a syllable or word. Pitch accent prediction is performed using three different techniques: N-gram models of syllable sequences, dynamic programming to match sequences of features, and decision trees. Using a C4.5 decision tree trained on a wide range of features, most notably each word's orthographic form and information extracted from the syntactic parse of the sentence, our feature set achieved a balanced error rate of 46.6%. This compares with the feature set used in [11] which had a balanced error rate of 55.55%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-224"
  },
  "ni07_interspeech": {
   "authors": [
    [
     "Xinqiang",
     "Ni"
    ],
    [
     "Yining",
     "Chen"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Ping",
     "Zhang"
    ]
   ],
   "title": "An unsupervised approach to automatic prosodic annotation",
   "original": "i07_0486",
   "page_count": 4,
   "order": 225,
   "p1": "486",
   "pn": "489",
   "abstract": [
    "Accent is probably the most prominent part in prosodic events. Automatic accent labeling is important for both speech synthesis and automatic speech understanding. However, manually labeling data for traditional supervised learning is expensive and time consuming. In this paper, we propose an unsupervised learning algorithm to label accent automatically. First, we assume all content words are accented. We build an initial acoustic model with accented vowels in content words and high confidence unaccented vowels in function words. Then an iterative progress is executed to convergence. Experimental results show that this unsupervised learning algorithm achieves about 90% agreement on accent labeling. Compared with 84.3%, the accuracy of a typical linguistic classifier, a 30% relative error reduction is obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-225"
  },
  "inanoglu07_interspeech": {
   "authors": [
    [
     "Zeynep",
     "Inanoglu"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "A system for transforming the emotion in speech: combining data-driven conversion techniques for prosody and voice quality",
   "original": "i07_0490",
   "page_count": 4,
   "order": 226,
   "p1": "490",
   "pn": "493",
   "abstract": [
    "This paper describes a system that combines independent transformation techniques to endow a neutral utterance with some required target emotion. The system consists of three modules that are each trained on a limited amount of speech data and act on differing temporal layers. F0 contours are modelled and generated using context-sensitive syllable HMMs, while durations are transformed using phone-based relative decision trees. For spectral conversion which is applied at the segmental level, two methods were investigated: a GMM-based voice conversion approach and a codebook selection approach. Converted test data were evaluated for three emotions using an independent emotion classifier as well as perceptual listening tests. The listening test results show that perception of sadness output by our system was comparable with the perception of human sad speech while the perception of surprise and anger was around 5% worse than that of a human speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-226"
  },
  "chiang07_interspeech": {
   "authors": [
    [
     "Chen-Yu",
     "Chiang"
    ],
    [
     "Hsiu-Min",
     "Yu"
    ],
    [
     "Yih-Ru",
     "Wang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "An automatic prosody labeling method for Mandarin speech",
   "original": "i07_0494",
   "page_count": 4,
   "order": 227,
   "p1": "494",
   "pn": "497",
   "abstract": [
    "A new model-based automatic prosody labeling method for Mandarin speech is proposed. It first introduces four models to describe the relationships of the prosody tags to be labeled, the prosodic features of the speech signals, and the linguistic features of the associated texts. It then employs a sequential optimization procedure to estimate parameters of these four models and find all prosody tags. Experimental results on the Sinica Tree-Bank corpus showed that most prosody tags labeled were meaningful and the estimated parameters of these four models matched well with our a priori knowledge about Mandarin prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-227"
  },
  "hirose07_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Corpus-based generation of prosodic features from text based on generation process model",
   "original": "i07_1274",
   "page_count": 4,
   "order": 228,
   "p1": "1274",
   "pn": "1277",
   "abstract": [
    "A total scheme of generating prosodic features from a text input was constructed. The method consists of corpus-based prediction of pauses, phone durations and fundamental frequencies ((F0's), in this order, and information predicted in an earlier process is utilized in the following processes. Since prediction of F0's is done on the command values of F0 contour generation process model instead of direct F0 values, a stable and flexible control of F0 contours is possible. By adding constraints on the accent command timings as a post processing, a better quality was realized when speech was synthesized using prosodic features generated by the method. Validity of the developed method was confirmed through the listening test of the synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-228"
  },
  "tian07_interspeech": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Imre",
     "Kiss"
    ]
   ],
   "title": "Novel eigenpitch-based prosody model for text-to-speech synthesis",
   "original": "i07_1278",
   "page_count": 4,
   "order": 229,
   "p1": "1278",
   "pn": "1281",
   "abstract": [
    "Prosody is an inherent supra-segmental feature in speech that human speakers employ to express, for example, attitude, emotion, intent and attention. In text-to-speech (TTS) systems, high naturalness can only be achieved if the prosody of the output is appropriate. The importance of prosody is even more crucial for tonal languages, such as Mandarin Chinese, in which the tone of each syllable is described by its pitch contour. In this paper, we propose a novel prosody modeling approach that uses the concept of syllable-based eigenpitch. The approach has been implemented in our Mandarin TTS system resulting in less than 0.1% error variance. The results obtained in practical experiments have confirmed the good performance of the proposed technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-229"
  },
  "strom07_interspeech": {
   "authors": [
    [
     "Volker",
     "Strom"
    ],
    [
     "Ani",
     "Nenkova"
    ],
    [
     "Robert",
     "Clark"
    ],
    [
     "Yolanda",
     "Vazquez-Alvarez"
    ],
    [
     "Jason",
     "Brenier"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Dan",
     "Jurafsky"
    ]
   ],
   "title": "Modelling prominence and emphasis improves unit-selection synthesis",
   "original": "i07_1282",
   "page_count": 4,
   "order": 230,
   "p1": "1282",
   "pn": "1285",
   "abstract": [
    "We describe the results of large scale perception experiments showing improvements in synthesising two distinct kinds of prominence: standard pitch-accent and strong emphatic accents. Previously prominence assignment has been mainly evaluated by computing accuracy on a prominence-labelled test set. By contrast we integrated an automatic pitch-accent classifier into the unit selection target cost and showed that listeners preferred these synthesised sentences. We also describe an improved recording script for collecting emphatic accents, and show that generating emphatic accents leads to further improvements in the fiction genre over incorporating pitch accent only. Finally, we show differences in the effects of prominence between child-directed speech and news and fiction genres.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-230"
  },
  "takada07_interspeech": {
   "authors": [
    [
     "Seiya",
     "Takada"
    ],
    [
     "Yuji",
     "Yagi"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "A framework of reply speech generation for concept-to-speech conversion in spoken dialogue systems",
   "original": "i07_1286",
   "page_count": 4,
   "order": 231,
   "p1": "1286",
   "pn": "1289",
   "abstract": [
    "Due to recent advancements in speech technologies, a large number of spoken dialogue systems have been constructed. However, since most of them adopt existing text-to-speech synthesizers, it is rather difficult to reflect the linguistic information obtained during the reply sentence generation well in output speech. A framework is necessary for correctly reflecting higher-level linguistic information, such as syntactic structure and discourse information. We have constructed a spoken dialogue system on road guidance and realized concept-to-speech conversion, where output speech is generated in a unified process. Tag LISP forms keep the syntactic structures throughout the process in order to reflect the linguistic information in the prosody of output speech. Furthermore, by making it possible to insert not only words but also phrase templates in tags, various sentences were generated with a minor increase of templates. Validity of the methods is shown through experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-231"
  },
  "stocksmeier07_interspeech": {
   "authors": [
    [
     "Thorsten",
     "Stocksmeier"
    ],
    [
     "Stefan",
     "Kopp"
    ],
    [
     "Dafydd",
     "Gibbon"
    ]
   ],
   "title": "Synthesis of prosodic attitudinal variants in German backchannel ja",
   "original": "i07_1290",
   "page_count": 4,
   "order": 232,
   "p1": "1290",
   "pn": "1293",
   "abstract": [
    "Feedback utterances are an important part of any dialog between humans. When two or more persons talk, they use short backchannel utterances to signal understanding and interest in the conversation. Surprisingly little is known about the relationship between the accompanying prosody and the meaning of feedback perceived by the dialog partner. We present a qualitative modelling study of 12 synthesized German ja (yes) interjections that shows the influence of prosodic features on emotional and pragmatic perception of this kind of feedback. Listeners perceived utterances as bored, hesitant, or happy and agreeing depending on the prosodic parameters used for synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-232"
  },
  "li07d_interspeech": {
   "authors": [
    [
     "Ke",
     "Li"
    ],
    [
     "Yoko",
     "Greenberg"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Inter-language prosodic style modification experiment using word impression vector for communicative speech generation",
   "original": "i07_1294",
   "page_count": 4,
   "order": 233,
   "p1": "1294",
   "pn": "1297",
   "abstract": [
    "To confirm the language independency of a communicative prosody generation from input word impression vector, we synthesized communicative Mandarin speech using prosodic characteristics of communicative Japanese speech. The fundamental frequency and duration characteristics of one-word \"n\" utterances of Japanese were copied to Mandarin through input word attributes. From the subjective impressions of an input word, a three-dimensional vector was calculated through Multi-Dimensional Scaling analysis. Three dimensions reflecting impressions of confident-doubtful, allowable-unacceptable and positive-negative correspond to systematic prosodic variations; F0 height, F0 dynamics and duration. Subjective evaluation of synthesized speech showed the possibility of communicative prosody generation from input word impression vector language independently.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-233"
  },
  "crammer07_interspeech": {
   "authors": [
    [
     "Koby",
     "Crammer"
    ]
   ],
   "title": "A conservative aggressive subspace tracker",
   "original": "i07_0498",
   "page_count": 4,
   "order": 234,
   "p1": "498",
   "pn": "501",
   "abstract": [
    "The need to track a subspace describing well a stream of points arises in many signal processing applications. In this work, we present a very efficient algorithm using a machine learning approach, which its goal is to de-noise the stream of input points. The algorithm guarantees the orthonormality of the representation it uses. We demonstrate the merits of our approach using simulations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-234"
  },
  "nilsson07_interspeech": {
   "authors": [
    [
     "Mattias",
     "Nilsson"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Mutual information and the speech signal",
   "original": "i07_0502",
   "page_count": 4,
   "order": 235,
   "p1": "502",
   "pn": "505",
   "abstract": [
    "Mutual information is commonly used in speech processing in the context of statistical mapping. Examples are the optimization of speech or speaker recognition algorithms, the computation of performance bounds on such algorithms, and bandwidth extension of narrow-band speech signals. It is generally ignored that speech-signal derived data usually have an intrinsic dimensionality that is lower than the dimensionality of the observation vectors (the dimensionality of the embedding space). In this paper, we show that such reduced dimensionality can affect the accuracy of the mutual information estimate significantly. We introduce a new method that removes the effects of singular probability density functions. The method does not require prior knowledge of the intrinsic dimensionality of the data. It is shown that the method is appropriate for speech-derived data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-235"
  },
  "ezzat07_interspeech": {
   "authors": [
    [
     "Tony",
     "Ezzat"
    ],
    [
     "Jake",
     "Bouvrie"
    ],
    [
     "Tomaso",
     "Poggio"
    ]
   ],
   "title": "Spectro-temporal analysis of speech using 2-d Gabor filters",
   "original": "i07_0506",
   "page_count": 4,
   "order": 236,
   "p1": "506",
   "pn": "509",
   "abstract": [
    "We present a 2-D spectro-temporal Gabor filterbank based on the 2-D Fast Fourier Transform, and show how it may be used to analyze localized patches of a spectrogram. We argue that the 2-D Gabor filterbank has the capacity to decompose a patch into its underlying dominant spectro-temporal components, and we illustrate the response of our filterbank to different speech phenomena such as harmonicity, formants, vertical onsets/offsets, noise, and overlapping simultaneous speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-236"
  },
  "dekens07_interspeech": {
   "authors": [
    [
     "Tomas",
     "Dekens"
    ],
    [
     "Mike",
     "Demol"
    ],
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Piet",
     "Verhoeve"
    ]
   ],
   "title": "A comparative study of speech rate estimation techniques",
   "original": "i07_0510",
   "page_count": 4,
   "order": 237,
   "p1": "510",
   "pn": "513",
   "abstract": [
    "In this paper we evaluate the performance of 8 different speech rate estimators [1, 2, 3, 4, 5] previously described in the literature by applying them on a multilingual test database [6]. All the estimators show an underestimation at high speech rates and some also suffer from an overestimation at low speech rates. Overall the tested methods obtain high correlation coefficients with the reference speech rate. The Temporal Correlation and Selected Sub-band Correlation method (tcssbc), which uses sub-band and time domain correlation for detecting the number of vowels or diphthongs present in the speech signal, shows little errors and appears to be the most appropriate overall technique for speech rate estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-237"
  },
  "falk07_interspeech": {
   "authors": [
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Hua",
     "Yuan"
    ],
    [
     "Wai-Yip",
     "Chan"
    ]
   ],
   "title": "Spectro-temporal processing for blind estimation of reverberation time and single-ended quality measurement of reverberant speech",
   "original": "i07_0514",
   "page_count": 4,
   "order": 238,
   "p1": "514",
   "pn": "517",
   "abstract": [
    "Auditory spectro-temporal representations of reverberant speech are investigated for blind estimation of reverberation time ( RT) and for single-ended measurement of speech quality. The auditory representations are obtained from an eight-filter filterbank which is used to extract the modulation spectra from temporal envelopes of the speech signal. Gaussian mixture models (GMM), one for each modulation channel and trained on clean speech signals, serve as reference models of normative speech behavior. Consistency measures, computed between reverberant test signals and each GMM, are mapped to an estimated RT and to an estimated quality score. Experiments show that the proposed measures achieve superior performance relative to current \"state-of-art\" algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-238"
  },
  "waterschoot07_interspeech": {
   "authors": [
    [
     "Toon van",
     "Waterschoot"
    ],
    [
     "Marc",
     "Moonen"
    ]
   ],
   "title": "Linear prediction of audio signals",
   "original": "i07_0518",
   "page_count": 4,
   "order": 239,
   "p1": "518",
   "pn": "521",
   "abstract": [
    "Linear prediction (LP) is a valuable tool for speech analysis and coding, due to the efficiency of the autoregressive model for speech signals. In audio analysis and coding, the sinusoidal model is much more popular, which is partly due to the poor performance of audio LP. By examining audio LP from a spectral estimation point of view, we observe that the distribution of the audio signal's dominant frequencies in the Nyquist interval is a critical factor determining LP performance. In this framework, we describe five existing alternative LP methods and illustrate how they all attempt to solve the observed frequency distribution problem.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-239"
  },
  "magi07_interspeech": {
   "authors": [
    [
     "Carlo",
     "Magi"
    ],
    [
     "Tom",
     "Bäckström"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Stabilised weighted linear prediction - a robust all-pole method for speech processing",
   "original": "i07_0522",
   "page_count": 4,
   "order": 240,
   "p1": "522",
   "pn": "525",
   "abstract": [
    "Weighted linear prediction (WLP) is a method to compute all-pole models of speech by applying temporal weighting of the residual energy. By using short-time energy (STE) as a weighting function, the algorithm over-weight those samples that fit the underlying speech production model well. The current work introduces a modified WLP method, stabilised weighted linear prediction (SWLP) leading always to stable all-pole models whose performance can be adjusted by changing the length (denoted by M) of the STE window. With a large M value, the SWLP spectra become similar to conventional LP spectra. A small value of M results in SWLP filters similar to those computed by the minimum variance distortionless response (MVDR) method. The study compares the performances of SWLP, MVDR, and conventional LP in spectral modelling of speech sounds corrupted by Gaussian additive white noise. Results indicate that SWLP is the most robust method against noise especially with a small M value.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-240"
  },
  "rudoy07_interspeech": {
   "authors": [
    [
     "Daniel",
     "Rudoy"
    ],
    [
     "Daniel N.",
     "Spendley"
    ],
    [
     "Patrick J.",
     "Wolfe"
    ]
   ],
   "title": "Conditionally linear Gaussian models for estimating vocal tract resonances",
   "original": "i07_0526",
   "page_count": 4,
   "order": 241,
   "p1": "526",
   "pn": "529",
   "abstract": [
    "Vocal tract resonances play a central role in the perception and analysis of speech. Here we consider the canonical task of estimating such resonances from an observed acoustic waveform, and formulate it as a statistical model-based tracking problem. In this vein, Deng and colleagues recently showed that a robust linearization of the formant-to-cepstrum map enables the effective use of a Kalman filtering framework. We extend this model both to account for the uncertainty of speech presence by way of a censored likelihood formulation, as well as to explicitly model formant cross-correlation via a vector autoregression, and in doing so retain a conditionally linear and Gaussian framework amenable to efficient estimation schemes. We provide evaluations using a recently introduced public database of formant trajectories, for which results indicate improvements from twenty to over 30% per formant in terms of root mean square error, relative to a contemporary benchmark formant analysis tool.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-241"
  },
  "schnell07_interspeech": {
   "authors": [
    [
     "Karl",
     "Schnell"
    ],
    [
     "Arild",
     "Lacroix"
    ]
   ],
   "title": "Time-varying pre-emphasis and inverse filtering of speech",
   "original": "i07_0530",
   "page_count": 4,
   "order": 242,
   "p1": "530",
   "pn": "533",
   "abstract": [
    "In this contribution, a time-varying linear prediction method is applied to speech processing. In contrast to the commonly used linear prediction approach, the proposed time-varying method considers the continuous time evolution of the vocal tract and, additionally, avoids block-wise processing. On the assumption that the linear predictor coefficients evolve linearly in sections and continuously over the whole signal, the optimum time-varying coefficients can be determined quasi-analytically by a least mean square approach. The investigations show that the method fits very well the realization of a time-varying pre-emphasis. Furthermore, the results show that the method is suitable for time-varying inverse filtering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-242"
  },
  "thiemann07_interspeech": {
   "authors": [
    [
     "Joachim",
     "Thiemann"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Reconstructing audio signals from modified non-coherent hilbert envelopes",
   "original": "i07_0534",
   "page_count": 4,
   "order": 243,
   "p1": "534",
   "pn": "537",
   "abstract": [
    "In this paper, we present a speech and audio analysis-synthesis method based on a Basilar Membrane (BM) model. The audio signal is represented in this method by the Hilbert envelopes of the responses to complex gammatone filters uniformally spaced on a critical band scale. We show that for speech and audio signals, a perceptually equivalent signal can be reconstructed from the envelopes alone by an iterative procedure that estimates the associated carrier for the envelopes. The rate requirement of the envelope information is reduced by low-pass filtering and sampling, and it is shown that it is possible to recover a signal without audible distortion from the sampled envelopes. This may lead to improved perceptual coding methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-243"
  },
  "nguyen07_interspeech": {
   "authors": [
    [
     "Binh Phu",
     "Nguyen"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A flexible spectral modification method based on temporal decomposition and Gaussian mixture model",
   "original": "i07_0538",
   "page_count": 4,
   "order": 244,
   "p1": "538",
   "pn": "541",
   "abstract": [
    "This paper presents a new spectral modification method to solve two drawbacks of conventional spectral modification methods, insufficient smoothness of the modified spectra between frames and ineffective spectral modification. To overcome the insufficient smoothness, a speech analysis technique called temporal decomposition (TD) is used to model the spectral evolution. Instead of modifying the speech spectra frame by frame, we only need to modify event targets and event functions, and the smoothness of the modified speech is ensured by the shape of the event functions. To overcome the ineffective spectral modification, we explore Gaussian mixture model (GMM) parameters for an input of TD to model the spectral envelope, and develop a new method of modifying GMM parameters in accordance with formant scaling factors. Experimental results show that the effectiveness of the proposed method is verified in terms of the smoothness of the modified speech and the effective spectral modification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-244"
  },
  "darch07_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Darch"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "A comparison of estimated and MAP-predicted formants and fundamental frequencies with a speech reconstruction application",
   "original": "i07_0542",
   "page_count": 4,
   "order": 245,
   "p1": "542",
   "pn": "545",
   "abstract": [
    "This work compares the accuracy of fundamental frequency and formant frequency estimation methods and maximum a posteriori (MAP) prediction from MFCC vectors with hand-corrected references. Five fundamental frequency estimation methods are compared to fundamental frequency prediction from MFCC vectors in both clean and noisy speech. Similarly, three formant frequency estimation and prediction methods are compared. An analysis of estimation and prediction accuracy shows that prediction from MFCCs provides the most accurate voicing classification across clean and noisy speech. On clean speech, fundamental frequency estimation outperforms prediction from MFCCs, but as noise increases the performance of prediction is significantly more robust than estimation. Formant frequency prediction is found to be more accurate than estimation in both clean and noisy speech. A subjective analysis of the estimation and prediction methods is also made by reconstructing speech from the acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-245"
  },
  "deng07_interspeech": {
   "authors": [
    [
     "Huiqun",
     "Deng"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Effect of incomplete glottal closures on estimates of glottal waves via inverse filtering of vowel sounds",
   "original": "i07_0546",
   "page_count": 4,
   "order": 246,
   "p1": "546",
   "pn": "549",
   "abstract": [
    "Glottal waves obtained via inverse filtering vowel sounds may contain residual vocal-tract resonances due to incomplete glottal closures. This paper investigates the effect of incomplete glottal closures on the estimates of the glottal waves via inverse filtering. It shows that such a residual resonance appears as stationary ripples superimposed on the derivatives of the original glottal wave over a whole glottal cycle. Knowing this, one can determine if there are significant resonances of vocal tracts in the obtained glottal waves. It also shows that given an incomplete glottal closure, better estimates of glottal waves can be obtained from large lip-opening vowel sounds than from other sounds. The glottal waves obtained from /scripta/ produced by male and female subjects are presented. The obtained glottal waves during rapid vocal-fold collisions exhibit transient positive derivatives, which are explained by the air squeezed by the colliding vocal folds and the air from the glottal chink.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-246"
  },
  "kalgaonkar07_interspeech": {
   "authors": [
    [
     "Kaustubh",
     "Kalgaonkar"
    ],
    [
     "Mark A.",
     "Clements"
    ]
   ],
   "title": "Vocal tract and area function estimation with both lip and glottal losses",
   "original": "i07_0550",
   "page_count": 4,
   "order": 247,
   "p1": "550",
   "pn": "553",
   "abstract": [
    "Traditional algorithms simplify the lattice recursion for evaluation of the PARCOR's by localizing the loss in vocal tract at one of its ends, the lips or the glottis. In this paper we present a framework for mapping to pseudo areas the VT transfer function with no rigid constraints on the losses in system, thereby allowing losses to be present at both the lips and glottis. This method allows us to calculate the reflection coefficients at both the glottis (rG) and the lips (rLip).\n",
    "The area functions obtained from these new PARCOR's, have better temporal (inter-frame) and spatial (intra-frame) predictability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-247"
  },
  "guruprasad07_interspeech": {
   "authors": [
    [
     "S",
     "Guruprasad"
    ],
    [
     "B",
     "Yegnanarayana"
    ],
    [
     "K Sri Rama",
     "Murty"
    ]
   ],
   "title": "Detection of instants of glottal closure using characteristics of excitation source",
   "original": "i07_0554",
   "page_count": 4,
   "order": 248,
   "p1": "554",
   "pn": "557",
   "abstract": [
    "In this paper, we propose a method for detection of glottal closure instants (GCI) in the voiced regions of speech signals. The method is based on periodicity of significant excitations of the vocal tract system. The key idea is the computation of coherent covariance sequence, which overcomes the effect of dynamic range of the excitation source signal, while preserving the locations of significant excitations. The Hilbert envelope of linear prediction residual is used as an estimate of the source of excitation of the vocal tract system. Performance of the proposed method is evaluated in terms of the deviation between true GCIs and hypothesized GCIs, using clean speech and degraded speech signals. The signal-to-noise ratio (SNR) of speech signals in the vicinity of GCIs has significant bearing on the performance of the proposed method. The proposed method is accurate and robust for detection of GCIs, even in the presence of degradations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-248"
  },
  "sturmel07_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Sturmel"
    ],
    [
     "Christophe",
     "D'Alessandro"
    ],
    [
     "Boris",
     "Doval"
    ]
   ],
   "title": "A comparative evaluation of the zeros of z transform representation for voice source estimation",
   "original": "i07_0558",
   "page_count": 4,
   "order": 249,
   "p1": "558",
   "pn": "561",
   "abstract": [
    "A new method for voice source estimation is evaluated and compared to Linear Prediction (LP) inverse filtering methods (autocorrelation LPC, covariance LPC and IAIF [1]). The method is based on a causal/anticausal model of the voice source and the ZZT (Zeros of Z-Transform) representation [2] for causal/anticausal signal separation. A database containing synthetic speech with various voice source settings and natural speech with acoustic and electro-glottographic signals was recorded. Formal evaluation of source estimation is based on spectral distances. The results show that the ZZT causal/anticausal decomposition method outperforms LP in voice source estimation both for synthetic and natural signals. However, its computational load is much heavier (despite a very simple principle) and the method seems sensitive to noise and computation precision errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-249"
  },
  "harma07_interspeech": {
   "authors": [
    [
     "Aki",
     "Härmä"
    ]
   ],
   "title": "Ambient telephony: scenarios and research challenges",
   "original": "i07_0562",
   "page_count": 4,
   "order": 250,
   "p1": "562",
   "pn": "565",
   "abstract": [
    "Telecommunications at home is changing rapidly. Many people have moved from the traditional PSTN phone to the mobile phone. Now for increasingly many people Voice-over-IP telephony on a PC platform is becoming the primary technology for voice communications. In this tutorial paper we give an overview of some of the current trends and try to characterize the next generation of home telephony, in particular, the concept of ambient telephony. We give an overview of the research challenges in the development of ambient telephone systems and introduce some potential solutions and scenarios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-250"
  },
  "obuchi07_interspeech": {
   "authors": [
    [
     "Yasunari",
     "Obuchi"
    ],
    [
     "Akio",
     "Amano"
    ]
   ],
   "title": "Always listening to you: creating exhaustive audio database in home environments",
   "original": "i07_0566",
   "page_count": 4,
   "order": 251,
   "p1": "566",
   "pn": "569",
   "abstract": [
    "In this paper, we describe a novel audio database recorded in home environments. The database contains continuous sounds from morning to evening, no matter what the subject is doing, although some utterances to invoke speech recognition are included in the data. It tells us how often speech interface is used, how speech interface is activated erroneously when it is not called, and how people speak when they really want to use speech recognition. The database also features parallel recording using microphone arrays, which is expected to improve the performance of speech/non-speech detection and speech recognition under noisy conditions. Preliminary experiments show that the speech/non-speech detection performance of the trigger-initiated activation system is relatively high, but that of the automatic activation system is not satisfactory. Adopting array-based and F0-based detection algorithms produces a slight rise of the precision/recall curve, but more research is necessary to realize a life with ubiquitous speech interface of home appliances, in which machines are always listening to you.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-251"
  },
  "schmalenstroeer07_interspeech": {
   "authors": [
    [
     "Joerg",
     "Schmalenstroeer"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Joint speaker segmentation, localization and identification for streaming audio",
   "original": "i07_0570",
   "page_count": 4,
   "order": 252,
   "p1": "570",
   "pn": "573",
   "abstract": [
    "In this paper we investigate the problem of identifying and localizing speakers with distant microphone arrays, thus extending the classical speaker diarization task to answer the question \"who spoke when\n",
    "and where\". We consider a streaming audio scenario, where the diarization output is to be generated in realtime with as low latency as possible. Rather than carrying out the individual segmentation and classification tasks (speech detection, change detection, gender/speaker classification) sequentially, we propose a simultaneous segmentation and classification by applying a Viterbi decoder. It uses a transition matrix estimated online from position information and speaker change hypotheses, instead of fixed transition probabilities. This avoids early hard decisions and is shown to outperform the sequential approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-252"
  },
  "lu07b_interspeech": {
   "authors": [
    [
     "Yan-Chen",
     "Lu"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Active binaural distance estimation for dynamic sources",
   "original": "i07_0574",
   "page_count": 4,
   "order": 253,
   "p1": "574",
   "pn": "577",
   "abstract": [
    "A method for estimating sound source distance in dynamic auditory ‘scenes’ using binaural data is presented. The technique requires little prior knowledge of the acoustic environment. It consists of feature extraction for two dynamic distance cues, motion parallax and acoustic τ, coupled with an inference framework for distance estimation. Sequential and non-sequential models are evaluated using simulated anechoic and reverberant spaces. Sequential approaches based on particle filtering more than halve the distance estimation error in all conditions relative to the non-sequential models. These results confirm the value of active behaviour and probabilistic reasoning in auditorily-inspired models of distance perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-253"
  },
  "borgstrom07_interspeech": {
   "authors": [
    [
     "Bengt J.",
     "Borgström"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "A packetization and variable bitrate interframe compression scheme for vector quantizer-based distributed speech recognition",
   "original": "i07_0578",
   "page_count": 4,
   "order": 254,
   "p1": "578",
   "pn": "581",
   "abstract": [
    "We propose a novel packetization and variable bitrate compression scheme for DSR source coding, based on the Group of Pictures concept from video coding. The proposed algorithm simultaneously packetizes and further compresses source coded features using the high interframe correlation of speech, and is compatible with a variety of VQ-based DSR source coders. The algorithm approximates vector quantizers as Markov Chains, and empirically trains the corresponding probability parameters. Feature frames are then compressed as I-frames, P-frames, or B-frames, using Huffman tables. The proposed scheme can perform lossless compression, but is also robust to lossy compression through VQ pruning or frame puncturing. To illustrate its effectiveness, we applied the proposed algorithm to the ETSI DSR source coder. The algorithm provided compression rates of up to 31.60% with negligible recognition accuracy degradation, and rates of up to 71.15% with performance degradation under 1.0%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-254"
  },
  "wolfel07_interspeech": {
   "authors": [
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "Channel selection by class separability measures for automatic transcriptions on distant microphones",
   "original": "i07_0582",
   "page_count": 4,
   "order": 255,
   "p1": "582",
   "pn": "585",
   "abstract": [
    "Channel selection is important for automatic speech recognition as the signal quality of one channel might be significantly better than those of the other channels and therefore, microphone array or blind source separation techniques might not lead to improvements over the best single microphone. The mayor challenge, however, is to find this particular channel who is leading to the most accurate classification. In this paper we present a novel channel selection method, based on class separability, to improve multi-source far distance speech-to-text transcriptions. Class separability measures have the advantage, compared to other methods such as the signal to noise ratio (SNR), that they are able to evaluate the channel quality on the actual features of the recognition system.\n",
    "We have evaluated on NISTs RT-07 development set and observe significant improvements in word accuracy over SNR based channel selection methods. We have also used this technique in NISTs RT-07 evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-255"
  },
  "wyatt07_interspeech": {
   "authors": [
    [
     "Danny",
     "Wyatt"
    ],
    [
     "Tanzeem",
     "Choudhury"
    ],
    [
     "Jeff",
     "Bilmes"
    ]
   ],
   "title": "Conversation detection and speaker segmentation in privacy-sensitive situated speech data",
   "original": "i07_0586",
   "page_count": 4,
   "order": 256,
   "p1": "586",
   "pn": "589",
   "abstract": [
    "We present privacy-sensitive methods for (1) automatically finding multi-person conversations in spontaneous, situated speech data and (2) segmenting those conversations into speaker turns. The methods protect privacy through a feature set that is rich enough to capture conversational styles and dynamics, but not sufficient for reconstructing intelligible speech. Experimental results show that the conversation finding method outperforms earlier approaches and that the speaker segmentation method is a significant improvement to the only other known privacy-sensitive method for speaker segmentation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-256"
  },
  "abad07_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "Carlos",
     "Segura"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Audio-based approaches to head orientation estimation in a smart-room",
   "original": "i07_0590",
   "page_count": 4,
   "order": 257,
   "p1": "590",
   "pn": "593",
   "abstract": [
    "The head orientation of human speakers in a smart-room affects the quality of the signals recorded by far-field microphones, and consequently influences the performance of the technologies deployed based on those signals. Additionally, knowing the orientation in these environments can be useful for the development of several multimodal advanced services, for instance, in microphone network management. Consequently, head orientation estimation has recently become a growing interesting research topic. In this paper, we propose two different approaches to head orientation estimation on the basis of multi-microphone recordings: first, an approach based on the generalization of the well-known SRP-PHAT speaker localization algorithm, and second a new approach based on measurements of the ratio between the high and the low band speech energies. Promising results are obtained in both cases, with a generalized better performance of the algorithms based on speaker localization methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-257"
  },
  "ion07_interspeech": {
   "authors": [
    [
     "Valentin",
     "Ion"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Multi-resolution soft features for channel-robust distributed speech recognition",
   "original": "i07_0594",
   "page_count": 4,
   "order": 258,
   "p1": "594",
   "pn": "597",
   "abstract": [
    "In this paper we introduce soft features of variable resolution for robust distributed speech recognition over channels exhibiting packet losses. The underlying rationale is that lost feature vectors can never be reconstructed perfectly and therefore reconstruction is carried out at a lower resolution than the resolution of the originally sent features. By doing so, enormous reductions in computational effort can be achieved at a graceful or even no degradation in word accuracy. In experiments conducted on the Aurora II database we obtained for example a reduction of a factor of 30 in computation time for the reconstruction of the soft features without an effect on the word error rate. The proposed method is fully compatible with the ETSI DSR standard, as there are no changes involved in the front-end processing and the transmission format.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-258"
  },
  "su07_interspeech": {
   "authors": [
    [
     "Yi",
     "Su"
    ],
    [
     "Frederick",
     "Jelinek"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Large-scale random forest language models for speech recognition",
   "original": "i07_0598",
   "page_count": 4,
   "order": 259,
   "p1": "598",
   "pn": "601",
   "abstract": [
    "The random forest language model (RFLM) has shown encouraging results in several automatic speech recognition (ASR) tasks but has been hindered by practical limitations, notably the space-complexity of RFLM estimation from large amounts of data. This paper addresses large-scale training and testing of the RFLM via an efficient disk-swapping strategy that exploits the recursive structure of a binary decision tree and the local access property of the tree-growing algorithm, redeeming the full potential of the RFLM, and opening avenues of further research, including useful comparisons with n-gram models. Benefits of this strategy are demonstrated by perplexity reduction and lattice rescoring experiments using a state-of-the-art ASR system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-259"
  },
  "akita07_interspeech": {
   "authors": [
    [
     "Yuya",
     "Akita"
    ],
    [
     "Yusuke",
     "Nemoto"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "PLSA-based topic detection in meetings for adaptation of lexicon and language model",
   "original": "i07_0602",
   "page_count": 4,
   "order": 260,
   "p1": "602",
   "pn": "605",
   "abstract": [
    "A topic detection approach based on a probabilistic framework is proposed to realize topic adaptation of speech recognition systems for long speech archives such as meetings. Since topics in such speech are not clearly defined unlike news stories, we adopt a probabilistic representation of topics based on probabilistic latent semantic analysis (PLSA). A topical sub-space is constructed by PLSA, and speech segments are projected to the subspace, then each segment is represented by a vector which consists of topic probabilities obtained by the projection. Topic detection is performed by clustering these vectors, and topic adaptation is done by collecting relevant texts based on the similarity in this probabilistic representation. In experimental evaluations, the proposed approach demonstrated significant reduction of perplexity and out-of-vocabulary rates as well as robustness against ASR errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-260"
  },
  "sako07_interspeech": {
   "authors": [
    [
     "Atsushi",
     "Sako"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Language modeling using PLSA-based topic HMM",
   "original": "i07_0606",
   "page_count": 4,
   "order": 261,
   "p1": "606",
   "pn": "609",
   "abstract": [
    "In this paper, we propose a PLSA-based language model for sports live speech. This model is implemented in unigram rescaling technique that combines a topic model and an n-gram. In conventional method, unigram rescaling is performed with a topic distribution estimated from a history of recognized transcription. This method can improve the performance; however it cannot express topic transition. Incorporating concept of topic transition, it is expected to improve the recognition performance. Thus the proposed method employs a \"Topic HMM\" instead of a history to estimate the topic distribution. The Topic HMM is a Discrete Ergodic HMM that expresses typical topic distributions and topic transition probabilities. Word accuracy results indicate an improvement over tri-gram and PLSA-based conventional method using a recognized history.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-261"
  },
  "pan07c_interspeech": {
   "authors": [
    [
     "Yi-cheng",
     "Pan"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Lexicon adaptation with reduced character error (LARCE) - a new direction in Chinese language modeling",
   "original": "i07_0610",
   "page_count": 4,
   "order": 262,
   "p1": "610",
   "pn": "613",
   "abstract": [
    "Good language modeling relies on good predefined lexicons. For Chinese, since there are no text word boundaries and the concept of \"word\" is not very well defined, constructing good lexicons is difficult. In this paper, we propose lexicon adaptation with reduced character error (LARCE), which learns new word tokens based on the criterion of reduced adaptation corpus error rate. In this approach, a multi-character string is taken as a new \"word\" as long as it is helpful in reducing the error rate, and minimum number of new, high-quality words can be obtained. This algorithm is based on character-based consensus networks. In initial experiments on Chinese broadcast news, it is shown that LARCE not only significantly outperforms PAT-tree-based word extraction algorithms, but even outperforms manually augmented lexicons. It is believed the concept is equally useful for other character-based languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-262"
  },
  "wu07c_interspeech": {
   "authors": [
    [
     "Meng-Sung",
     "Wu"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Minimum rank error training for language modeling",
   "original": "i07_0614",
   "page_count": 4,
   "order": 263,
   "p1": "614",
   "pn": "617",
   "abstract": [
    "Discriminative training techniques have been successfully developed for many pattern recognition applications. In speech recognition, discriminative training aims to minimize the metric of word error rate. However, in an information retrieval system, the best performance should be achieved by maximizing the average precision. In this paper, we construct the discriminative n-gram language model for information retrieval following the metric of minimum rank error (MRE) rather than the conventional metric of minimum classification error. In the optimization procedure, we maximize the average precision and estimate the language model towards attaining the smallest ranking loss. In the experiments on ad-hoc retrieval using TREC collections, the proposed MRE language model performs better than the maximum likelihood and the minimum classification error language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-263"
  },
  "wang07d_interspeech": {
   "authors": [
    [
     "Wen",
     "Wang"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Integrating MAP, marginals, and unsupervised language model adaptation",
   "original": "i07_0618",
   "page_count": 4,
   "order": 264,
   "p1": "618",
   "pn": "621",
   "abstract": [
    "We investigate the integration of various language model adaptation approaches for a cross-genre adaptation task to improve Mandarin ASR system performance on a recently introduced new genre, broadcast conversation (BC). Various language model adaptation strategies are investigated and their efficacies are evaluated based on ASR performance, including unsupervised language model adaptation from ASR transcripts and ways to integrate supervised Maximum A Posteriori (MAP) and marginal adaptation within the unsupervised adaptation framework. We found that by effectively combining these adaptation approaches, we can achieve as much as 1.3% absolute gain (6% relative) on the final recognition error rate in the BC genre.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-264"
  },
  "yamazaki07_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Yamazaki"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Haruo",
     "Yokota"
    ]
   ],
   "title": "Dynamic language model adaptation using presentation slides for lecture speech recognition",
   "original": "i07_2349",
   "page_count": 4,
   "order": 265,
   "p1": "2349",
   "pn": "2352",
   "abstract": [
    "We propose a dynamic language model adaptation method that uses the temporal information from lecture slides for lecture speech recognition. The proposed method consists of two steps. First, the language model is adapted with the text information extracted from all the slides of a given lecture. Next, the text information of a given slide is extracted based on temporal information and used for local adaptation. Hence, the language model, used to recognize speech associated with the given slide changes dynamically from one slide to the next. We evaluated the proposed method with the speech data from four Japanese lecture courses. Our experiments show the effectiveness of our proposed method, especially for keyword detection. The F-measure error rate for lecture keywords was reduced by 2.4%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-265"
  },
  "munteanu07_interspeech": {
   "authors": [
    [
     "Cosmin",
     "Munteanu"
    ],
    [
     "Gerald",
     "Penn"
    ],
    [
     "Ron",
     "Baecker"
    ]
   ],
   "title": "Web-based language modelling for automatic lecture transcription",
   "original": "i07_2353",
   "page_count": 4,
   "order": 266,
   "p1": "2353",
   "pn": "2356",
   "abstract": [
    "Universities have long relied on written text to share knowledge. As more lectures are made available on-line, these must be accompanied by textual transcripts in order to provide the same access to information as textbooks. While Automatic Speech Recognition (ASR) is a cost-effective method to deliver transcriptions, its accuracy for lectures is not yet satisfactory. One approach for improving lecture ASR is to build smaller, topic-dependent Language Models (LMs) and combine them (through LM interpolation or hypothesis space combination) with general-purpose, large-vocabulary LMs. In this paper, we propose a simple solution for lecture ASR with similar or better Word Error Rate reductions (as well as topic-specific keyword identification accuracies) than combination-based approaches. Our method eliminates the need for two types of LMs by exploiting the lecture slides to collect a web corpus appropriate for modelling both the conversational and the topic-specific styles of lectures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-266"
  },
  "alumae07_interspeech": {
   "authors": [
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Toomas",
     "Kirt"
    ]
   ],
   "title": "LSA-based language model adaptation for highly inflected languages",
   "original": "i07_2357",
   "page_count": 4,
   "order": 267,
   "p1": "2357",
   "pn": "2360",
   "abstract": [
    "This paper presents a language model topic adaptation framework for highly inflected languages. In such languages, sub-word units are used as basic units for language modeling. Since such units carry little semantic information, they are not very suitable for topic adaptation. We propose to lemmatize the corpus of training documents before constructing a latent topic model. To adapt language model, we use few lemmatized training sentences to find a set of documents that are semantically close to the current document. Fast marginal adaptation of sub-word trigram language model is used for adapting the background model. Experiments on a set of Estonian test texts show that the proposed approach gives a 19% decrease in language model perplexity. A statistically significant decrease in perplexity is observed already when using just two sentences for adaptation. We also show that the model employing lemmatization gives consistently better results than the unlemmatized model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-267"
  },
  "heidel07_interspeech": {
   "authors": [
    [
     "Aaron",
     "Heidel"
    ],
    [
     "Hung-an",
     "Chang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Language model adaptation using latent dirichlet allocation and an efficient topic inference algorithm",
   "original": "i07_2361",
   "page_count": 4,
   "order": 268,
   "p1": "2361",
   "pn": "2364",
   "abstract": [
    "We present an effort to perform topic mixture-based language model adaptation using latent Dirichlet allocation (LDA). We use probabilistic latent semantic analysis (PLSA) to automatically cluster a heterogeneous training corpus, and train an LDA model using the resultant topic-document assignments. Using this LDA model, we then construct topic-specific corpora at the utterance level for interpolation with a background language model during language model adaptation. We also present a novel iterative algorithm for LDA topic inference. Very encouraging results were obtained in preliminary experiments with broadcast news in Mandarin Chinese.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-268"
  },
  "yaman07_interspeech": {
   "authors": [
    [
     "Sibel",
     "Yaman"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Structural Bayesian language modeling and adaptation",
   "original": "i07_2365",
   "page_count": 4,
   "order": 269,
   "p1": "2365",
   "pn": "2368",
   "abstract": [
    "We propose a language modeling and adaptation framework using Bayesian structural maximum a posteriori (SMAP) principle, in which each n-gram event is embedded in a branch of a tree structure. The nodes in the first layer of this tree structure represent the unigrams, and those in the second layer represent the bigrams, and so on. Each node in the tree structure has an associated hyper-parameter representing the information about the prior distribution, and a count representing the number of times the word sequence occurs in the domain-specific data. In general, the hyper-parameters depend on the observation frequency of not only the node event but also its parent node of lower order n-gram event. Our automatic speech recognition experiments using the Wall Street Journal corpus verify that the proposed SMAP language model adaptation achieves a 5.6% relative improvement over maximum likelihood language models obtained with the same training and adaptation data sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-269"
  },
  "martins07b_interspeech": {
   "authors": [
    [
     "Ciro",
     "Martins"
    ],
    [
     "António J. S.",
     "Teixeira"
    ],
    [
     "João",
     "Neto"
    ]
   ],
   "title": "Vocabulary selection for a broadcast news transcription system using a morpho-syntactic approach",
   "original": "i07_2369",
   "page_count": 4,
   "order": 270,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "Although the vocabularies of ASR systems are designed to achieve high coverage for the expected domain, out-of-vocabulary (OOV) words cannot be avoided. Particularly, for daily and real-time transcription of Broadcast News (BN) data in highly inflected languages, the rapid vocabulary growth leads to high OOV word rates. To overcome this problem, we present a new morpho-syntactic approach to dynamically select the target vocabulary for this particular domain by trading off between the OOV word rate and vocabulary size.\n",
    "We evaluate this approach against the common selection strategy based on word frequency. Experiments have been carried out for a European Portuguese BN transcription system. Results computed on seven news shows, yields a relative reduction of 37.8% in OOV word rate against the baseline system and 5.5% when compared with the word frequency common approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-270"
  },
  "bach07_interspeech": {
   "authors": [
    [
     "Nguyen",
     "Bach"
    ],
    [
     "Mohamed",
     "Noamany"
    ],
    [
     "Ian",
     "Lane"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Handling OOV words in Arabic ASR via flexible morphological constraints",
   "original": "i07_2373",
   "page_count": 4,
   "order": 271,
   "p1": "2373",
   "pn": "2376",
   "abstract": [
    "We propose a novel framework to detect and recognize out-of-vocabulary (OOV) words in automated speech recognition (ASR). In the proposed framework a hybrid language model combining words and sub-word units is incorporated during ASR decoding then three different OOV words recognition methods are applied to generate OOV word hypotheses. Specifically, dictionary lookup, morphological composition, and direct phoneme-to-grapheme. The proposed approach successfully reduced WER by 1.9% and 1.6% for ASR systems with recognition vocabularies of 30K and 219K. Moreover, the proposed approach correctly recognized 5% of OOV words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-271"
  },
  "justo07_interspeech": {
   "authors": [
    [
     "Raquel",
     "Justo"
    ],
    [
     "M. Inés",
     "Torres"
    ]
   ],
   "title": "Phrases in category-based language models for Spanish and basque ASR",
   "original": "i07_2377",
   "page_count": 4,
   "order": 272,
   "p1": "2377",
   "pn": "2380",
   "abstract": [
    "In this work, we integrate phrases or segments of words into class n-gram language models in order to take advantage of two information sources: words and categories. Two different approaches to this kind of models are proposed and formulated. The models were integrated into an Automatic Speech Recognition system and subsequently evaluated in terms of word error rate. The experiments, carried out over two different databases and languages, demonstrate that a language model based on categories composed by phrases can outperform classical class n-gram language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-272"
  },
  "arsoy07_interspeech": {
   "authors": [
    [
     "Ebru",
     "Arısoy"
    ],
    [
     "Haşim",
     "Sak"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Language modeling for automatic turkish broadcast news transcription",
   "original": "i07_2381",
   "page_count": 4,
   "order": 273,
   "p1": "2381",
   "pn": "2384",
   "abstract": [
    "The aim of this study is to develop a speech recognition system for Turkish broadcast news. State-of-the-art speech recognition systems utilize statistical models. A large amount of data is required to reliably estimate these models. For this study, a large Turkish Broadcast News database, consisting of the speech signal and corresponding transcriptions, is being collected. In this paper, information about this database and experiments performed using the system developed on the collected data are presented. In addition to the baseline system, various sub-word language models are investigated. Lexical stem-endings are proposed as a novel unit for language modeling and are shown to perform better than surface stem-endings and morphs. Currently, our best systems have lower than 20% error on clean speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-273"
  },
  "calhoun07_interspeech": {
   "authors": [
    [
     "Sasha",
     "Calhoun"
    ]
   ],
   "title": "Predicting focus through prominence structure",
   "original": "i07_0622",
   "page_count": 4,
   "order": 274,
   "p1": "622",
   "pn": "625",
   "abstract": [
    "Focus is central to our control of information flow in dialogue. Spoken language understanding systems therefore need to be able to detect focus automatically. It is well known that prominence is a key marker of focus in English, however, the relationship is not straight-forward. We present focus prediction models built using the NXT Switchboard corpus. We claim that a focus is more likely if a word is more prominent than expected given its syntactic, semantic and discourse properties. Crucially, the perception of prominence arises not only from acoustic cues, but also the position in prosodic structure. Our focus prediction results, along with a study showing the acoustic properties of focal accents vary by structural position, support our claims. As a largely novel task, these results are an important first step in detecting focus for spoken language applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-274"
  },
  "bulut07_interspeech": {
   "authors": [
    [
     "Murtaza",
     "Bulut"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Analysis of emotional speech prosody in terms of part of speech tags",
   "original": "i07_0626",
   "page_count": 4,
   "order": 275,
   "p1": "626",
   "pn": "629",
   "abstract": [
    "Representation of emotions in terms of acoustic features of well defined lexical elements is desired for development of emotional speech processing systems. For that purpose, in this paper, the interaction between emotions and part of speech (POS) tags is investigated. Utterances from 3 speakers in angry, happy, sad, and neutral emotions are used to statistically analyze the effects of emotion, POS tag type, position of the tag, and speaker factors on tag duration, energy, and F0 variables. It is found that the main effects of emotion, tag type, and position are significant. Results also show that the effect of emotion is significantly dependent on position, but not on POS tag type. The effect of position is noticeable. POS tags located in the first half of sentences have shorter durations, higher energy, and higher F0 values.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-275"
  },
  "liu07b_interspeech": {
   "authors": [
    [
     "Fang",
     "Liu"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "The neutral tone in question intonation in Mandarin",
   "original": "i07_0630",
   "page_count": 4,
   "order": 276,
   "p1": "630",
   "pn": "633",
   "abstract": [
    "This study investigates how the neutral tone, when preceded by different full tones under different focus conditions, behaves in question intonation in Mandarin. Results indicate that 1) the preceding full/neutral tone largely determines the local F0 trajectory of the neutral tone, but the latter also gradually converges over the course of several neutral tone syllables, 2) post-focus lowering, which is caused by the effect of focus, occurs in both neutral-tone-ending and High-tone-ending sentences, with the interrogative intonation in questions realized as an upward shift starting from the focused word, and 3) sentence-final neutral tone has a falling contour even in questions, thus contrasting with sentence-final High tone, which has a rising contour in questions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-276"
  },
  "rochetcapellan07_interspeech": {
   "authors": [
    [
     "Amélie",
     "Rochet-Capellan"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Rafael",
     "Laboissière"
    ],
    [
     "Arturo",
     "Galvàn"
    ]
   ],
   "title": "Pointing to a target while naming it with /pata/ or /tapa/: the effect of consonants and stress position on jaw-finger coordination",
   "original": "i07_0634",
   "page_count": 4,
   "order": 277,
   "p1": "634",
   "pn": "637",
   "abstract": [
    "This study investigates jaw-finger coordination in a task consisting in pointing to a target while naming it with a /pata/ or a /tapa/ utterance stressed either on the first ('CVCV) or on the second (CV'CV) syllable. Optotrack measurements of jaw and finger displacements show that for 'CVCV names, the moment at with the finger reaches the target alignment is synchronized with the maximum of the first jaw opening motion. For CV'CV names, the synchronization occurs between the moment at which the finger leaves the target-alignment position and the maximum of the jaw opening motion for the second vowel. This pattern of synchronization does not depend on the target position or on the consonants order. These results add some support to theories involving the coordination of orofacial and brachiomanual gestures in the development and phylogeny of human languages. They call for more investigations on the link between speech and brachiomanual gestures in face-to-face communication.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-277"
  },
  "hide07_interspeech": {
   "authors": [
    [
     "Øydis",
     "Hide"
    ],
    [
     "Steven",
     "Gillis"
    ],
    [
     "Paul",
     "Govaerts"
    ]
   ],
   "title": "Suprasegmental aspects of pre-lexical speech in cochlear implanted children",
   "original": "i07_0638",
   "page_count": 4,
   "order": 278,
   "p1": "638",
   "pn": "641",
   "abstract": [
    "This paper investigates suprasegmental features in the pre-lexical speech of congenitally hearing impaired children who received a Nucleus-24 multichannel cochlear implant between 5 and 20 months of age. Single, independent bi-syllabic spontaneous babbling productions were analyzed acoustically at different stages in the babbling period. Fundamental frequency, pitch change in terms of direction and degree, and duration were analyzed for each vowel. The results were compared with those of a control group of normally hearing children, and analyzed with respect to length of cochlear implant experience. Few differences were found between the normally hearing children and the cochlear implanted children in terms of suprasegmental aspects, indicating that a cochlear implant provides fundamental improvement already in pre-lexical speech. Nevertheless, differences in pitch variations between the two groups of children at the end of the babbling period may signify a weakness related to the cochlear implant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-278"
  },
  "niebuhr07_interspeech": {
   "authors": [
    [
     "Oliver",
     "Niebuhr"
    ]
   ],
   "title": "Categorical perception in intonation: a matter of signal dynamics?",
   "original": "i07_0642",
   "page_count": 4,
   "order": 279,
   "p1": "642",
   "pn": "645",
   "abstract": [
    "Results of recent perception experiments revealed that the signalling of rising-falling F0 peak categories in German intonation involves an interplay of F0 and intensity. Moreover, combining identification judgements and reaction times suggests that the abruptness of the perceptual change between the categories is determined by the signal dynamics in the sense of the durations of the F0 peak movements and intensity transitions. This undermines the use of categorical perception as an instrument to detect phonological intonation categories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-279"
  },
  "aboutabit07_interspeech": {
   "authors": [
    [
     "Noureddine",
     "Aboutabit"
    ],
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Jeanne",
     "Clarke"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "A HMM recognition of consonant-vowel syllables from lip contours: the cued speech case",
   "original": "i07_0646",
   "page_count": 4,
   "order": 280,
   "p1": "646",
   "pn": "649",
   "abstract": [
    "Cued Speech (CS) is a manual code that complements lip-reading to enhance speech perception from visual input. The phonetic translation of CS gestures needs to combine the manual CS information with information from the lips, taking into account the desynchronization delay (Attina et al. [1], Aboutabit et al. [2]) between these two flows of information. This paper focuses on HMM recognition of the lip flow for Consonant Vowel (CV) syllables in the French Cued Speech production context. The CV syllables are considered in term of viseme groups that are compatible with the CS system. The HMM modeling is based on parameters derived from both the inner and outer lip contours. The global recognition score of CV syllable reaches 80.3%. This study shows that the errors are mainly observed on consonant groups in the context of high and mid-high rounded vowels. In contrast, CV syllables for anterior non rounded vowels ([ a, ε͂, i, œ͂, e, ε]) and for low and mid-low rounded vowels ([ã,ɔ,œ]) are well recognized (in average 87%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-280"
  },
  "lucey07_interspeech": {
   "authors": [
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "A unified approach to multi-pose audio-visual ASR",
   "original": "i07_0650",
   "page_count": 4,
   "order": 281,
   "p1": "650",
   "pn": "653",
   "abstract": [
    "The vast majority of studies in the field of audio-visual automatic speech recognition (AVASR) assumes frontal images of a speaker's face, but this cannot always be guaranteed in practice. Hence our recent research efforts have concentrated on extracting visual speech information from non-frontal faces, in particular the profile view. The introduction of additional views to an AVASR system increases the complexity of the system, as it has to deal with the different visual features associated with the various views. In this paper, we propose the use of linear regression to find a transformation matrix based on synchronous frontal and profile visual speech data, which is used to normalize the visual speech in each viewpoint into a single uniform view. In our experiments for the task of multi-speaker lipreading, we show that this \"pose-invariant\" technique reduces train/test mismatch between visual speech features of different views, and is of particular benefit when there is more training data for one viewpoint over another (e.g. frontal over profile).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-281"
  },
  "seymour07_interspeech": {
   "authors": [
    [
     "Rowan",
     "Seymour"
    ],
    [
     "Darryl",
     "Stewart"
    ],
    [
     "Ji",
     "Ming"
    ]
   ],
   "title": "Audio-visual integration for robust speech recognition using maximum weighted stream posteriors",
   "original": "i07_0654",
   "page_count": 4,
   "order": 282,
   "p1": "654",
   "pn": "657",
   "abstract": [
    "In this paper, we demonstrate for the first time, the robustness of the Maximum Stream Posterior (MSP) method for audio-visual integration on a large speaker- independent speech recognition task in noisy conditions. Furthermore, we show that the method can be generalised and improved by using a softer weighting scheme to account for moderate noise conditions. We call this generalised method the Maximum Weighted Stream Posterior (MWSP) method. In addition, we carry out the first tests of the Posterior Union Model approach for audio-visual integration. All of the methods are compared in digit recognition tests involving various audio and video noise levels and conditions including tests where both modalities are affected by noise. We also introduce a novel form of noise called\n",
    "jitter which is used to simulate camera movement. The results verify that the MSP approach is robust and that its generalised form (MWSP) can lead to further improvements in moderate noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-282"
  },
  "hueber07_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Bruce",
     "Denby"
    ],
    [
     "Gérard",
     "Dreyfus"
    ],
    [
     "Maureen",
     "Stone"
    ]
   ],
   "title": "Continuous-speech phone recognition from ultrasound and optical images of the tongue and lips",
   "original": "i07_0658",
   "page_count": 4,
   "order": 283,
   "p1": "658",
   "pn": "661",
   "abstract": [
    "The article describes a video-only speech recognition system for a \"silent speech interface\" application, using ultrasound and optical images of the voice organ. A one-hour audiovisual speech corpus was phonetically labeled using an automatic speech alignment procedure and robust visual feature extraction techniques. HMM-based stochastic models were estimated separately on the visual and acoustic corpus. The performance of the visual speech recognition system is compared to a traditional acoustic-based recognizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-283"
  },
  "zhu07_interspeech": {
   "authors": [
    [
     "Bo",
     "Zhu"
    ],
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Multimodal speech recognition with ultrasonic sensors",
   "original": "i07_0662",
   "page_count": 4,
   "order": 284,
   "p1": "662",
   "pn": "665",
   "abstract": [
    "In this research we explore multimodal speech recognition by augmenting acoustic information with that obtained by an ultrasonic emitter and receiver. After designing a hardware component to generate a stereo audio/ultrasound signal, we extract sub-band ultrasonic features that supplement conventional MFCC-based audio measurements. A simple interpolation method is used to combine audio and ultrasound model likelihoods. Experiments performed on a noisy continuous digit recognition task indicate that the addition of ultrasonic information reduces word error rates by 24-29% over a wide range of acoustic SNR (20-0 dB).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-284"
  },
  "dean07_interspeech": {
   "authors": [
    [
     "David",
     "Dean"
    ],
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Tim",
     "Wark"
    ]
   ],
   "title": "Fused HMM-adaptation of multi-stream HMMs for audio-visual speech recognition",
   "original": "i07_0666",
   "page_count": 4,
   "order": 285,
   "p1": "666",
   "pn": "669",
   "abstract": [
    "A technique known as fused hidden Markov models (FHMMs) was recently proposed as an alternative multi-stream modelling technique for audio-visual\n",
    "speaker recognition. In this paper we show that for audio-visual speech recognition (AVSR), FHMMs can be adopted as a novel method of training synchronous MSHMMs. MSHMMs, as proposed by several authors for use in AVSR, are jointly trained on both the audio and visual modalities. In contrast our proposed FHMM adaptation method can be used to adapt the multi-stream models from single-stream audio HMMs, and in the process, better model the video speech in the final model when compared to jointly-trained MSHMMs. By experiments conducted on the XM2VTS database we show that the improved video performance of the FHMM-adapted MSHMMs results in an improvement in AVSR performance over jointly-trained MSHMMs at all levels of audio noise, and provide significant advantage in high noise environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-285"
  },
  "ishi07_interspeech": {
   "authors": [
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Analysis of head motions and speech in spoken dialogue",
   "original": "i07_0670",
   "page_count": 4,
   "order": 286,
   "p1": "670",
   "pn": "673",
   "abstract": [
    "With the aim of automatically generating head motions from speech, analyses are conducted for verifying the relations between head motions and linguistic and paralinguistic information carried by speech. Analyses are conducted on motion captured data during natural dialogue. Analysis results showed that nods frequently occur during speech utterances, not only for expressing dialog acts such as agreement and affirmation, but also as indicative of syntactic or semantic units, appearing at the last syllable of the phrases, in strong phrase boundaries. The paper also analyzes the dependence on linguistic, prosodic and voice quality information of other head motions, like shakes and tilts, and discuss about the potentiality for their use in automatic generation of head motions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-286"
  },
  "larsen07_interspeech": {
   "authors": [
    [
     "Lars Bo",
     "Larsen"
    ],
    [
     "Kasper L.",
     "Jensen"
    ],
    [
     "Søren",
     "Larsen"
    ],
    [
     "Morten",
     "Rasmussen"
    ]
   ],
   "title": "A paradigm for mobile speech-centric services",
   "original": "i07_0674",
   "page_count": 4,
   "order": 287,
   "p1": "674",
   "pn": "677",
   "abstract": [
    "The work presented in this paper describes a new paradigm for speech interaction on mobile devices. A general framework for a distributed architecture is introduced and described. This is followed by a discussion of how to design multi modal interfaces affording spoken input. The solution has been to create an architecture capable of supporting several alternative GUIs, e.g. with spoken input, stylus input or a combination. Speech GUIs are designed entirely without GUI widgets requiring stylus or button input, instead relying on highlighting parts of text to create emphasis and steer the users' attention. This is exemplified through the presentation of a prototype for a Car Rental application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-287"
  },
  "campr07_interspeech": {
   "authors": [
    [
     "Pavel",
     "Campr"
    ],
    [
     "Marek",
     "Hrúz"
    ],
    [
     "Miloš",
     "Železný"
    ]
   ],
   "title": "Design and recording of Czech sign language corpus for automatic sign language recognition",
   "original": "i07_0678",
   "page_count": 4,
   "order": 288,
   "p1": "678",
   "pn": "681",
   "abstract": [
    "We describe the design, recording and content of a Czech Sign Language database in this paper. The database is intended for training and testing of sign language recognition (SLR) systems. The UWB-06-SLR-A database contains video data of 15 signers recorded from 3 different views, two of them capture whole body and provide 3D motion data, and third one is focused on signer's face and provide data for face expression feature extraction and for lipreading.\n",
    "The corpus consists of nearly 5 hours of processed and annotated video files which were recorded in laboratory conditions using static illumination. The whole corpus is annotated and pre-processed to be ready to use in SLR experiments. It is composed of 25 selected signs from Czech Sign Language. Each signer performed all of these signs with 5 repetitions. Altogether the database contains more than 5500 video files where each file contains one isolated sign.\n",
    "The purpose of the corpus is to provide data for evaluation of visual parameterizations and sign language recognition techniques. The corpus is pre-processed and each video file is supplemented with a XML data file. It provides information about performed sign (name of sign, type of sign), signer (identification, left or right-handed person), scene (camera position, calibration matrices) and pre-processed data (regions of interests, hands and head trajectories in 3D space).\n",
    "The presented database is collected, preprocessed and is ready to use for subsequent experiments on sign language recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-288"
  },
  "edlund07_interspeech": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Pushy versus meek - using avatars to influence turn-taking behaviour",
   "original": "i07_0682",
   "page_count": 4,
   "order": 289,
   "p1": "682",
   "pn": "685",
   "abstract": [
    "The flow of spoken interaction between human interlocutors is a widely studied topic. Amongst other things, studies have shown that we use a number of facial gestures to improve this flow - for example to control the taking of turns. This type of gestures ought to be useful in systems where an animated talking head is used, be they systems for computer mediated human-human dialogue or spoken dialogue systems, where the computer itself uses speech to interact with users. In this article, we show that a small set of simple interaction control gestures and a simple model of interaction can be used to influence users' behaviour in an unobtrusive manner. The results imply that such a model may improve the flow of computer mediated interaction between humans under adverse circumstances, such as network latency, or to create more human-like spoken human-computer interaction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-289"
  },
  "wand07_interspeech": {
   "authors": [
    [
     "Michael",
     "Wand"
    ],
    [
     "Szu-Chen Stan",
     "Jou"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Wavelet-based front-end for electromyographic speech recognition",
   "original": "i07_0686",
   "page_count": 4,
   "order": 290,
   "p1": "686",
   "pn": "689",
   "abstract": [
    "In this paper we present our investigations on the potential of wavelet-based preprocessing for surface electromyographic speech recognition. We implemented several variants of the Discrete Wavelet Transform and applied them to electromyographical data. First we examined different transforms with various filters and decomposition levels and found that the Redundant Discrete Wavelet Transform performs the best among all tested wavelet transforms. Furthermore, we compared the best wavelet transform to our EMG optimized spectral- and time-domain features. The results showed that the best wavelet transform slightly outperforms the optimized features with 30.9% word error rate compared to 32% for the optimized EMG spectral and time-domain features. Both numbers were achieved on a 108 word vocabulary test set using phone based acoustic models trained on continuously spoken speech captured by EMG.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-290"
  },
  "ferre07_interspeech": {
   "authors": [
    [
     "Gaëlle",
     "Ferré"
    ],
    [
     "Roxane",
     "Bertrand"
    ],
    [
     "Philippe",
     "Blache"
    ],
    [
     "Robert",
     "Espesser"
    ],
    [
     "Stéphane",
     "Rauzy"
    ]
   ],
   "title": "Intensive gestures in French and their multimodal correlates",
   "original": "i07_0690",
   "page_count": 4,
   "order": 291,
   "p1": "690",
   "pn": "693",
   "abstract": [
    "This paper relates a pilot study on intensive gestures in French - e.g. gestures which accompany speech and participate in the highlighting of some discourse elements which the paper means to determine. The study is based on spontaneous French informal conversation and the intensive gestures correlates we looked at pertained to the morphological, prosodic and gestural dimensions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-291"
  },
  "ouni07_interspeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Kais",
     "Ouni"
    ]
   ],
   "title": "Aspects of visual speech in Arabic",
   "original": "i07_0694",
   "page_count": 4,
   "order": 292,
   "p1": "694",
   "pn": "697",
   "abstract": [
    "In this paper, we present a study of visual speech in Arabic. More specifically, we performed a lipreading recognition experiment on Arabic, where a set of consonant-vowel stimuli were presented as visual-only speech and participants were asked to report what they recognized. The overall lipreading scores were consistent with other experiments in other languages. The resulting consonant confusion matrix shows that some of the phonemes were well discriminated, however, for others it depends on the context. Results are discussed based on the category of phonemes and the vowel context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-292"
  },
  "burnham07_interspeech": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Jessica",
     "Reynolds"
    ],
    [
     "Guillaume",
     "Vignali"
    ],
    [
     "Sandra",
     "Bollwerk"
    ],
    [
     "Caroline",
     "Jones"
    ]
   ],
   "title": "Rigid vs non-rigid face and head motion in phone and tone perception",
   "original": "i07_0698",
   "page_count": 4,
   "order": 293,
   "p1": "698",
   "pn": "701",
   "abstract": [
    "There is recent evidence that the visual concomitants, not only of the articulation of phones (consonants & vowels), but also of tones (fundamental frequency variations that signal lexical meaning in tone languages) facilitate speech perception. Analysis of speech production data from a Cantonese speaker suggests that the source of this perceptual information for tones involve rigid motion of the head rather than non-rigid face motion. A perceptual discrimination study was conducted using OPTOTRAK output in which rigid or non-rigid motion of the head could be presented independently, using two conditions: one in which words to be discriminated only differed in tone, and another in which they only differed in phone. The results suggest that non-rigid motion is the critical determinant for successful discrimination of phones, whereas both non-rigid and rigid motion are required for the discrimination of tones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-293"
  },
  "kjellstrom07_interspeech": {
   "authors": [
    [
     "Hedvig",
     "Kjellström"
    ],
    [
     "Olov",
     "Engwall"
    ],
    [
     "Sherif Mahdy",
     "Abdou"
    ],
    [
     "Olle",
     "Bälter"
    ]
   ],
   "title": "Audio-visual phoneme classification for pronunciation training applications",
   "original": "i07_0702",
   "page_count": 4,
   "order": 294,
   "p1": "702",
   "pn": "705",
   "abstract": [
    "We present a method for audio-visual classification of Swedish phonemes, to be used in computer-assisted pronunciation training. The probabilistic kernel-based method is applied to the audio signal and/or either a principal or an independent component (PCA or ICA) representation of the mouth region in video images. We investigate which representation (PCA or ICA) that may be most suitable and the number of components required in the base, in order to be able to automatically detect pronunciation errors in Swedish from audio-visual input. Experiments performed on one speaker show that the visual information help avoiding classification errors that would lead to gravely erroneous feedback to the user; that it is better to perform phoneme classification on audio and video separately and then fuse the results, rather than combining them before classification; and that PCA outperforms ICA for fewer than 50 components.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-294"
  },
  "grauwinkel07_interspeech": {
   "authors": [
    [
     "Katja",
     "Grauwinkel"
    ],
    [
     "Britta",
     "Dewitt"
    ],
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Visual information and redundancy conveyed by internal articulator dynamics in synthetic audiovisual speech",
   "original": "i07_0706",
   "page_count": 4,
   "order": 295,
   "p1": "706",
   "pn": "709",
   "abstract": [
    "This paper reports results of a study investigating the visual information conveyed by the dynamics of internal articulators. Intelligibility of synthetic audiovisual speech with and without visualization of the internal articulator movements was compared. Additionally speech recognition scores were contrasted before and after a short learning lesson in which articulator trajectories were explained, once with and once without motion of internal articulators. Results show that the motion information of internal articulator dynamics did not lead to significant different recognition scores at first, and that only in case of this additional visual information the training lesson was able to significantly increase visual and audiovisual speech intelligibility. After the learning lesson with all internal articulatory movements the visual recognition could be enhanced to a higher degree than the audiovisual recognition. The absolute increase of visual recognition could not be integrated completely into audiovisual recognition. It could be shown that this was due to redundant information conveyed by auditory and visual sources of information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-295"
  },
  "zhou07b_interspeech": {
   "authors": [
    [
     "Wei",
     "Zhou"
    ],
    [
     "Zengfu",
     "Wang"
    ]
   ],
   "title": "A speech rate related lip movement model for speech animation",
   "original": "i07_0710",
   "page_count": 4,
   "order": 296,
   "p1": "710",
   "pn": "713",
   "abstract": [
    "A novel lip movement model related to speech rate is proposed in this paper. The model is constructed based on the research results on the viscoelasticity of skin-muscle tissue and the quantitative relationship between lip muscle force and speech rate. In order to show the validity of the model, we have applied it to our Chinese speech animation system. The experimental results show that our system can synthesize the individualized speech animation with high naturalness at different speech rates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-296"
  },
  "wu07d_interspeech": {
   "authors": [
    [
     "Guanyong",
     "Wu"
    ],
    [
     "Jie",
     "Zhu"
    ]
   ],
   "title": "An extension 2DPCA based visual feature extraction method for audio-visual speech recognition",
   "original": "i07_0714",
   "page_count": 4,
   "order": 297,
   "p1": "714",
   "pn": "717",
   "abstract": [
    "Two dimensional principal component analysis (2DPCA) has been proposed for face recognition as an alternative to traditional PCA transform [1]. In this paper, we extend this approach to the visual feature extraction for audio-visual speech recognition (AVSR). First, a two-stage 2DPCA transform is conducted to extract the visual features. Then, the visemic linear discriminant analysis (LDA) is applied for post extraction processing. We investigate the presented method comparing with traditional PCA and 2DPCA. Experimental results show that the extension 2DPCA can reduce the dimension of 2DPCA and represent the testing mouth images better than PCA does; Moreover, 2DPCA+LDA needs less computation and has a better performance than PCA+LDA in the visual-only speech recognition; Finally, further experimental results demonstrate that our AVSR system using the extension 2DPCA method provides significant enhancement of robustness in noisy environments compared to the audio-only speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-297"
  },
  "lee07b_interspeech": {
   "authors": [
    [
     "Soo-jong",
     "Lee"
    ],
    [
     "Jun",
     "Park"
    ],
    [
     "Eung-kyeu",
     "Kim"
    ]
   ],
   "title": "Preventing an external acoustic noise from being misrecognized as a speech recognition object by confirming the lip movement image signal",
   "original": "i07_0718",
   "page_count": 4,
   "order": 298,
   "p1": "718",
   "pn": "721",
   "abstract": [
    "This paper describes an attempt to prevent an external acoustic noise from being misrecognized as a speech recognition object by confirming the lip movement image signal of a speaker as well as the analysis of the acoustic energy in the speech activity detection procedure, which is the preprocess phase of the speech recognition. An image camera for a PC is added to the existing speech recognition environment, and the collected image is analyzed to capture the movement of lips and classify whether it is acoustic speech made by a human or not. It is possible to determine to continue the recognition process based on the confirmation result of image signal data stored in the shared memory.\n",
    "We combined a speech recognition processor and an image recognizer, and the interworking function successfully operated at the rate of 99.3%. In the case of a subject facing the image camera and speaking, processing normally progressed to the output of the speech recognition result. However, the speech recognition result was not obtained without facing the camera, since the acoustic energy is regarded as noise if any lip movement is not confirmed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-298"
  },
  "hofer07_interspeech": {
   "authors": [
    [
     "Gregor",
     "Hofer"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "Automatic head motion prediction from speech data",
   "original": "i07_0722",
   "page_count": 4,
   "order": 299,
   "p1": "722",
   "pn": "725",
   "abstract": [
    "In this paper we present a novel approach to generate a sequence of head motion units given some speech. The modelling approach is based on the notion that head motion can be divided into a number of short homogeneous units that can be modelled individually. The system is based on Hidden Markov Models (HMM), which are trained on motion units and act as a sequence generator. They can be evaluated by an accuracy measure. A database of motion capture data was collected and manually annotated for head motion and is used to train the models. It was found that the model is good at distinguishing high activity regions from regions with less activity with accuracies around 75 percent. Furthermore the model is able to distinguish different head motion patterns based on speech features somewhat reliably, with accuracies reaching almost 70 percent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-299"
  },
  "denda07b_interspeech": {
   "authors": [
    [
     "Yuki",
     "Denda"
    ],
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Yoichi",
     "Yamashita"
    ]
   ],
   "title": "Omnidirectional audio-visual talker localizer with dynamic feature fusion based on validity and reliability criteria",
   "original": "i07_0726",
   "page_count": 4,
   "order": 300,
   "p1": "726",
   "pn": "729",
   "abstract": [
    "Talker localization is indispensable in video conferencing. Statistical audio-visual (AV) talker localizers that fuse AV features based on prior statistical property are ideals. However, statistical property must be estimated prior to the AV feature fusion procedure. To overcome this problem, this paper proposes a novel robust and omnidirectional AV talker localizer that dynamically fuses AV features based on validity and reliability criteria for eliminating prior statistical property. Direction estimation of speech arriving using equilateral triangular microphone array and human position detection using an omnidirectional video camera extract AV features from captured AV signals. Validity criterion, called audio- or visual-localization counter, validates both features. Reliability criterion, called evaluator of directional-speech arriving, acts as weight for dynamic AV feature fusion. The results of talker localization experiments in an actual office room confirmed that the proposed AV localizer based on dynamic feature fusion is superior to that of the conventional localizer that utilizes either audio or visual features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-300"
  },
  "campbell07_interspeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Damien",
     "Douxchamps"
    ]
   ],
   "title": "Processing image and audio information for recognising discourse participation status through features of face and voice",
   "original": "i07_0730",
   "page_count": 4,
   "order": 301,
   "p1": "730",
   "pn": "733",
   "abstract": [
    "This paper describes a system based on a 360-degree camera with a single microphone that detects speech activity in a roundtable context for the purpose of estimating discourse participation status information for each member present. We have obtained 97% accuracy in detecting participants and have shown that the use of non-verbal and backchannel speech information is a useful indicator of participant status in a discourse.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-301"
  },
  "wojcicki07_interspeech": {
   "authors": [
    [
     "Kamil K.",
     "Wójcicki"
    ],
    [
     "Stephen",
     "So"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "The effect of the additivity assumption on time and frequency domain wiener filtering for speech enhancement",
   "original": "i07_0798",
   "page_count": 4,
   "order": 302,
   "p1": "798",
   "pn": "801",
   "abstract": [
    "In this paper, we investigate the validity of the common assumption made in Wiener filtering that the clean speech and noise signals are uncorrelated under short-time analysis typically used for speech enhancement. In order to achieve this we have performed speech enhancement experiments, where speech corrupted by additive white Gaussian noise is enhanced by a Wiener filter designed in the time as well as the frequency domains. Results of oracle-style experiments confirm that the inclusion of the additivity assumption in Wiener filtering results in negligible degradation of enhanced speech quality. Informal listening tests show that the background noise resulting from time domain enhancement to be more tolerable than the background noise resulting from frequency domain framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-302"
  },
  "li07e_interspeech": {
   "authors": [
    [
     "Junfeng",
     "Li"
    ],
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Satoshi",
     "Hongo"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Yôiti",
     "Suzuki"
    ]
   ],
   "title": "Noise reduction based on adaptive β-order generalized spectral subtraction for speech enhancement",
   "original": "i07_0802",
   "page_count": 4,
   "order": 303,
   "p1": "802",
   "pn": "805",
   "abstract": [
    "Though spectral subtraction has widely been used for speech enhancement, the spectral order β set in spectral subtraction is generally fixed to some constants, resulting in the performance limitation to a certain degree. In this paper, we first analyze the performance of the β-order generalized spectral subtraction in terms of the gain function to highlight its dependence on the value of spectral order β. Based on the analysis results and considering the non-uniform effect of real-world noise on speech signal, we further propose an adaptive β-order generalized spectral subtraction in which the spectral order β is adaptively updated according to the signal-to-noise ratio in each critical band frame by frame as in a sigmoid function. Experimental results in various noise conditions illustrate the superiority of the proposed method with regard to the traditional spectral subtraction methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-303"
  },
  "das07_interspeech": {
   "authors": [
    [
     "Amit",
     "Das"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Class constrained ROVER based speech enhancement",
   "original": "i07_0806",
   "page_count": 4,
   "order": 304,
   "p1": "806",
   "pn": "809",
   "abstract": [
    "A phoneme class based speech enhancement algorithm is proposed that is derived from the family of constrained iterative enhancement schemes. The algorithm is a Rover based solution that overcomes three limitations of the iterative scheme. It removes the dependency of the terminating iteration, employs direct phoneme class constraints, and achieves suppression of audible noise. In the Rover scheme, the degraded utterance is partitioned into segments based on class, and class specific constraints are applied on each segment using a hard decision method. To alleviate the effect of hard decision errors, a GMM based maximum likelihood (ML) soft decision method is also introduced. Performance evaluation is done using Itakura-Saito, segSNR, and PESQ metrics for four noise types at two SNRs. It is shown that the proposed algorithm outperforms other baseline algorithms like Auto-LSP and log-MMSE for all noise types and levels and achieves a greater degree of consistency in improving quality for most phoneme classes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-304"
  },
  "deger07_interspeech": {
   "authors": [
    [
     "Erhan",
     "Deger"
    ],
    [
     "Md. Khademul Islam",
     "Molla"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Md. Kamrul",
     "Hasan"
    ]
   ],
   "title": "EMD based soft-thresholding for speech enhancement",
   "original": "i07_0810",
   "page_count": 4,
   "order": 305,
   "p1": "810",
   "pn": "813",
   "abstract": [
    "This paper introduces a novel speech enhancement method based on Empirical Mode Decomposition (EMD) and soft-thresholding algorithms. A modified soft thresholding strategy is adapted to the intrinsic mode functions (IMF) of the noisy speech. Due to the characteristics of EMD, each obtained IMF of the noisy signal will have different noise and speech energy distribution, thus will have a different noise variance. Based on this specific noise variance, by applying the proposed thresholding algorithm to each IMF separately, it is possible to effectively extract the existing noise components. The experimental results suggest that the proposed method is significantly more effective in removing the noise components from the noisy speech signal compared to recently reported techniques. The significantly better SNR improvement and the speech quality prove the superiority of the proposed algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-305"
  },
  "borowicz07_interspeech": {
   "authors": [
    [
     "Adam",
     "Borowicz"
    ],
    [
     "Alexander",
     "Petrovsky"
    ]
   ],
   "title": "An approximate solution for perceptually constrained signal subspace speech enhancement method",
   "original": "i07_0814",
   "page_count": 4,
   "order": 306,
   "p1": "814",
   "pn": "817",
   "abstract": [
    "In this paper we present a low-complexity version of perceptually constrained signal subspace method (PCSS) for speech enhancement. An approximate solution is presented in a new form which provides perceptually optimal residual noise shaping. The proposed approach does not require a whitening transformation and is sub-optimal for coloured noise. A comparative evaluation of selected methods is performed using objective speech quality measures and informal listening tests. The results show that the approximate method outperforms conventional one and gives comparable results as the exact solution in common situations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-306"
  },
  "fingscheidt07_interspeech": {
   "authors": [
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Suhadi",
     "Suhadi"
    ]
   ],
   "title": "Quality assessment of speech enhancement systems by separation of enhanced speech, noise, and echo",
   "original": "i07_0818",
   "page_count": 4,
   "order": 307,
   "p1": "818",
   "pn": "821",
   "abstract": [
    "Quality assessment of speech enhancement systems is a nontrivial task, especially when (residual) noise and echo signal components occur. We present a signal separation scheme that allows for a detailed analysis of unknown speech enhancement systems in a black box test scenario. Our approach separates the speech, (residual) noise, and (residual) echo component of the speech enhancement system in the sending direction (uplink direction). This makes it possible to independently judge the speech degradation and the noise and echo attenuation/ degradation. While state of the art tests always try to judge the sending direction signal mixture, our new scheme allows a more reliable analysis in shorter time. It will be very useful for testing hands-free devices in practice as well as for testing speech enhancement algorithms in research and development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-307"
  },
  "aicha07_interspeech": {
   "authors": [
    [
     "Anis Ben",
     "Aicha"
    ],
    [
     "Sofia Ben",
     "Jebara"
    ]
   ],
   "title": "Perceptual musical noise reduction using critical bands tonality coefficients and masking thresholds",
   "original": "i07_0822",
   "page_count": 4,
   "order": 308,
   "p1": "822",
   "pn": "825",
   "abstract": [
    "Speech enhancement techniques using spectral subtraction have the drawback of generating an annoying musical noise. We develop a new post-processing method for reducing it in each critical-band. In the proposed technique, the difference between tonality coefficients of the noisy speech and the denoised one constitutes one step for detection. Next, using a modified Johnston masking threshold, we detect the so-called \"critical-band musical noise\". The reduction is simply done by undertaking the power spectral density of detected musical noise under the masking thresholds. Simulation results using different criteria are presented to validate proposed ideas and to show that enhanced speech is characterized by low distortion and inaudible musical noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-308"
  },
  "mauler07_interspeech": {
   "authors": [
    [
     "Dirk",
     "Mauler"
    ],
    [
     "Anil M.",
     "Nagathil"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "On optimal estimation of compressed speech for hearing aids",
   "original": "i07_0826",
   "page_count": 4,
   "order": 309,
   "p1": "826",
   "pn": "829",
   "abstract": [
    "When noise reduction (NR) and dynamic compression (CP) systems are concatenated in a hearing aid or in a cochlear implant we observe undesired interaction effects like the degradation of the global SNR. A reason for this might be that the optimization of the NR algorithm is performed with respect to the uncompressed clean speech only. In this contribution we propose an alternative approach which integrates the CP task in the derivation of the NR algorithm. By this we get novel MMSE and MAP optimal estimators for the compressed clean speech. An analysis of the behavior of the proposed solutions reveals that the differences to a serial concatenation of NR and CP are in general small. In case of the widely used MMSE log spectral amplitude (LSA) estimator [1] we show that the combined optimization is identical to a serial concatenation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-309"
  },
  "hendriks07_interspeech": {
   "authors": [
    [
     "Richard C.",
     "Hendriks"
    ],
    [
     "Jesper",
     "Jensen"
    ],
    [
     "Richard",
     "Heusdens"
    ]
   ],
   "title": "DFT domain subspace based noise tracking for speech enhancement",
   "original": "i07_0830",
   "page_count": 4,
   "order": 310,
   "p1": "830",
   "pn": "833",
   "abstract": [
    "Most DFT domain based speech enhancement methods are dependent on an estimate of the noise power spectral density (PSD). For non-stationary noise sources it is desirable to estimate the noise PSD also in spectral regions where speech is present. In this paper a new method for noise tracking is presented, based on eigenvalue decompositions of correlation matrices that are constructed from time series of noisy DFT coefficients. The presented method can estimate the noise PSD at time-frequency points where both speech and noise are present. In comparison to state-of-the-art noise tracking algorithms the proposed algorithm reduces the estimation error between the estimated and the true noise PSD and improves segmental SNR when combined with an enhancement system with several dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-310"
  },
  "krishnamurthy07_interspeech": {
   "authors": [
    [
     "Nitish",
     "Krishnamurthy"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Noise tracking for speech systems in adverse environments",
   "original": "i07_0834",
   "page_count": 4,
   "order": 311,
   "p1": "834",
   "pn": "837",
   "abstract": [
    "In the design of speech systems, the primary focus is the speech oriented task with the secondary emphasis on sustaining performance under varying operating conditions. Variation in environmental conditions is one of the most important factors that impact speech system performance. In this study, we propose a framework for noise tracking. The proposed noise tracking algorithm is compared with Martin's [1] and Cohen's [2] estimation scheme's for speech enhancement in non-stationary noise conditions. The noise tracking scheme is evaluated over a corpus of three noise types including Babble (BAB), Large Crowd(LCR), and Machine Gun (MGN). The noise modeling scheme for tracking results in a measureable level of improvement for all the noise types (e.g., a 13.7% average relative improvement in Itakura-Saito(IS) measure over 9 noise conditions). This framework is therefore useful for speech applications requiring effective performance for non-stationary environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-311"
  },
  "essebbar07_interspeech": {
   "authors": [
    [
     "Abderrahman",
     "Essebbar"
    ],
    [
     "Tristan",
     "Poinsard"
    ]
   ],
   "title": "Speech enhancement using multi-reference noise reduction in a vehicle environment",
   "original": "i07_0838",
   "page_count": 4,
   "order": 312,
   "p1": "838",
   "pn": "841",
   "abstract": [
    "This paper presents a multi-reference noise reduction system for use in a vehicle. It is aimed at enhancing the driver's speech in noisy driving environments for applications such as voice commands or hands free phone. First, our system objective is briefly presented as well as the problems of classical techniques, like Beamforming, may face in such a harsh environment as a vehicle cabin. Second, a brief analysis of noises aboard a road vehicle is done. Third, the noise reduction architecture comprising a linear and non linear block with two respective non acoustic noise references is presented. Finally, some results obtained in real driving conditions are presented and analysed. Both human listening tests and speech recognition tests prove our system increases global performances compared to what is obtained with classical single channel speech processing methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-312"
  },
  "warsitz07_interspeech": {
   "authors": [
    [
     "Ernst",
     "Warsitz"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Dang Hai Tran",
     "Vu"
    ]
   ],
   "title": "Blind adaptive principal eigenvector beamforming for acoustical source separation",
   "original": "i07_0842",
   "page_count": 4,
   "order": 313,
   "p1": "842",
   "pn": "845",
   "abstract": [
    "For separating multiple speech signals given a convolutive mixture, time-frequency sparseness of the speech sources can be exploited. In this paper we present a multi-channel source separation method based on the concept of approximate disjoint orthogonality of speech signals. Unlike binary masking of single-channel signals as e.g. applied in the DUET algorithm we use a likelihood mask to control the adaptation of blind principal eigenvector beamformers. Furthermore orthogonal projection of the adapted beamformer filters leads to mutually orthogonal filter coefficients thus enhancing the demixing performance. Experimental results in terms of the achievable signal-to-interference ratio (SIR) and a perceptual speech quality measure are given for the proposed method and are compared to the DUET algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-313"
  },
  "koldovsky07_interspeech": {
   "authors": [
    [
     "Zbyněk",
     "Koldovský"
    ],
    [
     "Petr",
     "Tichavský"
    ]
   ],
   "title": "Time-domain blind audio source separation using advanced ICA methods",
   "original": "i07_0846",
   "page_count": 4,
   "order": 314,
   "p1": "846",
   "pn": "849",
   "abstract": [
    "In this paper, a prototype of novel algorithm for blind separation of convolutive mixtures of audio sources is proposed. The method works in time-domain, and it is based on the recently very successful algorithm EFICA for Independent Component Analysis, which is an enhanced version of more famous FastICA. Performance of the new algorithm is very promising, at least, comparable to other (mostly frequency domain) algorithms. Audio separation examples are included.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-314"
  },
  "lee07c_interspeech": {
   "authors": [
    [
     "S. W.",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Model-based speech separation with single-microphone input",
   "original": "i07_0850",
   "page_count": 4,
   "order": 315,
   "p1": "850",
   "pn": "853",
   "abstract": [
    "Prior knowledge of familiar auditory patterns is essential for separating sound sources in human auditory processing. Speech recognition modeling is one probabilistic way for capturing these familiar auditory patterns. In this paper we focus on separating speech sources with a single-microphone input only. A model-based algorithm is proposed to generate target speech by estimating its spectral envelope trajectory and filtering irrelevant harmonic structure of the interference. The spectral trajectory is optimally regenerated in the form of line spectrum pair (LSP) parameters. Experiments on separating mixed speech sources are presented. Objective evaluation shows that interference is significantly reduced and the output speech is highly intelligible and sounds fairly clear.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-315"
  },
  "kinoshita07_interspeech": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Masato",
     "Miyoshi"
    ]
   ],
   "title": "Multi-step linear prediction based speech dereverberation in noisy reverberant environment",
   "original": "i07_0854",
   "page_count": 4,
   "order": 316,
   "p1": "854",
   "pn": "857",
   "abstract": [
    "A speech signal captured by a distant microphone is generally contaminated by reverberation and background noise, which severely degrade the automatic speech recognition (ASR) performance. In this paper, we first extend a previously proposed single channel dereverberation algorithm to a multi-channel scenario. The method estimates late reflections using multi-channel multi-step linear prediction, and then suppresses them in the power spectral domain. Second, we analyze the effect of additive noise on the proposed method and provide one solution to the noisy reverberant environment. Experimental results show that the proposed method achieves good dereverberation in noisy reverberant environments, and can significantly improve the ASR performance to that obtained for a non-reverberant environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-316"
  },
  "lee07d_interspeech": {
   "authors": [
    [
     "Seung Yeol",
     "Lee"
    ],
    [
     "Jong Won",
     "Shin"
    ],
    [
     "Hwan Sik",
     "Yun"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "A statistical model based post-filtering algorithm for residual echo suppression",
   "original": "i07_0858",
   "page_count": 4,
   "order": 317,
   "p1": "858",
   "pn": "861",
   "abstract": [
    "In this paper, we propose a novel residual echo suppression (RES) algorithm constructed in the acoustic echo canceller. In the proposed approach, we introduce a statistical model to detect the signal components of the output signal and the state of signal is classified into four distinct hypothesis depending on the activity of near-end signal and residual echo. For hypothesis testing, the conventional likelihood ratio test is performed to make an optimal decision. The parameters specified in terms of the power spectral densities can be obtained by updating according to the hypothesis testing results and we can obtain the optimal RES filter by adopting the estimated parameters. The experimental results show that the proposed algorithm yields improved performance compared to that of the previous RES technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-317"
  },
  "huang07_interspeech": {
   "authors": [
    [
     "Xiaoshan",
     "Huang"
    ],
    [
     "Xiaoqun",
     "Zhao"
    ]
   ],
   "title": "An optimal speech enhancement under speech uncertainty probability and masking property of auditory system",
   "original": "i07_0862",
   "page_count": 4,
   "order": 318,
   "p1": "862",
   "pn": "865",
   "abstract": [
    "Recently, I. Cohen has presented causal and noncausal algorithms to modify the classic decision-directed approach for prior SNR. It is well-known that prior SNR is critical to trade off the musical noise level and the audible clearness level in spectral subtraction speech enhancement. However, all these algorithms conflict with statistical signal model more or less. To adjust smoothing parameters which play an important role on the recursive procedure of prior SNR and noise spectrum estimate more reasonably, we present novel speech uncertainty state model which capitalizes on the masking property of auditory system, and propose a new modified approach which employs speech uncertainty probability to make automatic adaptation of smoothing parameters. Novel algorithm is capable of eliminating musical noise meanwhile lowering speech distortion by remaining original speech in the case of inaudible noise under masking threshold. Experiments confirm that novel algorithm is superior to classic methods, particularly at low SNR environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-318"
  },
  "maier07_interspeech": {
   "authors": [
    [
     "Viktoria",
     "Maier"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Temporal episodic memory model: an evolution of minerva2",
   "original": "i07_0866",
   "page_count": 4,
   "order": 319,
   "p1": "866",
   "pn": "869",
   "abstract": [
    "This paper introduces a new model for automatic speech recognition (ASR) called TEMM - Temporal Episodic Memory Model. TEMM is derived from a simulation of human episodic memory called Minerva2, and it not only overcomes the inability of Minerva2 to use temporal sequence for recognition flexibly, but it also employs a prediction mechanism as an additional source of information. The performance of TEMM on an ASR task is compared to state-of-the-art HMM/GMM baseline systems, and a first analysis shows both promising results and a need to further stabilise the consistency of the output of the new model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-319"
  },
  "coro07_interspeech": {
   "authors": [
    [
     "Gianpaolo",
     "Coro"
    ],
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Fulvio",
     "Caropreso"
    ]
   ],
   "title": "Speech recognition with factorial-HMM syllabic acoustic models",
   "original": "i07_0870",
   "page_count": 4,
   "order": 320,
   "p1": "870",
   "pn": "873",
   "abstract": [
    "Approaches in Automatic Speech Recognition based on classic acoustic models seem not to exploit all the information lying in a speech signal; furthermore decoding procedures have real time constraints preventing the system to achieve optimal alignment between acoustic models and signal. In this paper, we present an approach to speech recognition in which Factorial Hidden Markov Models (FHMM) are used as syllabic acoustic models. An alignment algorithm is used for unit decoding. As applicative domain we choose numbers (range 0-999,999) uttered in Italian. Syllabic accuracy in our model is 84.81%, correctness on numbers is 77.74%. Aim of the experiment is to show that the performances of FHMMs lie in the ability to retrieve the presence of two different temporal dynamics in a speech segments: the former with a quasi-segmental timing, the latter presenting a quasi-syllabic trend. Moreover, we evaluate a unit decoding process based on a dynamic programming algorithm in order to exploit the acoustic models performances at best.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-320"
  },
  "wachter07_interspeech": {
   "authors": [
    [
     "Mathias De",
     "Wachter"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Patrick",
     "Wambacq"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Evaluating acoustic distance measures for template based recognition",
   "original": "i07_0874",
   "page_count": 4,
   "order": 321,
   "p1": "874",
   "pn": "877",
   "abstract": [
    "In this paper we investigate the behaviour of different acoustic distance measures for template based speech recognition in light of the combination of acoustic distances, linguistic knowledge and template concatenation fluency costs. To that end, different acoustic distance measures are compared on tasks with varying levels of fluency/linguistic constraints. We show that the adoption of those constraints invariably results in an acoustically clearly suboptimal template sequence being chosen as the winning hypothesis. There are strong implications for the design of acoustic distance measures: distance measures that are optimal for frame based classification may prove to be suboptimal for full sentence recognition. In particular, we show this is the case when comparing the Euclidean and the recently introduced adaptive kernel local Mahalanobis distance measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-321"
  },
  "han07_interspeech": {
   "authors": [
    [
     "Yan",
     "Han"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Hierarchical acoustic modeling based on random-effects regression for automatic speech recognition",
   "original": "i07_0878",
   "page_count": 4,
   "order": 322,
   "p1": "878",
   "pn": "881",
   "abstract": [
    "Recent research on human intelligence [1] suggests that the auditory system has a hierarchical structure, in which the lower levels store individual properties, and the upper levels store the group properties of utterances. However, most of the conventional automatic recognizers adopt a single level model structure. In structure-based models, such as HMM and parametric trajectory models, only the group properties of utterances are modeled. In template-based models, only the individual properties of utterances are exploited. In this paper, we propose a novel hierarchical acoustic model to simulate the human auditory hierarchy, in which both the group and the individual properties of utterances can be explicitly addressed. Furthermore, we developed two evaluation methods, namely bottom-up and top-down test, to simulate the prediction-verification loops in human hearing. The model is evaluated on a TIMIT vowel classification task. The proposed hierarchical model significantly outperforms parametric trajectory models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-322"
  },
  "hamalainen07_interspeech": {
   "authors": [
    [
     "Annika",
     "Hämäläinen"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Construction and analysis of multiple paths in syllable models",
   "original": "i07_0882",
   "page_count": 4,
   "order": 323,
   "p1": "882",
   "pn": "885",
   "abstract": [
    "In this paper, we construct multi-path syllable models using phonetic knowledge for initialising the parallel paths, and a data-driven solution for their re-estimation. We hypothesise that the richer topology of multi-path syllable models would be better at accounting for pronunciation variation than context-dependent phone models that can only account for the effects of left and right neighbours. We show that parallel paths that are initialised with phonetic knowledge and then re-estimated do indeed result in different trajectories in feature space. Yet, this does not result in better recognition performance. We suggest explanations for this finding, and provide the reader with important insights into the issues playing a role in pronunciation variation modelling with multi-path syllable models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-323"
  },
  "espywilson07_interspeech": {
   "authors": [
    [
     "Carol Y.",
     "Espy-Wilson"
    ],
    [
     "Tarun",
     "Pruthi"
    ],
    [
     "Amit",
     "Juneja"
    ],
    [
     "Om",
     "Deshmukh"
    ]
   ],
   "title": "Landmark-based approach to speech recognition: an alternative to HMMs",
   "original": "i07_0886",
   "page_count": 4,
   "order": 324,
   "p1": "886",
   "pn": "889",
   "abstract": [
    "In this paper, we compare a Probabilistic Landmark-Based speech recognition System (LBS) which uses Knowledge-based Acoustic Parameters (APs) as the front-end with an HMM-based recognition system that uses the Mel-Frequency Cepstral Coefficients as its front end. The advantages of LBS based on APs are (1) the APs are normalized for extra-linguistic information, (2) acoustic analysis at different landmarks may be performed with different resolutions and with different APs, (3) LBS outputs multiple acoustic landmark sequences that signal perceptually significant regions in the speech signal, (4) it may be easier to port this system to another language since the phonetic features captured by the APs are universal, and (5) LBS can be used as a tool for uncovering and subsequently understanding variability. LBS also has a probabilistic framework that can be combined with pronunciation and language models in order to make it more scalable to large vocabulary recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-324"
  },
  "asakawa07_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Asakawa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Automatic recognition of connected vowels only using speaker-invariant representation of speech dynamics",
   "original": "i07_0890",
   "page_count": 4,
   "order": 325,
   "p1": "890",
   "pn": "893",
   "abstract": [
    "Speech acoustics vary due to differences in gender, age, microphone, room, lines, and a variety of factors. In speech recognition research, to deal with these inevitable non-linguistic variations, thousands of speakers in different acoustic conditions were prepared to train acoustic models of individual phonemes. Recently, a novel representation of speech dynamics was proposed [1, 2], where the above non-linguistic factors are effectively removed from speech as if pitch information is removed from spectrum by its smoothing. This representation captures only speaker- and microphone-invariant speech dynamics and no absolute or static acoustic properties such as spectrums are used. With them, speaker identity has to remain in speech representation. In our previous study, the new representation was applied to recognizing a sequence of isolated vowels [3]. The proposed method with a single training speaker outperformed the conventional HMMs trained with more than four thousand speakers even in the case of noisy speech. The current paper shows the initial results of applying the dynamic representation to recognizing continuous speech, that is connected vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-325"
  },
  "togneri07_interspeech": {
   "authors": [
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "A structured speech model parameterized by recursive dynamics and neural networks",
   "original": "i07_0894",
   "page_count": 4,
   "order": 326,
   "p1": "894",
   "pn": "897",
   "abstract": [
    "We present in this paper an overview of the Hidden Dynamic Model (HDM) paradigm, exemplifying parametric construction of structure-based speech models that can be used for recognition purposes. We explore a general class of the HDM that uses recursive, autoregression functions to represent the hidden speech dynamics, and uses neural networks to represent the functional relationship between the hidden and observed speech vectors. This type of state-space formulation of the HDM is reviewed in terms of model construction, a parameter estimation technique, and a decoding method. We also present some typical experimental results on the use of this type of HDMs for phonetic recognition and for automatic vocal tract resonance tracking. We further provide analyses on the computational complexity (for decoding) and the parameter size of the HDM in comparison with the HMM. Finally, we discuss several key issues related to future exploration of the HDM paradigm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-326"
  },
  "deng07b_interspeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Structure-based and template-based automatic speech recognition - comparing parametric and non-parametric approaches",
   "original": "i07_0898",
   "page_count": 4,
   "order": 327,
   "p1": "898",
   "pn": "901",
   "abstract": [
    "This paper provides an introductory tutorial for the Interspeech07 special session on \"Structure-Based and Template-Based Automatic Speech Recognition\". The purpose of the special session is to bring together researchers who have special interest in novel techniques that are aimed at overcoming weaknesses of HMMs for acoustic modeling in speech recognition. Numerous such approaches have been taken over the past dozen years, which can be broadly classified into structured-based (parametric) and template-based (non-parametric) ones. In this paper, we will provide an overview of both approaches, focusing on the incorporation of long-range temporal dependencies of the speech features and phonetic detail in speech recognition algorithms. We will provide a high-level survey on major existing work and systems using these two types of \"beyond-HMM\" frameworks. The contributed papers in this special session will elaborate further on the related topics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-327"
  },
  "grangier07_interspeech": {
   "authors": [
    [
     "David",
     "Grangier"
    ],
    [
     "Samy",
     "Bengio"
    ]
   ],
   "title": "Learning the inter-frame distance for discriminative template-based keyword detection",
   "original": "i07_0902",
   "page_count": 4,
   "order": 328,
   "p1": "902",
   "pn": "905",
   "abstract": [
    "This paper proposes a discriminative approach to template-based keyword detection. We introduce a method to learn the distance used to compare acoustic frames, a crucial element for template matching approaches. The proposed algorithm estimates the distance from data, with the objective to produce a detector maximizing the Area Under the receiver operating Curve (AUC), i.e. the standard evaluation measure for the keyword detection problem. The experiments performed over a large corpus, SpeechDatII, suggest that our model is effective compared to an HMM system, e.g. the proposed approach reaches 93.8% of averaged AUC compared to 87.9% for the HMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-328"
  },
  "yu07c_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Handling phonetic context and speaker variation in a structure-based speech recognizer",
   "original": "i07_0906",
   "page_count": 4,
   "order": 329,
   "p1": "906",
   "pn": "909",
   "abstract": [
    "Recently we have developed a novel type of structure-based speech recognizer, which uses parameterized, non-recursive \"hidden\" trajectory model of vocal tract resonances (VTR) or formants to capture the dynamic structure of long-range speech coarticulation and reduction. The underlying model of this recognizer carries out bi-directional FIR filtering on the piecewise constant sequences of the VTR targets. In this paper, we elaborate on two key aspects of the model. First, the phonetic context controls the movement direction and thus the formation of the VTR trajectories. This provides \"structured\" context dependency for speech acoustics without using context dependent parameters as required by HMMs. Second, VTR targets as the key context-independent parameters of the model vary across speakers. We describe an effective target-value normalization algorithm that can be applied to both training and unknown test speakers. We report experimental results demonstrating the effectiveness of the normalization algorithm in the context of structure-based speech recognition. We also provide computational analysis on the HTM-based speech decoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-329"
  },
  "segbroeck07_interspeech": {
   "authors": [
    [
     "Maarten Van",
     "Segbroeck"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Vector-quantization based mask estimation for missing data automatic speech recognition",
   "original": "i07_0910",
   "page_count": 4,
   "order": 330,
   "p1": "910",
   "pn": "913",
   "abstract": [
    "The application of Missing Data Theory (MDT) has shown to improve the robustness of automatic speech recognition (ASR) systems. A crucial part in a MDT-based recognizer is the computation of the reliability masks from noisy data. To estimate accurate masks in environments with unknown, non-stationary noise statistics only weak assumptions can be made about the noise and we need to rely on a strong model for the speech. In this paper, we present a missing data detector that uses harmonicity in the noisy input signal and a vector quantizer (VQ) to confine speech models to a subspace. The resulting system can deal with additive and convolutional noise and shows promising results on the Aurora4 large vocabulary database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-330"
  },
  "demange07_interspeech": {
   "authors": [
    [
     "Sébastien",
     "Demange"
    ],
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Accurate marginalization range for missing data recognition",
   "original": "i07_0914",
   "page_count": 4,
   "order": 331,
   "p1": "914",
   "pn": "917",
   "abstract": [
    "Missing data recognition has been proposed to increase noise robustness of automatic speech recognition. This strategy relies on the use of a spectrographic mask that gives information about the true clean speech energy of a corrupted signal. This information is then used to refine the data process during the decoding step. We propose in this work a new mask that provides more information about the clean speech contribution than classical masks based on a Signal to Noise Ratio (SNR) thresholding. The proposed mask is described and compared to another missing data approach based on SNR thresholding. Experimental results show a significant word error rate reduction induced by the proposed approach. Moreover, the proposed mask outperforms the ETSI advanced front-end on the HIWIRE corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-331"
  },
  "kuhne07_interspeech": {
   "authors": [
    [
     "Marco",
     "Kühne"
    ],
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Sven",
     "Nordholm"
    ]
   ],
   "title": "Smooth soft mel-spectrographic masks based on blind sparse source separation",
   "original": "i07_0918",
   "page_count": 4,
   "order": 332,
   "p1": "918",
   "pn": "921",
   "abstract": [
    "This paper investigates the use of DUET, a recently proposed blind source separation method, as front-end for missing data speech recognition. Based on the attenuation and delay estimation in stereo signals soft time-frequency masks are designed to extract a target speaker from a mixture containing multiple speech sources. A postprocessing step is introduced in order to remove isolated mask points that can cause insertion errors in the speech decoder. The results for connected digit experiments in a multi-speaker environment demonstrate that the proposed soft masks closely match the performance of the oracle mask designed with a priori knowledge of the source spectra.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-332"
  },
  "laidler07_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Laidler"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Neil D.",
     "Lawrence"
    ]
   ],
   "title": "Model-driven detection of clean speech patches in noise",
   "original": "i07_0922",
   "page_count": 4,
   "order": 333,
   "p1": "922",
   "pn": "925",
   "abstract": [
    "Listeners may be able to recognise speech in adverse conditions by \"glimpsing\" time-frequency regions where the target speech is dominant. Previous computational attempts to identify such regions have been source-driven, using primitive cues. This paper describes a model-driven approach in which the likelihood of spectro-temporal patches of a noisy mixture representing speech is given by a generative model. The focus is on patch size and patch modelling. Small patches lead to a lack of discrimination, while large patches are more likely to contain contributions from other sources. A \"cleanness\" measure reveals that a good patch size is one which extends over a quarter of the speech frequency range and lasts for 40 ms. Gaussian mixture models are used to represent patches. A compact representation based on a 2D discrete cosine transform leads to reasonable speech/background discrimination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-333"
  },
  "stern07_interspeech": {
   "authors": [
    [
     "Richard M.",
     "Stern"
    ],
    [
     "Evandro B.",
     "Gouvêa"
    ],
    [
     "Govindarajan",
     "Thattai"
    ]
   ],
   "title": "polyaural array processing for automatic speech recognition in degraded environments",
   "original": "i07_0926",
   "page_count": 4,
   "order": 334,
   "p1": "926",
   "pn": "929",
   "abstract": [
    "In this paper we present a new method of signal processing for robust speech recognition using multiple microphones. The method, loosely based on the human binaural hearing system, consists of passing the speech signals detected by multiple microphones through bandpass filtering and nonlinear halfwave rectification operations, and then cross-correlating the outputs from each channel within each frequency band. These operations provide rejection of off-axis interfering signals. These operations are repeated (in a non-physiological fashion) for the negative of the signal, and an estimate of the desired signal is obtained by combining the positive and negative outputs. We demonstrate that the use of this approach provides substantially better recognition accuracy than delay-and-sum beamforming using the same sensors for target signals in the presence of additive broadband and speech maskers. Improvements in reverberant environments are tangible but more modest.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-334"
  },
  "morales07b_interspeech": {
   "authors": [
    [
     "Nicolás",
     "Morales"
    ],
    [
     "Liang",
     "Gu"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Adding noise to improve noise robustness in speech recognition",
   "original": "i07_0930",
   "page_count": 4,
   "order": 335,
   "p1": "930",
   "pn": "933",
   "abstract": [
    "In this work we explore a technique for increasing recognition accuracy on speech affected by corrupting noise of an undetermined nature, by the addition of a known and well-behaved noise (masking noise). The same type of noise used for masking is added to the training data, thus reducing the gap between training and test conditions, independent of the type of corrupting noise, or whether it is stationary or not. While still in an early development stage, the new approach shows consistent improvements in accuracy and robustness for a variety of conditions, where no use is made of a-priori knowledge of the corrupting noise. The approach is shown to be of particular interest to the case of cross-talk corrupting noise, a complicated situation in speech recognition for which the relative gain with the proposed approach is over 24%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-335"
  },
  "foslerlussier07_interspeech": {
   "authors": [
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Laura",
     "Dilley"
    ],
    [
     "Na'im",
     "Tyson"
    ],
    [
     "Mark",
     "Pitt"
    ]
   ],
   "title": "The buckeye corpus of speech: updates and enhancements",
   "original": "i07_0934",
   "page_count": 4,
   "order": 336,
   "p1": "934",
   "pn": "937",
   "abstract": [
    "This paper describes recent progress in the development of the Buckeye Corpus of Speech, a phonetically labeled corpus of conversational American English speech, first described in [1]. With the publication of the second phase of transcription, the corpus has nearly doubled in size from the first release. We briefly give an overview of the corpus, report on additional studies of inter-labeler agreement, and describe a new GUI designed to facilitate searching the annotated speech files.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-336"
  },
  "barroso07_interspeech": {
   "authors": [
    [
     "N.",
     "Barroso"
    ],
    [
     "A.",
     "Ezeiza"
    ],
    [
     "N.",
     "Gilisagasti"
    ],
    [
     "K. López de",
     "Ipiña"
    ],
    [
     "A.",
     "López"
    ],
    [
     "J. M.",
     "López"
    ]
   ],
   "title": "Development of multimodal resources for multilingual information retrieval in the basque context",
   "original": "i07_0938",
   "page_count": 4,
   "order": 337,
   "p1": "938",
   "pn": "941",
   "abstract": [
    "The development of Automatic Index Systems requires appropriate Multimodal Resources (MR) to design all the components of the system. The project the authors are involved in implements a baseline Multimodal Index System for users in the Basque Country, so it is essential to cover all the languages spoken: Basque, Spanish, and French. Since the specific goal tackled in this work is the development of a system to search information in audio files, this paper summarizes the ongoing efforts to develop resources for the Multilingual Continuous Speech Recognition system that would be available for researchers interested in this field.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-337"
  },
  "schwartz07_interspeech": {
   "authors": [
    [
     "Reva",
     "Schwartz"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "Joseph",
     "Campbell"
    ],
    [
     "Shelley",
     "Paget"
    ],
    [
     "Julie",
     "Vonwiller"
    ],
    [
     "Dominique",
     "Estival"
    ],
    [
     "Christopher",
     "Cieri"
    ]
   ],
   "title": "Construction of a phonotactic dialect corpus using semiautomatic annotation",
   "original": "i07_0942",
   "page_count": 4,
   "order": 338,
   "p1": "942",
   "pn": "945",
   "abstract": [
    "In this paper, we discuss rapid, semiautomatic annotation techniques of detailed phonological phenomena for large corpora. We describe the use of these techniques for the development of a corpus of American English dialects. The resulting annotations and corpora will support both large-scale linguistic dialect analysis and automatic dialect identification. We delineate the semiautomatic annotation process that we are currently employing and, a set of experiments we ran to validate this process. From these experiments, we learned that the use of ASR techniques could significantly increase the throughput and consistency of human annotators.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-338"
  },
  "abdennadher07_interspeech": {
   "authors": [
    [
     "Slim",
     "Abdennadher"
    ],
    [
     "Mohamed",
     "Aly"
    ],
    [
     "Dirk",
     "Bühler"
    ],
    [
     "Wolfgang",
     "Minker"
    ],
    [
     "Johannes",
     "Pittermann"
    ]
   ],
   "title": "BECAM tool - a semi-automatic tool for bootstrapping emotion corpus annotation and management",
   "original": "i07_0946",
   "page_count": 4,
   "order": 339,
   "p1": "946",
   "pn": "949",
   "abstract": [
    "Corpus annotation is an important aspect in speech applications where stochastic models need to be trained and evaluated. Multimodal corpora are also annotated. Moreover, corpus annotation is an essential phase in the construction of emotion recognizer engines. Large corpora, as they are essential to construct representative knowledge bases, have been a problem for corpus annotators. Time consumed for labeling such corpora is very significant. Furthermore, manageability becomes more arduous and tedious. In this paper, we propose a semi-automatic tool, called BECAM tool, that will help corpus annotators in managing and annotating large sample emotion corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-339"
  },
  "cieri07_interspeech": {
   "authors": [
    [
     "Christopher",
     "Cieri"
    ],
    [
     "Linda",
     "Corson"
    ],
    [
     "David",
     "Graff"
    ],
    [
     "Kevin",
     "Walker"
    ]
   ],
   "title": "Resources for new research directions in speaker recognition: the mixer 3, 4 and 5 corpora",
   "original": "i07_0950",
   "page_count": 4,
   "order": 340,
   "p1": "950",
   "pn": "953",
   "abstract": [
    "This paper describes new language resources designed to support research in speaker recognition. It begins with a brief overview of collections protocols, motivates the shift from the Switchboard protocol to the Mixer protocol, summarizes yields from the earliest phase of Mixer collection and then describes more recent phases, yields and expected yields and lessons learned.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-340"
  },
  "heeman07_interspeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Andy",
     "McMillin"
    ],
    [
     "J. Scott",
     "Yaruss"
    ]
   ],
   "title": "Intercoder reliability in annotating complex disfluencies",
   "original": "i07_0954",
   "page_count": 4,
   "order": 341,
   "p1": "954",
   "pn": "957",
   "abstract": [
    "In previous work, we presented an annotation scheme that can describe complex disfluencies. In this paper, we first show the prevalence of complex disfluencies and illustrate the types of distinctions that our scheme allows. Second, we present an annotation tool that allows the scheme to be easily applied. Third, we present the results of a reliability study in annotating complex disfluencies with the annotation tool. We find that subjects, even with a minimal amount of training, achieve high intercoder agreement. This work will help pave the way for speech recognizers to precisely model the structure of disfluencies, both for understanding conversational speech of non-stutterers and for assessing stuttering severity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-341"
  },
  "radfar07_interspeech": {
   "authors": [
    [
     "M. H.",
     "Radfar"
    ],
    [
     "R. M.",
     "Dansereau"
    ]
   ],
   "title": "Single channel speech separation using maximum a posteriori estimation",
   "original": "i07_0958",
   "page_count": 4,
   "order": 342,
   "p1": "958",
   "pn": "961",
   "abstract": [
    "We present a new approach for separating two speech signals when only a single recording of their additive mixture is available. In this approach, log spectra of the sources are estimated using maximum a posteriori estimation given the mixture's log spectrum and the probability density functions of the sources. It is shown that the estimation leads to a two-state, non-linear filter whose states are controlled by the means of the sources. The first state of the filter is expressed using a combination of two Wiener filters whose parameters are controlled by the means and variances of the sources and noise variance and the second state is expressed by the means of the sources. Through the experiments, conducted on a wide variety of mixtures, we show that the MAP based estimator outperforms the methods which use binary mask filtering or Wiener filtering for the separation task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-342"
  },
  "suhadi07_interspeech": {
   "authors": [
    [
     "Suhadi",
     "Suhadi"
    ],
    [
     "Tim",
     "Fingscheidt"
    ]
   ],
   "title": "Speech enhancement with improved a posteriori SNR computation",
   "original": "i07_0962",
   "page_count": 4,
   "order": 343,
   "p1": "962",
   "pn": "965",
   "abstract": [
    "In speech enhancement, the decision-directed (DD) approach to compute the a priori SNR is often used to reduce the musical tones. However, the constant DD weighting factor very close to one results in more speech distortion during transitional speech segments. Contrarily, a time-varying weighting factor gives less speech distortion but with more residual noise in speech pause. In this contribution we present a new a posteriori SNR computation to relax the dependence on the decision-directed weighting factor. By computing the a posteriori SNR with a time-varying weighting factor, we actually derive a correction factor to the time-varying DD weighting factor resulting in less speech distortion during transitions, as well as less residual noise in speech pause.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-343"
  },
  "tat07_interspeech": {
   "authors": [
    [
     "Thang Vu",
     "Tat"
    ],
    [
     "Germine",
     "Seide"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Method of LP-based blind restoration for improving intelligibility of bone-conducted speech",
   "original": "i07_0966",
   "page_count": 4,
   "order": 344,
   "p1": "966",
   "pn": "969",
   "abstract": [
    "Bone-conducted (BC) speech in an extremely noisy environment is stable against surrounding noise so that it may be able to be used instead of air-conducted (AC) speech for communication. However, it has very poor sound quality and its intelligibility is degraded when transmitted through bone conduction. Therefore, voice-quality and the intelligibility of BC speech need to be blindly improved in actual speech communication and this is a challenging new topic in the speech signal-processing field. We proposed an LP-based model to restore BC speech to improve its voice-quality in a previous study. While other methods such as Long-term Fourier transform need to use numerous AC speech parameters to restore BC speech, the proposed model can blindly restore BC speech by predicting BC-LP coefficients from AC-LP coefficients. We improved the proposed model by (1) extending long-term processing to frame-basis processing, (2) using LSF coefficients on LP representation, and (3) using a recurrent neural network for predicting parameters. We evaluated the improved model in comparison with other models to find out whether the model could adequately improve voice quality and the intelligibility of BC speech, using objective measures (LSD, MCD, and LCD) and carrying out Modified Rhyme Tests (MRTs). An evaluation of these three improvements to the LP-based model proved the practicability of blind-BC restoration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-344"
  },
  "falk07b_interspeech": {
   "authors": [
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Svante",
     "Stadler"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ],
    [
     "Wai-Yip",
     "Chan"
    ]
   ],
   "title": "Noise suppression based on extending a speech-dominated modulation band",
   "original": "i07_0970",
   "page_count": 4,
   "order": 345,
   "p1": "970",
   "pn": "973",
   "abstract": [
    "Previous work on bandpass modulation filtering for noise suppression has resulted in unwanted perceptual artifacts and decreased speech clarity. Artifacts are introduced mainly due to half-wave rectification, which is employed to correct for negative power spectral values resultant from the filtering process. In this paper, modulation frequency estimation (i.e., bandwidth extension) is used to improve perceptual quality. Experiments demonstrate that speech-component lowpass modulation content can be reliably estimated from bandpass modulation content of speech-plus-noise components. Subjective listening tests corroborate that improved quality is attained when the removed speech lowpass modulation content is compensated for by the estimate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-345"
  },
  "abolhassani07_interspeech": {
   "authors": [
    [
     "Amin Haji",
     "Abolhassani"
    ],
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Mohamed-Faouzi",
     "Harkat"
    ]
   ],
   "title": "Speech enhancement using PCA and variance of the reconstruction error model identification",
   "original": "i07_0974",
   "page_count": 4,
   "order": 346,
   "p1": "974",
   "pn": "977",
   "abstract": [
    "We present in this paper a subspace approach for enhancing a noisy speech signal. The original algorithm for model identification from which we have derived our method has been used in the field of fault detection and diagnosis. This algorithm is based on principal component analysis in which the optimal subspace selection is provided by a variance of the reconstruction error (VRE) criterion. This choice overcomes many limitations encountered with other selection criteria, like overestimation of the signal subspace or the need for empirical parameters. We have also extended our subspace algorithm to take into account the case of colored and babble noise. The performance evaluation, which is made on the Aurora database shows that our method provides a higher noise reduction and a lower signal distortion than existing enhancement methods. Our algorithm succeeds in enhancing the noisy speech in all noisy conditions without introducing artifacts such as \"musical noise\".\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-346"
  },
  "shin07_interspeech": {
   "authors": [
    [
     "Jong Won",
     "Shin"
    ],
    [
     "Woohyung",
     "Lim"
    ],
    [
     "Junesig",
     "Sung"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Speech reinforcement based on partial specific loudness",
   "original": "i07_0978",
   "page_count": 4,
   "order": 347,
   "p1": "978",
   "pn": "981",
   "abstract": [
    "In the presence of background noise, the perceptual loudness of speech signal significantly decreases resulting in the deterioration of intelligibility and clarity. In this paper, we propose a novel approach to enhance the quality of speech signal when the additive noise cannot be directly controlled. Specifically, we propose an approach which reinforces the speech signal so that the partial loudness in each band can be maintained to the level almost the same to that measured without the effect of background noise. To find a suitable reinforcement rule, the loudness perception model proposed by Moore et al. [1] is adopted. Experimental results show that the loudness of the original noise-free speech can be restored by the proposed reinforcement algorithm and the proposed algorithm can enhance the perceived quality of speech signal under various noise environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-347"
  },
  "rathcke07_interspeech": {
   "authors": [
    [
     "Tamara",
     "Rathcke"
    ],
    [
     "Jonathan",
     "Harrington"
    ]
   ],
   "title": "The phonetics and phonology of high and low tones in two falling f0-contours in standard German",
   "original": "i07_0982",
   "page_count": 4,
   "order": 348,
   "p1": "982",
   "pn": "985",
   "abstract": [
    "The present paper reports the results of an imitation experiment developed to evaluate empirically the validity of the AM-analyses given for two falling f0-patterns in German by different researchers. We look at the phonetic realisations of temporal alignment and frequency scaling of high and low tonal targets in varying syllabic environments. The effects of two phonetic factors were tested: (1) syllable structure of the postnuclear part of a phrase and (2) syllabic structure of the nuclear syllable. The results show that scaling and alignment are affected by the investigated factors in an unexpected way, so that predictions from different AM-analyses could not be confirmed by the data. We discussed the implications of the results in the light of the proposed analyses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-348"
  },
  "john07_interspeech": {
   "authors": [
    [
     "Tina",
     "John"
    ],
    [
     "Jonathan",
     "Harrington"
    ]
   ],
   "title": "Temporal alignment of creaky voice in neutralised realisations of an underlying, post-nasal voicing contrast in German",
   "original": "i07_0986",
   "page_count": 4,
   "order": 349,
   "p1": "986",
   "pn": "989",
   "abstract": [
    "The aim of the present experiment was to investigate the acoustic phonetic cues that could underlie a post-stress voicing distinction which, when considered on a segmental basis, appears to be neutralised. The data concern the difference in German between minimal pairs such as ‘Enten’ and ‘enden’ which in more casual speaking styles appear to show schwa and oral stop deletion and a surface realisation as a neutralised creaky voice nasal. We extracted from the Kiel Corpus all such contrasts that were judged by trained transcribers to have been neutralised in this way. We measured the spectral slope over the first two harmonics and the time at which the spectral slope first changed significantly. Our results show that, contrary to segmentally-based assumptions, /t, d/ were distinguished depending on the onset time of creaky voice relative to the preceding vowel. These data are consistent with a model in which cues to segmental contrasts may be distributed non-segmentally in time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-349"
  },
  "demol07_interspeech": {
   "authors": [
    [
     "Mike",
     "Demol"
    ],
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Piet",
     "Verhoeve"
    ]
   ],
   "title": "The duration of speech pauses in a multilingual environment",
   "original": "i07_0990",
   "page_count": 4,
   "order": 350,
   "p1": "990",
   "pn": "993",
   "abstract": [
    "In this paper we present a study of speech pauses at three different speaking rates, based on the analysis of four hours of read speech in six European languages. Our results confirm earlier observations that the logarithmic duration of the pauses can be well approximated by a bi-Gaussian distribution. We found this also to be true at slow and fast speaking rates. Our analysis further shows that, as far as the long speech pauses are concerned, similar strategies are used in all languages considered. For speaking slowly, speakers increase the total amount of pauses and they use a wider range of pause durations. Overall, there appeared to be no striking change in the average, nor in the variance of the distribution of the pause durations. For speaking rapidly, speakers decrease the amount of pauses used and they refrain from using the longest pauses that occur in their normal speech. Overall, this results in a lower average and a smaller variance of the pause durations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-350"
  },
  "gibbon07_interspeech": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ],
    [
     "Jolanta",
     "Bachan"
    ],
    [
     "Grażyna",
     "Demenko"
    ]
   ],
   "title": "Syllable timing patterns in Polish: results from annotation mining",
   "original": "i07_0994",
   "page_count": 4,
   "order": 351,
   "p1": "994",
   "pn": "997",
   "abstract": [
    "Previous studies of duration variation in syllable constituents have yielded results for Polish which are clear outliers in relation to those for other languages. We report on a study of this issue in the context of TTS development, using a large annotated database. Global and local duration distance measures are applied to phoneme and syllable level units, and generalised iambic and trochaic duration patterns are compared with grammatical structure. The study suggests that Polish is more syllable-timed than previously thought, and that there is tendentially a relationship between syllable duration patterns and word sequences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-351"
  },
  "kalimeris07_interspeech": {
   "authors": [
    [
     "Constandinos",
     "Kalimeris"
    ],
    [
     "Stelios",
     "Bakamidis"
    ]
   ],
   "title": "Minimal pairs and functional loads of sound contrasts obtained from a list of modern greek words",
   "original": "i07_0998",
   "page_count": 4,
   "order": 352,
   "p1": "998",
   "pn": "1001",
   "abstract": [
    "This paper reports on the initial results of our investigation into the distribution of speech sounds across the lexicon of Modern Greek (MG). The data we discuss ultimately derive from the list of orthographic word-types of a large general corpus of written MG. The orthographic word-types were automatically transcribed into their respective citation forms. Minimal pairs were automatically extracted from the resultant list of citation forms. The Functional Load (FL) of each sound opposition was computed as a function of (a) the length of citation forms, (b) the position of each sound contrast within citation forms and (c) the number of minimal pairs pertinent to each opposition in question. The body of data yielded by this study will be used for further research in MG phonology as well as for the improvement of the performance of Automatic Speech Recognition applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-352"
  },
  "wissing07_interspeech": {
   "authors": [
    [
     "Daan",
     "Wissing"
    ]
   ],
   "title": "More on acoustic correlates of stress",
   "original": "i07_1002",
   "page_count": 4,
   "order": 353,
   "p1": "1002",
   "pn": "1005",
   "abstract": [
    "The power of various relatively unknown parameters of stress [1, 2] was investigated. They were either derived from the latter, or from the physiological process of phonation. Two stimulus Afrikaans words in and out of focal accented sentence position were read by three Afrikaans female participants. The stressed and unstressed vowel /A/ was investigated in the two contexts. Effect sizes and multiple regression analysis results were used in determining the descriptive and explanatory power of the parameters as to the acoustic correlates of stress. The results substantiate the groundbreaking work of [1, 2] in this regard, but in some instances they disprove these. Many of the parameters proved to be quite powerful constructs, in some cases surpassing the known ones in strength. The successful derivation of such parameters from the physiological basis of the phonation process is demonstrated. Special attention is paid as to the description of vowel reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-353"
  },
  "woehrling07_interspeech": {
   "authors": [
    [
     "Cécile",
     "Woehrling"
    ],
    [
     "Philippe Boula de",
     "Mareüil"
    ]
   ],
   "title": "Comparing praat and snack formant measurements on two large corpora of northern and southern French",
   "original": "i07_1006",
   "page_count": 4,
   "order": 354,
   "p1": "1006",
   "pn": "1009",
   "abstract": [
    "We compare formant frequency measurements between two authoritative tools (Praat and Snack), two large corpora (of face-to-face and telephone speech) and two French varieties (northern and southern). There are both an evaluation of formant tracking (as well as related filtering techniques) and an application to find out salient pronunciation traits. Despite differences between Praat and Snack with regard to telephone speech (Praat yielding greater F1 values), results seem to converge to suggest that northern and southern French varieties mainly differ in the second formant of the open /O/. /O/ fronting in northern French (with F2 values greater than 1100 Hz for males and 1200 Hz for females) is by far the most discriminating feature provided by decision trees applied to oral vowel formants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-354"
  },
  "barry07_interspeech": {
   "authors": [
    [
     "William",
     "Barry"
    ],
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "Ingmar",
     "Steiner"
    ]
   ],
   "title": "The phonetic exponency of phrasal accentuation in French and German",
   "original": "i07_1010",
   "page_count": 4,
   "order": 355,
   "p1": "1010",
   "pn": "1013",
   "abstract": [
    "The acoustic-phonetic properties of words spoken with three different levels of accentuation (de-accented, pre-nuclear and nuclear accented in broad-focus and nuclear accented in narrow-focus) are examined in question-answer elicited sentences and iterative imitations (on the syllable da) produced by six French and six German speakers. Normalised parameter values allow a comparative weighting of the properties employed in differentiating the three levels of accentuation. Clear differences are found between French and German in the weighting hierarchy of the acoustic properties.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-355"
  },
  "christodoulou07_interspeech": {
   "authors": [
    [
     "Christiana",
     "Christodoulou"
    ]
   ],
   "title": "Phonetic geminates in cypriot greek: the case of voiceless plosives",
   "original": "i07_1014",
   "page_count": 4,
   "order": 356,
   "p1": "1014",
   "pn": "1017",
   "abstract": [
    "The research presented in this paper provides evidence toward the existence of geminates in Cypriot Greek (hereinafter, CyG). Toward this end, statistical analysis supports significant durational differences in closure duration, the cross-linguistic correlate to gemination. However, since the language maintains an audible distinction between the two phonemic categories and since closure duration cannot be measured in utterance initial environments, another/alternative correlate is necessary. Contrary to previous studies [2] and [7] this paper argues, supported by highly significant durational differences, that VOT is the primary correlate to gemination for CyG geminate plosives, because this correlate\n",
    "is available utterance initially. Preliminary statistical analysis suggests an effect of the vowel following the target segment, a fact that could facilitate in resolving the phonological representation of utterance initial geminates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-356"
  },
  "williams07_interspeech": {
   "authors": [
    [
     "Darcie",
     "Williams"
    ],
    [
     "François",
     "Poiré"
    ]
   ],
   "title": "Predicting vowel duration in spontaneous canadian French speech",
   "original": "i07_1018",
   "page_count": 4,
   "order": 357,
   "p1": "1018",
   "pn": "1021",
   "abstract": [
    "This study examines variables influencing vowel duration of French spoken in Windsor, Ontario, in order to see whether their respective effects on vowel duration are organised hierarchically. We first consider the data distribution of four female speakers before carrying out a statistical principal components analysis. Our results show that the variables are classified into three underlying factors: syllable structure, syllable position and vowel properties. This last factor group includes the factors phonological vowel class and diphthong status, and always explains the majority of the variability in vowel duration. Syllable position also accounts for some of this variation in certain cases. The consistent hierarchy of these factors across the statistical analyses confirms that a vowel's properties are the most important in determining its duration, followed first by the syllable's position in the utterance, and second by the syllabic structure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-357"
  },
  "chow07_interspeech": {
   "authors": [
    [
     "Ivan",
     "Chow"
    ],
    [
     "François",
     "Poiré"
    ]
   ],
   "title": "Rhotic variation and schwa epenthesis in windsor French",
   "original": "i07_1022",
   "page_count": 4,
   "order": 358,
   "p1": "1022",
   "pn": "1025",
   "abstract": [
    "This study investigates two idiosyncratic phenomena found in the French dialect of the Windsor area in SW Ontario. (1) Rhotics in this dialect are pronounced in three phonetic varieties: the dorsal fricative, the alveolar approximant, and the alveolar tap. (2) An epenthetic schwa is also found in the phonetic realization in certain phonemic contexts. Through phonetic analysis of recorded speech and statistical data analyses, we explore whether the phonological context is a good predictor of the phonetic realization of rhotics and schwa epenthesis. Amongst the independent factors, the manner and place of articulation of the phonemes preceding and following the rhotics are best predictors for the type of rhotics, the presence/absence of schwa epenthesis, as well as for the duration of the epenthetic schwa. The type of rhotic realization is a pre-condition for schwa epenthesis. Finally, duration of phonological schwas is also significantly longer than that of epenthetic schwas in schwa-rhotic sequences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-358"
  },
  "burki07_interspeech": {
   "authors": [
    [
     "Audrey",
     "Bürki"
    ],
    [
     "Cécile",
     "Fougeron"
    ],
    [
     "Cédric",
     "Gendrot"
    ]
   ],
   "title": "On the categorical nature of the process involved in schwa elision in French",
   "original": "i07_1026",
   "page_count": 4,
   "order": 359,
   "p1": "1026",
   "pn": "1029",
   "abstract": [
    "This paper examines the nature of the process involved in optional schwa elision in French. More specifically, it aims at testing whether this process is gradual or categorical, on the basis of an analysis of the distribution of the duration of over 4000 schwas extracted from a large corpus of continuous speech. The distribution observed is bimodal, with absent schwas (at 0 ms duration) on one side, and realized schwas on the other side, the two groups being separated by a small gap in the distribution. Realized schwas present cases of strong temporal reduction, but this reduction does not show a continuous pattern toward zero duration, as would be predicted if schwa elision was the end-point of a gradual reduction process. Different interpretations of this distribution are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-359"
  },
  "hu07b_interspeech": {
   "authors": [
    [
     "Yue-Ning",
     "Hu"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Chao",
     "Huang"
    ],
    [
     "Yan-Ning",
     "Zhang"
    ]
   ],
   "title": "Exploring tonal variations via context-dependent tone models",
   "original": "i07_1030",
   "page_count": 4,
   "order": 360,
   "p1": "1030",
   "pn": "1033",
   "abstract": [
    "In this paper, we study tonal variations by training context-dependent tone models from a large speech corpus. Each model represents a tone-in-context and can output a stylized f0 pattern for it. With these tone models, it becomes tangible to investigate f0-variations with plenty of factors. Six contextual factors are investigated to describe tones in this work. We find that impact of a factor varies across tones as well as the three states of a tone. Normally, onset pitch of a tone is determined jointly by syllable position and left tone, while, the offset pitch is mainly determined by syllable position. For a neutral tone, its pitch level is mainly depended on the left tone and syllable position affects its offset pitch. Both current vowel and right tone influence the pitch level, yet their impacts are weaker than syllable position and left tone, except for T3, the low tone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-360"
  },
  "martin07_interspeech": {
   "authors": [
    [
     "Philippe",
     "Martin"
    ],
    [
     "Jun",
     "Li"
    ]
   ],
   "title": "Acoustic analysis of the neutral tone in Mandarin",
   "original": "i07_1034",
   "page_count": 4,
   "order": 361,
   "p1": "1034",
   "pn": "1037",
   "abstract": [
    "East Asian Languages such as Mandarin do have lexical tones in their phonological system. Pronounced in isolation, the fundamental frequency contours produced by these tones are relatively stable and their shapes well described phonetically. However, modifications can occur, not only in the well known case where two consecutive third tones are realized with a tone two - tone three sequence, but in other contexts as well, producing either one of the three other tones available in the phonological system or a so called neutral tone. In this paper, specific acoustic characteristics of neutral tones resulting from sequences of three or more T3 tones are investigated. In particular, values of the fundamental frequency F0 glissando were evaluated and compared to a perception threshold. Other melodic features were considered as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-361"
  },
  "ho07_interspeech": {
   "authors": [
    [
     "Rerrario Shui-Ching",
     "Ho"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "F<sub>0</sub> analysis of perceptual distance among Cantonese level tones",
   "original": "i07_1038",
   "page_count": 4,
   "order": 362,
   "p1": "1038",
   "pn": "1041",
   "abstract": [
    "This paper presents an acoustical analysis of the pitch height of the four level tones of Cantonese in search for a quantitative relationship of their perceptual distance. Our preliminary measurements and calculations give the first evidence that the conventional representations were mostly mistaken.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-362"
  },
  "hsu07_interspeech": {
   "authors": [
    [
     "Chang-wen",
     "Hsu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Extended powered cepstral normalization (p-CN) with range equalization for robust features in speech recognition",
   "original": "i07_1106",
   "page_count": 4,
   "order": 363,
   "p1": "1106",
   "pn": "1109",
   "abstract": [
    "Cepstral normalization has been popularly used as a powerful approach to produce robust features for speech recognition. A new approach of Powered Cepstral Normalization (P-CN) was recently proposed to normalize the MFCC parameters in the r1-th order powered domain, where r1 > 1.0, and then transform the features back by an 1/r2 power order to a better recognition domain, and it was shown to produce robust features. Here we further extend P-CN to a more effective and efficient form, in which we can on-line find good values of r2 for each utterance in real time based on the concept of dynamic range equalization. The basic idea is that the difference in dynamic ranges of feature parameters is in fact a good indicator for the mismatch degrading the recognition performance. Extensive experimental results showed that the Extended P-CN with range equalization proposed in this paper significantly outperforms the conventional Cepstral Normalization and P-CN in all noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-363"
  },
  "sakai07_interspeech": {
   "authors": [
    [
     "Makoto",
     "Sakai"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Selection of optimal dimensionality reduction method using chernoff bound for segmental unit input HMM",
   "original": "i07_1110",
   "page_count": 4,
   "order": 364,
   "p1": "1110",
   "pn": "1113",
   "abstract": [
    "To precisely model the time dependency of features, segmental unit input HMM with a dimensionality reduction method has been widely used for speech recognition. Linear discriminant analysis (LDA) and heteroscedastic discriminant analysis (HDA) are popular approaches to reduce the dimensionality. We have proposed another dimensionality reduction method called power linear discriminant analysis (PLDA) to select the best dimensionality reduction method that yields the highest recognition performance. This selection process on the basis of trial and error requires much time to train HMMs and to test the recognition performance for each dimensionality reduction method.\n",
    "In this paper we propose a performance comparison method without training or testing. We show that the proposed method using the Chernoff bound can rapidly and accurately evaluate the relative recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-364"
  },
  "tyagi07_interspeech": {
   "authors": [
    [
     "Vivek",
     "Tyagi"
    ]
   ],
   "title": "Fepstrum: an improved modulation spectrum for ASR",
   "original": "i07_1114",
   "page_count": 4,
   "order": 365,
   "p1": "1114",
   "pn": "1117",
   "abstract": [
    "In our previous work [3, 4], we have introduced fepstrum; an improved modulation spectrum estimation technique that overcomes certain theoretical as well as practical shortcomings in the previously published modulation spectrum related techniques [11, 13, 14]. In [3], we have also shown that fepstrum is an exact dual of the well known quantity, real cepstrum. In this paper, we provide further extensive ASR results using the fepstrum features over the TIMIT core test-set phoneme recognition task using a triphone context dependent HMM recognizer. Moreover, fepstrum performance is rigorously benchmarked against a competitive MFCC baseline, other best results reported on the same task [7, 8, 9] and a heterogeneous and multiple classifier based technique [5]. In our experiments, a simple concatenation of fepstrum and MFCC composite feature is used to train a conventional hidden Markov model Gaussian mixture model (HMM-GMM) recognizer. This composite feature achieves a phoneme recognition accuracy of 74.6% on the TIMIT core test-set which is 1.8% absolute better than the MFCC HMM-GMM recognizer accuracy of 72.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-365"
  },
  "macho07_interspeech": {
   "authors": [
    [
     "Dušan",
     "Macho"
    ]
   ],
   "title": "Narrowband to wideband feature expansion for robust multilingual ASR",
   "original": "i07_1118",
   "page_count": 4,
   "order": 366,
   "p1": "1118",
   "pn": "1121",
   "abstract": [
    "To build high quality wideband acoustic models for automatic speech recognition (ASR), a large amount of wideband speech training data is required. However, for a particular language, one may have available a lot of narrowband data, but only a limited amount of wideband data. This paper deals with such situation and proposes a narrowband to wideband expansion algorithm that expands the narrowband signal ASR features to wideband ASR features. The algorithm is tested in two practical situations comprising sufficient amount and insufficient amount of original wideband training data. Tests show that using a combination of wideband features and expanded features does not harm the ASR performance when having a sufficient amount of the original wideband data, and it improves the ASR performance significantly when only a limited amount of wideband data is originally available. In the presented multilingual tests, a unique expansion model is trained for four languages from the Speecon database. Availability of different amounts of wideband training data is considered, including the case when no wideband data is available. ASR experiments for each language confirm that the addition of expanded features to the wideband model training enhances the models and provides better results than using the limited amount of wideband data only. In all tests, the ETSI standard noise-robust front-end is used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-366"
  },
  "li07f_interspeech": {
   "authors": [
    [
     "Weifeng",
     "Li"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Non-linear spectral contrast stretching for in-car speech recognition",
   "original": "i07_1122",
   "page_count": 4,
   "order": 367,
   "p1": "1122",
   "pn": "1125",
   "abstract": [
    "In this paper, we present a novel feature normalization method in the log-scaled spectral domain for improving the noise robustness of speech recognition front-ends. In the proposed scheme, a non-linear contrast stretching is added to the outputs of log mel-filterbanks (MFB) to imitate the adaptation of the auditory system under adverse conditions. This is followed by a two-dimensional filter to smooth out the processing artifacts. The proposed MFCC front-ends perform remarkably well on CENSREC-2 in-car database with an average relative improvement of 29.3% compared to baseline MFCC system. It is also confirmed that the proposed processing in log MFB domain can be integrated with conventional cepstral post-processing techniques to yield further improvements. The proposed algorithm is simple and requires only a small extra computation load.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-367"
  },
  "li07g_interspeech": {
   "authors": [
    [
     "Xiao-Bing",
     "Li"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Clustering-based two-dimensional linear discriminant analysis for speech recognition",
   "original": "i07_1126",
   "page_count": 4,
   "order": 368,
   "p1": "1126",
   "pn": "1129",
   "abstract": [
    "In this paper, a new, Clustering-based Two-Dimensional Linear Discriminant Analysis (Clustering-based 2DLDA) method is proposed for extracting discriminant features in Automatic Speech Recognition (ASR). Based on Two-Dimensional Linear Discriminant Analysis (2DLDA), which works with data represented in matrix space and is adopted to extract discriminant information in a joint spectral-temporal domain, Clustering-based 2DLDA integrates the cluster information in each class by redefining the between-class scatter matrix to tackle the fact that many clusters exist in each state in Hidden Markov Model (HMM)-based ASR. The method was evaluated in the TiDigits connected-digit string recognition and the TIMIT continuous phoneme recognition. Experimental results show that 2DLDA yields a slight improvement on the recognition performance over classical LDA, and our proposed Clustering-based 2DLDA outperforms 2DLDA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-368"
  },
  "kubo07_interspeech": {
   "authors": [
    [
     "Yotaro",
     "Kubo"
    ],
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Akira",
     "Kurematsu"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "A study on temporal features derived by analytic signal",
   "original": "i07_1130",
   "page_count": 4,
   "order": 369,
   "p1": "1130",
   "pn": "1133",
   "abstract": [
    "Traditional feature extraction methods for automatic speech recognition (ASR), such as MFCC (Mel-frequency cepstral coefficients) and PLP (perceptual linear prediction) [6], are extracted from short-term spectral envelopes and can be used to realize promising ASR systems. On the other hand, features extracted by TRAPs-like classifiers [2] are based on long-term envelopes of narrow-band signals. These two forms of feature extractions use a mutual representation of energy in narrow band signals.\n",
    "We have developed a feature extraction system that depends on not only the energy but also the modulation of carrier signals. Carrier signals involve attributes such as the spectral centroid, spectral gradient, number of zero-crossing points, and frequency modulation. Some experiments show that not only the spectral envelope and its modulation but also the zero-crossing points and frequency modulation form a significant portion of human speech perception [4].\n",
    "In this study, we propose a method of carrier analysis, evaluate this method, and discuss the effectiveness of carrier analysis for ASR. Our method can reduce the phoneme error rate from 45.7% to 38.6%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-369"
  },
  "zahorian07_interspeech": {
   "authors": [
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "Tara",
     "Singh"
    ],
    [
     "Hongbing",
     "Hu"
    ]
   ],
   "title": "Dimensionality reduction of speech features using nonlinear principal components analysis",
   "original": "i07_1134",
   "page_count": 4,
   "order": 370,
   "p1": "1134",
   "pn": "1137",
   "abstract": [
    "One of the main practical difficulties for automatic speech recognition is the large dimensionality of acoustic feature spaces and the subsequent training problems collectively referred to as the \"curse of dimensionality.\" Many linear techniques, most notably principal components analysis (PCA) and linear discriminant analysis (LDA) and several variants have been used to reduce dimensionality while attempting to preserve variability and discriminability of classes in the feature space. However, these orthogonal rotations of the feature space are suboptimal if data are distributed primarily on curved subspaces embedded in the higher dimensional feature spaces. In this paper, two neural network based nonlinear transformations are used to represent speech data in reduced dimensionality subspaces. It is shown that a subspace computed with the explicit intent of maximizing classification accuracy is far superior to a subspace derived as to minimize mean square representation error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-370"
  },
  "sanand07_interspeech": {
   "authors": [
    [
     "D. R.",
     "Sanand"
    ],
    [
     "D. Dinesh",
     "Kumar"
    ],
    [
     "S.",
     "Umesh"
    ]
   ],
   "title": "Linear transformation approach to VTLN using dynamic frequency warping",
   "original": "i07_1138",
   "page_count": 4,
   "order": 371,
   "p1": "1138",
   "pn": "1141",
   "abstract": [
    "In the paper, we present a novel linear transformation approach to frequency warping during vocal tract length normalisation (VTLN) using the idea of dynamic frequency warping (DFW). Linear transformation among the mel-frequency cepstral coefficients (MFCC) provides computational advantage of not having to recompute features for each warp factor in VTLN. The proposed method uses the idea of separating the smoothing and the frequency warping operations in the feature extraction stage unlike the conventional approach where both operations are integrated into the filter-bank operation. The advantage of the proposed DFW approach is that, we can obtain a transformation matrix for any arbitrary warping even when we do not know the functional form or mapping of the warping function. We compare the performance of the proposed method along with approaches proposed in [4] and [5] on one phone classification and two digit recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-371"
  },
  "alencar07_interspeech": {
   "authors": [
    [
     "Vladimir Fabregas Surigué de",
     "Alencar"
    ],
    [
     "Abraham",
     "Alcaim"
    ]
   ],
   "title": "Features interpolation domain for distributed speech recognition and performance for ITU-t g.723.1 CODEC",
   "original": "i07_1142",
   "page_count": 4,
   "order": 372,
   "p1": "1142",
   "pn": "1145",
   "abstract": [
    "In this paper, we examine the best domain to perform features interpolation in Distributed Speech Recognition (DSR) systems. We show that the only one domain where a performance gain can be achieved from the linear interpolation procedure is in the Line Spectral Frequencies (LSF) domain. A DSR scenario where the ITU-T G.723.1 codec is employed is also investigated. The recognition feature generated from the reconstructed speech is highly sensitive to the encoding noise. We have also shown that the LSF quantization scheme used by the G.723.1 codec decreases the recognition performance by approximately 2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-372"
  },
  "sato07_interspeech": {
   "authors": [
    [
     "Shoei",
     "Sato"
    ],
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Shinich",
     "Homma"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Tohru",
     "Takagi"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Dynamic integration of multiple feature streams for robust real-time LVCSR",
   "original": "i07_1146",
   "page_count": 4,
   "order": 373,
   "p1": "1146",
   "pn": "1149",
   "abstract": [
    "We present a novel method of integrating the likelihoods of multiple feature streams for robust speech recognition. The integration algorithm dynamically calculates a frame-wise stream weight so that a heavier weight is given to a stream that is robust to a variety of noisy environments or speaking styles. Such a robust stream is expected to bring out discriminative ability. The weight is calculated in real time from mutual information between an input stream and active HMM states in a search space. In this paper, we describe three features that are extracted through auditory filters by taking into account the human auditory system extracting amplitude and frequency modulations. These features are expected to provide complementary clues for speech recognition. Speech recognition experiments using field reports and spontaneous commentary from Japanese broadcast news showed that the proposed method reduced error words by 9% relative to the best result obtained from a single stream.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-373"
  },
  "matsumasa07_interspeech": {
   "authors": [
    [
     "Hironori",
     "Matsumasa"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ],
    [
     "Ichao",
     "Li"
    ],
    [
     "Toshitaka",
     "Nakabayashi"
    ]
   ],
   "title": "PCA-based feature extraction for fluctuation in speaking style of articulation disorders",
   "original": "i07_1150",
   "page_count": 4,
   "order": 374,
   "p1": "1150",
   "pn": "1153",
   "abstract": [
    "We investigated the speech recognition of a person with articulation disorders resulting from athetoid cerebral palsy. Recently, the accuracy of speaker-independent speech recognition has been remarkably improved by the use of stochastic modeling of speech. However, the use of those acoustic models causes degradation of speech recognition for a person with different speech styles (e.g., articulation disorders). In this paper, we discuss our efforts to build an acoustic model for a person with articulation disorders. The articulation of the first speech tends to become unstable due to strain on muscles and that causes degradation of speech recognition. Therefore, we propose a robust feature extraction method based on PCA (Principal Component Analysis) instead of MFCC. Its effectiveness is confirmed by word recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-374"
  },
  "valente07b_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Multi-stream features combination based on dempster-shafer rule for LVCSR system",
   "original": "i07_1154",
   "page_count": 4,
   "order": 375,
   "p1": "1154",
   "pn": "1157",
   "abstract": [
    "This paper investigates the combination of two streams of acoustic features. Extending our previous work on small vocabulary task, we show that combination based on Dempster-Shafer rule outperforms several classical rules like sum, product and inverse entropy weighting even in LVCSR systems. We analyze results in terms of Frame Error Rate and Cross Entropy measures. Experimental framework uses meeting transcription task and results are provided on RT05 evaluation data. Results are consistent with what has been previously observed on smaller databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-375"
  },
  "singhmiller07_interspeech": {
   "authors": [
    [
     "Natasha",
     "Singh-Miller"
    ],
    [
     "Michael",
     "Collins"
    ],
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "Dimensionality reduction for speech recognition using neighborhood components analysis",
   "original": "i07_1158",
   "page_count": 4,
   "order": 376,
   "p1": "1158",
   "pn": "1161",
   "abstract": [
    "Previous work has considered methods for learning projections of high-dimensional acoustic representations to lower dimensional spaces. In this paper we apply the neighborhood components analysis (NCA) [2] method to acoustic modeling in a speech recognizer. NCA learns a projection of acoustic vectors that optimizes a criterion that is closely related to the classification accuracy of a nearest-neighbor classifier. We introduce regularization into this method, giving further improvements in performance. We describe experiments on a lecture transcription task, comparing projections learned using NCA and HLDA [1]. Regularized NCA gives a 0.7% absolute reduction in WER over HLDA, which corresponds to a relative reduction of 1.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-376"
  },
  "su07b_interspeech": {
   "authors": [
    [
     "Dan",
     "Su"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "Probabilistic latent speaker analysis for large vocabulary speech recognition",
   "original": "i07_1162",
   "page_count": 4,
   "order": 377,
   "p1": "1162",
   "pn": "1165",
   "abstract": [
    "Trajectory folding problem is intrinsic for HMM-based speech recognition systems in which each state is modeled by a mixture of Gaussian components. In this paper, a probabilistic latent semantic analysis (PLSA)-based approach is proposed for use in speech recognition systems to alleviate this problem. The basic idea is that different speech trajectories are strongly correlated with speaker variation, and different speakers may have high scores on certain Gaussian components consistently. Thus, PLSA is adopted to perform co-occurrence analysis between Gaussian components and speakers and provide additional source of information to constrain searching path during decoding procedure. Experimental results show that 11.2% and 2.7% relative reduction on word error rate can be achieved on a homogeneous test set and the 2004 863 evaluation set, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-377"
  },
  "prasanna07_interspeech": {
   "authors": [
    [
     "S. R. Mahadeva",
     "Prasanna"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "MRASTA and PLP in automatic speech recognition",
   "original": "i07_1166",
   "page_count": 4,
   "order": 378,
   "p1": "1166",
   "pn": "1169",
   "abstract": [
    "This work explores different methods for combining estimated posterior probabilities from Multi-RASTA (MRASTA) and Perceptual Linear Prediction (PLP) features for Automatic Speech Recognition (ASR). The improved performance by the ASR system indicates the complementary nature of information present in MRASTA and PLP. Among the different combining methods explored, product gives best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-378"
  },
  "bruckl07_interspeech": {
   "authors": [
    [
     "Markus",
     "Brückl"
    ]
   ],
   "title": "Women's vocal aging: a longitudinal approach",
   "original": "i07_1170",
   "page_count": 4,
   "order": 379,
   "p1": "1170",
   "pn": "1173",
   "abstract": [
    "A quasi-experimental longitudinal paired-samples study was carried out to explore, whether aging for 5 years can (1) audibly and (2) measurably change women's vocalisations, and if so, on which acoustic information (3) the listeners' performance possibly could relay on and (4) which parameters can contribute to detect the chronological difference.\n",
    "Results indicate that (1) listeners can significantly correctly judge this difference based on sustained /i/ and /u/ vowels, but much better based on (spontaneous) speech samples. (2) Parameters depicting pitch, vowel resonance, voice perturbations, tremor and spectral energy distributions differ (significantly) between chronologically and perceptually younger and older samples. (3) Listeners tend to judge vowel samples as older, if increased (amplitude) perturbations can be measured, but in speech samples there seem to be overriding and objectively more reliable features. (4) The most reliable age-indicating measures in this study in speech samples are durations / tempo measures - in vowels F0 and tremor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-379"
  },
  "cnockaert07_interspeech": {
   "authors": [
    [
     "Laurence",
     "Cnockaert"
    ],
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Canan",
     "Ozsancak"
    ],
    [
     "Pascal",
     "Auzou"
    ],
    [
     "Francis",
     "Grenez"
    ]
   ],
   "title": "Effect of intensive voice therapy on vocal tremor for parkinson speakers",
   "original": "i07_1174",
   "page_count": 4,
   "order": 380,
   "p1": "1174",
   "pn": "1177",
   "abstract": [
    "The effect of intensive voice therapy (Lee Silverman Voice Treatment, LSVT) on vocal tremor features of Parkinson speakers is presented. Vocal tremor is the low-frequency variation of the vocal frequency. Its features differ for Parkinson and normophonic speakers. Here, vocal tremor features have been estimated for a corpus of speakers with Parkinson's disease, recorded before and after intensive voice therapy. Results show that the treatment has significant effects on vocal tremor amplitude: Vocal tremor amplitude has decreased right after treatment. After six month, it has increased again, but is still lower than before treatment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-380"
  },
  "alpan07_interspeech": {
   "authors": [
    [
     "A.",
     "Alpan"
    ],
    [
     "A.",
     "Kacha"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Assessment of vocal dysperiodicities in connected disordered speech",
   "original": "i07_1178",
   "page_count": 4,
   "order": 381,
   "p1": "1178",
   "pn": "1181",
   "abstract": [
    "The aim of the presentation is to investigate acoustic analysis of connected speech by means of an average-equalized and energy-equalized variogram to extract vocal dysperiodicities. The variogram enables positioning a current and a lagged analysis frame in adjacent speech cycles to track inter-cycle dysperiodicities. Average and energy equalization of the analysis frames are options that make it possible to compensate for slow deterministic changes of the speech signal amplitude in connected speech. The instantaneous dysperiodicity trace has been summarized by means of segmental and global signal-to-dysperiodicity ratios. Results show that signal-to-dysperiodicity ratios obtained by variogram analysis correlate strongly with the perceived degree of hoarseness when the analysis frames are energy-equalized. Equalizing the frame averages removes small artifacts in the instantaneous dysperiodicity trace that are caused by sound-to-sound transients or intrusive low-frequency noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-381"
  },
  "laukkanen07_interspeech": {
   "authors": [
    [
     "Anne-Maria",
     "Laukkanen"
    ],
    [
     "Jaromír",
     "Horáček"
    ],
    [
     "Pavel",
     "Švancara"
    ],
    [
     "Elina",
     "Lehtinen"
    ]
   ],
   "title": "Effects of FE modelled consequences of tonsillectomy on perceptual evaluation of voice",
   "original": "i07_1182",
   "page_count": 4,
   "order": 382,
   "p1": "1182",
   "pn": "1185",
   "abstract": [
    "This study aimed to investigate the effects of a tonsillectomy on the perceived overall voice quality and timbre. Computer simulations of five Czech vowels were made, including both the calculated resonance effects of large tonsils (size 1.6 cm3) and the resonances without tonsils. The simulations were made using a finite element model of the vocal tract, based on magnetic resonance images. The size and shape of the tonsils were ascertained from clinical data. The generated pressure outputs were transformed into sound records presented to 10 trained listeners. Formant frequencies of the simulated samples were measured. The samples with and without tonsils did not differ significantly from each other in voice quality. F3 was significantly lower and the timbre was darker without tonsils. Thus, the effects of tonsillectomy on voice may be perceptible, at least in the case of large tonsils. The effect, however, may disappear in time due to changes in the tissue and due to compensatory changes in articulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-382"
  },
  "leeuw07_interspeech": {
   "authors": [
    [
     "Irma M. Verdonck-de",
     "Leeuw"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Li Ying",
     "Chao"
    ],
    [
     "Rico N. P. M.",
     "Rinkel"
    ],
    [
     "Pepijn A.",
     "Borggreven"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "C. René",
     "Leemans"
    ]
   ],
   "title": "Speech quality after major surgery of the oral cavity and oropharynx with microvascular soft tissue reconstruction",
   "original": "i07_1186",
   "page_count": 4,
   "order": 383,
   "p1": "1186",
   "pn": "1189",
   "abstract": [
    "Speech quality of patients with oral or oropharyngeal carcinoma was assessed by perceptual and acoustic-phonetic analyses. Speech recordings of running speech of patients before and 6 and 12 months after treatment for oral or oropharyngeal cancer and of 18 control speakers were evaluated regarding intelligibility, nasality and articulation, which revealed deteriorated speech in 20% of the patients before treatment, and in 75% 6-12 months after treatment. Acoustic analyses comprised formant, duration, perturbation and noise measures of the vowels /i/, /a/, and /u/ and were performed on the speech samples 6 months after treatment and the controls. Patients appeared to have a smaller vowel space compared to controls, which was clearly related to speech intelligibility. Furthermore, voice perturbation appeared to be higher in patients. Although oropharyngeal treatment does not effect the function of the larynx itself, the acoustic coupling between source and filter may effect the smoothness of the voicing characteristics. The presented speech analyses may serve as part of an outcome measurement protocol for assessing efficacy of speech rehabilitation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-383"
  },
  "bruijn07_interspeech": {
   "authors": [
    [
     "Christel de",
     "Bruijn"
    ],
    [
     "Sandra",
     "Whiteside"
    ]
   ],
   "title": "Voice fatigue and use of speech recognition: a study of voice quality ratings",
   "original": "i07_1190",
   "page_count": 4,
   "order": 384,
   "p1": "1190",
   "pn": "1193",
   "abstract": [
    "Previous studies have suggested the use of speech recognition software may be related to the development of voice problems. The aim of this study is to investigate the effects of using such software on perceptual voice quality. In particular, the variables type of speech recognition (discrete and continuous) and vocal load of a speaker are considered. One of the most consistent results was a rise in pitch, a common finding in voice fatigue studies. It is interpreted as part of a hyperfunctional mechanism countering early signs of voice fatigue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-384"
  },
  "bonastre07b_interspeech": {
   "authors": [
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "A.",
     "Ghio"
    ],
    [
     "A.",
     "Giovanni"
    ],
    [
     "G.",
     "Pouchoulin"
    ],
    [
     "J.",
     "Révis"
    ],
    [
     "B.",
     "Teston"
    ],
    [
     "P.",
     "Yu"
    ]
   ],
   "title": "Complementary approaches for voice disorder assessment",
   "original": "i07_1194",
   "page_count": 4,
   "order": 385,
   "p1": "1194",
   "pn": "1197",
   "abstract": [
    "This paper describes two comparative studies of voice quality assessment based on complementary approaches. The first study was undertaken on 449 speakers (including 391 dysphonic patients) whose voice quality was evaluated in parallel by a perceptual judgment and objective measurements on acoustic and aerodynamic data. Results showed that a nonlinear combination of 7 parameters allowed the classification of 82% voice samples in the same grade as the jury. The second study relates to the adaptation of Automatic Speaker Recognition (ASR) techniques to pathological voice assessment. The system designed for this particular task relies on a GMM based approach, which is the state-of-the-art for ASR. Experiments conducted on 80 female voices provide promising results, underlining the interest of such an approach. We benefit from the multiplicity of theses techniques to evaluate the methodological situation which points fundamental differences between these complementary approaches (bottom-up vs. top-down, global vs. analytic). We also discuss some theoretical aspects about relationship between acoustic measurement and perceptual mechanisms which are often forgotten in the performance race.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-385"
  },
  "pouchoulin07_interspeech": {
   "authors": [
    [
     "G.",
     "Pouchoulin"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "A.",
     "Ghio"
    ],
    [
     "A.",
     "Giovanni"
    ]
   ],
   "title": "Frequency study for the characterization of the dysphonic voices",
   "original": "i07_1198",
   "page_count": 4,
   "order": 386,
   "p1": "1198",
   "pn": "1201",
   "abstract": [
    "Concerned with pathological voice assessment, this paper aims at characterizing dysphonia in the frequency domain for a better understanding of relating phenomena while most of the studies have focused only on improving classification systems for diagnosis help purposes. In this context, a GMM-based automatic classification system is applied on different frequency ranges in order to investigate which ones are relevant for dysphonia characterization. Experiment results demonstrate that the low frequencies [0-3000]Hz are more relevant for dysphonia discrimination compared with higher frequencies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-386"
  },
  "boucher07_interspeech": {
   "authors": [
    [
     "Victor J.",
     "Boucher"
    ]
   ],
   "title": "Acoustic correlates of laryngeal-muscle fatigue: findings for a phonometric prevention of acquired voice pathologies",
   "original": "i07_1202",
   "page_count": 4,
   "order": 387,
   "p1": "1202",
   "pn": "1205",
   "abstract": [
    "This presentation focuses on the problem of defining valid acoustic correlates of vocal fatigue seen as a physiological condition that can lead to voice pathologies. Several findings are reported based on a corpus of recordings involving electromyography (EMG) of laryngeal muscles and voice acoustics. The recordings were obtained in sessions of vocal effort extending across 12-14 hours. A known technique for estimating muscle fatigue is applied involving \"spectral compression\" of EMG potentials. The results show critical changes at given times of day. In examining the effects of these changes on voice acoustics, there is no linear correlation with respect to conventional acoustic parameters, but peaks in voice tremor occur at points of critical change in muscle fatigue. Further results are presented showing the need to take into account compensatory muscle actions in defining phonometric signs of vocal fatigue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-387"
  },
  "maier07b_interspeech": {
   "authors": [
    [
     "Andreas",
     "Maier"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Emeka",
     "Nkenke"
    ]
   ],
   "title": "Automatic scoring of the intelligibility in patients with cancer of the oral cavity",
   "original": "i07_1206",
   "page_count": 4,
   "order": 388,
   "p1": "1206",
   "pn": "1209",
   "abstract": [
    "After surgical treatment of cancer of the oral cavity patients often suffer from functional restrictions such as speech disorders. In this paper we present a novel approach to assess the outcome of the treatment w.r.t. the intelligibility of the patient using the result of an automatic speech recognition system. The word recognition rate was taken as intelligibility score. Compared to four speech experts this method yields results that are as good as the best speech expert compared to the other experts. The correlation between our system and the mean opinion of the experts is .92. Furthermore we show that our system has better performance than the average expert and is more reliable.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-388"
  },
  "duchateau07_interspeech": {
   "authors": [
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Leen",
     "Cleuren"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Pol",
     "Ghesquière"
    ]
   ],
   "title": "Automatic assessment of children's reading level",
   "original": "i07_1210",
   "page_count": 4,
   "order": 389,
   "p1": "1210",
   "pn": "1213",
   "abstract": [
    "In this paper, an automatic system for the assessment of reading in children is described and evaluated. The assessment is based on a reading test with 40 words, presented one by one to the child by means of a computerized reading tutor. The score that expresses the child's reading performance is calculated as the total time needed to read the 40 words divided by the number of correctly read words. In each grade, children are classified in 5 groups based on their score as provided by human annotators. We show that when the score for a child is assessed automatically using a speech recognizer, a classification can be obtained with a substantial agreement (Cohen's Kappa over 0.6) with the human classification. As all children in the experiments were classified either correctly or in an adjoining group, we can conclude that the proposed system can provide large time gains in current manual classification procedures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-389"
  },
  "ferrer07b_interspeech": {
   "authors": [
    [
     "Carlos",
     "Ferrer"
    ],
    [
     "María E.",
     "Hernández-Díaz"
    ],
    [
     "Eduardo",
     "González"
    ]
   ],
   "title": "Using waveform matching techniques in the measurement of shimmer in voiced signals",
   "original": "i07_1214",
   "page_count": 4,
   "order": 390,
   "p1": "1214",
   "pn": "1217",
   "abstract": [
    "In this work several approaches of amplitude contours estimation for shimmer measurement are analyzed and compared. The approaches covered incorporate a waveform matching procedure proposed in this work, based on existent least squares measures. The experimental comparisons evaluate each method's sensitivity to periodicity perturbations like jitter, shimmer, and noise, as well as their combination. The waveform matching technique shows a better overall performance than the other methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-390"
  },
  "fraile07_interspeech": {
   "authors": [
    [
     "R.",
     "Fraile"
    ],
    [
     "J. I.",
     "Godino-Llorente"
    ],
    [
     "N.",
     "Sáenz-Lechón"
    ],
    [
     "V.",
     "Osma-Ruiz"
    ],
    [
     "P.",
     "Gómez-Vilda"
    ]
   ],
   "title": "Analysis of the impact of analogue telephone channel on MFCC parameters for voice pathology detection",
   "original": "i07_1218",
   "page_count": 4,
   "order": 391,
   "p1": "1218",
   "pn": "1221",
   "abstract": [
    "In this paper, the feasibility of a system developed for the remote diagnosis of voice pathologies is analysed. More specifically, the performance of MFCC-based pathology detectors over speech transmitted through an analogue telephone channel is studied. Results indicate that MFCC are voice features fairly robust to amplitude distortion and almost insensitive to phase distortion, but the efficiency of a voice pathology detector based on these features is clearly decreased when the speech samples are transmitted through a telephone channel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-391"
  },
  "manfredi07_interspeech": {
   "authors": [
    [
     "C.",
     "Manfredi"
    ],
    [
     "L.",
     "Bocchi"
    ],
    [
     "G.",
     "Cantarella"
    ],
    [
     "G.",
     "Peretti"
    ],
    [
     "G.",
     "Guidi"
    ],
    [
     "V.",
     "Mezzatesta"
    ]
   ],
   "title": "Objective parameters from videokymographic images: a user-friendly interface",
   "original": "i07_1222",
   "page_count": 4,
   "order": 392,
   "p1": "1222",
   "pn": "1225",
   "abstract": [
    "Videolaryngostroboscopy (VLS) is a first choice examination for diagnosis of several laryngeal pathologies. However, in case of strong a-periodicity of the vocal sound, it becomes ineffective in describing subsequent phases of the vocal cycle. To overcome such limitation, a new technique, named videokymography (VKG) has been developed. VKG delivers images from a single line selected from the whole VLS image, at the speed of approximately 8000 line-images/s. However, despite its usefulness, until now no quantitative analysis of VKG images is commercially available. This paper presents a new tool for measuring and tracking quantitative parameters from VKG images. It performs evaluation of left-to-right period, amplitude and phase ratios, as well as of phase symmetry index. Robust techniques for edge detection have been implemented, to reduce both noise and artefacts. The new tool has been provided with a user-friendly interface for managing patients' data and image analysis, according to a set of parameters that can be easily adjusted by the user. VKG images from one non-dysphonic and seven dysphonic subjects were analysed, providing objective parameters useful in diagnosis support.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-392"
  },
  "house07_interspeech": {
   "authors": [
    [
     "David",
     "House"
    ]
   ],
   "title": "Integrating audio and visual cues for speaker friendliness in multimodal speech synthesis",
   "original": "i07_1250",
   "page_count": 4,
   "order": 393,
   "p1": "1250",
   "pn": "1253",
   "abstract": [
    "This paper investigates interactions between audio and visual cues to friendliness in questions in two perception experiments. In the first experiment, manually edited parametric audio-visual synthesis was used to create the stimuli. Results were consistent with earlier findings in that a late, high final focal accent peak was perceived as friendlier than an earlier, lower focal accent peak. Friendliness was also effectively signaled by visual facial parameters such as a smile, head nod and eyebrow raising synchronized with the final accent. Consistent additive effects were found between the audio and visual cues for the subjects as a group and individually showing that subjects integrate the two modalities. The second experiment used data-driven visual synthesis where the database was recorded by an actor instructed to portray anger and happiness. Friendliness was correlated to the happy database, but the effect was not as strong as for the parametric synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-393"
  },
  "wesseling07_interspeech": {
   "authors": [
    [
     "Wieneke",
     "Wesseling"
    ],
    [
     "R. J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The influence of masking words on the prediction of TRPs in a shadowed dialog",
   "original": "i07_1254",
   "page_count": 4,
   "order": 394,
   "p1": "1254",
   "pn": "1257",
   "abstract": [
    "It is well known that listeners can ignore disturbances in speech and rely on context to interpolate the message. This fact is used to determine the importance of individual words for projecting Transition Relevance Places, TRPs. Subjects were asked to shadow manipulated pre-recorded dialogs with minimal responses, saying ‘ah’ when they feel it is appropriate. In these dialogs, at random, of each utterance, either one of the last four words was replaced by white noise (masked condition), or no word was replaced (non masked condition). The reaction times were analyzed for effects of masked words. The presence of masked words, even prominent words, did not affect the response times of our subjects unless the very last word of the utterance was masked. This indicates that listeners are able to seamlessly interpolate the missing words and only need the identity of the last word to determine the exact position of the TRP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-394"
  },
  "laskowski07_interspeech": {
   "authors": [
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Susanne",
     "Burger"
    ]
   ],
   "title": "Analysis of the occurrence of laughter in meetings",
   "original": "i07_1258",
   "page_count": 4,
   "order": 395,
   "p1": "1258",
   "pn": "1261",
   "abstract": [
    "Automatic speech understanding in natural multiparty conversation settings stands to gain from parsing not only verbal but also non-verbal vocal communicative behaviors. In this work, we study the most frequently annotated non-verbal behavior, laughter, whose detection has clear implications for speech understanding tasks, and for the automatic recognition of affect in particular. To complement existing acoustic descriptions of the phenomenon, we explore the temporal patterning of laughter over the course of conversation, with a view towards its automatic segmentation and detection. We demonstrate that participants vary extensively in their use of laughter, and that laughter differs from speech in its duration and in the regularity of its occurrence. We also show that laughter and speech are quite dissimilar in terms of the degree of simultaneous vocalization by multiple participants, and in terms of the probability of transitioning into and out of vocalization overlap states.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-395"
  },
  "barkhuysen07_interspeech": {
   "authors": [
    [
     "Pashiera",
     "Barkhuysen"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Incremental perception of acted and real emotional speech",
   "original": "i07_1262",
   "page_count": 4,
   "order": 396,
   "p1": "1262",
   "pn": "1265",
   "abstract": [
    "This paper reports on an experiment using the gating paradigm to test the recognition speed for various emotional expressions from a speaker's face. In a perception experiment, subjects were presented with video clips of speakers who displayed negative or positive emotions, which were either acted or real. The clips were shown in successive segments (gates) of increasing duration. Results show that subjects are surprisingly accurate in their recognition of the various emotions, as they already reach high recognition scores in the first gate (after only 160 milliseconds). Interestingly, the recognition speed is faster for positive than negative emotions, in line with comparable valency effects reported by Leppänen and Hietanen (2003). Finally, the gating results confirm earlier findings that acted emotions are perceived as more intense than true emotions (Wilting et al., 2006), as the former get more extreme recognition scores than the latter, already after a short period of exposure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-396"
  },
  "schlangen07_interspeech": {
   "authors": [
    [
     "David",
     "Schlangen"
    ],
    [
     "Raquel",
     "Fernández"
    ]
   ],
   "title": "Speaking through a noisy channel - experiments on inducing clarification behaviour in human-human dialogue",
   "original": "i07_1266",
   "page_count": 4,
   "order": 397,
   "p1": "1266",
   "pn": "1269",
   "abstract": [
    "We report results of an experiment on inducing communication problems in human-human dialogue. We set up a voice-only cooperative task where we manipulated one channel by replacing (in real-time, at random points) all signal with noise. Altogether around 10% of the speaker's signal was thus removed. We found an increase in clarification requests of a form that has previously been hypothesised to be used mainly for clarifying acoustic problems. We also found a correlation between the percentage of an utterance being manipulated and the use of devices for pointing out error locations. From our findings, we derive a gold-standard policy for clarification behaviour.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-397"
  },
  "dalessandro07_interspeech": {
   "authors": [
    [
     "Christophe",
     "D'Alessandro"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Sylvain Le",
     "Beux"
    ]
   ],
   "title": "Computerized chironomy: evaluation of hand-controlled intonation reiteration",
   "original": "i07_1270",
   "page_count": 4,
   "order": 398,
   "p1": "1270",
   "pn": "1273",
   "abstract": [
    "Chironomy means in this paper of intonation modeling in terms of hand movements. An experiment in hand-controlled intonation reiteration is described. A system for real-time intonation modification driven by a graphic tablet is presented. This system is used for reiterating a speech corpus (sentences of 1 to 9 syllables, natural and reiterant speech). The subjects also produced vocal imitation of the same corpus. Correlation and distances between natural and reiterated intonation contours are measured. These measures show that chironomic reiteration and vocal reiteration give comparable, and good, results. This paves the way to several applications in expressive intonation synthesis and to a new intonation modeling paradigm in terms of movements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-398"
  },
  "habernal07_interspeech": {
   "authors": [
    [
     "Ivan",
     "Habernal"
    ],
    [
     "Miloslav",
     "Konopík"
    ]
   ],
   "title": "JAAE: the java abstract annotation editor",
   "original": "i07_1298",
   "page_count": 4,
   "order": 399,
   "p1": "1298",
   "pn": "1301",
   "abstract": [
    "Recent trends in NLP (Natural Language Processing) are heading towards a stochastic processing of natural language. Stochastic methods, however, usually demand a lot of annotated training data. In most cases, the annotation of the data has to be done manually by a team of annotators and it is a highly time-consuming and expensive process. Thus we tried to develop an efficient and user-friendly editor that would aid human annotators to create the annotated data. We offer this editor for free. The developed editor is described in this article.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-399"
  },
  "nagino07_interspeech": {
   "authors": [
    [
     "Goshu",
     "Nagino"
    ],
    [
     "Makoto",
     "Shozakai"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "How to judge reusability of existing speech corpora for target task by utilizing statistical multidimensional scaling",
   "original": "i07_1302",
   "page_count": 4,
   "order": 400,
   "p1": "1302",
   "pn": "1305",
   "abstract": [
    "In order to develop a target speech recognition system with less cost of time and money, reusability of existing speech corpora is becoming one of the most important issues. This paper proposes a new technique to judge the reusability of existing speech corpora for a target task by utilizing a statistical multidimensional scaling method. In an experiment using twelve tasks in five speech corpora, our proposed method could show high correlation to the cross task recognition performance and judge the reusability of existing speech corpora correctly for the target task with lower cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-400"
  },
  "rutten07_interspeech": {
   "authors": [
    [
     "Peter",
     "Rutten"
    ]
   ],
   "title": "Feasibility of constructing an expressive speech corpus from television soap opera dialogue",
   "original": "i07_1306",
   "page_count": 4,
   "order": 401,
   "p1": "1306",
   "pn": "1309",
   "abstract": [
    "This paper presents a study into the feasibility of extracting a corpus of expressive speech from television soap opera dialogue. We investigated how dialogue can be extracted from television production tapes, and what kind of signal quality may be expected. We analysed to what extent the scripts that are used in television production can provide a transcription of the actual dialogue. From the scripts we also estimated how much dialogue speech we can expect to find for each character. We based our analysis on 7 seasons (1145 episodes) of a soap opera produced by the Flemish broadcaster VRT. The results show that processing 100 episodes can result in 3 hours of speech for one of the main characters, or 2.5 hours of dialogue between two of the main characters. The scripts, however, do not provide a quick win for automatic annotation of the corpus - they do not provide sufficiently accurate transcriptions of the dialogue that was actually spoken by the actors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-401"
  },
  "orr07_interspeech": {
   "authors": [
    [
     "Rosemary",
     "Orr"
    ],
    [
     "Bernat González i",
     "Llinares"
    ],
    [
     "Françoise",
     "Petersen"
    ],
    [
     "Helge",
     "Hüttenrauch"
    ],
    [
     "Martin",
     "Böcker"
    ],
    [
     "Michael",
     "Tate"
    ]
   ],
   "title": "Collection of empirical data for standardization of generic vocabularies in speech driven ICT devices and services",
   "original": "i07_1310",
   "page_count": 4,
   "order": 402,
   "p1": "1310",
   "pn": "1313",
   "abstract": [
    "This paper describes a method of collecting multilingual speech data for use in the compilation of spoken command vocabularies for ICT devices and services in the EU, the EFTA countries and Turkey and Russia. The resulting vocabularies will be published as a European standard, for use by industry in the production of such applications. The context of this work is the EU i2010 framework for addressing the main challenges and developments in ICT up to 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-402"
  },
  "selmini07_interspeech": {
   "authors": [
    [
     "Antonio Marcos",
     "Selmini"
    ],
    [
     "Fábio",
     "Violaro"
    ]
   ],
   "title": "Acoustic-phonetic features for refining the explicit speech segmentation",
   "original": "i07_1314",
   "page_count": 4,
   "order": 403,
   "p1": "1314",
   "pn": "1317",
   "abstract": [
    "This paper describes the refinement of the automatic speech segmentation into phones obtained via Hidden Markov Models (HMM). This refinement is based on acoustic-phonetic features associated to different phone classes. The proposed system was evaluated using both a small speaker dependent Brazilian Portuguese speech database and a speaker independent speech database (TIMIT). The refinement was applied to the boundaries obtained by just running the Viterbi's algorithm on the HMMs associated to the different utterances. Improvements of 30% and 13% were achieved in the percentage of segmentation errors below 20 ms for the speaker dependent and speaker independent databases respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-403"
  },
  "lecouteux07_interspeech": {
   "authors": [
    [
     "B.",
     "Lecouteux"
    ],
    [
     "Georges",
     "Linarès"
    ],
    [
     "Frédéric",
     "Beaugendre"
    ],
    [
     "Pascal",
     "Nocera"
    ]
   ],
   "title": "Text island spotting in large speech databases",
   "original": "i07_1318",
   "page_count": 4,
   "order": 404,
   "p1": "1318",
   "pn": "1321",
   "abstract": [
    "This paper addresses the problem of using journalist prompts or closed captions to build corpora for training speech recognition systems. Generally, these text documents are imperfect transcripts which suffer from the lack of timestamps. We propose a method combining a driven decoding algorithm and a fast-match process allowing to spot text-segments. This method is evaluated both on the French ESTER ([1]) corpus and on a large database composed of records from the Radio Television Belge Francophone (RTBF) associated to real prompts. Results show very good performance in terms of spotting; we observed a F-measure of about 98% on spotting the real text island provided by the RTBF corpus. Moreover, the decoding driven by the imperfect transcript island outperforms significantly the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-404"
  },
  "paek07_interspeech": {
   "authors": [
    [
     "Tim",
     "Paek"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Christopher",
     "Meek"
    ]
   ],
   "title": "People watcher: a game for eliciting human-transcribed data for automated directory assistance",
   "original": "i07_1322",
   "page_count": 4,
   "order": 405,
   "p1": "1322",
   "pn": "1325",
   "abstract": [
    "Automated Directory Assistance (ADA) allows users to request telephone or address information of residential and business listings using speech recognition. Because callers often express listings differently than how they are registered in the directory, ADA systems require transcriptions of alternative phrasings for directory listings as training data, which can be costly to acquire. As such, a framework in which data can be contributed voluntarily by large numbers of Internet users has tremendous value. In this paper, we introduce People Watcher, a computer game that elicits transcribed, alternative user phrasings for directory listings while at the same time entertaining players. Data generated from the game not only overlapped actual audio transcriptions, but resulted in a statistically significant 15% relative reduction in semantic error rate when utilized for ADA. Furthermore, semantic accuracy was not statistically different than using the actual audio transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-405"
  },
  "kun07_interspeech": {
   "authors": [
    [
     "Andrew",
     "Kun"
    ],
    [
     "Tim",
     "Paek"
    ],
    [
     "Zeljko",
     "Medenica"
    ]
   ],
   "title": "The effect of speech interface accuracy on driving performance",
   "original": "i07_1326",
   "page_count": 4,
   "order": 406,
   "p1": "1326",
   "pn": "1329",
   "abstract": [
    "With the proliferation of cell phones around the world, governments have been enacting legislation prohibiting the use of cell phones during driving without a \"hands-free\" kit, bringing automotive speech recognition to the forefront of public safety. At the same time, the trend in cell phone hardware has been to create smaller and thinner devices with greater computational power and functional complexity, making speech the most viable modality for user input. Given the important role that automotive speech recognition is likely to play in consumer lives, we explore how the accuracy of the speech engine, the use of the push-to-talk button, and the type of dialog repair employed by the interface influences driving performance. In experiments conducted with a driving simulator, we found that the accuracy of the speech engine and its interaction with the use of the push-to-talk button does impact driving performance significantly, but the type of dialog repair employed does not. We discuss the implications of these findings on the design of automotive speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-406"
  },
  "zhang07b_interspeech": {
   "authors": [
    [
     "Hua",
     "Zhang"
    ],
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Context constrained-generalized posterior probability for verifying phone transcriptions",
   "original": "i07_1330",
   "page_count": 4,
   "order": 407,
   "p1": "1330",
   "pn": "1333",
   "abstract": [
    "A new statistical confidence measure, Context Constrained- Generalized Posterior probability (CC-GPP), is proposed for verifying phone transcriptions in speech databases. Different from generalized posterior probability (GPP), CC-GPP is computed by considering string hypotheses that bear a focused phone with partially matched left and right contexts. Parameters used for CC-GPP include context window length, a minimal number of matched context phones, and verification thresholds. They are determined by minimizing verification errors in a development set. Evaluated on a test set of 500 sentences that consist of 2.1% phone errors, CCGPP achieves 99.6% accuracy and 78.7% recall when 90% of the phones are accepted.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-407"
  },
  "angkititrakul07_interspeech": {
   "authors": [
    [
     "Pongtep",
     "Angkititrakul"
    ],
    [
     "DongGu",
     "Kwak"
    ],
    [
     "SangJo",
     "Choi"
    ],
    [
     "JeongHee",
     "Kim"
    ],
    [
     "Anh",
     "PhucPhan"
    ],
    [
     "Amardeep",
     "Sathyanarayana"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Getting start with UTDrive: driver-behavior modeling and assessment of distraction for in-vehicle speech systems",
   "original": "i07_1334",
   "page_count": 4,
   "order": 408,
   "p1": "1334",
   "pn": "1337",
   "abstract": [
    "This paper describes our first step for advances in human-machine interactive systems for in-vehicle environments of the UTDrive project. UTDrive is part of an on-going international collaboration to collect and research rich multi-modal data recorded for modeling behavior while the driver is interacting with speech-activated systems or performing other secondary tasks. A simultaneous second goal is to better understand speech characteristics of the driver undergoing additional cognitive load since dialog systems are generally not formulated for high task-stress environment (e.g., driving a vehicle). The corpus consists of audio, video, brake/gas pedal pressure, forward distance, GPS information, and CAN-Bus information. The resulting corpus, analysis, and modeling will contribute to more effective speech systems which are able to sense driver cognitive distraction/stress and adapt itself to the driver's cognitive capacity and driving situations for improved safety while driving.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-408"
  },
  "kolluru07_interspeech": {
   "authors": [
    [
     "BalaKrishna",
     "Kolluru"
    ],
    [
     "Yoshihiko",
     "Gotoh"
    ]
   ],
   "title": "Relative evaluation of informativeness in machine generated summaries",
   "original": "i07_1338",
   "page_count": 4,
   "order": 409,
   "p1": "1338",
   "pn": "1341",
   "abstract": [
    "This paper is concerned with the relative evaluation of information content in summaries. We study the effect of crossing the summary-question pairs for a comprehension test based summary evaluation. Using the scheme, machine generated and human authored summaries from the broadcast news stories are evaluated. The approach does not use absolute scores. Instead it relies on a relative comparison, effectively alleviating the subjectivity of individual summary authors. The evaluation indicates that less than half (44%) of information is shared between human authored summaries of roughly 15 words. On the other hand, 27% of information in machine generated summaries is shared with human authored summaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-409"
  },
  "takezawa07_interspeech": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Masahide",
     "Mizushima"
    ],
    [
     "Tohru",
     "Shimizu"
    ],
    [
     "Genichiro",
     "Kikui"
    ]
   ],
   "title": "A method for evaluating task-oriented spoken dialog translation systems based on communication efficiency",
   "original": "i07_1342",
   "page_count": 4,
   "order": 410,
   "p1": "1342",
   "pn": "1345",
   "abstract": [
    "We propose a method for measuring communication efficiency from the viewpoint of conveying essential information in task-oriented spoken dialog translation. We present the results of one dialog experiment using speech-to-speech translation systems and a similar experiment using the Wizard of Oz method, which was carried out using hidden interpreters instead of a speech-to-speech translation system. We also present the relative performance score of the speech-to-speech translation system, which was obtained by comparing the machine's performance with that of humans, i.e., hidden interpreters. Finally, we discuss the relationship between users' linguistic behavior and system performance. We found that users of the system tended to make shorter utterances without decreasing the number of essential items needed to achieve a task and to improve transmission efficiency by using strategy to control dialogs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-410"
  },
  "hooijdonk07_interspeech": {
   "authors": [
    [
     "Charlotte van",
     "Hooijdonk"
    ],
    [
     "Edwin",
     "Commandeur"
    ],
    [
     "Reinier",
     "Cozijn"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Erwin",
     "Marsi"
    ]
   ],
   "title": "Using eye movements for online evaluation of speech synthesis",
   "original": "i07_1346",
   "page_count": 4,
   "order": 411,
   "p1": "1346",
   "pn": "1349",
   "abstract": [
    "This paper describes an eye tracking experiment to study the processing of diphone synthesis, unit selection synthesis, and human speech taking segmental and suprasegmental speech quality into account. The results showed that both factors influenced the processing of human and synthetic speech, and confirmed that eye tracking is a promising albeit time consuming research method to evaluate synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-411"
  },
  "li07h_interspeech": {
   "authors": [
    [
     "Jian",
     "Li"
    ],
    [
     "Dmitry",
     "Sityaev"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "Sentence level intelligibility evaluation for Mandarin text-to-speech systems using semantically unpredictable sentences",
   "original": "i07_1350",
   "page_count": 4,
   "order": 412,
   "p1": "1350",
   "pn": "1353",
   "abstract": [
    "Intelligibility assessment is one of the important aspects in the text-to-speech system (TTS) evaluation. Several intelligibility assessment\n",
    "methods have been proposed and successfully applied to European\n",
    "languages, both at word level and sentence level. Since Mandarin\n",
    "has its own unique features, these methods must be modified when applying to Mandarin. The word level assessment methods such as DRT and MRT have successfully been modified and extended to Mandarin (e.g. CDRT, CDRT-tone and CMRT). Sentence level assessment methods, on the other hand, have not been well studied for Mandarin. This paper focuses on the Semantically Unpredictable Sentences (SUS) test, which is one of the most commonly used sentence level\n",
    "assessment methods, and considers several important aspects\n",
    "of the SUS test design when extending it to Mandarin. It also compares the SUS test for Mandarin with CDRT, CDRT-tone and CMRT.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-412"
  },
  "kessens07_interspeech": {
   "authors": [
    [
     "Judith",
     "Kessens"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "N-best: the northern- and southern-dutch benchmark evaluation of speech recognition technology",
   "original": "i07_1354",
   "page_count": 4,
   "order": 413,
   "p1": "1354",
   "pn": "1357",
   "abstract": [
    "In this paper, we describe N-best 2008, the first Large Vocabulary Speech Recognition (LVCSR) benchmark evaluation held for the Dutch language. Both the accent as spoken in the Netherlands (Northern-Dutch) and in Belgium (Southern-Dutch or Flemish), will be evaluated. The evaluation tasks are broadcast news (BN) and conversational telephone speech (CTS). The N-best evaluation will take place in the spring of 2008 and is open to all research institutes and industries on voluntary basis. The goals of this first N-best evaluation is to define, set-up and conduct a Dutch LVCSR benchmark evaluation. In this paper, we will describe the state-of-the-art of Dutch LVCSR, recognition problems that are typical for the Dutch language, and the evaluation protocol.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-413"
  },
  "holter07_interspeech": {
   "authors": [
    [
     "Trym",
     "Holter"
    ],
    [
     "Svein",
     "Sørsdal"
    ]
   ],
   "title": "A MAP based approach to adaptive speech intelligibility measurements",
   "original": "i07_1358",
   "page_count": 4,
   "order": 414,
   "p1": "1358",
   "pn": "1361",
   "abstract": [
    "This paper presents an adaptive procedure applied to measurements of speech intelligibility using the modified rhyme test. It is argued that the required speech-to-noise (SNR) ratio could be estimated with sufficient accuracy with as few as 25 utterances. The present procedure is based on the maximum a posteriori criterion, and it is demonstrated how the standard deviation of the SNR estimate can be improved by about 0.5 dB compared to a previously published method based on the maximum likelihood procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-414"
  },
  "boonsuk07_interspeech": {
   "authors": [
    [
     "Sirinoot",
     "Boonsuk"
    ],
    [
     "Proadpran",
     "Punyabukkana"
    ],
    [
     "Atiwong",
     "Suchato"
    ]
   ],
   "title": "Phone boundary detection using selective refinements and context-dependent acoustic features",
   "original": "i07_1362",
   "page_count": 4,
   "order": 415,
   "p1": "1362",
   "pn": "1365",
   "abstract": [
    "Accurate placement of phone boundaries results in better performance of speech recognition systems as well as in the  quality of concatenative speech synthesis. This study proposes a post-processing technique to refine the locations of phone boundaries provided by HMM-based forced alignment. The context-dependent Linear Discriminant Analysis (LDA) classifiers together with a confidence scoring scheme are utilized to improve the precision of locating phone boundaries. Every acoustic feature is not always suitable for locating boundaries between every type of phonetic segment. Therefore, feature selections are performed based on the boundary types. The proposed context-dependent refinement results in a 43.9% error reduction in locating phone boundaries compared to the ones obtained from an HMM-based force alignment. The average deviation, from manually labeled boundaries, is reduced from 1.4 to 1.0 frame when the frame size used is 10 milliseconds.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-415"
  },
  "tan07_interspeech": {
   "authors": [
    [
     "Tien-Ping",
     "Tan"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Modeling context and language variation for non-native speech recognition",
   "original": "i07_1429",
   "page_count": 4,
   "order": 416,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "Non-native speakers often face difficulty in pronouncing like the native speakers. This paper proposes to model pronunciation variation in non-native speaker's speech using only acoustics models, without the need for the corpus. Variation in term of context and language will be modeled. The combination of both modeling resulted in the reduction of absolute WER as much as 16% and 6% for native Vietnamese and Chinese speakers of French.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-416"
  },
  "zhao07b_interspeech": {
   "authors": [
    [
     "Xufang",
     "Zhao"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "An evaluation of cross-language adaptation and native speech training for rapid HMM construction based on very limited training data",
   "original": "i07_1433",
   "page_count": 4,
   "order": 417,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "As the needs and opportunities for speech technology applications in a variety of languages have grown, methods for rapid transfer of speech technology across languages have become a practical concern. Previous works focus on the comparison of different adaptation algorithms, for example, MAP (Maximum A Posterior), Bootstrap, and MLLR (Maximum Likelihood Linear Regression) on speaker adaptation. However, a very interesting point is that, with increasing adaptation corpora, the performance of direct native speech training may already exceed the performance of cross-language adaptation. If it is true, there should be a threshold for the size of an adaptation corpus. In general, transferring acoustic knowledge is useful when there is not enough training data available. This paper presents a systematic comparison of the relative effectiveness of cross-language adaptation and native speech training, using transfer from English to Mandarin as a test case. This study found that cross-language adaptation does not produce better acoustic models than the direct native speech training approach even using limited training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-417"
  },
  "markov07_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Never-ending learning with dynamic hidden Markov network",
   "original": "i07_1437",
   "page_count": 4,
   "order": 418,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "Current automatic speech recognition systems have two distinctive modes of operation: training and recognition. After the training, system parameters are fixed, and if a mismatch between training and testing conditions occurs, an adaptation procedure is commonly applied. However, the adaptation methods change the system parameters in such a way that previously learned knowledge is irrecoverably destroyed. In searching for a solution to this problem and motivated by the results of recent neuro-biological studies, we have developed a network of hidden Markov states that is capable of unsupervised on-line adaptive learning while preserving the previously acquired knowledge. Speech patterns are represented by state sequences or paths through the network. The network can detect previously unseen patterns, and if such a new pattern is encountered, it is learned by adding new states and transitions to the network. Paths and states corresponding to spurious events or \"noises\" and, therefore, rarely visited, are gradually removed. Thus, the network can grow and shrink when needed, i.e. it dynamically changes its structure. The learning process continues as long as the network lasts, i.e. theoretically forever, so it is called neverending learning. The output of the network is the best state sequence and the decoding is done concurrently with the learning. Thus the network always operates in a single learning/decoding mode. Initial experiments with a small database of isolated spelled letters showed that the Dynamic Hidden Markov network is indeed capable of never-ending learning and can perfectly recognize previously learned speech patterns.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-418"
  },
  "breslin07_interspeech": {
   "authors": [
    [
     "C.",
     "Breslin"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Building multiple complementary systems using directed decision trees",
   "original": "i07_1441",
   "page_count": 4,
   "order": 419,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "Large vocabulary speech recognition systems typically use a combination of multiple systems to obtain the final hypothesis. For combination to give gains, the systems being combined must be complementary, i.e. they must make different errors. Often, complementary systems are chosen simply by training multiple systems, performing all combinations, and selecting the best. This approach becomes time consuming as more potential systems are considered, and hence recent work has looked at explicitly building systems to be complementary to each other. This paper considers building multiple complementary systems based on directed decision trees, and combining them within a multi-pass adaptive framework. The tree divergence is introduced for easy comparison of trees without having to build entire systems. Experiments are presented on a Broadcast News Arabic task, and show that gains can be achieved by using more than one complementary system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-419"
  },
  "nanjo07_interspeech": {
   "authors": [
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Yuichi",
     "Oku"
    ],
    [
     "Takehiko",
     "Yoshimi"
    ]
   ],
   "title": "Automatic speech recognition framework for multilingual audio contents",
   "original": "i07_1445",
   "page_count": 4,
   "order": 420,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "Automatic speech recognition (ASR) for multilingual audio contents, such as international conference recordings and broadcast news, is addressed. For handling such contents efficiently, a simultaneous ASR is promising. Conventionally, ASR has been performed independently, namely language by language, although multilingual speech, which consists of utterances in several languages representing the same meaning, is available. In this paper, we discuss a bilingual speech recognition framework based on statistical ASR and machine translation (MT) in which bilingual ASR is performed simultaneously and complementarily. Then, according to Japanese speech recognition with corresponding English text and MT, we shows the framework works well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-420"
  },
  "bouselmi07_interspeech": {
   "authors": [
    [
     "G.",
     "Bouselmi"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "I.",
     "Illina"
    ]
   ],
   "title": "Combined acoustic and pronunciation modelling for non-native speech recognition",
   "original": "i07_1449",
   "page_count": 4,
   "order": 421,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "In this paper, we present several adaptation methods for non-native speech recognition. We have tested pronunciation modelling, MLLR and MAP non-native pronunciation adaptation and HMM models retraining on the HIWIRE foreign accented English speech database. The \"phonetic confusion\" scheme we have developed consists in associating to each spoken phone several sequences of confused phones. In our experiments, we have used different combinations of acoustic models representing the canonical and the foreign pronunciations: spoken and native models, models adapted to the non-native accent with MAP and MLLR. The joint use of pronunciation modelling and acoustic adaptation led to further improvements in recognition accuracy. The best combination of the above mentioned techniques resulted in a relative word error reduction ranging from 46% to 71%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-421"
  },
  "emori07_interspeech": {
   "authors": [
    [
     "Tadashi",
     "Emori"
    ],
    [
     "Yoshifumi",
     "Onishi"
    ],
    [
     "Koichi",
     "Shinoda"
    ]
   ],
   "title": "Automatic estimation of scaling factors among probabilistic models in speech recognition",
   "original": "i07_1453",
   "page_count": 4,
   "order": 422,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "We propose an efficient new method for estimating scaling factors among probabilistic models in speech recognition. Most speech recognition systems consist of an acoustic and a language model, and require scaling factors to balance probabilities among them. The scaling factors are conventionally optimized in recognition tests. In our proposed method, the scaling factors are regarded as parameters of a log-linear model, and they are estimated using a gradient-ascent method based on the\n",
    "maximum a posteriori probability criterion. Posterior probability is computed using word-lattices. We employ an iteration technique which repeats a word-lattice-generation/scaling-factor-estimation process, and the resulting scaling factor estimation is robust with respect to the changes in initial values. In experiments, estimated scaling factors were nearly identical to optimal values obtained in a greedy grid search, and they changed little with variations in initial values.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-422"
  },
  "stoimenov07_interspeech": {
   "authors": [
    [
     "Emilian",
     "Stoimenov"
    ],
    [
     "John",
     "McDonough"
    ]
   ],
   "title": "Memory efficient modeling of polyphone context with weighted finite-state transducers",
   "original": "i07_1457",
   "page_count": 4,
   "order": 423,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "In earlier work, we derived a transducer HC that translates from sequences of Gaussian mixture models directly to phone sequences. The HC transducer was statically expanded then determinized and minimized. In this work, we present a refinement of the correct algorithm whereby the initial\n",
    "HC transducer is incrementally expanded and immediately determinized. This technique avoids the need for a full expansion of the initial\n",
    "HC, and thereby reduces the random access memory required to produce the determinized HC by a factor of more than five. With the incremental algorithm, we were able to construct HC for a semi-continuous acoustic model with 16,000 distributions which reduced the word error rate from 34.1% to 32.9% with respect to a fully-continuous system with 4,000 distributions on the lecture meeting portion of the NIST RT05 data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-423"
  },
  "pylypenko07_interspeech": {
   "authors": [
    [
     "Valeriy",
     "Pylypenko"
    ]
   ],
   "title": "Extra large vocabulary continuous speech recognition algorithm based on information retrieval",
   "original": "i07_1461",
   "page_count": 4,
   "order": 424,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "This paper presents a new two-pass algorithm for Extra Large (more than 1M words) Vocabulary Continuous Speech recognition based on the Information Retrieval (ELVIRCOS). The principle of this approach is to decompose a recognition process into two passes where the first pass builds the word subset for the second pass recognition by using information retrieval procedure. Word graph composition for continuous speech is presented. With this approach a high performance for large vocabulary speech recognition can be obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-424"
  },
  "hetherington07_interspeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "PocketSUMMIT: small-footprint continuous speech recognition",
   "original": "i07_1465",
   "page_count": 4,
   "order": 425,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "We present PocketSUMMIT, a small-footprint version of our SUMMIT continuous speech recognition system. With portable devices becoming smaller and more powerful, speech is increasingly becoming an important input modality on these devices. PocketSUMMIT is implemented as a variable-rate continuous density hidden Markov model with diphone context-dependent models. We explore various Gaussian parameter quantization schemes and find 8:1 compression or more is achievable with little reduction in accuracy. We also show how the quantized parameters can be used for rapid table lookup. We explore first-pass language model pruning in a finite-state transducer (FST) framework, as well as FST and n-gram weight quantization and bit packing, to further reduce memory usage. PocketSUMMIT is currently able to run a moderate vocabulary conversational speech recognition system in real time in a few MB on current PDAs and smart phones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-425"
  },
  "cincarek07_interspeech": {
   "authors": [
    [
     "Tobias",
     "Cincarek"
    ],
    [
     "Izumi",
     "Shindo"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Development of preschool children subsystem for ASR and q&a in a real-environment speech-oriented guidance task",
   "original": "i07_1469",
   "page_count": 4,
   "order": 426,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "The development of a module for speech recognition and answer generation for preschool children for a speech-oriented guidance system is described. This topic requires extra treatment because the performance is still disproportionally low to children of higher age, there is a growing business demand and only relatively few research on preschool children ASR has been carried out. This is especially true for building practical applications. A real-environment speech database with more than 12,000 utterances of Japanese preschool children and more than 60,000 utterances of school children are employed for system development. The difference between preschool children's and standard pronunciation is narrowed by introducing uniform reference transcriptions and pronunciation modeling. Furthermore, language and acoustic model are optimized. Final evaluation shows, that the speech-oriented guidance system's response accuracy can be improved by more than 12% absolute.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-426"
  },
  "ma07b_interspeech": {
   "authors": [
    [
     "Chengyuan",
     "Ma"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A study on word detector design and knowledge-based pruning and rescoring",
   "original": "i07_1473",
   "page_count": 4,
   "order": 427,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "Detection of speech attributes, phones and words is a key component of a detection-based automatic speech recognition framework in the automatic speech attribute transcription project. This paper presents a two-stage approach, keyword-filler network method followed by knowledge-based pruning and rescoring, for detection of any given word in continuous speech. Different from conventional keyword spotting systems, both content words and function words are considered in this study. To reduce the high miss, a modified grammar network for word detection is proposed. Then knowledge sources from landmark detection, attributes detection and other spectral cues were combined together to remove the unlikely putative segments from the hypothesized word candidates. This study has been evaluated on the WSJ0 corpus under matched and mismatched acoustic conditions. When comparing with the conventional keyword spotting system, we found the proposed word detector greatly improves the detection performance. The figure-of-merits for content and function words were improved from 48.8% to 61.5%, and 22.3% to 33.1% respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-427"
  },
  "colthurst07_interspeech": {
   "authors": [
    [
     "Thomas",
     "Colthurst"
    ],
    [
     "Tresi",
     "Arvizo"
    ],
    [
     "Chia-Lin",
     "Kao"
    ],
    [
     "Owen",
     "Kimball"
    ],
    [
     "Stephen A.",
     "Lowe"
    ],
    [
     "David R. H.",
     "Miller"
    ],
    [
     "Jim Van",
     "Sciver"
    ]
   ],
   "title": "Parameter tuning for fast speech recognition",
   "original": "i07_1477",
   "page_count": 4,
   "order": 428,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "We describe a novel method for tuning the decoding parameters of a speech-to-text system so as to minimize word error rate (WER) subject to an over-all time constraint. When applied to three sub-realtime systems for recognizing English conversational telephone speech, the method gave speed improvements of up to 21.1% while at the same time reducing WER by up to 6.7%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-428"
  },
  "bosch07_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Bert",
     "Cranen"
    ]
   ],
   "title": "A computational model for unsupervised word discovery",
   "original": "i07_1481",
   "page_count": 4,
   "order": 429,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "We present an unsupervised algorithm for the discovery of words and word-like fragments from the speech signal, without using an upfront defined lexicon or acoustic phone models. The algorithm is based on a combination of acoustic pattern discovery, clustering, and temporal sequence learning. It exploits the acoustic similarity between multiple acoustic tokens of the same words or word-like fragments. In its current form, the algorithm is able to discover words in speech with low perplexity (connected digits). Although its performance still falls off compared to mainstream ASR approaches, the value of the algorithm is its potential to serve as a computational model in two research directions. First, the algorithm may lead to an approach for speech recognition that is fundamentally liberated from the modelling constraints in conventional ASR. Second, the proposed algorithm can be interpreted as a computational model of language acquisition that takes actual speech as input and is able to find words as ‘emergent’ properties from raw input.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-429"
  },
  "meyer07b_interspeech": {
   "authors": [
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Matthias",
     "Wächter"
    ],
    [
     "Thomas",
     "Brand"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Phoneme confusions in human and automatic speech recognition",
   "original": "i07_1485",
   "page_count": 4,
   "order": 430,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "A comparison between automatic speech recognition (ASR) and human speech recognition (HSR) is performed as prerequisite for identifying sources of errors and improving feature extraction in ASR. HSR and ASR experiments are carried out with the same logatome database which consists of nonsense syllables. Two different kinds of signals are presented to human listeners: First, noisy speech samples are converted to Mel-frequency cepstral coefficients which are resynthesized to speech, with information about voicing and fundamental frequency being discarded. Second, the original signals with added noise are presented, which is used to evaluate the loss of information caused by the process of resynthesis. The analysis also covers the degradation of ASR caused by dialect or accent and shows that different error patterns emerge for ASR and HSR. The information loss induced by the calculation of ASR features has the same effect as a deterioration of the SNR by 10 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-430"
  },
  "ohta07_interspeech": {
   "authors": [
    [
     "Kengo",
     "Ohta"
    ],
    [
     "Masatoshi",
     "Tsuchiya"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Construction of spoken language model including fillers using filler prediction model",
   "original": "i07_1489",
   "page_count": 4,
   "order": 431,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "This paper proposes a novel method to construct a spoken language model including fillers from a corpus including no fillers using a filler prediction model. It consists of two submodels: a filler insertion model which predicts places where fillers should be inserted, and a filler selection model which predicts appropriate fillers for given places. It converts a corpus that covers domain-relevant topics but includes no fillers into a corpus that contains fillers as well as domain-relevant topics. The experiment against the corpus of spontaneous Japanese shows that language models constructed by the proposed method achieve quite near performance of the traditional trigram language model constructed from the real spontaneous corpus including fillers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-431"
  },
  "kumaran07_interspeech": {
   "authors": [
    [
     "Raghunandan",
     "Kumaran"
    ],
    [
     "Jeff",
     "Bilmes"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Attention shift decoding for conversational speech recognition",
   "original": "i07_1493",
   "page_count": 4,
   "order": 432,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "We introduce a novel approach to decoding in speech recognition (termed attention-shift decoding) that attempts to mimic aspects of human speech recognition responsible for robustness in processing conversational speech. Our approach is a radical departure from traditional decoding algorithms for speech recognition. We propose a method to first identify reliable regions of the speech signal and then use these to help decode the unreliable regions, thus conditioning on potentially non-consecutive portions of the signal. We test this approach in a second-pass rescoring framework and compare it to standard second-pass rescoring. On a conversational telephone speech recognition task (EARS RT-03 CTS evaluation), our approach shows an improvement of 2.6% absolute when using oracle information for detecting the reliable regions, and 0.4% absolute when detecting the reliable regions automatically.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-432"
  },
  "mihajlik07_interspeech": {
   "authors": [
    [
     "Péter",
     "Mihajlik"
    ],
    [
     "Tibor",
     "Fegyó"
    ],
    [
     "Zoltán",
     "Tüske"
    ],
    [
     "Pavel",
     "Ircing"
    ]
   ],
   "title": "A morpho-graphemic approach for the recognition of spontaneous speech in agglutinative languages - like Hungarian",
   "original": "i07_1497",
   "page_count": 4,
   "order": 433,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "A coupled acoustic- and language-modeling approach is presented for the recognition of spontaneous speech primarily in agglutinative languages. The effectiveness of the approach in large vocabulary spontaneous speech recognition is demonstrated on the Hungarian MALACH corpus. The derivation of morphs from word forms is based on a statistical morphological segmentation tool while the mapping of morphs into graphemes is obtained trivially by splitting each morph into individual letters. Using morphs instead of words in language modeling gives significant WER reductions in case of both phoneme- and grapheme-based acoustic modeling. The improvements are larger after speaker adaptation of the acoustic models. In conclusion, morphophonemic and the proposed morpho-graphemic ASR approaches yield the same best WERs, which are significantly lower than the word-based baselines but essentially without language dependent rules or pronunciation dictionaries in the latter case.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-433"
  },
  "yang07f_interspeech": {
   "authors": [
    [
     "Mei",
     "Yang"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Andreas",
     "Kathol"
    ]
   ],
   "title": "A semi-supervised learning approach for morpheme segmentation for an Arabic dialect",
   "original": "i07_1501",
   "page_count": 4,
   "order": 434,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "We present a semi-supervised learning approach which utilizes a heuristic model for learning morpheme segmentation for Arabic dialects. We evaluate our approach by applying morpheme segmentation to the training data of a statistical machine translation (SMT) system. Experiments show that our approach is less sensitive to the availability of annotated stems than a previous rule-based approach and learns 12% more segmentations on our Iraqi Arabic data. When applied in an SMT system, our approach yields a 8% relative reduction in the training vocabulary size and a 0.8% relative reduction in the out-of-vocabulary (OOV) rate on the test set, again as compared to the rule-based approach. Finally, our approach also results in a modest increase in BLEU scores.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-434"
  },
  "huyssteen07_interspeech": {
   "authors": [
    [
     "Gerhard B. van",
     "Huyssteen"
    ],
    [
     "Martin J.",
     "Puttkammer"
    ]
   ],
   "title": "Accelerating the annotation of lexical data for less-resourced languages",
   "original": "i07_1505",
   "page_count": 4,
   "order": 435,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "The development of digital resources is an expensive and time-consuming endeavor, especially in the case of less-resourced languages. In this paper, we describe a freely available, open-source system, called TurboAnnotate, for bootstrapping linguistic data for machine-learning purposes, or for manually creating gold standards or other annotated lists. A detailed description of the design and functionalities of the tool is given, focusing on how the requirements of end-users are being addressed through it. It is indicated that TurboAnnotate does not only promise to help increase the accuracy of human annotators, but also to save enormously on human effort in terms of time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-435"
  },
  "draxler07_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "On web-based creation of speech resources for less-resourced languages",
   "original": "i07_1509",
   "page_count": 4,
   "order": 436,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "Web-based creation of speech resources is a new paradigm for producing spoken language resources. It is particularly suited for less resourced languages, i.e. languages for which no readily available speech resources exist. This paper maps the speech resource creation tasks to the client-server architecture of the WWW. It presents two tools that have been developed for web-based speech resource creation, and it demonstrates the effectiveness of this approach by three use cases: 1) high bandwidth recordings of new speaker populations in geographically distributed locations, 2) recordings in adverse recording environments, e.g. hospitals, and 3) field recordings of endangered languages. The only infrastructure requirements are electricity for the equipment and an Internet connection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-436"
  },
  "martinovic07_interspeech": {
   "authors": [
    [
     "Miroslav",
     "Martinović"
    ],
    [
     "Srdjan",
     "Vesić"
    ],
    [
     "Goran",
     "Rakić"
    ]
   ],
   "title": "Building an information retrieval system for serbian - challenges and solutions",
   "original": "i07_1513",
   "page_count": 4,
   "order": 437,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "We describe challenges encountered while building an information retrieval system for Serbian language. Approaches designed and adopted to handle them are depicted and illuminated in this paper. As a backbone of our system, we used SMART retrieval system which we augmented with features necessary to deal with specificities of the Serbian alphabet. In addition, morphological richness of the language accentuated implications of the text preprocessing phase. During this phase, we devised two algorithms which increased retrieval precision by 14% and 27%, respectively. Testing was conducted using two gigabyte EBART collection of Serbian newspaper articles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-437"
  },
  "pauw07_interspeech": {
   "authors": [
    [
     "Guy De",
     "Pauw"
    ],
    [
     "Peter Waiganjo",
     "Wagacha"
    ]
   ],
   "title": "Bootstrapping morphological analysis of gĩkũyũ using unsupervised maximum entropy learning",
   "original": "i07_1517",
   "page_count": 4,
   "order": 438,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "This paper describes a proof-of-the-principle experiment in which maximum entropy learning is used for the automatic induction of shallow morphological features for the resource-scarce Bantu language of Gĩkũyũ. This novel approach circumvents the limitations of typical unsupervised morphological induction methods that employ minimum-edit distance metrics to establish morphological similarity between words. The experimental results show that the unsupervised maximum entropy learning approach compares favorably to those of the established AutoMorphology method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-438"
  },
  "gros07_interspeech": {
   "authors": [
    [
     "Jerneja Žganec",
     "Gros"
    ],
    [
     "Stanislav",
     "Gruden"
    ]
   ],
   "title": "The voiceTRAN machine translation system",
   "original": "i07_1521",
   "page_count": 4,
   "order": 439,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "Freely available tools and language resources were used to build the VoiceTRAN statistical machine translation (SMT) system. Various configuration variations of the system are presented and evaluated. The VoiceTRAN SMT system outperformed the baseline conventional rule-based MT system in both English-Slovenian in-domain test setups. To further increase the generalization capability of the translation model for lower-coverage out-of-domain test sentences, an \"MSD-recombination\" approach was proposed. This approach not only allows a better exploitation of conventional translation models, but also performs well in the more demanding translation direction; that is, into a highly inflectional language. Using this approach in the out-of-domain setup of the English-Slovenian JRC-ACQUIS task, we have achieved significant improvements in translation quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-439"
  },
  "paulo07_interspeech": {
   "authors": [
    [
     "Sérgio",
     "Paulo"
    ],
    [
     "Luís C.",
     "Oliveira"
    ]
   ],
   "title": "MuLAS: a framework for automatically building multi-tier corpora",
   "original": "i07_1525",
   "page_count": 4,
   "order": 440,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "The Multi- Level Alignment System (MuLAS) is the L2F tool for building multi-tier speech corpora with reduced or no human intervention at all. MuLAS automatically combines information coming from external speech annotations, human or machine-generated, with the text-based utterance descriptions that it creates, in order to build more reliable and complete descriptions of the spoken utterances.\n",
    "This paper presents our methods for multi-tier annotation synchronization, which lie behind the MuLAS operation. Such methods have allowed us to expand the building of multi-tier corpora to new languages without spending too much effort. MuLAS has been successfully applied to the building of multi-tier corpora for speech synthesis in American and British English, European Portuguese and German. Natural prosody generation has benefited from MuLAS, too, since prosodic models can be derived from corpora built by MuLAS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-440"
  },
  "ringersma07_interspeech": {
   "authors": [
    [
     "Jacquelijn",
     "Ringersma"
    ],
    [
     "Marc",
     "Kemps-Snijders"
    ]
   ],
   "title": "Creating multimedia dictionaries of endangered languages using LEXUS",
   "original": "i07_1529",
   "page_count": 4,
   "order": 441,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "This paper reports on the development of a flexible web based lexicon tool, LEXUS. LEXUS is targeted at linguists involved in language documentation (of endangered languages). It allows the creation of lexica within the structure of the proposed ISO LMF standard and uses the proposed concept naming conventions from the ISO data categories, thus enabling interoperability, search and merging. LEXUS also offers the possibility to visualize language, since it provides functionalities to include audio, video and still images to the lexicon. With LEXUS it is possible to create semantic network knowledge bases, using typed relations. The LEXUS tool is free for use.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-441"
  },
  "loftsson07_interspeech": {
   "authors": [
    [
     "Hrafn",
     "Loftsson"
    ],
    [
     "Eiríkur",
     "Rögnvaldsson"
    ]
   ],
   "title": "IceNLP: a natural language processing toolkit for icelandic",
   "original": "i07_1533",
   "page_count": 4,
   "order": 442,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "Icelandic is a morphologically complex language, for which language technology resources are scarce. Only a few years ago, it could be stated that language technology was practically non-existent in Iceland. In this paper, we describe the development of an NLP toolkit for processing the language, the challenges faced and the decisions made during development. The current version of the toolkit consists of a tokeniser/sentence segmentiser, a morphological analyser, a linguistic rule-based tagger, and a finite-state parser. The development of our toolkit is a step towards building a Basic Language Resource Toolkit (BLARK) for the Icelandic language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-442"
  },
  "peche07_interspeech": {
   "authors": [
    [
     "Marius",
     "Peche"
    ],
    [
     "Marelie",
     "Davel"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Phonotactic spoken language identification with limited training data",
   "original": "i07_1537",
   "page_count": 4,
   "order": 443,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "We investigate the addition of a new language, for which limited resources are available, to a phonotactic language identification system. Two classes of approaches are studied: in the first class, only existing phonetic recognizers are employed, whereas an additional phonetic recognizer in the new language is created for the second class. It is found that the number of acoustic recognizers employed plays a crucial role in determining the recognition accuracy for the new language. We study different approaches to incorporating a language for which audio-only data is available (no pronunciation dictionaries or transcriptions) and find that if more than about 2000 training utterances are available, a bootstrapped acoustic model for the new language can improve accuracy substantially.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-443"
  },
  "abate07_interspeech": {
   "authors": [
    [
     "Solomon Teferra",
     "Abate"
    ],
    [
     "Wolfgang",
     "Menzel"
    ]
   ],
   "title": "Automatic speech recognition for an under-resourced language - amharic",
   "original": "i07_1541",
   "page_count": 4,
   "order": 444,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "In this paper we present the development of an Automatic Speech Recognition System (ASRS) for Amharic using limited available resources and the freely available speech toolkit (HTK). There are phonological, dialectal, orthographic and morphological features of Amharic that challenge the development of ASRSs. The problem of resource scarcity is also a hindrance to the research and development initiatives in the area of Amharic ASR. Dealing with these language and resource related problems, we have developed syllable- and triphone-based ASR for Amharic and achieved 90.43% and 91.31% word recognition accuracy, respectively, on the evaluation test set of 5k vocabulary.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-444"
  },
  "nimaan07_interspeech": {
   "authors": [
    [
     "Abdillahi",
     "Nimaan"
    ],
    [
     "Pascal",
     "Nocera"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Information retrieval strategies for accessing african audio corpora",
   "original": "i07_1545",
   "page_count": 4,
   "order": 445,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "In this paper we present a first approach to access African oral corpora, combining automatic speech recognition and information retrieval. Firstly, we present the principal characteristics of our Somali speech recognizer [8] and the results obtained on real audio archives gathered from Djibouti Radio. Secondly, we present a Hybrid Language Model (HLM) including words and sub-words to improve the robustness against OOV words. We proceed to Information Retrieval experiments with various strategies. We search on the different outputs of the ASR system (words, sub-words and hybrid). We finally present a new strategy combining sub-words and words to enhance the information retrieval results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-445"
  },
  "siivola07_interspeech": {
   "authors": [
    [
     "Vesa",
     "Siivola"
    ],
    [
     "Mathias",
     "Creutz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Morfessor and variKN machine learning tools for speech and language technology",
   "original": "i07_1549",
   "page_count": 4,
   "order": 446,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "This paper introduces two recent open source software packages developed for unsupervised natural language modeling. The Morfessor program segments words automatically into morpheme-like units without any rule-based morphological analyzers. The VariKN toolkit trains language models producing a compact set of high-order n-grams utilizing state-of-art Kneser-Ney smoothing. As an example, this paper shows how to construct a language model for speech recognition in multiple languages utilizing only a minimal amount of linguistic resources. Morfessor and VariKN also have other applications in text understanding, information retrieval and machine translation. Unsupervised machine learning techniques are particularly well suited for the development of systems for less-resourced languages, because they do not depend on manually designed morphological or syntactical analyzers or annotated data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-446"
  },
  "jongtaveesataporn07_interspeech": {
   "authors": [
    [
     "Markpong",
     "Jongtaveesataporn"
    ],
    [
     "Issara",
     "Thienlikit"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Towards better language modeling for Thai LVCSR",
   "original": "i07_1553",
   "page_count": 4,
   "order": 447,
   "p1": "1553",
   "pn": "1556",
   "abstract": [
    "One of the difficulties of Thai language modeling is the process of text corpus preparation. Because there is no explicit word boundary marker in written Thai text, word segmentation must be performed prior to training a language model. This paper presents two approaches to language model construction for Thai LVCSR based on pseudo-morpheme merging. The first approach merges pseudo-morphemes using forward and reverse bi-grams. The second approach utilizes the C4.5 decision tree to merge pseudo-morphemes based on multiple features. The performance of ASR systems with language models built using these methods are better than systems which use only pseudo-morpheme or lexicon-based word segmentation. These approaches produce results comparable to that obtained by the system utilizing manual segmentation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-447"
  },
  "raymond07_interspeech": {
   "authors": [
    [
     "Christian",
     "Raymond"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Generative and discriminative algorithms for spoken language understanding",
   "original": "i07_1605",
   "page_count": 4,
   "order": 448,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "Spoken Language Understanding (SLU) for conversational systems (SDS) aims at extracting concept and their relations from spontaneous speech. Previous approaches to SLU have modeled concept relations as stochastic semantic networks ranging from generative approach to discriminative. As spoken dialog systems complexity increases, SLU needs to perform understanding based on a richer set of features ranging from a-priori knowledge, long dependency, dialog history, system belief, etc. This paper studies generative and discriminative approaches to modeling the sentence segmentation and concept labeling. We evaluate algorithms based on Finite State Transducers (FST) as well as discriminative algorithms based on Support Vector Machine sequence classifier based and Conditional Random Fields (CRF). We compare them in terms of concept accuracy, generalization and robustness to annotation ambiguities. We also show how non-local non-lexical features (e.g. a-priori knowledge) can be modeled with CRF which is the best performing algorithm across tasks. The evaluation is carried out on two SLU tasks of different complexity, namely ATIS and MEDIA corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-448"
  },
  "iosif07_interspeech": {
   "authors": [
    [
     "Elias",
     "Iosif"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "A soft-clustering algorithm for automatic induction of semantic classes",
   "original": "i07_1609",
   "page_count": 4,
   "order": 449,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "In this paper, we propose a soft-decision, unsupervised clustering algorithm that generates semantic classes automatically using the probability of class membership for each word, rather than deterministically assigning a word to a semantic class. Semantic classes are induced using an unsupervised, automatic procedure that uses a context-based similarity distance to measure semantic similarity between words. The proposed soft-decision algorithm is compared with various \"hard\" clustering algorithms, e.g., [1], and it is shown to improve semantic class induction performance in terms of both precision and recall for a travel reservation corpus. It is also shown that additional performance improvement is achieved by combining (auto-induced) semantic with lexical information to derive the semantic similarity distance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-449"
  },
  "gravano07_interspeech": {
   "authors": [
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Stefan",
     "Benus"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Shira",
     "Mitchell"
    ],
    [
     "Ilia",
     "Vovsha"
    ]
   ],
   "title": "Classification of discourse functions of affirmative words in spoken dialogue",
   "original": "i07_1613",
   "page_count": 4,
   "order": 450,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "We present results of a series of machine learning experiments that address the classification of the discourse function of single affirmative cue words such as alright, okay and mm-hm in a spoken dialogue corpus. We suggest that a simple discourse/sentential distinction is not sufficient for such words and propose two additional classification sub-tasks: identifying (a) whether such words convey acknowledgment or agreement, and (b) whether they cue the beginning or end of a discourse segment. We also study the classification of each individual word into its most common discourse functions. We show that models based on contextual features extracted from the time-aligned transcripts approach the error rate of trained human aligners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-450"
  },
  "minescu07_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Minescu"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Conditional use of word lattices, confusion networks and 1-best string hypotheses in a sequential interpretation strategy",
   "original": "i07_1617",
   "page_count": 4,
   "order": 451,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "Within the context of a deployed spoken dialog service, this study presents a new interpretation strategy based on the sequential use of different ASR output representations: 1-best strings, word lattices and confusion networks. The goal is to reject as early as possible in the decoding process the non-relevant messages containing non-speech or out-of-domain content. This is done through the 1-pass of the ASR decoding process thanks to specific acoustic and language models. A confusion network (CN) is then calculated for the remaining messages and another rejection process is applied with the confidence measures obtained in the CN. The messages kept at this stage are considered relevant; therefore the search for the best interpretation is applied to a richer search space than just the 1-best word string: either the whole CN or the whole word lattice. An improved, SLU oriented, CN generation algorithm is also proposed that significantly reduces the size of the CN obtained while improving the recognition performance. This strategy is evaluated on a large corpus of real users' messages obtained from a deployed service.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-451"
  },
  "kolar07_interspeech": {
   "authors": [
    [
     "Jáchym",
     "Kolář"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Speaker adaptation of language models for automatic dialog act segmentation of meetings",
   "original": "i07_1621",
   "page_count": 4,
   "order": 452,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "Dialog act (DA) segmentation in meeting speech is important for meeting understanding. In this paper, we explore speaker adaptation of hidden event language models (LMs) for DA segmentation using the ICSI Meeting Corpus. Speaker adaptation is performed using a linear combination of the generic speaker-independent LM and an LM trained on only the data from individual speakers. We test the method on 20 frequent speakers, on both reference word transcripts and the output of automatic speech recognition. Results indicate improvements for 17 speakers on reference transcripts, and for 15 speakers on automatic transcripts. Overall, the speaker-adapted LM yields statistically significant improvement over the baseline LM for both test conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-452"
  },
  "albalate07_interspeech": {
   "authors": [
    [
     "Amparo",
     "Albalate"
    ],
    [
     "Dimitar",
     "Dimitrov"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "Unsupervised categorisation approaches for technical support automated agents",
   "original": "i07_1625",
   "page_count": 4,
   "order": 453,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "In this paper we describe an unsupervised approach for the automated categorisation of utterances into predefined categories of symptoms (or problems) within the framework of a technical support automated agent. The utterance classification is performed based on an iterative K-means clustering method. In order to improve the lower accuracy typical of unsupervised algorithms, we have analysed two different enhancements of the classification algorithm. The first method exploits the affinity among words by automatically extracting classes of semantically equivalent terms. The second approach consists of a disambiguation technique based on a new criterion to estimate the relevance of terms for the classification. An analysis of the results of an experimental evaluation performed on a corpus of 34848 utterances concludes the paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-453"
  },
  "wohlmayr07_interspeech": {
   "authors": [
    [
     "Michael",
     "Wohlmayr"
    ],
    [
     "Marián",
     "Képesi"
    ]
   ],
   "title": "Joint position-pitch extraction from multichannel audio",
   "original": "i07_1629",
   "page_count": 4,
   "order": 454,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "Recently, a method for joint extraction of pitch and location information from two-channel recordings has been introduced. This framework offers a new, natural representation of all acoustic sources in the auditory scene, and has potential to be used as front-end in applications such as advanced tracking of multiple speakers in conference rooms. In this paper, we explore basic properties of this method and propose improvements in performance by using circular arrangements of multiple microphones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-454"
  },
  "kim07c_interspeech": {
   "authors": [
    [
     "Hyun Soo",
     "Kim"
    ]
   ],
   "title": "Morphological pre-processing technique and its applications on speech signal",
   "original": "i07_1633",
   "page_count": 4,
   "order": 455,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "The properties and applications of morphological filters for speech analysis are investigated. We introduce and investigate a novel nonlinear spectral envelope estimation method based on morphological operations, which is found to be very robust against noise. This method is also compared with the spectral envelope estimation vocoder (SEEVOC) method. A simple method for the optimum selection of the structuring set size without using pitch information is proposed. Also, a new concept of higher order peaks is introduced and found to be beneficial. The morphological approach is then used for a new pitch estimation method. The harmonic-plus-noise decomposition is used to develop a novel and flexible noise reduction method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-455"
  },
  "pelle07_interspeech": {
   "authors": [
    [
     "Patricia A.",
     "Pelle"
    ],
    [
     "Claudio F.",
     "Estienne"
    ]
   ],
   "title": "A pitch extraction system based on phase locked loops and consensus decision",
   "original": "i07_1637",
   "page_count": 4,
   "order": 456,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "In this work a very low error rate pitch estimation system is presented, which is also very robust against noise. Two key aspects of the system are mainly responsible of such good behavior: on the one hand we use a multiple estimation scheme based on PLL's. These devices provide us with robust information about the period of the speech signal harmonics. By combining this information with an additional independent estimation it is possible to obtain a robust estimation of f0. On the other hand, multiple estimations are combined in a stage that assesses each of them, retaining the more reliable ones. A final agreement value between these qualified estimations is the final result of the system. This consensus decision significantly improves the initial estimation accuracy. Overall performance is assessed by comparing our system to the get_f0 algorithm, under clean and noisy conditions. We show that our system outperforms get_f0 over all presented conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-456"
  },
  "legat07_interspeech": {
   "authors": [
    [
     "Milan",
     "Legát"
    ],
    [
     "Jindřich",
     "Matoušek"
    ],
    [
     "Daniel",
     "Tihelka"
    ]
   ],
   "title": "A robust multi-phase pitch-mark detection algorithm",
   "original": "i07_1641",
   "page_count": 4,
   "order": 457,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "This paper describes a robust multi-phase algorithm for marking of pitch pulses in speech using both glottal and speech signals. In the first phase, the glottal signal is used for the estimation of the fundamental frequency (f0) contour of the given sentence. Next, pitch mark candidates are generated on the basis of both glottal and speech signals. In the third phase, the best sequence of pitch marks is found in the set of the candidates. Finally, this pitch mark sequence is post-processed. One of the features of the new method is that every pitch mark detected is given confidence, so that problematic pitch mark subsequences can be located. The algorithm was tested and compared with other pitch-mark detection methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-457"
  },
  "molla07_interspeech": {
   "authors": [
    [
     "Md. Khademul Islam",
     "Molla"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Md. Kamrul",
     "Hasan"
    ]
   ],
   "title": "Pitch estimation of noisy speech signals using empirical mode decomposition",
   "original": "i07_1645",
   "page_count": 4,
   "order": 458,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "This paper presents a pitch estimation method of noisy speech signal using empirical mode decomposition (EMD). The normalized autocorrelation function (NACF) of the noisy speech signal is decomposed into a finite set of band-limited signals termed as intrinsic mode functions (IMFs) using EMD. The periodicity of one IMF is supposed to be equal to the accurate pitch period. A conventional autocorrelation based pitch period detection method is used to select the IMF with pitch period. The accurate pitch period is obtained from the selected IMF. The pitch estimation performance in term of gross pitch error (GPE) of the proposed algorithm is compared with recently proposed methods. The experimental results show that the EMD based algorithm performs better in pitch estimation of noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-458"
  },
  "hirst07_interspeech": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Hyongsil",
     "Cho"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Hyunji",
     "Yu"
    ]
   ],
   "title": "Evaluating two versions of the momel pitch modelling algorithm on a corpus of read speech in Korean",
   "original": "i07_1649",
   "page_count": 4,
   "order": 459,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "The Momel algorithm provides an automatic factoring of raw fundamental frequency into two components: a microprosodic component, corresponding to local variations of pitch caused by the phonetic nature of the speech segments and a macroprosodic component corresponding to the overall pitch pattern of the utterance which is then represented as a sequence of pitch targets. An earlier evaluation estimated the overall efficiency of the algorithm (F-measure) at around 95% on a corpus of read speech for 5 European languages and at around 93% for a corpus of spontaneous speech. In this paper we present the results of the evaluation of the output of two versions of the Momel algorithm as compared with manually corrected pitch targets for a corpus of just over 2 hours of read speech in Korean (40 continuous 5-sentence passages, each read by 5 male and 5 female speakers). The results show that the new version of the Momel algorithm performs systematically better than the earlier version.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-459"
  },
  "hussein07_interspeech": {
   "authors": [
    [
     "Hussein",
     "Hussein"
    ],
    [
     "Oliver",
     "Jokisch"
    ]
   ],
   "title": "Hybrid electroglottograph and speech signal based algorithm for pitch marking",
   "original": "i07_1653",
   "page_count": 4,
   "order": 460,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "Pitch marking is very significant in speech signal processing. In a text-to-speech (TTS) system based on the Time-Domain Pitch-Synchronous Overlap-Add (TD-PSOLA) method, robust estimation of pitch marks (PM) is especially important to the modification of the time and pitch scale of a speech signal in order to match it to that of the target speaker. The aim of this paper is to improve the accuracy of automatic Pitch Mark Algorithms (PMA). Therefore, we propose a hybrid method for pitch marking that combines the advantages of the Electroglottograph (EGG) and the speech signals. We evaluate this hybrid algorithm for pitch marking against pitch mark algorithm used by Praat program [1]. The results of the evaluation indicate that the suggested method provides better performance than PMA based on EGG signal or speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-460"
  },
  "droppo07_interspeech": {
   "authors": [
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "A fine pitch model for speech",
   "original": "i07_2757",
   "page_count": 4,
   "order": 461,
   "p1": "2757",
   "pn": "2760",
   "abstract": [
    "An accurate model for the structure of speech is essential to many speech processing applications, including speech enhancement, synthesis, recognition, and coding. This paper explores some deficiencies of standard harmonic methods of modeling voiced speech. In particular, they ignore the effect of fundamental frequency changing within an analysis frame, and the fact that the fundamental frequency is not a continuously varying parameter, but a side effect of a series of discrete events.\n",
    "We present an alternative, time-series based framework for modeling the voicing structure of speech called the fine pitch model. By precisely modeling the voicing structure, it can more accurately account for the content in a voiced speech segment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-461"
  },
  "ghosh07_interspeech": {
   "authors": [
    [
     "Prasanta Kumar",
     "Ghosh"
    ],
    [
     "Antonio",
     "Ortega"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Pitch period estimation using multipulse model and wavelet transform",
   "original": "i07_2761",
   "page_count": 4,
   "order": 462,
   "p1": "2761",
   "pn": "2764",
   "abstract": [
    "Wavelet transform-based pitch period estimation is well known in the literature. This approach to pitch estimation assumes that the glottis closures are correlated with the maxima in the adjacent scales of the wavelet transform and for pitch period estimation, one needs to detect these correlated maxima across these scales, which is often prone to error especially in the case of noisy signals. In this paper, we develop an optimization scheme in the wavelet framework using a multipulse excitation model for the speech signal and the pitch period is estimated as a result of this optimization. We report experiments on both clean and noisy conditions and show that the proposed optimization works better than wide used heuristic approach for maxima detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-462"
  },
  "heckmann07_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Frank",
     "Joublin"
    ],
    [
     "Christian",
     "Goerick"
    ]
   ],
   "title": "Combining rate and place information for robust pitch extraction",
   "original": "i07_2765",
   "page_count": 4,
   "order": 463,
   "p1": "2765",
   "pn": "2768",
   "abstract": [
    "In this paper we propose an algorithm for the robust extraction of pitch combining both temporal (rate) and pattern matching (place) techniques. Following a transformation into the spectral domain via the application of a Gammatone filter bank the rate information is extracted in each band via the zero crossing distances in that band. Next a comb filter with teeth at the harmonics of the current fundamental frequency hypothesis is set up, reflecting the pattern matching aspect. The signals below the teeth of the comb filter are analyzed upon consistency. This yields an allocation pattern for the filter. The current allocation pattern is compared to prototypical ones allowing the suppression of side peaks at harmonics and sub-harmonics of the true fundamental. A comparison to a state of the art autocorrelation based algorithm is performed showing significantly better results for our algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-463"
  },
  "christensen07_interspeech": {
   "authors": [
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Ning",
     "Ma"
    ],
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Integrating pitch and localisation cues at a speech fragment level",
   "original": "i07_2769",
   "page_count": 4,
   "order": 464,
   "p1": "2769",
   "pn": "2772",
   "abstract": [
    "This paper proposes a novel speech-fragment based approach for processing binaural data to improve the estimation of speech source locations in reverberant, multi-speaker recordings. The technique employs two stages. First, a robust multi-pitch tracking algorithm is used to locate local spectro-temporal ‘speech fragments’ - regions where the energy in the mixture is dominated by a single speech source. Second, robust localisation estimates are formed by integrating interaural time difference cues over each speech fragment. The technique is applied to the analysis of more than five hours of two-party meetings that have been constructed from a mixture of binaural mannequin recordings. It is shown that estimating location at the speech fragment level produces better results than conventional location-estimate smoothing techniques leading to a an increase in relative frame accuracy rate of more than 35%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-464"
  },
  "lienard07_interspeech": {
   "authors": [
    [
     "Jean-Sylvain",
     "Liénard"
    ],
    [
     "François",
     "Signol"
    ],
    [
     "Claude",
     "Barras"
    ]
   ],
   "title": "Speech fundamental frequency estimation using the alternate comb",
   "original": "i07_2773",
   "page_count": 4,
   "order": 465,
   "p1": "2773",
   "pn": "2776",
   "abstract": [
    "Reliable estimation of speech fundamental frequency is crucial in the perspective of speech separation. We show that the gross errors on F0 measurement occur for particular configurations of the periodic structure to be estimated and the other periodic structure used to achieve this estimation. The error families are characterized by a set of two positive integers. The Alternate Comb method uses this knowledge to cancel most of the erroneous solutions. Its efficiency is assessed by an evaluation using a classical pitch database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-465"
  },
  "rosenberg07_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Detecting pitch accent using pitch-corrected energy-based predictors",
   "original": "i07_2777",
   "page_count": 4,
   "order": 466,
   "p1": "2777",
   "pn": "2780",
   "abstract": [
    "Previous work has shown that the energy components of frequency subbands with a variety of frequencies and bandwidths predict pitch accent with various degrees of accuracy, and produce correct predictions for distinct subsets of data points. In this paper, we describe a series of experiments exploring techniques to leverage the predictive power of these energy components by including pitch and duration features - other known correlates to pitch accent. We perform these experiments on Standard American English read, spontaneous and broadcast news speech, each corpus containing at least four speakers. Using an approach by which we correct energy-based predictions using pitch and duration information prior to using a majority voting classifier, we were able to detect pitch accent in read, spontaneous and broadcast news speech at 84.0%, 88.3% and 88.5% accuracy, respectively. Human performance at pitch accent detection is generally taken to be between 85% and 90%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-466"
  },
  "chatterjee07_interspeech": {
   "authors": [
    [
     "Saikat",
     "Chatterjee"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Normalized two stage SVQ for minimum complexity wide-band LSF quantization",
   "original": "i07_1657",
   "page_count": 4,
   "order": 467,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "We develop a two stage split vector quantization method with optimum bit allocation, for achieving minimum computational complexity. This also results in much lower memory requirement than the recently proposed switched split vector quantization method. To improve the rate-distortion performance further, a region specific normalization is introduced, which results in 1 bit/vector improvement over the typical two stage split vector quantizer, for wide-band LSF quantization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-467"
  },
  "zhang07c_interspeech": {
   "authors": [
    [
     "Peng",
     "Zhang"
    ],
    [
     "Chang-chun",
     "Bao"
    ]
   ],
   "title": "A novel 2kb/s waveform interpolation speech coder based on non-negative matrix factorization",
   "original": "i07_1661",
   "page_count": 4,
   "order": 468,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "In this paper, a 2kb/s Waveform Interpolation speech coder is proposed based on non-negative matrix factorization (NMF). In characteristic waveforms (CWs) decomposition, band-partitioning initialization constraints were set to basis vectors before NMF was carried out. This decomposition method only requires speech signal from the current frame, and can yield high decomposition quality with low computational complexity. Besides, the high dimensional CWs matrix can be expressed by the low dimensional coding matrix, and this has facilitated the CWs quantization. The listening test shows that the proposed 2kb/s NMF-WI coder can give smooth speech with quality close to 2.4kb/s SVD-based WI coder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-468"
  },
  "ismail07_interspeech": {
   "authors": [
    [
     "Ahmed",
     "Ismail"
    ],
    [
     "Yasser",
     "Dakroury"
    ],
    [
     "Hazem",
     "Abbas"
    ]
   ],
   "title": "A novel energy distribution comparison approach for robust speech spectrum vector quantization",
   "original": "i07_1665",
   "page_count": 4,
   "order": 469,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "Vector Quantization (VQ) has been extensively used in speech vocoders. The training process normally requires a very large training-set. This paper introduces a novel energy distribution comparison distortion measure for the high-band speech spectrum that enables the vector quantizer to operate given a relatively small training-set. This measure has been used in the construction of a segmental vocoder using the pitch period as segments. A description of the proposed approach, the Energy-Mass distortion measure, is given and compared to the use of MFCC as a distortion measure showing the ability of the proposed approach to better represent the speech formants, when operating under the small training-set constraint. Finally, the performance of the new Energy-Mass is evaluated using the Spectral Distortion (SD). Speech quality perceived by the receiver is evaluated using the recently standardized objective quality measure PESQ, where an improvement of 0.3 PESQ score was obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-469"
  },
  "ismail07b_interspeech": {
   "authors": [
    [
     "Ahmed",
     "Ismail"
    ],
    [
     "Yasser",
     "Dakroury"
    ],
    [
     "Hazem",
     "Abbas"
    ]
   ],
   "title": "Novel low-band phase representation for low bit-rate speech coding",
   "original": "i07_1669",
   "page_count": 4,
   "order": 470,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "Vector Quantization (VQ) has been extensively used in speech vocoders. Phase information is often ignored or coarsely represented in parametric coders because of the difficulties facing phase quantization. This paper introduces a novel distortion measure for the low-band speech signal that takes phase information into consideration, with no increase in the bit-rate. This measure has been used in the construction of a segmental vocoder, which is using the pitch period as segments. A description of the proposed Time-Domain Phase- Aware (TDPA) distortion measure is given and compared to the use of the MFCC as a distortion measure showing the effect of the phase information represented in the TDPA model on improving the inter-frame correlation of the synthesized speech. Finally, the performance of the TDPA is evaluated using the Segmental Signal-to-Noise Ratio (SNR), and Spectral Distortion (SD). Speech quality is evaluated using the recently standardized objective quality measure PESQ.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-470"
  },
  "wu07e_interspeech": {
   "authors": [
    [
     "Chun-Feng",
     "Wu"
    ],
    [
     "Cheng-Lung",
     "Lee"
    ],
    [
     "Wen-Whei",
     "Chang"
    ]
   ],
   "title": "Perceptual-based playout mechanisms for multi-stream voice over IP networks",
   "original": "i07_1673",
   "page_count": 4,
   "order": 471,
   "p1": "1673",
   "pn": "1676",
   "abstract": [
    "Packet loss and delay are two essential problems to real-time voice transmission over best-effort packet networks. In the proposed system, multiple descriptions of the speech are transmitted to take advantage of largely uncorrelated delay and loss characteristics on different network paths. Adaptive playout scheduling of multiple voice streams is formulated as an optimization problem leading to a better delay-loss tradeoff. Also proposed is a perceptually motivated optimization criterion based on a simplified version of the ITU-T E-model. Experimental results show that the proposed playout buffer algorithm improves the delay-loss tradeoff as well as speech reconstruction quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-471"
  },
  "zopf07_interspeech": {
   "authors": [
    [
     "Robert",
     "Zopf"
    ],
    [
     "Jes",
     "Thyssen"
    ],
    [
     "Juin-Hwey",
     "Chen"
    ]
   ],
   "title": "Time-warping and re-phasing in packet loss concealment",
   "original": "i07_1677",
   "page_count": 4,
   "order": 472,
   "p1": "1677",
   "pn": "1680",
   "abstract": [
    "This paper proposes two techniques to improve packet loss concealment (PLC). In the first technique, time-warping is used to stretch or shrink the time axis of the signal received in the first good frame after frame loss to align it with the extrapolated signal used to conceal the bad frame. This aligning procedure avoids any destructive interference that might otherwise occur when the two signals are out of phase and overlap-added. The second technique may be applied to speech codecs with memory, particularly suited for backward-adaptive systems. In this technique, called \"re-phasing\", the internal states of the codec are phase-aligned with the signal in the first good frame. Both techniques are part of the ITU-T G.722 Appendix III packet loss concealment standard and provide significant quality improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-472"
  },
  "agiomyrgiannakis07_interspeech": {
   "authors": [
    [
     "Yannis",
     "Agiomyrgiannakis"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "The harmonic model codec (HMC) framework for voIP",
   "original": "i07_1681",
   "page_count": 4,
   "order": 473,
   "p1": "1681",
   "pn": "1684",
   "abstract": [
    "A framework for joint source/channel coding of speech is presented. It is based on a harmonic representation of the speech signal and facilitates efficient quantization of harmonic amplitudes and phases both in a single description and a multiple description setting. Furthermore, it combines high-quality packet loss concealment with efficient source coding and multiple description coding. Two proof-of-concept codecs are presented; a single description codec that is equivalent to iLBC in terms of bitrate and quality but more robust in conditions of increased packet losses and a multiple description codec that is capable of accepting loss rates up to 40% for a DCR score of 3.8.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-473"
  },
  "agiomyrgiannakis07b_interspeech": {
   "authors": [
    [
     "Yannis",
     "Agiomyrgiannakis"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Bit-erasure channel decoding for GMM-based multiple description coding",
   "original": "i07_1685",
   "page_count": 4,
   "order": 474,
   "p1": "1685",
   "pn": "1688",
   "abstract": [
    "Multiple Description Coding (MDC) is a plausible way to use the diversity of packet networks to increase the robustness of the transmission to packet losses. The redundancy that is introduced via MDC can also be used to increase the robustness of the transmission to bit-errors. This paper presents a novel decoding method for GMM (Gaussian Mixture Model)-based MDC in the presence of detected bit-errors. Particularly for speech transmission over bit-erasure channels, is shown that the proposed method considerably improves the quality of the received speech spectral envelopes when one side-description is damaged. In highly correlated descriptions, for example, single and double bit-errors can almost be corrected.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-474"
  },
  "yuan07_interspeech": {
   "authors": [
    [
     "Hua",
     "Yuan"
    ],
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Wai-Yip",
     "Chan"
    ]
   ],
   "title": "Degradation-classification assisted single-ended quality measurement of speech",
   "original": "i07_1689",
   "page_count": 4,
   "order": 475,
   "p1": "1689",
   "pn": "1692",
   "abstract": [
    "We propose an algorithm to classify speech degradations at network endpoints and to estimate the speech quality based on the degradation classification decision. Perceptual features from degraded speech signals are used to form statistical reference models of different degradation classes. Consistency measures, calculated between degraded speech signals and the reference models, are used to train a degradation classifier and mean opinion score (MOS) mappings. The quality of a received speech signal is estimated based on its degradation class and the MOS mapping associated with the class. Experimental results show that the proposed algorithm achieves high classification accuracy, and degradation classification improves the accuracy of the quality estimate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-475"
  },
  "raake07_interspeech": {
   "authors": [
    [
     "Alexander",
     "Raake"
    ],
    [
     "Sascha",
     "Spors"
    ],
    [
     "Jens",
     "Ahrens"
    ],
    [
     "Jitendra",
     "Ajmera"
    ]
   ],
   "title": "Concept and evaluation of a downward-compatible system for spatial teleconferencing using automatic speaker clustering",
   "original": "i07_1693",
   "page_count": 4,
   "order": 476,
   "p1": "1693",
   "pn": "1696",
   "abstract": [
    "In multi-party teleconferencing, the transport of separate speech streams to a particular user and the subsequent spatial rendering of the different streams enables a more efficient communication. A simple means of spatial presentation at client side is that of binaural rendering and headphone presentation. For downward-compatibility, e.g. when the transport mechanism does not support multiple parallel downlink streams, a system is proposed that combines an automatic speaker classification mechanism with a spatial rendering of the segregated streams. The combined system aims at a better separability of the speakers than conventional systems. The paper details the two basic components, namely automatic speaker classification, and binaural rendering. Based on a first evaluation of the approach, a proof of concept is provided, and directions for further improvement are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-476"
  },
  "lee07e_interspeech": {
   "authors": [
    [
     "Min-Ki",
     "Lee"
    ],
    [
     "Kyung-Tae",
     "Kim"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Dae Hee",
     "Youn"
    ]
   ],
   "title": "Speech quality estimation using packet loss effects in CELP-type speech coders",
   "original": "i07_1697",
   "page_count": 4,
   "order": 477,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "This paper proposes an objective quality assessment method for voice communication systems using packet loss information. Based on the fact that the effect of packet loss to perceptual quality varies depending on the signal characteristics of lost packets, different weighting factors are applied to predicting overall quality. Considering the key paradigm of low bit rate speech coders such as parameter prediction in consecutive frames, we also include the effect of neighborhood frames. To verify the performance of the proposed algorithm, we apply it into two well-known speech codecs, G.729 and AMR-NB. Simulation results with a large speech database verify the superiority of the proposed algorithm. The normalized correlation between degraded listening quality (LQ) scale of the PESQ and the proposed method is 0.9121 at G.729A and 0.9289 at AMR codec based on SMV classification of input signal and 0.8505 at G.729A and 0.8586 at AMR codec based on SMV classification of decoded signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-477"
  },
  "oshikiri07_interspeech": {
   "authors": [
    [
     "Masahiro",
     "Oshikiri"
    ],
    [
     "Hiroyuki",
     "Ehara"
    ],
    [
     "Toshiyuki",
     "Morii"
    ],
    [
     "Tomofumi",
     "Yamanashi"
    ],
    [
     "Kaoru",
     "Satoh"
    ],
    [
     "Koji",
     "Yoshida"
    ]
   ],
   "title": "An 8-32 kbit/s scalable wideband coder extended with MDCT-based bandwidth extension on top of a 6.8 kbit/s narrowband CELP coder",
   "original": "i07_1701",
   "page_count": 4,
   "order": 478,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "In this paper, we present a 6.8-32 kbit/s scalable speech and audio coder using a modified-discrete-cosine-transform (MDCT)-based bandwidth extension on top of a 6.8 kbit/s code-excited-linear-prediction (CELP) coder. The proposed coder comprises a 6.8 kbit/s narrowband CELP as its core-layer and eight enhancement layers with the bitrates of 0.8, 1.2, 3.2, or 4.0 kbit/s. After encoding of a narrowband signal by the core-layer, the first enhancement layer extends the bandwidth of a narrowband decoded signal, and the other enhancement layers increase the fidelity of an extended wideband signal or robustness against frame erasure conditions. Subjective evaluation test results demonstrate that the proposed coder outperforms G.729.1 for music signals at 16 and 24 kbit/s in particular with competitive or even better performance in other conditions like clean speech, background noise, and frame erasure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-478"
  },
  "wielgat07_interspeech": {
   "authors": [
    [
     "Robert",
     "Wielgat"
    ],
    [
     "Tomasz P.",
     "Zieliński"
    ],
    [
     "Paweł",
     "Świętojański"
    ],
    [
     "Piotr",
     "Żołądź"
    ],
    [
     "Daniel",
     "Król"
    ],
    [
     "Tomasz",
     "Woźniak"
    ],
    [
     "Stanisław",
     "Grabias"
    ]
   ],
   "title": "Comparison of HMM and DTW methods in automatic recognition of pathological phoneme pronunciation",
   "original": "i07_1705",
   "page_count": 4,
   "order": 479,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "In the paper recently proposed Human Factor Cepstral Coefficients (HFCC) are used to automatic recognition of pathological phoneme pronunciation in speech of impaired children and efficiency of this approach is compared to application of the standard Mel-Frequency Cepstral Coefficients (MFCC) as a feature vector. Both dynamic time warping (DTW), working on whole words or embedded phoneme patterns, and hidden Markov models (HMM) are used as classifiers in the presented research. Obtained results demonstrate superiority of combining HFCC features and modified phoneme-based DTW classifier.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-479"
  },
  "yu07d_interspeech": {
   "authors": [
    [
     "K.",
     "Yu"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Unsupervised training with directed manual transcription for recognising Mandarin broadcast audio",
   "original": "i07_1709",
   "page_count": 4,
   "order": 480,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "The performance of unsupervised discriminative training has been found to be highly dependent on the accuracy of the initial automatic transcription. This paper examines a strategy where a relatively small amount of poorly recognised data are manually transcribed to supplement the automatically transcribed data. Experiments were carried out on a Mandarin broadcast transcription task using both Broadcast News (BN) and Broadcast Conversation (BC) data. A range of experimental conditions are compared for both maximum likelihood and discriminative training using directed manual transcription. For BC data, using fully unsupervised discriminative training, only 17% of the reduction in character error rate (CER) from supervised training is obtained. By automatically selecting 18% of the data for manual transcription yields 50% of the CER gain from supervised training. The directed approach to selecting data outperforms the use of a random set of data for manual transcription.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-480"
  },
  "wu07f_interspeech": {
   "authors": [
    [
     "Hao",
     "Wu"
    ],
    [
     "Xihong",
     "Wu"
    ]
   ],
   "title": "Context dependent syllable acoustic model for continuous Chinese speech recognition",
   "original": "i07_1713",
   "page_count": 4,
   "order": 481,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "The choice of basic modeling unit in building acoustic model for a continuous Mandarin speech recognition task is a very important issue [1]. Unlike traditional phoneme or Initial/Finals (IFs) units based acoustic modeling methods, which usually suffer from the limitations of less accuracy in modeling intra-syllable variations and long scale temporal dependencies, in this paper, a practicable syllable based approach is presented. In contrast with IFs, syllable can implicitly model the intra-syllable variations in good accuracy. Also, by carefully choosing context modeling schemes and parameter tying methods, syllable based acoustic model can capture longer temporal variations while keeping the complexity of model well controlled. Meanwhile, considering the data unbalanced problem, multiple sized unit model based approaches are also implemented in this research. The experiment result shows the acoustic model based on the presented syllable based approach is effective in improving the performance of the Chinese continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-481"
  },
  "oikonomidis07_interspeech": {
   "authors": [
    [
     "Dimitris",
     "Oikonomidis"
    ],
    [
     "Vassilis",
     "Diakoloukas"
    ],
    [
     "Vassilis",
     "Digalakis"
    ]
   ],
   "title": "A sub-optimal viterbi-like search for linear dynamic models classification",
   "original": "i07_1717",
   "page_count": 4,
   "order": 482,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "This paper describes a Viterbi-like decoding algorithm applied on segment-models based on linear dynamic systems (LDMs). LDMs are a promising acoustic modeling scheme which can alleviate several of the limitations of the popular Hidden Markov Models (HMMs). There are several implementations of LDMs that can be found in the literature. For our decoding experiments we consider general identifiable forms of LDMs which allow increased state space dimensionality and relax most of the constraints found in other approaches. Results on the AURORA2 database show that our decoding scheme significantly outperforms standard HMMs, particularly under significant noise levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-482"
  },
  "heigold07_interspeech": {
   "authors": [
    [
     "Georg",
     "Heigold"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "On the equivalence of Gaussian HMM and Gaussian HMM-like hidden conditional random fields",
   "original": "i07_1721",
   "page_count": 4,
   "order": 483,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "In this work we show that Gaussian HMMs (GHMMs) are equivalent to GHMM-like Hidden Conditional Random Fields (HCRFs). Hence, improvements of HCRFs over GHMMs found in literature are not due to a refined acoustic modeling but rather come from the more robust formulation of the underlying optimization problem or spurious local optima. Conventional GHMMs are usually estimated with a criterion on segment level whereas hybrid approaches are based on a formulation of the criterion on frame level. In contrast to CRFs, these approaches do not provide scores or do not support more than two classes in a natural way. In this work we analyze these two classes of criteria and propose a refined frame based criterion, which is shown to be an approximation of the associated criterion on segment level. Experimental results concerning these issues are reported for the German digit string recognition task Sietill and the large vocabulary English European Parliament Plenary Sessions (EPPS) task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-483"
  },
  "scanzio07_interspeech": {
   "authors": [
    [
     "Stefano",
     "Scanzio"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ]
   ],
   "title": "Speeding-up neural network training using sentence and frame selection",
   "original": "i07_1725",
   "page_count": 4,
   "order": 484,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "Training Artificial Neural Networks (ANNs) with large amounts of speech data is a time intensive task due to the intrinsically sequential nature of the back-propagation algorithm.\n",
    "This paper presents an approach for training ANNs using sentence and frame selection. The goal is to speed-up the training process, and to balance the phonetic coverage of the selected frames, trying to mitigate the classification problems related to the prior probabilities of the individual phonetic classes.\n",
    "These techniques, together with a three-step training approach and software optimizations, reduced by an order of magnitude the training time of our models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-484"
  },
  "liu07c_interspeech": {
   "authors": [
    [
     "Linquan",
     "Liu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Makoto",
     "Akabane"
    ],
    [
     "Ruxin",
     "Chen"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Using a small development set to build a robust dialectal Chinese speech recognizer",
   "original": "i07_1729",
   "page_count": 4,
   "order": 485,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "To make full use of a small development data set to build a robust dialectal Chinese speech recognizer from a standard Chinese speech recognizer (based on Chinese Initial/Final, IF), a novel, simple but effective acoustic modeling method, named state-dependent phoneme-based model merging (SDPBMM), is proposed and evaluated, where a shared-state of standard tri-IF is merged with a state of dialectal mono-IF in terms of pronunciation variation modeling. Specifically, in order to deal with phonetic-level pronunciation variations in SDPBMM, distance-based pronunciation modeling is proposed based on a small dialectal Chinese data set. With a 40-minute Shanghai-dialectal Chinese data set, SDPBMM can achieve a significant syllable error rate (SER) reduction of 14.3% for dialectal Chinese with almost no performance degradation for standard Chinese. Experimentally, SDPBMM can also outperform the maximum likelihood linear regression (MLLR) adaptation and the pooled retraining methods with relative SER reductions by 2.8% and 10.6%, respectively. If SDPBMM is combined with the MLLR adaptation, another relative SER reduction of 3.3% can be further achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-485"
  },
  "molina07_interspeech": {
   "authors": [
    [
     "Carlos",
     "Molina"
    ],
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Fernando",
     "Huenupán"
    ],
    [
     "Claudio",
     "Garreton"
    ]
   ],
   "title": "Unsupervised re-scoring of observation probability in viterbi based on reinforcement learning by using confidence measure and HMM neighborhood",
   "original": "i07_1733",
   "page_count": 4,
   "order": 486,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "This paper proposes a new paradigm to compensate for mismatch condition in speech recognition. A two-step Viterbi decoding based on reinforcement learning is described. The idea is to strength or weaken HMM's by using Bayes-based confidence measure ( BBCM) and distances between models. If HMM's in the N-best list show a low BBCM, the second Viterbi decoding will prioritize the search on neighboring models according to their distances to the N-best HMM's. As shown here, a reduction of 6% in WER is achieved in a task which results difficult for standard MAP and MLLR adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-486"
  },
  "lin07c_interspeech": {
   "authors": [
    [
     "Shiuan-Sung",
     "Lin"
    ],
    [
     "François",
     "Yvon"
    ]
   ],
   "title": "Optimization on decoding graphs by discriminative training",
   "original": "i07_1737",
   "page_count": 4,
   "order": 487,
   "p1": "1737",
   "pn": "1740",
   "abstract": [
    "The three main knowledge sources used in the automatic speech recognition (ASR), namely the acoustic models, a dictionary and a language model, are usually designed and optimized in isolation. Our previous work [1] proposed a methodology for jointly tuning these parameters, based on the integration of the resources as a finite-state graph, whose transition weights are trained discriminatively. This paper extends the training framework to a large vocabulary task, the automatic transcription of French broadcast news. We propose several fast decoding techniques to make the training practical. Experiments show that a reduction of 1% absolute of word error rate (WER) can be obtained. We conclude the paper with an appraisal of the potential of this approach on large vocabulary ASR tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-487"
  },
  "huet07_interspeech": {
   "authors": [
    [
     "Stéphane",
     "Huet"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Pascale",
     "Sébillot"
    ]
   ],
   "title": "Morphosyntactic processing of n-best lists for improved recognition and confidence measure computation",
   "original": "i07_1741",
   "page_count": 4,
   "order": 488,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "We study the use of morphosyntactic knowledge to process N-best lists. We propose a new score function that combines the parts of speech (POS), language model, and acoustic scores at the sentence level. Experimental results, obtained for French broadcast news transcription, show a significant improvement of the word error rate with various decoding criteria commonly used in speech recognition. Interestingly, we observed more grammatical transcriptions, which translates into a better sentence error rate. Finally, we show that POS knowledge also improves posterior based confidence measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-488"
  },
  "li07i_interspeech": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Juan M.",
     "Huerta"
    ]
   ],
   "title": "How predictable is ASR confidence in dialog applications?",
   "original": "i07_1745",
   "page_count": 4,
   "order": 489,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "ASR confidence is a metric that reflects, to a large extent, the conditions under which a recognition task is being carried out as well as the reliability of the result. Because of this, ASR confidence constitutes a potentially useful feature in frameworks that attempt to asses the state of a dialog. In this paper we evaluate the predictability of ASR confidence based on knowledge of previously observed context-dependent confidences. We find out that the contextual confidence can be predicted with a standard prediction deviation less than 10% of the dynamic range of the confidence score, which represents a almost 40% relative reduction in standard deviation measure to a static confidence assumption baseline. Because our prediction is based on context, this predictability can be leveraged to produce an estimate of the expected average confidence until the end of a call based on the context path expected to be traversed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-489"
  },
  "allauzen07_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Allauzen"
    ]
   ],
   "title": "Error detection in confusion network",
   "original": "i07_1749",
   "page_count": 4,
   "order": 490,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "In this article, error detection for broadcast news transcription system is addressed in a post-processing stage. We investigate a logistic regression model based on features extracted from confusion networks. This model aims to estimate a confidence score for each confusion set and detect errors. Different kind of knowledge sources are explored such as the confusion set solely, statistical language model, and lexical properties. Impact of the different features are assessed and show the importance of those extracted from the confusion network solely. To enrich our modeling with information about the neighborhood, features of adjacent confusion sets are also added to the vector of features. Finally, a distinct processing of confusion sets is also explored depending on the value of their best posterior probability. To be compared with the standard ASR output, our best system yields to a significant improvement of the classification error rate from 17.2% to 12.3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-490"
  },
  "oba07_interspeech": {
   "authors": [
    [
     "Takanobu",
     "Oba"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "An approach to efficient generation of high-accuracy and compact error-corrective models for speech recognition",
   "original": "i07_1753",
   "page_count": 4,
   "order": 491,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "This paper focuses on an error-corrective method through reranking of hypotheses in speech recognition. Some recent work investigated corrective models that can be used to rescore hypotheses so that a hypothesis with a smaller error rate has a higher score. Discriminative training such as perceptron algorithm can be used to estimate such corrective models. In discriminative training, how to choose competitors is an important factor because the model parameters are estimated from the difference between the reference (or oracle hypothesis) and the competitors. In this paper, we investigate the way how to choose effective competitors for training corrective models. Particularly we focus on word error rate (WER) of each hypothesis and show that a higher WER hypothesis rather than the best-scored one works effectively as a competitor. In addition, we show that using only one competitor with the highest WER in an N-best list is very effective to generate accurate and compact corrective models in experiments with the Corpus of Spontaneous Japanese (CSJ).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-491"
  },
  "ketabdar07_interspeech": {
   "authors": [
    [
     "Hamed",
     "Ketabdar"
    ],
    [
     "Mirko",
     "Hannemann"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Detection of out-of-vocabulary words in posterior based ASR",
   "original": "i07_1757",
   "page_count": 4,
   "order": 492,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "Over the years, sophisticated techniques for utilizing the prior knowledge in the form of text-derived language model and in pronunciation lexicon evolved. However, their use has an undesirable effect: unexpected lexical items (words) in the phrase are replaced by acoustically acceptable in-vocabulary items [1]. This is the major source of error since the replacement often introduces additional errors [2, 3]. Improving the machine ability to handle these unexpected words would considerably increase the utility of speech recognition technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-492"
  },
  "braga07_interspeech": {
   "authors": [
    [
     "Daniela",
     "Braga"
    ],
    [
     "Luís",
     "Coelho"
    ],
    [
     "Fernando Gil V.",
     "Resende"
    ]
   ],
   "title": "Homograph ambiguity resolution in front-end design for portuguese TTS systems",
   "original": "i07_1761",
   "page_count": 4,
   "order": 493,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "In this paper, a module for homograph disambiguation in Portuguese Text-to-Speech (TTS) is proposed. This module works with a part-of-speech (POS) parser, used to disambiguate homographs that belong to different parts-of-speech, and a semantic analyzer, used to disambiguate homographs which belong to the same part-of-speech. The proposed algorithms are meant to solve a significant part of homograph ambiguity in European Portuguese (EP) (106 homograph pairs so far). This system is ready to be integrated in a Letter-to-Sound (LTS) converter. The algorithms were trained and tested with different corpora. The obtained experimental results gave rise to 97.8% of accuracy rate. This methodology is also valid for Brazilian Portuguese (BP), since 95 homographs pairs are exactly the same as in EP. A comparison with a probabilistic approach was also done and results were discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-493"
  },
  "choueiter07_interspeech": {
   "authors": [
    [
     "Ghinwa F.",
     "Choueiter"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "New word acquisition using subword modeling",
   "original": "i07_1765",
   "page_count": 4,
   "order": 494,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "In this paper, we use subword modeling to learn the pronunciations and spellings of new words. The subwords are generated with a context-free grammar, and are intermediate units between phonemes and syllables. We first evaluate the effectiveness of the subword model in automatically generating the spelling and pronunciation of new words. Then the subword model is embedded in a multi-stage recognizer which consists of word, subword, and letter recognizers. In a preliminary set of experiments, the hybrid system outperforms a large-vocabulary isolated word recognizer. The subword model is also used to improve the performance of the letter recognizer by generating a spelling cohort which is used to train a small letter n-gram. The small letter n-gram has a reduced perplexity compared to a much larger n-gram, and can be used by the letter recognizer for the spoken spelling mode. This could translate to an improved letter error rate in future letter recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-494"
  },
  "thomas07_interspeech": {
   "authors": [
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "Language identification of person names using CF-IOF based weighing function",
   "original": "i07_1769",
   "page_count": 4,
   "order": 495,
   "p1": "1769",
   "pn": "1772",
   "abstract": [
    "Information about the language of origin helps in generating pronunciation for foreign words, specially person names, in a text-to-speech synthesis system. It can be used to apply language specific letter-to-sound (LTS) rules to these words during synthesis. In this paper, we propose a novel approach for using substrings of a person name (called letter N-grams) to identify the language of its origin. We use a weight for the letter N-grams that is motivated by the techniques used in text document classification, different from the usual N-gram probabilities used in earlier approaches. We also propose a tree based approach to select the letter N-grams of different lengths for language identification. Several experiments have been conducted to evaluate the performance of the proposed approach and compare it with those of the earlier proposed approaches based on N-gram probabilities. We show an improvement in classification results over the earlier approaches without using any language specific rules.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-495"
  },
  "heuvel07_interspeech": {
   "authors": [
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Nanneke",
     "Konings"
    ]
   ],
   "title": "G2p conversion of names: what can we do (better)?",
   "original": "i07_1773",
   "page_count": 4,
   "order": 496,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "In this contribution it is shown that a good approach for the grapheme-to-phoneme conversion of proper names (e.g. person names, toponyms, etc), is to use a cascade of a general purpose grapheme-to-phoneme (G2P) converter and a special purpose phoneme-to-phoneme (P2P) converter. The G2P produces an initial transcription that is then transformed by the P2P. The latter is automatically trained on reference transcriptions of names belonging to the envisaged name category (e.g. toponyms). The P2P learning process is conceived in such a way that it can take account of high order determinants of pronunciation, such as specific syllables, name prefixes and name suffixes. The proposed methodology was successfully tested on person names and toponyms, but we believe that it will also offer substantial reductions of the cost for building pronunciation lexicons of other name categories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-496"
  },
  "thangthai07_interspeech": {
   "authors": [
    [
     "Ausdang",
     "Thangthai"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Anocha",
     "Ragchatjaroen"
    ],
    [
     "Sittipong",
     "Saychum"
    ]
   ],
   "title": "A learning method for Thai phonetization of English words",
   "original": "i07_1777",
   "page_count": 4,
   "order": 497,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "This article tackles the problem of transcribing English words using Thai phonological system. The problem exists in Thai, where modern writing often composes of English orthography, and transcribing using English phonology results unnatural. The proposed model is totally data-driven, starting by automatic grapheme-phoneme alignment, modeling transduction rules and predicting Thai syllabic-tones using learning machines. Three specific issues are addresses. The first one is involving English transcription information in transduction once the input English word appears in an English pronunciation dictionary. Second, more precise transduction rules can be obtained by a constraint of Thai syllable-structure. Lastly, the ambiguity in assigning tones to Thai pronunciations of English words is alleviated by introducing a learning machine. The proposed model achieves acceptable results in both objective and text-to-speech synthesis subjective tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-497"
  },
  "werner07_interspeech": {
   "authors": [
    [
     "Steffen",
     "Werner"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Spontaneous speech synthesis by pronunciation variant selection - a comparison to natural speech",
   "original": "i07_1781",
   "page_count": 4,
   "order": 498,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "In order to make synthetic speech more spontaneous we have introduced various duration control methods, which are based on word language model probability and on pronunciation variant selection algorithms. In former publications we considered the standalone algorithms [1]. In this paper we combine the change of the speaking rate according to the language model probability with an indirect change of the speaking rate. The latter is achieved by a pronunciation variant selection algorithm based on a variant sequence model.\n",
    "To evaluate the quality of the different approaches and to compare them to the canonical synthesis (as the state-of-the-art system), we performed various absolute category rating listening tests. In addition, we conducted the same test with natural speech to provide a further evaluation criterion. The results achieved in this paper show that a suitable sequence of pronunciation variants achieves a significant lower listening effort and a higher mean opinion score (MOS) for both synthetic and natural speech samples compared to the canonical pronunciation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-498"
  },
  "tsourakis07_interspeech": {
   "authors": [
    [
     "Nikos",
     "Tsourakis"
    ],
    [
     "Vassilis",
     "Digalakis"
    ]
   ],
   "title": "A generic methodology of converting transliterated text to phonetic strings case study: greeklish",
   "original": "i07_1785",
   "page_count": 4,
   "order": 499,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "In this work, we present a generic methodology for converting transliterated text (native language written with a non-native alphabet) to phonetic sequences. The goal is to create the same phonetic result that would be produced if a native speaker uttered the original text in native alphabet. In our work, we implemented the specific methodology as a front-end to a Text-to-Speech (TTS) server. To evaluate our algorithms we considered the case that corresponds to the Greek language, called Greeklish.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-499"
  },
  "singh07_interspeech": {
   "authors": [
    [
     "Rita",
     "Singh"
    ],
    [
     "Evandro B.",
     "Gouvêa"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Probabilistic deduction of symbol mappings for extension of lexicons",
   "original": "i07_1789",
   "page_count": 4,
   "order": 500,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "This paper proposes a statistical mapping-based technique for guessing pronunciations of novel words from their spellings. The technique is based on the automatic determination and utilization of unidirectional mappings between n-tuples of characters and n-tuples of phonemes, and may be viewed as a statistical extension of analogy-based pronunciation guessing algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-500"
  },
  "astrov07_interspeech": {
   "authors": [
    [
     "Sergey",
     "Astrov"
    ],
    [
     "Joachim",
     "Hofer"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "Use of syllable center detection for improved duration modeling in Chinese Mandarin connected digits recognition",
   "original": "i07_1793",
   "page_count": 4,
   "order": 501,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "&# 9;This paper describes practical approaches for improving Mandarin digit recognition accuracy, especially in cars. We consider syllable and subword unit durations as additional source of information. The explored approach was realized in two stages. First, the system performs standard speech recognition using acoustic spectral features. As a result, an n-best list of hypotheses is generated. In the second stage the hypothesis probabilities are re-estimated using duration models, thus, the hypotheses are reordered such that the correct ones are pushed to the top of the n-best list. In such a way the word error rate (WER) is reduced. We explore state of the art approach of duration n-grams. In order to eliminate the influence of speech rate variations, the durations are normalized to a relative speech rate, a 10% relative reduction of WER was achieved. A novel approach led to 13.3% WER reduction: the durations were normalized to a syllable rate obtained from the syllable center detector.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-501"
  },
  "pellegrini07_interspeech": {
   "authors": [
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Using phonetic features in unsupervised word decompounding for ASR with application to a less-represented language",
   "original": "i07_1797",
   "page_count": 4,
   "order": 502,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "In this paper, a data-driven word decompounding algorithm is described and applied to a broadcast news corpus in Amharic. The baseline algorithm has been enhanced in order to address the problem of increased phonetic confusability arising from word decompounding by incorporating phonetic properties and some constraints on recognition units derived from prior forced alignment experiments. Speech recognition experiments have been carried out to validate the approach. Out of vocabulary (OOV) words rates can be reduced by 30% to 40% and an absolute Word Error Rate (WER) reduction of 0.4% has been achieved. The algorithm is relatively language independent and requires minimal adaptation to be applied to other languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-502"
  },
  "qiang07_interspeech": {
   "authors": [
    [
     "Sheng",
     "Qiang"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Congfu",
     "Xu"
    ]
   ],
   "title": "Robust F0 modeling for Mandarin speech recognition in noise",
   "original": "i07_1801",
   "page_count": 4,
   "order": 503,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "The F0 contour plays an important role in recognizing spoken tonal languages like Mandarin Chinese. However, the discontinuity of F0 between voiced and unvoiced transition has traditionally been a bottleneck in creating a succinct statistical tone model for automatic speech recognition applications. By applying successfully the Multi-Space Distribution (MSD) to tone modeling, we recently reported a relative 24% reduction of tonal syllable errors on a Mandarin speech database. In this paper, we test MSD further in a noisy, continuous Mandarin digit recognition task, where eight types of noises are added to clean speech signals at five SNRs. The experimental results show that our MSD-based digit models can significantly improve the recognition performance in noise over a baseline system. Relative digit error rate reductions of 19.1% and 15.0% are obtained for noises seen and unseen in the training data, respectively. The improvements are also better than other reference systems where F0 information is incorporated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-503"
  },
  "seppi07_interspeech": {
   "authors": [
    [
     "Dino",
     "Seppi"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Georg",
     "Stemmer"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "Word duration modeling for word graph rescoring in LVCSR",
   "original": "i07_1805",
   "page_count": 4,
   "order": 504,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "A well-known unfavorable property of HMMs in speech recognition is their inappropriate representation of phone and word durations. This paper describes an approach to resolve this limitation by integrating explicit word duration models into an HMM-based speech recognizer. Word durations are represented by log-normal densities using a back-off strategy that approximates durations of words that have been observed seldom by a combination of the statistics of suitable sub-word units. Furthermore, two different normalization procedures are compared which reduce the influence of the implicit HMM duration distribution resulting from the state-to-state transition probabilities. Experiments on European parliamentary speeches in English and Spanish language show that the proposed approaches are effective and lead to small, but consistent reductions in the word error rate for large-vocabulary speech recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-504"
  },
  "tamburini07_interspeech": {
   "authors": [
    [
     "Fabio",
     "Tamburini"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "On automatic prominence detection for German",
   "original": "i07_1809",
   "page_count": 4,
   "order": 505,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "Perceptual prominence is an important indicator of a word's and syllable's lexical, syntactic, semantic and pragmatic status in a discourse. Its automatic annotation would be a valuable enrichment of large databases used in unit selection speech synthesis and speech recognition. While much research has been carried out on the interaction between prominence and acoustic factors, little progress has been made in its automatic annotation. Previous approaches to German relied on linguistic features in prominence detection, but a purely acoustic method would be advantageous. We applied an algorithm to German data that had been previously used for English and Italian. Both the algorithm and the data annotation encode prominence as a continuous rather than a categorical parameter. First results are encouraging, but again show that prominence perception relies on linguistic expectancies as well as acoustic patterns. Also, our results further strengthen the view that force accents are a more reliable cue to prominence than pitch accents in German.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-505"
  },
  "ananthakrishnan07_interspeech": {
   "authors": [
    [
     "Sankaranarayanan",
     "Ananthakrishnan"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Prosody-enriched lattices for improved syllable recognition",
   "original": "i07_1813",
   "page_count": 4,
   "order": 506,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "Automatic recognition of syllables is useful for many spoken language applications such as speech recognition and spoken document retrieval. Short-term spectral properties (such as mel-frequency cepstral coefficients, or MFCCs) are usually the features of choice for such systems, which typically ignore suprasegmental (prosodic) cues that manifest themselves at the syllable, word and utterance level. Previous work has shown that categorical representations of prosody correlate well with lexical entities. In this paper, we attempt to exploit this relationship by enriching syllable-level lattices, generated by a standard speech recognizer, with categorical prosodic events for improved syllable recognition performance. With the enriched lattices, we obtain a 2% relative improvement in syllable error rate over the baseline system on a read speech task (the Boston University Radio News Corpus).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-506"
  },
  "pinto07_interspeech": {
   "authors": [
    [
     "Joel",
     "Pinto"
    ],
    [
     "Andrew",
     "Lovitt"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Exploiting phoneme similarities in hybrid HMM-ANN keyword spotting",
   "original": "i07_1817",
   "page_count": 4,
   "order": 507,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "We propose a technique for generating alternative models for keywords in a hybrid hidden Markov model - artificial neural network (HMM-ANN) keyword spotting paradigm. Given a base pronunciation for a keyword from the lookup dictionary, our algorithm generates a new model for a keyword which takes into account the systematic errors made by the neural network and avoiding those models that can be confused with other words in the language. The new keyword model improves the keyword detection rate while minimally increasing the number of false alarms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-507"
  },
  "liu07d_interspeech": {
   "authors": [
    [
     "C. E.",
     "Liu"
    ],
    [
     "K.",
     "Thambiratnam"
    ],
    [
     "F.",
     "Seide"
    ]
   ],
   "title": "Online vocabulary adaptation using limited adaptation data",
   "original": "i07_1821",
   "page_count": 4,
   "order": 508,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "This paper presents a study of low-latency domain-independent online vocabulary adaptation using limited amounts of supporting text data. The target applications include blind indexing of Internet content, indexing of new content with low latency, and domains where Out-Of-Vocabulary (OOV) words are problematic. A number of methods to perform document-specific adaptation using a small amount of support metadata and the Internet are examined. It is shown that a combination of word feature fusion and cross-file statistics pooling provides robust adaptation. The best evaluated method achieved an absolute reduction of 27.6% in OOV detection false alarm rate over the baseline word feature thresholding methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-508"
  },
  "lee07f_interspeech": {
   "authors": [
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Mark A.",
     "Clements"
    ],
    [
     "Sorin",
     "Dusan"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Keith",
     "Johnson"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ]
   ],
   "title": "An overview on automatic speech attribute transcription (ASAT)",
   "original": "i07_1825",
   "page_count": 4,
   "order": 509,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "Automatic Speech Attribute Transcription (ASAT), an ITR project sponsored under the NSF grant (IIS-04-27113), is a cross-institute effort involving Georgia Institute of Technology, The Ohio State University, University of California at Berkeley, and Rutgers University. This project approaches speech recognition from a more linguistic perspective: unlike traditional ASR systems, humans detect acoustic and auditory cues, weigh and combine them to form theories, and then process these cognitive hypotheses until linguistically and pragmatically consistent speech understanding is achieved. A major goal of the ASAT paradigm is to develop a detection-based approach to automatic speech recognition (ASR) based on attribute detection and knowledge integration. We report on progress of the ASAT project, present a sharable platform for community collaboration, and highlight areas of potential interdisciplinary ASR research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-509"
  },
  "bromberg07_interspeech": {
   "authors": [
    [
     "Ilana",
     "Bromberg"
    ],
    [
     "Qian",
     "Qian"
    ],
    [
     "Jun",
     "Hou"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chengyuan",
     "Ma"
    ],
    [
     "Brett",
     "Matthews"
    ],
    [
     "Antonio",
     "Moreno-Daniel"
    ],
    [
     "Jeremy",
     "Morris"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Yu",
     "Wang"
    ]
   ],
   "title": "Detection-based ASR in the automatic speech attribute transcription project",
   "original": "i07_1829",
   "page_count": 4,
   "order": 510,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "We present methods of detector design in the Automatic Speech Attribute Transcription project. This paper details the results of a student-led, cross-site collaboration between Georgia Institute of Technology, The Ohio State University and Rutgers University. The work reported in this paper describes and evaluates the detection-based ASR paradigm and discusses phonetic attribute classes, methods of detecting framewise phonetic attributes and methods of combining attribute detectors for ASR.\n",
    "We use Multi-Layer Perceptrons, Hidden Markov Models and Support Vector Machines to compute confidence scores for several prescribed sets of phonetic attribute classes. We use Conditional Random Fields (CRFs) and knowledge-based rescoring of phone lattices to combine framewise detection scores for continuous phone recognition on the TIMIT database. With CRFs, we achieve a phone accuracy of 70.63%, outperforming the baseline and enhanced HMM systems, by incorporating all of the attribute detectors discussed in the paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-510"
  },
  "lin07d_interspeech": {
   "authors": [
    [
     "Chi-Yueh",
     "Lin"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Attribute-based Mandarin speech recognition using conditional random fields",
   "original": "i07_1833",
   "page_count": 4,
   "order": 511,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "Integrating phonetic knowledge into a speech recognizer is a possible way to further improve the performance of conventional HMM-based speech recognition methods. This paper presents a cascaded architecture which consists of attribute detection and conditional random field to make use of phonetic knowledge within the phone decoding process. The attribute detection can be implemented by using any effective feature extraction approaches. In this study, an HMM-based method is applied for attribute tagging of Mandarin speech. Then a conditional random field method which applies attribute labels as the input vectors is used to perform the speech recognition. The preliminary experiment result shows that the proposed method is very promising and worthy for further investigation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-511"
  },
  "strik07_interspeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "Comparing classifiers for pronunciation error detection",
   "original": "i07_1837",
   "page_count": 4,
   "order": 512,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "Providing feedback on pronunciation errors in computer assisted language learning systems requires that pronunciation errors be detected automatically. In the present study we compare four types of classifiers that can be used for this purpose: two acoustic-phonetic classifiers (one of which employs linear-discriminant analysis (LDA)), a classifier based on cepstral coefficients in combination with LDA, and one based on confidence measures (the so-called Goodness Of Pronunciation scores). The best results were obtained for the two LDA classifiers which produced accuracy levels of about 85-93%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-512"
  },
  "krajewski07_interspeech": {
   "authors": [
    [
     "Jarek",
     "Krajewski"
    ],
    [
     "Bernd",
     "Kröger"
    ]
   ],
   "title": "Using prosodic and spectral characteristics for sleepiness detection",
   "original": "i07_1841",
   "page_count": 4,
   "order": 513,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "This paper describes a promising sleepiness detection approach based on prosodic and spectral speech characteristics and illustrates the validity of this method by briefly discussing results from a sleep deprivation study (N=20). We conducted a within-subject sleep deprivation design (8.00 p.m. to 4.00 a.m.). During the night of sleep deprivation, a standardized self-report scale was used every hour just before the recordings to determine the sleepiness state. The speech material consisted of simulated driver assistance system phrases. In order to investigate sleepiness induced speech changes, a standard set of spectral and prosodic features were extracted from the sentences. After forward selection and a PCA were employed on the feature space in an attempt to prune redundant dimensions, LDA- and ANN-based classification models were trained. The best level-0 model (RA15, LDA) offers a mean accuracy rate of 80.0% for the two-class problem. Using an ensemble classification strategy (majority voting as meta-classifier) we achieved a accuracy rate of 88.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-513"
  },
  "ore07_interspeech": {
   "authors": [
    [
     "Brian M.",
     "Ore"
    ],
    [
     "Raymond E.",
     "Slyh"
    ]
   ],
   "title": "Score fusion for articulatory feature detection",
   "original": "i07_1845",
   "page_count": 4,
   "order": 514,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "Articulatory Features (AFs) describe the way in which the speech organs are used when producing speech sounds. Research has shown that incorporating this information into speech recognizers can lead to an increase in system performance. This paper considers English AF detection using Gaussian Mixture Models (GMMs) and Multi-Layer Perceptrons (MLPs). The scores from the GMM- and MLP-based detectors are fused using a second MLP, resulting in an average reduction of 8.24% in equal error rate compared to the individual systems. These detector outputs are used to form the feature set for a Hidden Markov Model (HMM) phone recognizer. It is shown that monophone models created using the proposed feature set perform comparably to triphone models trained using Mel-Frequency Cepstral Coefficients (MFCCs).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-514"
  },
  "otterson07_interspeech": {
   "authors": [
    [
     "Scott",
     "Otterson"
    ]
   ],
   "title": "Improved location features for meeting speaker diarization",
   "original": "i07_1849",
   "page_count": 4,
   "order": 515,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "This paper proposes several improvements to the correlation-based location features recently used in meeting speaker diarization. A speech-specific alternative to the generalized cross correlation phase transform (GCC-PHAT) algorithm is tested and shown to provide equal or better results without noise reduction or continuity-enforcing smoothing. The limitations of a single correlation reference waveform are discussed, and it is shown how a multi-band energy ratio feature can help overcome them, yielding significantly improved performance. An all-pairs correlation is also proposed, and when combined with energy ratios, it also improves upon the baseline system. However, the best combination is the baseline correlation features with energy ratios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-515"
  },
  "han07b_interspeech": {
   "authors": [
    [
     "Kyu J.",
     "Han"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A robust stopping criterion for agglomerative hierarchical clustering in a speaker diarization system",
   "original": "i07_1853",
   "page_count": 4,
   "order": 516,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "Agglomerative hierarchical clustering (AHC) is an unsupervised classification strategy of merging the closest pair of clusters recursively, and has been widely used in speaker diarization systems to classify speech segments by speaker identity. The most critical part in AHC is how to automatically stop the recursive process at the point when clustering error rate reaches its lowest possible value, for which a BIC-based stopping criterion has been widely used. However, this criterion is not robust to data source variation. In this paper, we examine the criterion to establish the cause for the robustness issue and, based on this, propose an improved stopping criterion. Experimental results based on meeting conversation excerpts randomly chosen from various meeting speech corpora indicate that the proposed criterion is superior to the BIC-based one, showing that clustering error rate is improved on average by 7.28% (absolute) and 34.16% (relative).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-516"
  },
  "huijbregts07_interspeech": {
   "authors": [
    [
     "Marijn",
     "Huijbregts"
    ],
    [
     "Chuck",
     "Wooters"
    ]
   ],
   "title": "The blame game: performance analysis of speaker diarization system components",
   "original": "i07_1857",
   "page_count": 4,
   "order": 517,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "In this paper we discuss the performance analysis of a speaker diarization system similar to the system that was submitted by ICSI at the NIST RT06s evaluation benchmark. The analysis that is based on a series of oracle experiments, provides a good understanding of the performance of each system component on a test set of twelve conference meetings used in previous NIST benchmarks. Our analysis shows that the speech activity detection component contributes most to the total diarization error rate (23%). The lack of ability to model overlapping speech is also a large source of errors (22%) followed by the component that creates the initial system models (15%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-517"
  },
  "aronowitz07b_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Trainable speaker diarization",
   "original": "i07_1861",
   "page_count": 4,
   "order": 518,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "This paper presents a novel framework for speaker diarization. We explicitly model intra-speaker inter-segment variability using a speaker-labeled training corpus and use this modeling to assess the speaker similarity between speech segments. Modeling is done by embedding segments into a segment-space using kernel-PCA, followed by explicit modeling of speaker variability in the segment-space. Our framework leads to a significant improvement in diarization accuracy. Finally, we present a similar method for bandwidth classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-518"
  },
  "huang07b_interspeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Karthik",
     "Visweswariah"
    ]
   ],
   "title": "Improving speaker diarization for CHIL lecture meetings",
   "original": "i07_1865",
   "page_count": 4,
   "order": 519,
   "p1": "1865",
   "pn": "1868",
   "abstract": [
    "Speaker diarization is often performed before automatic speech recognition (ASR) to label speaker segments. In this paper we present two simple schemes to improve the speaker diarization performance. The first is to iteratively refine GMM speaker models by frame level re-labeling and smoothing of the decision likelihood. The second is to use word level alignment information from the ASR process. We focus on the CHIL lecture meeting data. Our experiments on the NIST RT06 evaluation data show that these simple methods are quite effective in improving our baseline diarization system, with alignment information providing 1% absolute reduction in diarization error rate (DER) and the re-label smoothing providing an additional 3.51% absolute reduction in DER. The overall system generates a DER that is 6.8% relative better than the top performing system from the RT06 evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-519"
  },
  "le07b_interspeech": {
   "authors": [
    [
     "Viet-Bac",
     "Le"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Speaker diarization using normalized cross likelihood ratio",
   "original": "i07_1869",
   "page_count": 4,
   "order": 520,
   "p1": "1869",
   "pn": "1872",
   "abstract": [
    "In this paper, we present the Normalized Cross Likelihood Ratio (NCLR) and the advantages of using it in a speaker diarization system. First, the NCLR is used as a dissimilarity measure between two Gaussian speaker models in the speaker change detection step and its contribution to the performance of speaker change detection is compared with those of BIC and Hostelling's T2-Statistic measures. Then, the NCLR measure is modified to deal with multi-gaussian adapted models in the cluster recombination step. This step ends the step-by-step speaker diarization process after the BIC-based hierarchical clustering and the Viterbi re-segmentation steps. By comparing the NCLR measure with the CLR (Cross Likelihood Ratio) one, more than 30% of relative diarization error is reduced in ESTER evaluation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-520"
  },
  "lee07g_interspeech": {
   "authors": [
    [
     "Wai-Sum",
     "Lee"
    ]
   ],
   "title": "Tone production by the speakers of different age-and-gender groups",
   "original": "i07_1873",
   "page_count": 4,
   "order": 521,
   "p1": "1873",
   "pn": "1876",
   "abstract": [
    "This paper is an acoustic analysis of the pitch/F0 of the six long tones [55 33 22 21 25 23] in Cantonese produced by the male and female adult speakers and male and female child speakers. Results show that (i) the F0 patterns of the Cantonese tones for the speakers of different age-and-gender groups are similar, but the absolute F0 values differ. (ii) The difference in F0 between the adult and child speakers is large, but less between the child and female adult speakers. (iii) The difference in F0 is noticeable between the adult speakers of different genders, but not between the male and female child speakers. (iv) The difference in F0 across the speaker groups is uniformly scaled for different tone types. And (v) for all the six tones, the F0 value for the child speakers is approximately an octave higher than that for the male adult speakers and 1.2 to 1.3 times higher than that of the female adult speakers, and the F0 for the female adult speakers is slightly over a half octave higher than the F0 for the male adult speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-521"
  },
  "xu07_interspeech": {
   "authors": [
    [
     "Nan",
     "Xu"
    ],
    [
     "Denis",
     "Burnham"
    ],
    [
     "Christine",
     "Kitamura"
    ]
   ],
   "title": "Vowels and tones in infant directed speech: hyperarticulation for both, but different developmental patterns",
   "original": "i07_1877",
   "page_count": 4,
   "order": 522,
   "p1": "1877",
   "pn": "1880",
   "abstract": [
    "A number of studies have shown that mothers hyperarticulate vowels in their Infant Directed Speech to their 6-month-old infants. Here we investigate the possibility that such hyperarticulation might also occur for lexical tone for mother-infant dyads in tone language environments, and possible changes in mothers' vowel and such tone hyperarticulation in IDS across the infant's first year. IDS from a total of 22 native Cantonese speaking mothers was recorded, 11 when their infants were 3-, 6-, and 9-month-old, and another 11 when their infants were 6-, 9-, and 12-month-old, and mothers asked to use nine target words in their speech; one for each of the three corner vowels /i/, /a/ and /u/), and another six for each of the Cantonese tones on the vowel /i/. Vowel hyperarticulation was investigated using first and second formant values, and tones using fundamental frequency onset and offset [1]. Preliminary results for 5 mothers presented here indicate that both vowel and tone hyperarticulation occur, but that while vowel hyperarticulation emerges around 6 months and increases from 6 to 9 to 12 month, tone hyperarticulation occurs only at 6 and 9 months. The results suggest that, as for vowels, tone space is hyperarticulated in IDS, but returns to Adult Directed Speech levels earlier for tones than vowels. Possible reasons for this are discussed, as are future studies with other tone languages with smaller tonal inventories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-522"
  },
  "ko07_interspeech": {
   "authors": [
    [
     "Eon-Suk",
     "Ko"
    ]
   ],
   "title": "Acquisition of vowel duration in children speaking american English",
   "original": "i07_1881",
   "page_count": 4,
   "order": 523,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "This study is an acoustic investigation of the acquisition of vowel duration in children speaking American English. The primary goal was to find out when and how children begin to produce different vowel durations as a function of postvocalic voicing. A total of 803 longitudinal data extracted from the Providence Corpus were analyzed. The age range covered by the data was from 0;11 to 4;0. The findings are summarized as follows: (1) Children control the vowel duration conditioned by voicing before the age of 2. (2) They also make the durational distinction between the tense and lax vowels before the age of 2. (3) There is no developmental trend in the acquisition of the vowel duration conditioned by postvocalic voicing. The results suggest that children thoroughly learn the phonetic implementation of temporal parameter from the very early stage of speech production to such an extent as to make it appear as an automatic process.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-523"
  },
  "hirano07_interspeech": {
   "authors": [
    [
     "Hiroko",
     "Hirano"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Goh",
     "Kawai"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "F<sub>0</sub> models show Chinese speakers of Japanese insert intonational boundaries and drop pitch",
   "original": "i07_1885",
   "page_count": 4,
   "order": 524,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "We used a command-response additive F0 model to analyze F0 patterns of Japanese spoken by native speakers of Mandarin Chinese. Compared to native speakers of Japanese, we found that Chinese speakers exhibit the following characteristics: (a) higher pitch, (b) more phrases, (c) bunsetsu decomposition, and (d) utterance-final plunging. These characteristics physically manifest themselves as: (a) higher baseline F0, (b) more phrase commands, (c) more accent commands, and (d) negative commands. These characteristics may be subjectively perceived as: (a) tinnier speech (possible L1 marker but does not degrade communication), (b) disjoint phrases (requires mental consolidation), (c) choppy prosodic words (requires reconstruction), and (d) abrupt utterance termination (possibly misconstrued as emphatic or rude). We believe these difficulties arose from tonal and syllable-timed interference, which can be overcome by prosodic control and planning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-524"
  },
  "escudero07_interspeech": {
   "authors": [
    [
     "Paola",
     "Escudero"
    ],
    [
     "Jelle",
     "Kastelein"
    ],
    [
     "Klara",
     "Weiand"
    ],
    [
     "R. J. J. H. van",
     "Son"
    ]
   ],
   "title": "Formal modelling of L1 and L2 perceptual learning: computational linguistics versus machine learning",
   "original": "i07_1889",
   "page_count": 4,
   "order": 525,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "In this paper, we evaluate the adequacy of two widely used machine learning algorithms and a computational linguistic proposal to model L2 perceptual development. The three proposals are, in order, Nearest Neighbor, Naive Bayesian and Stochastic OT and the Gradual Learning Algorithm. We compared the three models' outputs to those of Spanish learners of Dutch who were asked to categorize synthetic stimuli as one of the 12 Dutch vowels. The empirical results of the human learners show that L2 learners differ significantly from native listeners, but also that their perceptual spaces tend to become more native-like with L2 proficiency. The results of the simulations show that all three algorithms are able to model listeners' data to a certain extent but that Stochastic OT and the Gradual Learning Algorithm, i.e. the linguistic model, best reproduces L1 and L2 data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-525"
  },
  "broersma07_interspeech": {
   "authors": [
    [
     "Mirjam",
     "Broersma"
    ]
   ],
   "title": "Kettle hinders cat, shadow does not hinder shed: activation of ‘almost embedded’ words in nonnative listening",
   "original": "i07_1893",
   "page_count": 4,
   "order": 526,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "A Cross-Modal Priming experiment investigated Dutch listeners' perception of English words. Target words were embedded in a carrier word (e.g.,\n",
    "cat in catalogue) or ‘almost embedded’ in a carrier word except for a mismatch in the perceptually difficult /æ/-/ε/ contrast (e.g., cat in kettle). Previous results showed a bias towards perception of /ε / over /æ/. The present study shows that presentation of carrier words either containing an /æ/ or an /ε/ led to long lasting inhibition of embedded or ‘almost embedded’ words with an /æ/, but not of words with an /ε /. Thus, both catalogue and kettle hindered recognition of cat, whereas neither schedule nor shadow hindered recognition of shed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-526"
  },
  "krstulovic07_interspeech": {
   "authors": [
    [
     "Sacha",
     "Krstulović"
    ],
    [
     "Anna",
     "Hunecke"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "An HMM-based speech synthesis system applied to German and its adaptation to a limited set of expressive football announcements",
   "original": "i07_1897",
   "page_count": 4,
   "order": 527,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "The paper assesses the capability of an HMM-based TTS system to produce German speech. The results are discussed in qualitative terms, and compared over three different choices of context features. In addition, the system is adapted to a small set of football announcements, in an exploratory attempt to synthesise expressive speech. We conclude that the HMMs are able to produce highly intelligible neutral German speech, with a stable quality, and that the expressivity is partially captured in spite of the small size of the football dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-527"
  },
  "gu07_interspeech": {
   "authors": [
    [
     "Liang",
     "Gu"
    ],
    [
     "Wei",
     "Zhang"
    ],
    [
     "Lazkin",
     "Tahir"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Statistical vowelization of Arabic text for speech synthesis in speech-to-speech translation systems",
   "original": "i07_1901",
   "page_count": 4,
   "order": 528,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "Vowelization presents a principle difficulty in building text-to-speech synthesizers for speech-to-speech translation systems. In this paper, a novel log-linear modeling method is proposed that takes into account vowel and diacritical information at both the word level and character level. A unique syllable based normalization algorithm is then introduced to enhance both word coverage and data consistency. A recursive data generation and model training scheme is further devised to jointly optimize speech synthesizers and vowelizers for an English-Arabic speech translation system. The diacritization error rate is reduced by over 50% in vowelization experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-528"
  },
  "liu07e_interspeech": {
   "authors": [
    [
     "Wu",
     "Liu"
    ],
    [
     "Dezhi",
     "Huang"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Xinnian",
     "Mao"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": "A pair-based language model for the robust lexical analysis in Chinese text-to-speech synthesis",
   "original": "i07_1905",
   "page_count": 4,
   "order": 529,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "This paper presents a robust method of lexical analysis for Chinese text-to-speech (TTS) synthesis using a pair-based Language Model (LM). The traditional way of Chinese lexical analysis simply regards the word segmentation and part-of-speech (POS) tagging as two separated phases. Each of them utilizes its own algorithms and models. Actually, the POS information is useful for word segmentation, and vice versa. Therefore, a pair-based language model is proposed to integrate basic word segmentation, POS tagging and named entity (NE) identification into a unified framework. The objective evaluation indicates that the proposed method achieves the top-level performance, and confirms its effectiveness in Chinese lexical analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-529"
  },
  "maia07_interspeech": {
   "authors": [
    [
     "R.",
     "Maia"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "A trainable excitation model for HMM-based speech synthesis",
   "original": "i07_1909",
   "page_count": 4,
   "order": 530,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "This paper introduces a novel excitation approach for speech synthesizers in which the final waveform is generated through parameters directly obtained from Hidden Markov Models (HMMs). Despite the attractiveness of the HMM-based speech synthesis technique, namely utilization of small corpora and flexibility concerning the achievement of different voice styles, synthesized speech presents a characteristic buzziness caused by the simple excitation model which is employed during the speech production. This paper presents an innovative scheme where mixed excitation is modeled through closed-loop training of a set of state-dependent filters and pulse trains, with minimization of the error between excitation and residual sequences. The proposed method shows effectiveness, yielding synthesized speech with quality far superior to the simple excitation baseline and comparable to the best excitation schemes thus far reported for HMM-based speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-530"
  },
  "steigner07_interspeech": {
   "authors": [
    [
     "Jochen",
     "Steigner"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "Cross-language phonemisation in German text-to-speech synthesis",
   "original": "i07_1913",
   "page_count": 4,
   "order": 531,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "We present a TTS component for transcribing English words in German text. In addition to loan words, whose form does not change, we also cover xenomorphs, English stems with German morphology. We motivate the need for such a processing component, and present the algorithm in some detail. In an evaluation on unseen material, we find a precision of 0.85 and a recall of 0.997.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-531"
  },
  "tachibana07_interspeech": {
   "authors": [
    [
     "Ryuki",
     "Tachibana"
    ],
    [
     "Tohru",
     "Nagano"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Masafumi",
     "Nishimura"
    ],
    [
     "Noboru",
     "Babaguchi"
    ]
   ],
   "title": "Preliminary experiments toward automatic generation of new TTS voices from recorded speech alone",
   "original": "i07_1917",
   "page_count": 4,
   "order": 532,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "To generate a new concatenative text-to-speech (TTS) voice from recordings of a human's voice, not only recordings but also additional information such as the transcriptions, prosodic labels, and the phonemic alignments are necessary. Since some of the information depends on the speaking style of the narrator, these types of information need to be manually added by listening to the recordings, which is costly and time consuming. To tackle this problem, we have been working on a totally trainable TTS system every component of which, including the text processing module, can be automatically trained from a speech corpus. In this paper, we refine the framework and propose several submodules to collect all of the linguistic and acoustic information necessary for generating a TTS voice from the recorded speech. Though completely automatic generation of a new voice is not yet possible, we report progress in the submodules by showing experimental results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-532"
  },
  "chomphan07_interspeech": {
   "authors": [
    [
     "Suphattharachai",
     "Chomphan"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Implementation and evaluation of an HMM-based Thai speech synthesis system",
   "original": "i07_2849",
   "page_count": 4,
   "order": 533,
   "p1": "2849",
   "pn": "2852",
   "abstract": [
    "This paper describes a novel approach to the realization of Thai speech synthesis. Spectrum, pitch, and phone duration are modeled simultaneously in a unified framework of HMM, and their parameter distributions are clustered independently by using a decision-tree based context clustering technique with different styles. A group of contextual factors which affect spectrum, pitch, and state duration, i.e., tone type, part of speech, are taken into account especially for a tonal language. The evaluation of the synthesized speech shows that tone correctness is significantly improved in some clustering styles, moreover the implemented system gives the better reproduction of prosody (or naturalness, in some sense) than the unit-selection-based system with the same speech database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-533"
  },
  "bonardo07_interspeech": {
   "authors": [
    [
     "Davide",
     "Bonardo"
    ],
    [
     "Enrico",
     "Zovato"
    ]
   ],
   "title": "Speech synthesis enhancement in noisy environments",
   "original": "i07_2853",
   "page_count": 4,
   "order": 534,
   "p1": "2853",
   "pn": "2856",
   "abstract": [
    "This paper reports recent activities made to improve the intelligibility of synthesized speech in noisy environments. Nowadays Text-To-Speech technologies (TTS) are used in many embedded devices like mobile phones, PDAs, car navigation systems, etc. This means that speech can be produced in different types of environments where background noise can significantly degrade the perception of the synthetic message and consequently its intelligibility.\n",
    "The features discussed in this paper are being developed and assessed inside the EU funded SHARE project whose goal is to develop a multimodal communication system supporting rescue operations and disaster management.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-534"
  },
  "schmid07_interspeech": {
   "authors": [
    [
     "Helmut",
     "Schmid"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Julia",
     "Weidenkaff"
    ]
   ],
   "title": "Tagging syllable boundaries with joint n-gram models",
   "original": "i07_2857",
   "page_count": 4,
   "order": 535,
   "p1": "2857",
   "pn": "2860",
   "abstract": [
    "This paper presents a statistical method for the segmentation of words into syllables which is based on a joint n-gram model. Our system assigns syllable boundaries to phonetically transcribed words. The syllabification task was formulated as a tagging task. The syllable tagger was trained on syllable-annotated phone sequences. In an evaluation using ten-fold cross-validation, the system correctly predicted the syllabification of German words with an accuracy by word of 99.85%, which clearly exceeds results previously reported in the literature. The best performance was observed for a context size of five preceding phones. A detailed qualitative error analysis suggests that a further reduction of the error rate by up to 90% is possible by eliminating inconsistencies in the training database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-535"
  },
  "xu07b_interspeech": {
   "authors": [
    [
     "Jun",
     "Xu"
    ],
    [
     "Dezhi",
     "Huang"
    ],
    [
     "Yongxin",
     "Wang"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Lianhong",
     "Cai"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": "Hierarchical non-uniform unit selection based on prosodic structure",
   "original": "i07_2861",
   "page_count": 4,
   "order": 536,
   "p1": "2861",
   "pn": "2864",
   "abstract": [
    "In speech synthesis systems based on wave concatenation, using longer units can generate more natural synthetic speech. In order to improve the usage of longer units in the corpus, this paper proposed a hierarchical non-uniform unit selection framework. Each layer included in the framework is an independent searching procedure which searches for different sized units and adopts suitable naturalness measuring functions related to the unit type. We have applied it to our Mandarin speech synthesis system according to the Chinese prosodic structure with respect to the statistical result in our corpus. Experiment result shows it outperforms our previous system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-536"
  },
  "birkholz07_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Control of an articulatory speech synthesizer based on dynamic approximation of spatial articulatory targets",
   "original": "i07_2865",
   "page_count": 4,
   "order": 537,
   "p1": "2865",
   "pn": "2868",
   "abstract": [
    "We present a novel approach to the generation of speech movements for an articulatory speech synthesizer. The movements of the articulators are modeled by dynamical third order linear systems that respond to sequences of simple motor commands. The motor commands are derived automatically from a high level schedule for the input phonemes. The proposed model considers velocity differences of the articulators and accounts for coarticulation between vowels and consonants. Preliminary tests of the model in the framework of an articulatory speech synthesizer indicate its potential to produce realistic speech movements and thereby to contribute to a higher quality of the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-537"
  },
  "nishizawa07_interspeech": {
   "authors": [
    [
     "Nobuyuki",
     "Nishizawa"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "A preselection method based on cost degradation from the optimal sequence for concatenative speech synthesis",
   "original": "i07_2869",
   "page_count": 4,
   "order": 538,
   "p1": "2869",
   "pn": "2872",
   "abstract": [
    "A novel unit preselection criterion for concatenative speech synthesis is proposed. To reduce the computational cost for unit selection, units that are unlikely to be selected should be pruned as preselection before Viterbi search. Since the criterion is defined as the difference between the cost of the locally optimal sequence where a unit is fixed and that of the globally optimal sequence, not only the target cost but also the concatenation cost can be taken into account in preselection. For real-time speech synthesis, a preselection method using decision trees, where a unit can be bound to multiple nodes of a tree, is also introduced. Results of a unit selection experiment show that the proposed method using decision trees built from 8-hour training data is superior in the costs of the selected units to the conventional online preselection based on target costs. The experimental results also show that the method is more effective where the computational cost is strongly limited.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-538"
  },
  "strecha07_interspeech": {
   "authors": [
    [
     "Guntram",
     "Strecha"
    ],
    [
     "Matthias",
     "Eichner"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Line cepstral quefrencies and their use for acoustic inventory coding",
   "original": "i07_2873",
   "page_count": 4,
   "order": 539,
   "p1": "2873",
   "pn": "2876",
   "abstract": [
    "Line spectral frequencies (LSF) are widely used in the field of speech coding. Due to its properties, the LSF are qualified for the quantisation and the efficient compression of speech signals. In this paper we introduce the line cepstral quefrencies (LCQ). They are derived from the cepstrum in the same manner as the LSF are derived from linear predictive coding (LPC) features. We show that the combination of the pole-zero transfer function of the cepstrum with the properties of LSF offers advantages for speech coding. We apply the LCQ features to compress an acoustic inventory, which is used for a low resource speech synthesis. It is shown that the compression performance of the LCQ features is better than those of the LSF features in terms of the mean spectral distance to the original inventory.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-539"
  },
  "cahill07_interspeech": {
   "authors": [
    [
     "Peter",
     "Cahill"
    ],
    [
     "Daniel",
     "Aioanei"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Articulatory acoustic feature applications in speech synthesis",
   "original": "i07_2877",
   "page_count": 4,
   "order": 540,
   "p1": "2877",
   "pn": "2880",
   "abstract": [
    "The quality of unit selection speech synthesisers depends significantly on the content of the speech database being used. In this paper a technique is introduced that can highlight mispronunciations and abnormal units in the speech synthesis voice database through the use of articulatory acoustic feature extraction to obtain an additional layer of annotation. A set of articulatory acoustic feature classifiers help minimise the selection of inappropriate units in the speech database and are shown to significantly improve the word error rate of a diphone synthesiser.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-540"
  },
  "krul07_interspeech": {
   "authors": [
    [
     "Aleksandra",
     "Krul"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "François",
     "Yvon"
    ],
    [
     "Cédric",
     "Boidin"
    ],
    [
     "Thierry",
     "Moudenc"
    ]
   ],
   "title": "Approaches for adaptive database reduction for text-to-speech synthesis",
   "original": "i07_2881",
   "page_count": 4,
   "order": 541,
   "p1": "2881",
   "pn": "2884",
   "abstract": [
    "This paper raises the issue of speech database reduction adapted to a specific domain for Text-To-Speech (TTS) synthesis application. We evaluate several methods: a database pruning technique based on the statistical behaviour of the unit selection algorithm and a database adaptation method based on the Kullback-Leibler divergence. The aim of the former is to eliminate the least selected units during the synthesis of a domain specific training corpus. The aim of the later approach is to build a reduced database whose unit distribution approximates a given target distribution. We evaluate these methods on several objective measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-541"
  },
  "tsai07b_interspeech": {
   "authors": [
    [
     "Richard Tzong-Han",
     "Tsai"
    ],
    [
     "Hsi-Chuan",
     "Hung"
    ],
    [
     "Hong-Jie",
     "Dai"
    ],
    [
     "Wen-Lian",
     "Hsu"
    ]
   ],
   "title": "Exploiting unlabeled internal data in conditional random fields to reduce word segmentation errors for Chinese texts",
   "original": "i07_2885",
   "page_count": 4,
   "order": 542,
   "p1": "2885",
   "pn": "2888",
   "abstract": [
    "The application of text-to-speech (TTS) conversion has become widely used in recent years. Chinese TTS faces several unique difficulties. The most critical is caused by the lack of word delimiters in written Chinese. This means that Chinese word segmentation (CWS) must be the first step in Chinese TTS. Unfortunately, due to the ambiguous nature of word boundaries in Chinese, even the best CWS systems make serious segmentation errors. Incorrect sentence interpretation causes TTS errors, preventing TTS's wider use in applications such as automatic customer services or computer reader systems for the visually impaired. In this paper, we propose a novel method that exploits unlabeled internal data to reduce word segmentation errors without using external dictionaries. To demonstrate the generality of our method, we verify our system on the most widely recognized CWS evaluation tool - the SIGHAN bakeoff, which includes datasets in both traditional and simplified Chinese. These datasets are provided by four representative academies or industrial research institutes in HK, Taiwan, Mainland China, and the U.S. Our experimental results show that with only internal data and unlabeled test data, our approach reduces segmentation errors by an average of 15% compared to the traditional approach. Moreover, our approach achieves comparable performance to the best CWS systems that use external resources. Further analysis shows that our method has the potential to become more accurate as the amount of test data increases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-542"
  },
  "kirkpatrick07_interspeech": {
   "authors": [
    [
     "Barry",
     "Kirkpatrick"
    ],
    [
     "Darragh",
     "O'Brien"
    ],
    [
     "Ronán",
     "Scaife"
    ],
    [
     "Andrew",
     "Errity"
    ]
   ],
   "title": "On the role of spectral dynamics in unit selection speech synthesis",
   "original": "i07_2889",
   "page_count": 4,
   "order": 543,
   "p1": "2889",
   "pn": "2892",
   "abstract": [
    "Cost functions employed in unit selection significantly influence the quality of speech output. Although unit selection can produce very natural sounding speech the quality can be inconsistent and is difficult to guarantee due to discontinuities between incompatible units. The join cost employed in unit selection to measure the suitability of concatenating speech units typically consists of sub costs representing the fundamental frequency and spectrum at the boundaries of each unit. In this study the role of spectral dynamics as a join cost in unit selection synthesis is explored. A number of spectral dynamic measures are tested for the task of detecting discontinuities. Results indicate that spectral dynamic measures correlate with human perception of discontinuity if the features are extracted appropriately. Spectral dynamic mismatch is found to be a source of discontinuity although results suggest this is likely to occur simultaneously with static spectral mismatch.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-543"
  },
  "langner07_interspeech": {
   "authors": [
    [
     "Brian",
     "Langner"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "ugloss: a framework for improving spoken language generation understandability",
   "original": "i07_2893",
   "page_count": 4,
   "order": 544,
   "p1": "2893",
   "pn": "2896",
   "abstract": [
    "Understandable spoken presentation of structured and complex information is a difficult task to do well. As speech synthesis is used in more applications, there is likely to be an increasing requirement to present complex information in an understandable manner. This paper introduces\n",
    "uGloss, a language generation framework designed to influence the understandability of spoken output. We describe relevant factors to its design and provide a general description of our algorithm. We compare our approach to human performance for a straightforward task, and discuss areas of improvement and our future goals for this work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-544"
  },
  "schnell07b_interspeech": {
   "authors": [
    [
     "Karl",
     "Schnell"
    ],
    [
     "Arild",
     "Lacroix"
    ]
   ],
   "title": "Combination of LSF and pole based parameter interpolation for model-based diphone concatenation",
   "original": "i07_2897",
   "page_count": 4,
   "order": 545,
   "p1": "2897",
   "pn": "2900",
   "abstract": [
    "For speech generation using small databases, spectral smoothing at the unit joints is necessary and can be realized by an interpolation of model parameters. For that purpose, the LSF are the best choice from the conventional parameter descriptions. This contribution shows how LSF interpolations can be improved using poles as parameters. The problem of the pole assignment between the two pole configurations at the unit joints is solved by pole tracking of an LSF transition. An inspection of the assignments determined by LSF transitions reveals unfavorable cases which can be corrected. A comparison between the LSF and the pole based interpolations shows that the LSF interpolations can be improved by the corrected pole assignments and by the trajectories of the poles. The investigations are performed using a diphone database which is analyzed by an extended LPC model in lattice structure including vocal tract losses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-545"
  },
  "prahallad07_interspeech": {
   "authors": [
    [
     "Kishore",
     "Prahallad"
    ],
    [
     "Arthur R.",
     "Toth"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Automatic building of synthetic voices from large multi-paragraph speech databases",
   "original": "i07_2901",
   "page_count": 4,
   "order": 546,
   "p1": "2901",
   "pn": "2904",
   "abstract": [
    "Large multi paragraph speech databases encapsulate prosodic and contextual information beyond the sentence level which could be exploited to build natural sounding voices. This paper discusses our efforts on automatic building of synthetic voices from large multi-paragraph speech databases. We show that the primary issue of segmentation of large speech file could be addressed with modifications to forced-alignment technique and that the proposed technique is independent of the duration of the audio file. We also discuss how this framework could be extended to build a large number of voices from public domain large multi-paragraph recordings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-546"
  },
  "gallardoantolin07_interspeech": {
   "authors": [
    [
     "A.",
     "Gallardo-Antolín"
    ],
    [
     "R.",
     "Barra"
    ],
    [
     "Marc",
     "Schröder"
    ],
    [
     "Sacha",
     "Krstulović"
    ],
    [
     "J. M.",
     "Montero"
    ]
   ],
   "title": "Automatic phonetic segmentation of Spanish emotional speech",
   "original": "i07_2905",
   "page_count": 4,
   "order": 547,
   "p1": "2905",
   "pn": "2908",
   "abstract": [
    "To achieve high quality synthetic emotional speech, unit-selection is the state-of-the-art technique. Nevertheless, a large expensive phonetically-segmented corpus is needed, and cost-effective automatic techniques should be studied. According to the HMM experiments in this paper: segmentation performance can depend heavily on the segmental or prosodic nature of the intended emotion (segmental emotions are more difficult to segment than prosodic ones), several emotions should be combined to obtain a larger training set (especially when prosodic emotions are involved; this is especially true for small training sets) and a combination of emphatic and non-emphatic emotional recordings (short sentences vs. long paragraphs) can degrade overall performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-547"
  },
  "lin07e_interspeech": {
   "authors": [
    [
     "Dacheng",
     "Lin"
    ],
    [
     "Yong",
     "Zhao"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Jieyu",
     "Zhao"
    ]
   ],
   "title": "Iterative unit selection with unnatural prosody detection",
   "original": "i07_2909",
   "page_count": 4,
   "order": 548,
   "p1": "2909",
   "pn": "2912",
   "abstract": [
    "Corpus-driven speech synthesis is hampered by the occurrence of occasional glitches which ruin the impression of the whole utterance. We propose an iterative unit selection integrated with an unnatural prosody detection model to identify any unnatural prosody. The system searches an optimal path in the lattice, verifies its naturalness by the unnatural prosody model and replaces the bad section with a better candidate, until it passes the verification test. In light of hypothesis testing, we show this trial-and-error approach takes effective advantage of abundant candidate samples in the database. Also, in contrast to conventional prosody prediction, an unnatural prosody detection model still leaves enough room for the prosody variations. Unnaturalness confidence measures are studied. The combined model can reduce the objective distortion by 16.3%. Perceptual experiments also confirm the proposed approach improves the synthetic speech quality appreciably.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-548"
  },
  "hanzlicek07_interspeech": {
   "authors": [
    [
     "Zdeněk",
     "Hanzlíček"
    ],
    [
     "Jindřich",
     "Matoušek"
    ]
   ],
   "title": "F0 transformation within the voice conversion framework",
   "original": "i07_1961",
   "page_count": 4,
   "order": 549,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "In this paper, several experiments on F0 transformation within the voice conversion framework are presented. The conversion system is based on a probabilistic transformation of line spectral frequencies and residual prediction. Three probabilistic methods of instantaneous F0 transformation are described and compared. Moreover, a new modification of inter-speaker residual prediction is proposed which utilizes the information on target F0 directly during the determination of suitable residuum. Preference listening tests confirmed that this modification outperformed the standard version of residual prediction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-549"
  },
  "erro07_interspeech": {
   "authors": [
    [
     "Daniel",
     "Erro"
    ],
    [
     "Asunción",
     "Moreno"
    ]
   ],
   "title": "Weighted frequency warping for voice conversion",
   "original": "i07_1965",
   "page_count": 4,
   "order": 550,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "This paper presents a new voice conversion method called Weighted Frequency Warping (WFW), which combines the well known GMM approach and the frequency warping approach. The harmonic plus stochastic model has been used to analyze, modify and synthesize the speech signal. Special phase manipulation procedures have been designed to allow the system to work in pitch-asynchronous mode. The experiments show that the proposed technique reaches a high degree of similarity between the converted and target speakers, and the naturalness and quality of the resynthesized speech is much higher than those of classical GMM-based systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-550"
  },
  "erro07b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Erro"
    ],
    [
     "Asunción",
     "Moreno"
    ]
   ],
   "title": "Frame alignment method for cross-lingual voice conversion",
   "original": "i07_1969",
   "page_count": 4,
   "order": 551,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "Most of the existing voice conversion methods calculate the optimal transformation function from a given set of paired acoustic vectors of the source and target speakers. The alignment of the phonetically equivalent source and target frames is problematic when the training corpus available is not parallel, although this is the most realistic situation. The alignment task is even more difficult in cross-lingual applications because the phoneme sets may be different in the involved languages. In this paper, a new iterative alignment method based on acoustic distances is proposed. The method is shown to be suitable for text-independent and cross-lingual voice conversion, and the conversion scores obtained in our evaluation experiments are not far from the performance achieved by using parallel training corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-551"
  },
  "nurminen07_interspeech": {
   "authors": [
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Jilei",
     "Tian"
    ],
    [
     "Victor",
     "Popa"
    ]
   ],
   "title": "Voicing level control with application in voice conversion",
   "original": "i07_1973",
   "page_count": 4,
   "order": 552,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "Speech processing related changes in the speech spectra may often lead to unwanted changes in the effective degree of voicing, which in turn may degrade the speech quality. This phenomenon is studied more closely in this paper, first on a theoretical level and then in the context of voice conversion. Moreover, a simple but efficient approach for avoiding the unwanted changes in the effective level of voicing is proposed. The usefulness of the proposed voicing level control is demonstrated in a practical voice conversion system. The compensation of the changes in the degree of voicing is found to reduce the average level of noise in the output and to enhance the perceptual speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-552"
  },
  "percybrooks07_interspeech": {
   "authors": [
    [
     "Winston S.",
     "Percybrooks"
    ],
    [
     "Elliot",
     "Moore"
    ]
   ],
   "title": "New algorithm for LPC residual estimation from LSF vectors for a voice conversion system",
   "original": "i07_1977",
   "page_count": 4,
   "order": 553,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "Voice conversion involves transforming segments of speech from a source speaker to make them to be perceived as if spoken by a target speaker. Generally, this process involves the estimation of vocal tract parameters and an excitation signal that match the target speaker. The work presented here proposes an algorithm for estimating the excitation residuals of the target speaker using a weighted combination of clustered residuals. The algorithm is subjected to objective and subjective comparisons to other basic types of residual estimation techniques for voice conversion. Tests were carried on 2 male and 2 female target speakers in an ideal setting. The overall goal of this work is to create an improved algorithm for estimating excitation residuals during voice conversion that maintain speaker recognizability and high synthesis quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-553"
  },
  "ohtani07_interspeech": {
   "authors": [
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speaker adaptive training for one-to-many eigenvoice conversion based on Gaussian mixture model",
   "original": "i07_1981",
   "page_count": 4,
   "order": 554,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "One-to-many eigenvoice conversion (EVC) allows the conversion of a specific source speaker into arbitrary target speakers. Eigenvoice Gaussian mixture model (EV-GMM) is trained in advance with multiple parallel data sets consisting of the source speaker and many pre-stored target speakers. The EV-GMM is adapted for arbitrary target speakers using only a few utterances by estimating a small number of free parameters. Therefore, the initial EV-GMM directly affects the conversion performance of the adapted EV-GMM. In order to prepare a better initial model, this paper proposes Speaker Adaptive Training (SAT) of a canonical EV-GMM in one-to-many EVC. Results of objective and subjective evaluations demonstrate that SAT causes significant improvements in the performance of EVC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-554"
  },
  "petkov07_interspeech": {
   "authors": [
    [
     "Petko N.",
     "Petkov"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Improving the phase vocoder approach to pitch-shifting",
   "original": "i07_1985",
   "page_count": 4,
   "order": 555,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "A class of methods known as phase vocoders allows for implementing pitch shifting in the spectral domain. We extend the approach of shifting the isolated harmonics of the spectrum by introducing a new technique for separating the sinusoidal components. Keeping together the main lobe and the side lobes, which result from convolution of the harmonics with the spectrum of the analysis window in the Fourier transform, we minimize the leakage of energy and the related phase compensation problems. Furthermore, we integrate a robust enhancement to the update of the phase, based on tracking of the energy envelope. The formant structure of the signal is preserved by means of an all-pole speech production model. The proposed modifications lead to significant improvement of the quality of the pitch-shifted speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-555"
  },
  "mesbahi07_interspeech": {
   "authors": [
    [
     "Larbi",
     "Mesbahi"
    ],
    [
     "Vincent",
     "Barreaud"
    ],
    [
     "Olivier",
     "Boeffard"
    ]
   ],
   "title": "Comparing GMM-based speech transformation systems",
   "original": "i07_1989",
   "page_count": 4,
   "order": 556,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "This article deals with a study on GMM-based voice conversion systems. We compare the main linear conversion functions found in the literature on an identical speech corpus. We insist in particular on the risks of over-fitting and over-smoothing. We propose three alternatives for robust conversion functions in order to minimize these risks. We show, on two experimental speech databases, that the approach suggested by Kain remains the more precise but leads to an over-fitting ratio of 1.72%. The alternatives which we propose, present an average degradation of 2.8% for a 0.52% over-fitting ratio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-556"
  },
  "kuo07b_interspeech": {
   "authors": [
    [
     "Jen-Wei",
     "Kuo"
    ],
    [
     "Hung-Yi",
     "Lo"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Improved HMM/SVM methods for automatic phoneme segmentation",
   "original": "i07_2057",
   "page_count": 4,
   "order": 557,
   "p1": "2057",
   "pn": "2060",
   "abstract": [
    "This paper presents improved HMM/SVM methods for a two-stage phoneme segmentation framework, which tries to imitate the human phoneme segmentation process. The first stage performs hidden Markov model (HMM) forced alignment according to the minimum boundary error (MBE) criterion. The objective is to align a phoneme sequence of a speech utterance with its acoustic signal counterpart based on MBE-trained HMMs and explicit phoneme duration models. The second stage uses the support vector machine (SVM) method to refine the hypothesized phoneme boundaries derived by HMM-based forced alignment. The efficacy of the proposed framework has been validated on two speech databases: the TIMIT English database and the MATBN Mandarin Chinese database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-557"
  },
  "shinozaki07_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Gaussian mixture optimization for HMM based on efficient cross-validation",
   "original": "i07_2061",
   "page_count": 4,
   "order": 558,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "A Gaussian mixture optimization method is explored using cross-validation likelihood as an objective function instead of the conventional training set likelihood. The optimization is based on reducing the number of mixture components by selecting and merging a pair of Gaussians step by step base on the objective function so as to remove redundant components and improve the generality of the model. Cross-validation likelihood is more appropriate for avoiding over-fitting than the conventional likelihood and can be efficiently computed using sufficient statistics. It results in a better Gaussian pair selection and provides a termination criterion that does not rely on empirical thresholds. Large-vocabulary speech recognition experiments on oral presentations show that the cross-validation method gives a smaller word error rate with an automatically determined model size than a baseline training procedure that does not perform the optimization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-558"
  },
  "zen07_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Model-space MLLR for trajectory HMMs",
   "original": "i07_2065",
   "page_count": 4,
   "order": 559,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "This paper proposes model-space Maximum Likelihood Linear Regression (mMLLR) based speaker adaptation technique for trajectory HMMs, which have been derived from HMMs by imposing explicit relationships between static and dynamic features. This model can alleviate two limitations of the HMM: constant statistics within a state and conditional independence assumption of state output probabilities without increasing the number of model parameters. Results in a continuous speech recognition experiments show that the proposed algorithm can adapt trajectory HMMs to a specific speaker and improve the performance of a trajectory HMM-based speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-559"
  },
  "ketabdar07b_interspeech": {
   "authors": [
    [
     "Hamed",
     "Ketabdar"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "In-context phone posteriors as complementary features for tandem ASR",
   "original": "i07_2069",
   "page_count": 4,
   "order": 560,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "In this paper, we present a method for integrating possible prior knowledge (such as phonetic and lexical knowledge), as well as acoustic context (e.g., the whole utterance) in the phone posterior estimation, and we propose to use the obtained posteriors as complementary posterior features in Tandem ASR configuration. These posteriors are estimated based on HMM state posterior probability definition (typically used in standard HMMs training). In this way, by integrating the appropriate prior knowledge and context, we enhance the estimation of phone posteriors. These new posteriors are called ‘in-context’ or HMM posteriors. We combine these posteriors as complementary evidences with the posteriors estimated from a Multi Layer Perceptron (MLP), and use the combined evidence as features for training and inference in Tandem configuration. This approach has improved the performance, as compared to using only MLP estimated posteriors as features in Tandem, on OGI Numbers , Conversational Telephone speech (CTS), and Wall Street Journal (WSJ) databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-560"
  },
  "qian07_interspeech": {
   "authors": [
    [
     "Qian",
     "Qian"
    ],
    [
     "Xiaodong",
     "He"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Phone-discriminating minimum classification error (p-MCE) training for phonetic recognition",
   "original": "i07_2073",
   "page_count": 4,
   "order": 561,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "In this paper, we report a study on performance comparisons of discriminative training methods for phone recognition using the TIMIT database. We propose a new method of phone-discriminating minimum classification error (P-MCE), which performs MCE training at the sub-string or phone level instead of at the traditional string level. Aiming at minimizing the phone recognition error rate, P-MCE nevertheless takes advantage of the well-known, efficient training routine derived from the conventional string-based MCE, using specially constructed one-best lists selected from phone lattices. Extensive investigations and comparisons are conducted between the P-MCE and other discriminative training methods including maximum mutual information (MMI), minimum phone or word error (MPE/MWE), and the other two MCE methods. The P-MCE outperforms most of experimented approaches on the standard TIMIT database in terms of the continuous phonetic recognition accuracy. P-MCE achieves comparable results with the MPE method which also aims at reducing phone-level recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-561"
  },
  "lamel07_interspeech": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "Abdel.",
     "Messaoudi"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Improved acoustic modeling for transcribing Arabic broadcast data",
   "original": "i07_2077",
   "page_count": 4,
   "order": 562,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "This paper summarizes our recent progress in improving the automatic transcription of Arabic broadcast audio data, and some efforts to address the challenges of the broadcast conversational speech. Our efforts are aimed at improving the acoustic, pronunciation and language models taking into account specificities of the Arabic language. In previous work we demonstrated that explicit modeling of short vowels improved recognition performance, even when producing non-vocalized hypotheses. In addition to modeling short vowels, consonant gemination and nunation are now explicitly modeled, alternative pronunciations have been introduced to better represent dialectical variants, and a duration model has been integrated. In order to facilitate training on Arabic audio data with non-vocalized transcripts a generic vowel model has been introduced. Compared with the previous system (used in the 2006 GALE evaluation) the relative word error rate has been reduced by over 10%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-562"
  },
  "mcdermott07_interspeech": {
   "authors": [
    [
     "Erik",
     "McDermott"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "String and lattice based discriminative training for the corpus of spontaneous Japanese lecture transcription task",
   "original": "i07_2081",
   "page_count": 4,
   "order": 563,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "This article aims to provide a comprehensive set of acoustic model discriminative training results for the Corpus of Spontaneous Japanese (CSJ) lecture speech transcription task. Discriminative training was carried out for this task using a 100,000 word trigram for several acoustic model topologies, using both diagonal and full covariance models, and using both string-based and lattice-based training paradigms. We describe our implementation of the proposal by Macherey et al. for numerical subtraction of the reference lattice statistics from the competitor lattice statistics during lattice-based Minimum Classification Error (MCE) training. We also present results for lattice-based training that does not use such subtraction, corresponding to the well-known Maximum Mutual Information (MMI) approach. Discriminative training yielded relative reductions in Word Error Rate of up to 13%. Specific problems encountered in implementing discriminative training for this task are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-563"
  },
  "kang07_interspeech": {
   "authors": [
    [
     "Byung-Ok",
     "Kang"
    ],
    [
     "Ho-Young",
     "Jung"
    ],
    [
     "Yun-Keun",
     "Lee"
    ]
   ],
   "title": "Discriminative noise adaptive training approach for an environment migration",
   "original": "i07_2085",
   "page_count": 4,
   "order": 564,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "A combined strategy of noise-adaptive training (NAT) and discriminative-based adaptation is proposed for effective migration of speech recognition systems to other noisy environments. NAT is an effective approach for real-field applications, but does not satisfy the minimum classification error (MCE) criterion for the recognition process and adapts poorly to new environments. The proposed method makes up for the weak points in discriminative adaptation strategies, and presents a new method for improving the MCE approach. Using this new method, experimental results show that the speech recognition system can successfully be migrated to other environments using specific-condition data of the target environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-564"
  },
  "chen07_interspeech": {
   "authors": [
    [
     "Jia-Yu",
     "Chen"
    ],
    [
     "Peder A.",
     "Olsen"
    ],
    [
     "John R.",
     "Hershey"
    ]
   ],
   "title": "Word confusability - measuring hidden Markov model similarity",
   "original": "i07_2089",
   "page_count": 4,
   "order": 565,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "We address the problem of word confusability in speech recognition by measuring the similarity between Hidden Markov Models (HMMs) using a number of recently developed techniques. The focus is on defining a word confusability that is accurate, in the sense of predicting artificial speech recognition errors, and computationally efficient when applied to speech recognition applications. It is shown by using the edit distance framework for HMMs that we can use statistical information measures of distances between probability distribution functions to define similarity or distance measures between HMMs. We use correlation between errors in a real speech recognizer and the HMM similarities to measure how well each technique works. We demonstrate significant improvements relative to traditional phone confusion weighted edit distance measures by use of a Bhattacharyya divergence-based edit distance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-565"
  },
  "deselaers07_interspeech": {
   "authors": [
    [
     "Thomas",
     "Deselaers"
    ],
    [
     "Georg",
     "Heigold"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Speech recognition with state-based nearest neighbour classifiers",
   "original": "i07_2093",
   "page_count": 4,
   "order": 566,
   "p1": "2093",
   "pn": "2096",
   "abstract": [
    "We present a system that uses nearest neighbour classification on the state level of the hidden Markov model. Common speech recognition systems nowadays use Gaussian mixtures with a very high number of densities. We propose to carry this idea to the extreme, such that each observation is a prototype of its own. This approach is well-known and widely used in other areas of pattern recognition and has some immediate advantages over other classification approaches, but has never been applied to speech recognition. We evaluate the proposed method on the SieTill corpus of continuous digit strings and on the large vocabulary EPPS English task. It is shown that nearest neighbour outperforms conventional systems when training data is sparse.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-566"
  },
  "teunen07_interspeech": {
   "authors": [
    [
     "Remco",
     "Teunen"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "HMM-based speech recognition using decision trees instead of GMMs",
   "original": "i07_2097",
   "page_count": 4,
   "order": 567,
   "p1": "2097",
   "pn": "2100",
   "abstract": [
    "In this paper, we experiment with decision trees as replacements for Gaussian mixture models to compute the observation likelihoods for a given HMM state in a speech recognition system. Decision trees have a number of advantageous properties, such as that they do not impose restrictions on the number or types of features, and that they automatically perform feature selection. In fact, due to the conditional nature of the decision tree evaluation process, the subset of features that is actually used during recognition depends on the input signal. Automatic state-tying can be incorporated directly into the acoustic model as well, and it too becomes a function of the input signal. Experimental results for the Aurora 2 speech database show that a system using decision trees offers state-of-the-art performance, even without taking advantage of its full potential.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-567"
  },
  "gollan07_interspeech": {
   "authors": [
    [
     "Christian",
     "Gollan"
    ],
    [
     "Stefan",
     "Hahn"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "An improved method for unsupervised training of LVCSR systems",
   "original": "i07_2101",
   "page_count": 4,
   "order": 568,
   "p1": "2101",
   "pn": "2104",
   "abstract": [
    "In this paper, we introduce an improved method for unsupervised training where the data selection or filtering process is done on state level. We describe in detail the setup of the experiments and introduce the state confidence scores on word and allophone state level for performing the data selection for mixture training on state level. Although we are using a relatively small amount of 180 hours of untranscribed recordings in addition to the available carefully manually transcribed transcriptions of 100 hours, we are able to significantly improve our final speaker adaptive acoustic model. Furthermore, we present promising results by doing system combination using the acoustic models trained on different confidence thresholds. These methods are evaluated on the EPPS corpus starting from the RWTH European English parliamentary speech transcription system. A significant improvement of 7% relative is achieved using less data for unsupervised training than conventional systems require.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-568"
  },
  "omar07b_interspeech": {
   "authors": [
    [
     "Mohamed Kamal",
     "Omar"
    ]
   ],
   "title": "A variational approach to robust maximum likelihood estimation for speech recognition",
   "original": "i07_2105",
   "page_count": 4,
   "order": 569,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "In many automatic speech recognition (ASR) applications, the data used to estimate the class-conditional feature probability density function (PDF) is noisy, and the test data is mismatched with the training data. Previous research has shown that the effect of this problem may be reduced by using models which take the effect of the noise into consideration, and by transforming the features or the models used in the classifier to adapt to new environments and speakers. This paper addresses the degradation in the performance of ASR systems due to small - possibly time-varying - perturbations of the training data. To approach this problem, we provide a computationally efficient algorithm for estimating the model parameters which maximize the sum of the log likelihood and the negative of a measure of the sensitivity of the estimated likelihood to these perturbations. This approach does not make any assumptions about the noise model during training. We present several large vocabulary speech recognition experiments that show significant recognition accuracy improvement compared to using the baseline maximum likelihood (ML) models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-569"
  },
  "yu07e_interspeech": {
   "authors": [
    [
     "Kai",
     "Yu"
    ],
    [
     "Rob A.",
     "Rutenbar"
    ]
   ],
   "title": "Generating small, accurate acoustic models with a modified Bayesian information criterion",
   "original": "i07_2109",
   "page_count": 4,
   "order": 570,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "Although Gaussian mixture models are commonly used in acoustic models for speech recognition, there is no standard method for determining the number of mixture components. Most models arbitrarily assign the number of mixture components with little justification. While model selection techniques with a mathematical derivation, such as the Bayesian information criterion (BIC), have been applied, these criteria focus on properly modeling the true distribution of individual tied-states (senones) without considering the entire acoustic model; this leads to suboptimal speech recognition performance. In this paper we present a method to generate statistically-justified acoustic models that consider inter-senone effects by modifying the BIC. Experimental results in the CMU Communicator domain show that in contrast to previous strategies, the new method generates not only attractively smaller acoustic models, but also ones with lower word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-570"
  },
  "bell07_interspeech": {
   "authors": [
    [
     "Peter",
     "Bell"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Sparse Gaussian graphical models for speech recognition",
   "original": "i07_2113",
   "page_count": 4,
   "order": 571,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "We address the problem of learning the structure of Gaussian graphical models for use in automatic speech recognition, a means of controlling the form of the inverse covariance matrices of such systems. With particular focus on data sparsity issues, we implement a method for imposing graphical model structure on a Gaussian mixture system, using a convex optimisation technique to maximise a penalised likelihood expression. The results of initial experiments on a phone recognition task show a performance improvement over an equivalent full-covariance system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-571"
  },
  "sakti07_interspeech": {
   "authors": [
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "An HMM acoustic model incorporating various additional knowledge sources",
   "original": "i07_2117",
   "page_count": 4,
   "order": 572,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "We introduce a method of incorporating additional knowledge sources into an HMM-based statistical acoustic model. The probabilistic relationship between information sources is first learned through a Bayesian network to easily integrate any additional knowledge sources that might come from any domain and then the global joint probability density function (PDF) of the model is formulated. Where the model becomes too complex and direct BN inference is intractable, we utilize a junction tree algorithm to decompose the global joint PDF into a linked set of local conditional PDFs. This way, a simplified form of the model can be constructed and reliably estimated using a limited amount of training data. Here, we apply this framework to incorporate accents, gender, and wide-phonetic knowledge information at the HMM phonetic model level. The performance of the proposed method was evaluated on an LVCSR task using two different types of accented English speech data. Experimental results revealed that our method improves word accuracy with respect to standard HMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-572"
  },
  "varjokallio07_interspeech": {
   "authors": [
    [
     "Matti",
     "Varjokallio"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Comparison of subspace methods for Gaussian mixture models in speech recognition",
   "original": "i07_2121",
   "page_count": 4,
   "order": 573,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "Speech recognizers typically use high-dimensional feature vectors to capture the essential cues for speech recognition purposes. The acoustics are then commonly modeled with a Hidden Markov Model with Gaussian Mixture Models as observation probability density functions. Using unrestricted Gaussian parameters might lead to intolerable model costs both evaluation- and storagewise, which limits their practical use only to some high-end systems. The classical approach to tackle with these problems is to assume independent features and constrain the covariance matrices to being diagonal. This can be thought as constraining the second order parameters to lie in a fixed subspace consisting of rank-1 terms. In this paper we discuss the differences between recently proposed subspace methods for GMMs with emphasis placed on the applicability of the models to a practical LVCSR system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-573"
  },
  "schultz07_interspeech": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Sameer",
     "Badaskar"
    ],
    [
     "Matthew",
     "Hornyak"
    ],
    [
     "John",
     "Kominek"
    ]
   ],
   "title": "SPICE: web-based tools for rapid language adaptation in speech processing systems",
   "original": "i07_2125",
   "page_count": 4,
   "order": 574,
   "p1": "2125",
   "pn": "2128",
   "abstract": [
    "In this paper we describe the design and implementation of a user interface for SPICE, a web-based toolkit for rapid prototyping of speech and language processing components. We report on the challenges and experiences gathered from testing these tools in an advanced graduate hands-on course, in which we created speech recognition, speech synthesis, and small-domain translation components for 10 different languages within only 6 weeks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-574"
  },
  "deprez07_interspeech": {
   "authors": [
    [
     "Filip",
     "Deprez"
    ],
    [
     "Jan",
     "Odijk"
    ],
    [
     "Jan De",
     "Moortel"
    ]
   ],
   "title": "Introduction to multilingual corpus-based concatenative speech synthesis",
   "original": "i07_2129",
   "page_count": 4,
   "order": 575,
   "p1": "2129",
   "pn": "2132",
   "abstract": [
    "This tutorial paper addresses foreign-language support in corpus-based concatenative text-to-speech systems. We give an overview of application domains where strictly monolingual speech synthesis is not sufficient and where multilingual text-to-speech is required or highly desirable. We describe two approaches to multilingual corpus-based speech synthesis: phoneme mapping on the one hand, and the creation of multilingual speech databases on the other. We list the strengths and weaknesses of both approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-575"
  },
  "stouten07b_interspeech": {
   "authors": [
    [
     "Frederik",
     "Stouten"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Recognition of foreign names spoken by native speakers",
   "original": "i07_2133",
   "page_count": 4,
   "order": 576,
   "p1": "2133",
   "pn": "2136",
   "abstract": [
    "It is a challenge to develop a speech recognizer that can handle the kind of lexicons encountered in an automatic attendant or car navigation application. Such lexicons can contain several 100K entries, mainly proper names. Many of these names are of a foreign origin, and native speakers can pronounce them in different ways, ranging from a completely\n",
    "nativized to a completely foreignized pronunciation. In this paper we propose a method that tries to deal with the observed pronunciation variability by introducing the concept of a foreignizable phoneme, and by combining standard acoustic models with a phonologically inspired back-off acoustic model. The main advantage of the approach is that it does not require any foreign phoneme models nor foreign speech data. For the recognition of English names by means of Dutch acoustic models, we obtained a reduction of the word error rate by more than 10% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-576"
  },
  "cordoba07b_interspeech": {
   "authors": [
    [
     "R.",
     "Cordoba"
    ],
    [
     "L. F.",
     "D'Haro"
    ],
    [
     "F.",
     "Fernandez-Martinez"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "R.",
     "Barra"
    ]
   ],
   "title": "Language identification using several sources of information with a multiple-Gaussian classifier",
   "original": "i07_2137",
   "page_count": 4,
   "order": 577,
   "p1": "2137",
   "pn": "2140",
   "abstract": [
    "We present several innovative techniques that can be applied in a PPRLM system for language identification (LID). To normalize the scores, eliminate the bias in the scores and improve the classifier, we compared the bias removal technique (up to 19% relative improvement (RI)) and a Gaussian classifier (up to 37% RI). Then, we include additional sources of information in different feature vectors of the Gaussian classifier: the sentence acoustic score (11% RI), the average acoustic score for each phoneme (11% RI), and the average duration for each phoneme (7.8% RI). The use of a multiple-Gaussian classifier with 4 feature vectors meant an additional 15.1% RI. Using 4 feature vectors instead of just PPRLM provides a 26.1% RI. Finally, we include additional acoustic HMMs of the same language with success (10% relative improvement). We will show how all these improvements have been mostly additive.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-577"
  },
  "solar07_interspeech": {
   "authors": [
    [
     "Carmen Del",
     "Solar"
    ],
    [
     "Guillermo",
     "Pérez"
    ],
    [
     "Eva",
     "Florencio"
    ],
    [
     "David",
     "Moral"
    ],
    [
     "Gabriel",
     "Amores"
    ],
    [
     "Pilar",
     "Manchón"
    ]
   ],
   "title": "Dynamic language change in MIMUS",
   "original": "i07_2141",
   "page_count": 4,
   "order": 578,
   "p1": "2141",
   "pn": "2144",
   "abstract": [
    "One of the most widely pursued goals in dialogue system development is the improvement of usability, which is mainly achieved by providing users with both friendly and manageable interfaces. The MIMUS dialogue system supports multimodal interactions, allowing the user to interact not only verbally but also graphically. In addition to this, MIMUS (MultIModal, University of Seville) is a multilingual system that enables the user to communicate (dynamically) in Spanish and English. The present paper describes the MIMUS architecture, the components that entail direct relationship with multilinguality, and the way in which languages can be dynamically switched within a single dialogue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-578"
  },
  "loof07b_interspeech": {
   "authors": [
    [
     "Jonas",
     "Lööf"
    ],
    [
     "Christian",
     "Gollan"
    ],
    [
     "Stefan",
     "Hahn"
    ],
    [
     "Georg",
     "Heigold"
    ],
    [
     "B.",
     "Hoffmeister"
    ],
    [
     "Christian",
     "Plahl"
    ],
    [
     "David",
     "Rybach"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "The RWTH 2007 TC-STAR evaluation system for european English and Spanish",
   "original": "i07_2145",
   "page_count": 4,
   "order": 579,
   "p1": "2145",
   "pn": "2148",
   "abstract": [
    "In this work, the RWTH automatic speech recognition systems developed for the third TC-STAR evaluation campaign 2007 are presented. The RWTH systems make systematic use of internal system combination, combining systems with differences in feature extraction, adaptation methods, and training data used. To take advantage of this, novel feature extraction methods were employed; this year saw the introduction of Gammatone features and MLP based phone posterior features. Further improvements were achieved using unsupervised training, and it is notable that these improvements were achieved using a fairly low amount of automatically transcribed data. Also contributing to the improvements over last year was the switch to MPE training, and the introduction of projecting SAT transforms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-579"
  },
  "koh07_interspeech": {
   "authors": [
    [
     "Eugene Chin Wei",
     "Koh"
    ],
    [
     "Hanwu",
     "Sun"
    ],
    [
     "Tin Lay",
     "Nwe"
    ],
    [
     "Trung Hieu",
     "Nguyen"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Susanto",
     "Rahardja"
    ]
   ],
   "title": "Using direction of arrival estimate and acoustic feature information in speaker diarization",
   "original": "i07_2149",
   "page_count": 4,
   "order": 580,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "This paper describes the I2R/NTU system submitted for the NIST Rich Transcription 2007 (RT-07) Meeting Recognition evaluation Multiple Distant Microphone (MDM) task. In our implementation, the Direction of Arrival (DOA) information is specifically used to perform speaker turn detection and clustering. Cluster purification is then carried out by performing GMM modeling on acoustic features. Finally, non-speech & silence removal is effected to remove unwanted segments. The system achieved an overall DER of 31.02% on the NIST Rich Transcription Spring 2006 evaluation tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-580"
  },
  "batista07_interspeech": {
   "authors": [
    [
     "Fernando",
     "Batista"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Nuno",
     "Mamede"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Recovering punctuation marks for automatic speech recognition",
   "original": "i07_2153",
   "page_count": 4,
   "order": 581,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "This paper shows results of recovering punctuation over speech transcriptions for a Portuguese broadcast news corpus. The approach is based on maximum entropy models and uses word, part-of-speech, time and speaker information. The contribution of each type of feature is analyzed individually. Separate results for each focus condition are given, making it possible to analyze the differences of performance between planned and spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-581"
  },
  "yeh07_interspeech": {
   "authors": [
    [
     "Jui-Feng",
     "Yeh"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Wei-Yen",
     "Wu"
    ]
   ],
   "title": "Disfluency correction of spontaneous speech using conditional random fields with variable-length features",
   "original": "i07_2157",
   "page_count": 4,
   "order": 582,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "This paper presents an approach to detecting and correcting edit disfluency based on conditional random fields with variable-length features. The variable-length features consist of word, chunk and sentence features. Conditional random fields (CRF) are adopted to model the properties of the edit disfluency, including repair, repetition and restart, for edit disfluency detection. For the evaluation of the proposed method, Mandarin conversational dialogue corpus (MCDC) is used. The detection error rate of edit word is 17.3%. Compared with DF-gram, Maximum Entropy and the approach combining language model and alignment model, the proposed approach achieves 11.7%, 8% and 3.9% improvements, respectively. The experimental results show that the proposed model outperforms other methods and efficiently detects and corrects edit disfluency in spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-582"
  },
  "huang07c_interspeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Vit",
     "Libal"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ]
   ],
   "title": "Detection, diarization, and transcription of far-field lecture speech",
   "original": "i07_2161",
   "page_count": 4,
   "order": 583,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "Speech processing of lectures recorded inside smart rooms has recently attracted much interest. In particular, the topic has been central to the Rich Transcription (RT) Meeting Recognition Evaluation campaign series, sponsored by NIST, with emphasis placed on benchmarking speech activity detection (SAD), speaker diarization (SPKR), speech-to-text (STT), and speaker-attributed STT (SASTT) technologies. In this paper, we present the IBM systems developed to address these tasks in preparation for the RT 2007 evaluation, focusing on the far-field condition of lecture data collected as part of European project CHIL. For their development, the systems are benchmarked on a subset of the RT Spring 2006 (RT06s) evaluation test set, where they yield significant improvements for all SAD, SPKR, and STT tasks over RT06s results; for example, a 16% relative reduction in word error rate is reported in STT, attributed to a number of system advances discussed here. Initial results are also presented on SASTT, a task newly introduced in 2007 in place of the discontinued SAD.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-583"
  },
  "hazen07c_interspeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Brennan",
     "Sherry"
    ],
    [
     "Mark",
     "Adler"
    ]
   ],
   "title": "Speech-based annotation and retrieval of digital photographs",
   "original": "i07_2165",
   "page_count": 4,
   "order": 584,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "In this paper we describe the development of a speech-based annotation and retrieval system for digital photographs. The system uses a client/server architecture which allows photographs to be captured and annotated on light-weight clients, such as mobile camera phones, and then processed, indexed and stored on networked servers. For speech-based retrieval we have developed a mixed grammar recognition approach which allows the speech recognition system to construct a single finite-state network combining context-free grammars, for recognizing and parsing query carrier phrases and metadata phrases, with an unconstrained statistical n-gram model for recognizing free-form search terms. Experiments demonstrating successful retrieval of photographs using purely speech-based annotation and retrieval are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-584"
  },
  "guz07_interspeech": {
   "authors": [
    [
     "Umit",
     "Guz"
    ],
    [
     "Sébastien",
     "Cuendet"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "Co-training using prosodic and lexical information for sentence segmentation",
   "original": "i07_2597",
   "page_count": 4,
   "order": 585,
   "p1": "2597",
   "pn": "2600",
   "abstract": [
    "We investigate the application of the co-training learning algorithm on the sentence boundary classification problem by using lexical and prosodic information. Co-training is a semi-supervised machine learning algorithm that uses multiple weak classifiers with a relatively small amount of labeled data and incrementally uses unlabeled data. The assumption in co-training is that the classifiers can co-train each other, as one can label samples that are difficult for the other. The sentence segmentation problem is very appropriate for the co-training method since it satisfies the main requirements of the co-training algorithm: the dataset can be described by two disjoint and natural views that are redundantly sufficient. In our case, the feature sets are capturing lexical and prosodic information. The experimental results on the ICSI Meeting (MRDA) corpus show the effectiveness of the co-training algorithm for this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-585"
  },
  "esteve07_interspeech": {
   "authors": [
    [
     "Yannick",
     "Estève"
    ],
    [
     "Sylvain",
     "Meignier"
    ],
    [
     "Paul",
     "Deléglise"
    ],
    [
     "Julie",
     "Mauclair"
    ]
   ],
   "title": "Extracting true speaker identities from transcriptions",
   "original": "i07_2601",
   "page_count": 4,
   "order": 586,
   "p1": "2601",
   "pn": "2604",
   "abstract": [
    "Automatic speaker diarization generally produces a generic label such a spkr1 rather than the true identity of the speaker. Recently, two approaches based on lexical rules were proposed to extract the true identity of the speaker from the transcriptions of the audio recording without any a priori acoustic information: one uses n-gram, the other one uses semantic classification trees (SCT). The latter was proposed by the authors of this paper. In this paper, the two methods are compared in experiments carried out on French broadcast news records from the ESTER 2005 evaluation campaign. Experiments are processed on manual and automatic transcriptions. On manual transcriptions, the n-gram-based approach can be more precise, but the automatic transcriptions, the SCT-based approach gives significantly the best results in terms of recall and precision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-586"
  },
  "fu07_interspeech": {
   "authors": [
    [
     "Rong",
     "Fu"
    ],
    [
     "Ian D.",
     "Benest"
    ]
   ],
   "title": "An improved speaker diarization system",
   "original": "i07_2605",
   "page_count": 4,
   "order": 587,
   "p1": "2605",
   "pn": "2608",
   "abstract": [
    "This paper describes an automatic speaker diarization system for natural, multi-speaker meeting conversations. Only one central microphone is used to record the meeting. The new system is robust to different acoustic environments - it requires neither pre-training models nor development sets to initialize the parameters. The new system determines the model complexity automatically. It adapts the segment model from a universal background model, and uses the cross-likelihood ratio instead of the Bayesian Information Criterion (BIC) for merging. Finally it uses an intra-cluster/inter-cluster ratio as the stopping criterion. Together this reduces the speaker diarization error rate from 21.76% to 17.21% compared with the baseline system [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-587"
  },
  "stuker07_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Christian",
     "Fügen"
    ],
    [
     "Florian",
     "Kraft"
    ],
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "The ISL 2007 English speech transcription system for european parliament speeches",
   "original": "i07_2609",
   "page_count": 4,
   "order": 588,
   "p1": "2609",
   "pn": "2612",
   "abstract": [
    "The project Technology and Corpora for Speech to Speech Translation (TC-STAR) aims at making a break-through in speech-to-speech translation research, significantly reducing the gap between the performance of machines and humans at this task. Technological and scientific progress is driven by periodic, competitive evaluations within the project. In this paper we describe the ISL speech transcription system for English European Parliament speeches with which we participated in the third TC-STAR evaluation campaign in the spring of 2007. The improvements over last year's system originate from a recognition hypotheses based segmentation, the utilization of unsupervised in-domain training material, a modified cross-system adaptation and combination scheme, and the enhancement of the language model through the use of web based training material.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-588"
  },
  "hwang07_interspeech": {
   "authors": [
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Xin",
     "Lei"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Ozgur",
     "Cetin"
    ],
    [
     "Gang",
     "Peng"
    ]
   ],
   "title": "Advances in Mandarin broadcast speech recognition",
   "original": "i07_2613",
   "page_count": 4,
   "order": 589,
   "p1": "2613",
   "pn": "2616",
   "abstract": [
    "We describe our continuing efforts to improve the UW-SRI-ICSI Mandarin broadcast speech recognizer. This includes increasing acoustic and text training data, adding discriminative features, incorporating frame-level discriminative training criterion, multiple-pass acoustic model (AM) cross adaptation, language model (LM) genre adaptation and system combination. The net effect without LM adaptation was a 24%-64% relative reduction in character error rates (CERs) on a variety of test sets. In addition, LM adaptation gave us another 6% of relative CER reduction on broadcast conversations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-589"
  },
  "ogata07_interspeech": {
   "authors": [
    [
     "Jun",
     "Ogata"
    ],
    [
     "Masataka",
     "Goto"
    ],
    [
     "Kouichirou",
     "Eto"
    ]
   ],
   "title": "Automatic transcription for a web 2.0 service to search podcasts",
   "original": "i07_2617",
   "page_count": 4,
   "order": 590,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "This paper describes speech recognition techniques that enable a Web 2.0 service \"PodCastle\" where users can search and read transcribed texts of podcasts, and correct recognition errors in those texts. Most previous speech recognizers had difficulties transcribing podcasts because podcasts include various kinds of contents recorded in different conditions and cover recent topics that tend to have many out-of-vocabulary words. To overcome such difficulties, we continuously improve speech recognizers by using information aggregated on the basis of Web 2.0. For example, a language model is adapted to a topic of the target podcast on the fly, the pronunciations of out-of-vocabulary words are obtained from a Web 2.0 service, and an acoustic model is trained by using the results of the error correction by anonymous users. The experiments we report in this paper show that our techniques produce promising results for podcasts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-590"
  },
  "tepperman07_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A text-free approach to assessing nonnative intonation",
   "original": "i07_2169",
   "page_count": 4,
   "order": 591,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "To compensate for the variability in native English intonation and the unpredictability of nonnative speech, we propose a new method of assessing nonnative intonation without any prior knowledge of the target text or phonetics. After recognition of tone events with HMMs and a bigram model of intonation, we define an utterance's automatic intonation score as the mean of the posterior probabilities for all recognized tone segments. On the ISLE corpus of learners' English, we find intonation scores generated by this technique have a 0.331 correlation with general pronunciation scores determined by native listeners. In comparison, the SRI Eduspeak system's proposal for pronunciation scoring based on suprasegmental features derived from prior knowledge of the target text yields a 0.247 correlation with listener scores on a similar corpus. Because it is text-free, our approach could be used to assess intonation outside of a strictly educational application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-591"
  },
  "lee07h_interspeech": {
   "authors": [
    [
     "John",
     "Lee"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Automatic generation of cloze items for prepositions",
   "original": "i07_2173",
   "page_count": 4,
   "order": 592,
   "p1": "2173",
   "pn": "2176",
   "abstract": [
    "Fill-in-the-blank questions, or cloze items, are commonly used in language learning applications. The benefits of personalized items, tailored to the user's interest and proficiency, have motivated research on automatic generation of cloze items. This paper is concerned with generating cloze items for prepositions, whose usage often poses problems for non-native speakers of English.\n",
    "The quality of a cloze item depends on the choice of distractors. We propose two methods, based on collocations and on non-native English corpora, to generate distractors for prepositions. Both methods are found to be more successful in attracting users than a baseline that relies only on word frequency, a common criterion in past research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-592"
  },
  "waple07_interspeech": {
   "authors": [
    [
     "Christopher",
     "Waple"
    ],
    [
     "Hongcui",
     "Wang"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Yasushi",
     "Tsubota"
    ],
    [
     "Masatake",
     "Dantsuji"
    ]
   ],
   "title": "Evaluating and optimizing Japanese tutor system featuring dynamic question generation and interactive guidance",
   "original": "i07_2177",
   "page_count": 4,
   "order": 593,
   "p1": "2177",
   "pn": "2180",
   "abstract": [
    "We are developing a new CALL system to aid students learning Japanese as a second language. This system is designed to allow students to create their own sentences based on visual prompts, receiving feedback based on their mistakes. The questions are dynamically generated, resulting in a large variety of challenges. The students may choose to receive guidance in order to complete each task, selecting the level of help that best suits their needs. A scoring system is also incorporated, which awards a grade to students based on the errors made and hints used. The trial of the system has been conducted with twenty one students, providing the statistics of actual errors and hint usages. With these data, we have trained the weights of the scoring system by taking into account the impact of each issue on the proficiency of the students. The validity of the estimated score is generally confirmed by predicting the proficiency of the students.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-593"
  },
  "cucchiarini07_interspeech": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Ambra",
     "Neri"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "ASR-based pronunciation training: scoring accuracy and pedagogical effectiveness of a system for dutch L2 learners",
   "original": "i07_2181",
   "page_count": 4,
   "order": 594,
   "p1": "2181",
   "pn": "2184",
   "abstract": [
    "A system for providing Computer Assisted Pronunciation Training for Dutch was developed, Dutch-CAPT, which appeared to be effective in improving pronunciation quality of L2 learners of Dutch. In this paper we describe the architecture of the system paying particular attention to the rationale behind this system, to the performance of the error detection algorithm and its relationship to the pedagogical effectiveness of the corrective feedback provided.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-594"
  },
  "tepperman07b_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Matthew",
     "Black"
    ],
    [
     "Patti",
     "Price"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Matteo",
     "Gerosa"
    ],
    [
     "Margaret",
     "Heritage"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A Bayesian network classifier for word-level reading assessment",
   "original": "i07_2185",
   "page_count": 4,
   "order": 595,
   "p1": "2185",
   "pn": "2188",
   "abstract": [
    "To automatically assess young children's reading skills as demonstrated by isolated words read aloud, we propose a novel structure for a Bayesian Network classifier. Our network models the generative story among speech recognition-based features, treating pronunciation variants and reading mistakes as distinct but not independent cues to a qualitative perception of reading ability. This Bayesian approach allows us to estimate the probabilistic dependencies among many highly-correlated features, and to calculate soft decision scores based on the posterior probabilities for each class. With all proposed features, the best version of our network outperforms the C4.5 decision tree classifier by 17% and a Naive Bayes classifier by 8%, in terms of correlation with speaker-level reading scores on the Tball data set. This best correlation of 0.92 approaches the expert inter-evaluator correlation, 0.95.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-595"
  },
  "holzapfel07_interspeech": {
   "authors": [
    [
     "Hartwig",
     "Holzapfel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Behavior models for learning and receptionist dialogs",
   "original": "i07_2189",
   "page_count": 4,
   "order": 596,
   "p1": "2189",
   "pn": "2192",
   "abstract": [
    "We present a dialog model for identifying persons, learning person names, and associated face IDs in a receptionist dialog. The proposed model allows a decomposition of the main dialog task into separate dialog behaviors which can be implemented separately and allow a mixture of handcrafted models and dialog strategies trained with reinforcement learning. The dialog model was implemented on our robot and tested in a number of experiments in a receptionist task. A Wizard-of-Oz experiment is used to evaluate the dialog structure, delivers information for the definition of metrics, and delivers a data corpus which is used to train a user simulation and component error model. Using these models we train a dialog module for learning a person's name with reinforcement learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-596"
  },
  "turunen07_interspeech": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Anssi",
     "Kainulainen"
    ],
    [
     "Aleksi",
     "Melto"
    ],
    [
     "Topi",
     "Hurtig"
    ]
   ],
   "title": "Design of a rich multimodal interface for mobile spoken route guidance",
   "original": "i07_2193",
   "page_count": 4,
   "order": 597,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "We present a design of a rich multimodal interface for mobile route guidance. The application provides public transport information in Finland, including support for pedestrian guidance when the user is changing between the means of transportation. The range of input and output modalities include speech synthesis, speech recognition, a fisheye GUI, haptics, contextual text input, physical browsing, physical gestures, non-speech audio, and global positioning information. Together, these modalities provide an interface that is accessible for a wide range of users including persons with various levels of visual impairment. In this paper we describe the functional aspects and the design of the interface of our publicly available prototype system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-597"
  },
  "theune07_interspeech": {
   "authors": [
    [
     "Mariët",
     "Theune"
    ],
    [
     "Dennis",
     "Hofs"
    ],
    [
     "Marco van",
     "Kessel"
    ]
   ],
   "title": "The virtual guide: a direction giving embodied conversational agent",
   "original": "i07_2197",
   "page_count": 4,
   "order": 598,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "We present the Virtual Guide, an embodied conversational agent that can give directions in a 3D virtual environment. We discuss how dialogue management, language generation and the generation of appropriate gestures are carried out in our system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-598"
  },
  "gandhe07_interspeech": {
   "authors": [
    [
     "Sudeep",
     "Gandhe"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "Creating spoken dialogue characters from corpora without annotations",
   "original": "i07_2201",
   "page_count": 4,
   "order": 599,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "Virtual humans are being used in a number of applications, including simulation-based training, multi-player games, and museum kiosks. Natural language dialogue capabilities are an essential part of their human-like persona. These dialogue systems have a goal of being believable and generally have to operate within the bounds of their restricted domains. Most dialogue systems operate on a dialogue-act level and require extensive annotation efforts. Semantic annotation and rule authoring have long been known as bottlenecks for developing dialogue systems for new domains. In this paper, we investigate several dialogue models for virtual humans that are trained on an unannotated human-human corpus. These are inspired by information retrieval and work on the surface text level. We evaluate these in text-based and spoken interactions and also against the upper baseline of human-human dialogues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-599"
  },
  "hui07_interspeech": {
   "authors": [
    [
     "Pui-Yu",
     "Hui"
    ],
    [
     "Zhengyu",
     "Zhou"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Complementarity and redundancy in multimodal user inputs with speech and pen gestures",
   "original": "i07_2205",
   "page_count": 4,
   "order": 600,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "We present a comparative analysis of multi-modal user inputs with speech and pen gestures, together with their semantically equivalent uni-modal (speech only) counterparts. The multimodal interactions are derived from a corpus collected with a Pocket PC emulator in the context of navigation around Beijing. We devise a cross-modality integration methodology that interprets a multi-modal input and paraphrases it as a semantically equivalent, uni-modal input. Thus we generate parallel multimodal (MM) and unimodal (UM) corpora for comparative study. Empirical analysis based on class trigram perplexities shows two categories of data: (PPMM = PPUM) and (PPMM UM). The former involves complementarity across modalities in expressing the user's intent, including occurrences of ellipses. The latter involves redundancy, which will be useful for handling recognition errors by exploring mutual reinforcements. We present explanatory examples of data in these two categories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-600"
  },
  "bell07b_interspeech": {
   "authors": [
    [
     "Linda",
     "Bell"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Children's convergence in referring expressions to graphical objects in a speech-enabled computer game",
   "original": "i07_2209",
   "page_count": 4,
   "order": 601,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "This paper describes an empirical study of children's spontaneous interactions with an animated character in a speech-enabled computer game. More specifically, it deals with convergence of referring expressions. 49 children were invited to play the game, which was initiated by a collaborative \"put-that-there\" task. In order to solve this task, the children had to refer to both physical objects and icons in a 3D environment. For physical objects, which were mostly referred to using straight-forward noun phrases, lexical convergence took place in 90% of all cases. In the case of the icons, the children were more innovative and spontaneously referred to them in many different ways. Even after being prompted by the system, lexical convergence took place for only 50% of the icons. In the cases where convergence did take place, the effect of the system's prompts were quite local, and the children quickly resorted to their original way of referring when naming new icons in later tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-601"
  },
  "kawatsu07_interspeech": {
   "authors": [
    [
     "Hiromi",
     "Kawatsu"
    ],
    [
     "Sumio",
     "Ohno"
    ]
   ],
   "title": "An analysis of individual differences in the f<sub>0</sub> contour and the duration of anger utterances at several degrees",
   "original": "i07_2213",
   "page_count": 4,
   "order": 602,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "Taking up anger emotion expressed by speech, prosodic features were analyzed in order to find out the relationship between the degree of anger and manifestations on the speech signal in terms of individual differences. As a result of analysis, there were some common features among the speakers, although there were some speaker-dependent features. About the baseline frequency and the magnitude of the first phrase command, common tendencies were found in all speakers. The amplitude of the accent command increases as the emotional degree increases on the whole. Some speakers emphasized accent commands at all positions within a sentence, some emphasized only near the end of a sentence. Speaking rate at the 1st and 4th phrases were faster than those at the 2nd and 3rd phrases for the utterance with emotion, although there was an individual difference in the effect of the emotional degree. It is very interesting that two aspects in prosody, i.e., an F0 contour and a speaking rate, might be complement each other in order to represent a difference of emotional degrees.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-602"
  },
  "arimoto07_interspeech": {
   "authors": [
    [
     "Yoshiko",
     "Arimoto"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "Acoustic features of anger utterances during natural dialog",
   "original": "i07_2217",
   "page_count": 4,
   "order": 603,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "This report focuses on an automatic estimation of speakers' anger emotion degree. Two kinds of pseudo-dialogs were held to collect spontaneous anger utterances during the natural Japanese dialog. In order to quantify the anger degree of utterances, a six-scale subjective evaluation was conducted to grade every utterance according to an anger emotion degree by twelve evaluators. With this data set, acoustic features of each utterance were examined to clarify what is the clue to estimate degree of anger utterances. To examine the possibility of automatic emotion estimation, we conducted experiment to estimate the degree of anger emotion automatically by multiple regression analysis using the acoustic parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-603"
  },
  "biadsy07_interspeech": {
   "authors": [
    [
     "Fadi",
     "Biadsy"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Wisam",
     "Dakka"
    ]
   ],
   "title": "Comparing american and palestinian perceptions of charisma using acoustic-prosodic and lexical analysis",
   "original": "i07_2221",
   "page_count": 4,
   "order": 604,
   "p1": "2221",
   "pn": "2224",
   "abstract": [
    "Charisma, the ability to lead by virtue of personality alone, is difficult to define but relatively easy to identify. However, cultural factors clearly affect perceptions of charisma. In this paper we compare results from parallel perception studies investigating charismatic speech in Palestinian Arabic and American English. We examine acoustic/prosodic and lexical correlates of charisma ratings to determine how the two cultures differ with respect to their views of charismatic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-604"
  },
  "busso07_interspeech": {
   "authors": [
    [
     "Carlos",
     "Busso"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Using neutral speech models for emotional speech analysis",
   "original": "i07_2225",
   "page_count": 4,
   "order": 605,
   "p1": "2225",
   "pn": "2228",
   "abstract": [
    "Since emotional speech can be regarded as a variation on neutral (non-emotional) speech, it is expected that a robust neutral speech model can be useful in contrasting different emotions expressed in speech. This study explores this idea by creating acoustic models trained with spectral features, using the emotionally-neutral TIMIT corpus. The performance is tested with two emotional speech databases: one recorded with a microphone (acted), and another recorded from a telephone application (spontaneous). It is found that accuracy up to 78% and 65% can be achieved in the binary and category emotion discriminations, respectively. Raw Mel Filter Bank (MFB) output was found to perform better than conventional MFCC, with both broad-band and telephone-band speech. These results suggest that well-trained neutral acoustic models can be effectively used as a front-end for emotion recognition, and once trained with MFB, it may reasonably work well regardless of the channel characteristics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-605"
  },
  "satoh07_interspeech": {
   "authors": [
    [
     "N.",
     "Satoh"
    ],
    [
     "K.",
     "Yamauchi"
    ],
    [
     "S.",
     "Matsunaga"
    ],
    [
     "M.",
     "Yamashita"
    ],
    [
     "R.",
     "Nakagawa"
    ],
    [
     "K.",
     "Shinohara"
    ]
   ],
   "title": "Emotion clustering using the results of subjective opinion tests for emotion recognition in infants' cries",
   "original": "i07_2229",
   "page_count": 4,
   "order": 606,
   "p1": "2229",
   "pn": "2232",
   "abstract": [
    "This paper proposes an emotion clustering procedure for emotion detection in infants' cries. Our clustering procedure is performed using the results of subjective opinion tests regarding the emotions expressed in infants' cries. Through the procedure, we obtain a tree data structure of emotion clusters that are generated by the progressive merging of emotions. Emotion merging is carried out on the condition that the objective function concerning the ambiguity of emotions that were detected in the opinion tests is minimized. Clustering experiments are performed on the results of opinion tests completed by infants' mothers and baby-rearing experts. The experimental results show that the proposed clustering, which considers the evaluation rank of each emotion, is superior to the clustering that is only concerned with the detection/nondetection of each emotion. Based on the clustering results, we performed a preliminary recognition experiment on two emotion clusters. According to the recognition results, the proposed emotion cluster achieves a detection rate of 75%, which shows the effectiveness of the proposed clustering procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-606"
  },
  "barra07_interspeech": {
   "authors": [
    [
     "R.",
     "Barra"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "J.",
     "Macias-Guarasa"
    ],
    [
     "J.",
     "Gutiérrez-Arriola"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "J. M.",
     "Pardo"
    ]
   ],
   "title": "On the limitations of voice conversion techniques in emotion identification tasks",
   "original": "i07_2233",
   "page_count": 4,
   "order": 607,
   "p1": "2233",
   "pn": "2236",
   "abstract": [
    "The growing interest in emotional speech synthesis urges effective emotion conversion techniques to be explored. This paper estimates the relevance of three speech components (spectral envelope, residual excitation and prosody) for synthesizing identifiable emotional speech, in order to be able to customize voice conversion techniques to the specific characteristics of each emotion. The analysis has been based on a listening test with a set of synthetic mixed-emotion utterances that draw their speech components from emotional and neutral recordings. Results prove the importance of transforming residual excitation for the identification of emotions that are not fully conveyed through prosodic means (such as cold anger or sadness in our Spanish corpus).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-607"
  },
  "dupuis07_interspeech": {
   "authors": [
    [
     "Kate",
     "Dupuis"
    ],
    [
     "Kathleen",
     "Pichora-Fuller"
    ]
   ],
   "title": "Use of lexical and affective prosodic cues to emotion by younger and older adults",
   "original": "i07_2237",
   "page_count": 4,
   "order": 608,
   "p1": "2237",
   "pn": "2240",
   "abstract": [
    "Older adults often report that, although they are able to hear conversations, they have difficulty attending to or understanding what is being said. Interactions between cognitive and perceptual processing are necessary for comprehension. It is possible that older adults have difficulty determining what type of emotional information is represented by the affective tone in which speech is spoken. Two studies were conducted using sentences to examine the use of lexical and affective prosodic cues to emotion by younger and older adults. The present studies are the first steps in a research programme concerning the effect of age on perceptual and cognitive interactions during comprehension of affective prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-608"
  },
  "gupta07_interspeech": {
   "authors": [
    [
     "Purnima",
     "Gupta"
    ],
    [
     "Nitendra",
     "Rajput"
    ]
   ],
   "title": "Two-stream emotion recognition for call center monitoring",
   "original": "i07_2241",
   "page_count": 4,
   "order": 609,
   "p1": "2241",
   "pn": "2244",
   "abstract": [
    "We present a technique for two-stream processing of speech signals for emotion detection. The first stream recognises emotion from acoustic features while the second stream recognises emotion from the semantics of the conversation. A probabilistic measure is derived for each of the individual streams and the emotion category from the two streams is recognised. The output of the two streams is combined to generate a score for a particular emotion category. The confidence level of each stream is used to weigh the scores from the two streams while generating the final score. This technique is extremely significant for call-center data that have some semantics associated with the speech.\n",
    "The proposed technique is evaluated on the LDC corpus and on the real-word call-center data. Experiments suggest that use of a two-stream process provides better results than the existing techniques of extracting emotion only from acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-609"
  },
  "grichkovtsova07_interspeech": {
   "authors": [
    [
     "Ioulia",
     "Grichkovtsova"
    ],
    [
     "Anne",
     "Lacheret"
    ],
    [
     "Michel",
     "Morel"
    ]
   ],
   "title": "The role of intonation and voice quality in the affective speech perception",
   "original": "i07_2245",
   "page_count": 4,
   "order": 610,
   "p1": "2245",
   "pn": "2248",
   "abstract": [
    "The perception value of intonation and voice quality is investigated for six affective states: anger, sadness, happiness, obviousness, doubt and irony. The main research question is whether the role of intonation and voice quality is equally important in the perception of studied affective states or whether one of them may be privileged. Six affective states were tested on utterances with natural lexical meaning. The transplantation paradigm was used in designing audio stimuli. Perception results show that each studied affective state has its own usage of prosody and voice quality. Some differences were found in the identification of emotions and attitudes. New questions risen from the present study and further directions of work are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-610"
  },
  "vlasenko07_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Vlasenko"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Andreas",
     "Wendemuth"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Combining frame and turn-level information for robust recognition of emotions within speech",
   "original": "i07_2249",
   "page_count": 4,
   "order": 611,
   "p1": "2249",
   "pn": "2252",
   "abstract": [
    "Current approaches to the recognition of emotion within speech usually use statistic feature information obtained by application of functionals on turn- or chunk levels. Yet, it is well known that thereby important information on temporal sub-layers as the frame-level is lost. We therefore investigate the benefits of integration of such information within turn-level feature space. For frame-level analysis we use GMM for classification and 39 MFCC and energy features with CMS. In a subsequent step output scores are fed forward into a 1.4k large-feature-space turn-level SVM emotion recognition engine. Thereby we use a variety of Low-Level-Descriptors and functionals to cover prosodic, speech quality, and articulatory aspects. Extensive test-runs are carried out on the public databases EMO-DB and SUSAS. Speaker-independent analysis is faced by speaker normalization. Overall results highly emphasize the benefits of feature integration on diverse time scales.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-611"
  },
  "schuller07_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Dino",
     "Seppi"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Thurid",
     "Vogt"
    ],
    [
     "Johannes",
     "Wagner"
    ],
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Laurence",
     "Vidrascu"
    ],
    [
     "Noam",
     "Amir"
    ],
    [
     "Loic",
     "Kessous"
    ],
    [
     "Vered",
     "Aharonson"
    ]
   ],
   "title": "The relevance of feature type for the automatic classification of emotional user states: low level descriptors and functionals",
   "original": "i07_2253",
   "page_count": 4,
   "order": 612,
   "p1": "2253",
   "pn": "2256",
   "abstract": [
    "In this paper, we report on classification results for emotional user states (4 classes, German database of children interacting with a pet robot). Six sites computed acoustic and linguistic features independently from each other, following in part different strategies. A total of 4244 features were pooled together and grouped into 12 low level descriptor types and 6 functional types. For each of these groups, classification results using Support Vector Machines and Random Forests are reported for the full set of features, and for 150 features each with the highest individual Information Gain Ratio. The performance for the different groups varies mostly between ≈ 50% and ≈ 60%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-612"
  },
  "quang07_interspeech": {
   "authors": [
    [
     "Vũ Minh",
     "Quang"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Eric",
     "Castelli"
    ]
   ],
   "title": "Automatic question detection: prosodic-lexical features and crosslingual experiments",
   "original": "i07_2257",
   "page_count": 4,
   "order": 613,
   "p1": "2257",
   "pn": "2260",
   "abstract": [
    "In this paper, we present our work on automatic question detection from the speech signal. We are interested in developing automatic detection system and investigate the portability of such system to a new language. The first goal of this paper is to propose and evaluate a combined approach for automatic question detection where prosodic features are augmented by the use of lexical features. It is shown that both early and late integration of theses features in a decision tree-based classifier improves the question detection performance compared to a baseline system using prosodic features only. The second goal of this paper is to conduct a crosslingual (French / Vietnamese) evaluation concerning the use of prosodic features. It is shown that our first system developed for French which uses an initial prosodic feature set can be improved using a new feature set that takes into account some specific prosodic characteristics of the Vietnamese tonal language. Both Vietnamese and French question detection systems obtain F-ratio performance around 80% on pre-segmented meeting and dialog utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-613"
  },
  "tachibana07b_interspeech": {
   "authors": [
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Keigo",
     "Kawashima"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Performance evaluation of HMM-based style classification with a small amount of training data",
   "original": "i07_2261",
   "page_count": 4,
   "order": 614,
   "p1": "2261",
   "pn": "2264",
   "abstract": [
    "This paper describes a classification technique for emotional expressions and speaking styles of speech using only a small amount of training data of a target speaker. We model spectral and fundamental frequency (F0) features simultaneously using multi-space probability distribution HMM (MSD-HMM), and adapt a speaker-independent neutral style model to a certain target speaker's style model with a small amount of data using MSD-MLLR which is extended MLLR for MSD-HMM. We perform classification experiments for professional narrators' speech and non-professional speakers' speech and evaluate the performance of proposed technique by comparing with other commonly used classifiers. We show that the proposed technique gives better result than the other classifiers when using a few sentences of target speaker's style data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-614"
  },
  "truong07_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Visualizing acoustic similarities between emotions in speech: an acoustic map of emotions",
   "original": "i07_2265",
   "page_count": 4,
   "order": 615,
   "p1": "2265",
   "pn": "2268",
   "abstract": [
    "In this paper, we introduce a visual analysis method to assess the discriminability and confusability between emotions according to automatic emotion classifiers. The degree of acoustic similarities between emotions can be defined in terms of distances that are based on pair-wise emotion discrimination experiments. By employing Multidimensional Scaling, the discriminability between emotions can then be visualized in a two-dimensional plot that is relatively easy to interpret. This ‘map of emotions’ is compared to the well-known ‘Feeltrace’ two-dimensional mapping of emotions. While there is correlation with the ‘arousal’ dimension of Feeltrace, it appears that the ‘valence’ dimension is difficult to relate to the acoustic map.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-615"
  },
  "hu07c_interspeech": {
   "authors": [
    [
     "Hao",
     "Hu"
    ],
    [
     "Ming-Xing",
     "Xu"
    ],
    [
     "Wei",
     "Wu"
    ]
   ],
   "title": "Fusion of global statistical and segmental spectral features for speech emotion recognition",
   "original": "i07_2269",
   "page_count": 4,
   "order": 616,
   "p1": "2269",
   "pn": "2272",
   "abstract": [
    "Speech emotion recognition is an interesting and challenging speech technology, which can be applied to broad areas. In this paper, we propose to fuse the global statistical and segmental spectral features at the decision level for speech emotion recognition. Each emotional utterance is individually scored by two recognition systems, the global statistics-based and segmental spectrum-based systems, and a weighted linear combination is applied to fuse their scores for final decision. Experimental results on an emotional speech database demonstrate that the global statistical and segmental spectral features are complementary, and the proposed fusion approach further improves the performance of the emotion recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-616"
  },
  "sethu07_interspeech": {
   "authors": [
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Julien",
     "Epps"
    ]
   ],
   "title": "Group delay features for emotion detection",
   "original": "i07_2273",
   "page_count": 4,
   "order": 617,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "This paper focuses on speech based emotion classification utilizing acoustic data. The most commonly used acoustic features are pitch and energy, along with prosodic information like the rate of speech. We propose the use of a novel feature based on the phase response of an all-pole model of the vocal tract obtained from linear predictive coefficients (LPC), in addition to the aforementioned features. We compare this feature to other commonly used acoustic features based on classification accuracy. The back-end of our system employs a probabilistic neural network based classifier. Evaluations conducted on the LDC Emotional Prosody speech corpus indicate the proposed features are well suited to the task of emotion classification. The proposed features are able to provide a relative increase in classification accuracy of about 14% over established features when combined with them to form a larger feature vector.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-617"
  },
  "muller07_interspeech": {
   "authors": [
    [
     "Christian",
     "Müller"
    ],
    [
     "Felix",
     "Burkhardt"
    ]
   ],
   "title": "Combining short-term cepstral and long-term pitch features for automatic recognition of speaker age",
   "original": "i07_2277",
   "page_count": 4,
   "order": 618,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "The most successful systems in previous comparative studies on speaker age recognition used short-term cepstral features modeled with Gaussian Mixture Models (GMMs) or applied multiple phone recognizers trained with the data of speakers of the respective class. Acoustic analyses, however, indicate that certain features such as pitch extracted from a longer span of speech correlate clearly with the speaker age although the systems based on those features have been inferior to the before mentioned approaches. In this paper, three novel systems combining short-term cepstral features and long-term features for speaker age recognition are compared to each other. A system combining GMMs using frame-based MFCCs and Support-Vector-Machines using long-term pitch performs best. The results indicate that the combination of the two feature types is a promising approach, which corresponds to findings in related fields like speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-618"
  },
  "enos07_interspeech": {
   "authors": [
    [
     "Frank",
     "Enos"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Detecting deception using critical segments",
   "original": "i07_2281",
   "page_count": 4,
   "order": 619,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "We present an investigation of segments that map to global lies, that is, the intent to deceive with respect to salient topics of the discourse. We propose that identifying the truth or falsity of these\n",
    "critical segments may be important in determining a speaker's veracity over the larger topic of discourse. Further, answers to key questions, which can be identified a priori, may represent emotional and cognitive\n",
    "hot spots, analogous to those observed by psychologists who study gestural and facial cues to deception. We present results of experiments that use two different definitions of critical segments and employ machine learning techniques that compensate for imbalances in the dataset. Using this approach, we achieve a performance gain of 23.8% relative to chance, in contrast with human performance on a similar task, which averages substantially below chance. We discuss the features used by the models, and consider how these findings can influence future research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-619"
  },
  "nose07_interspeech": {
   "authors": [
    [
     "Takashi",
     "Nose"
    ],
    [
     "Yoichi",
     "Kato"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Style estimation of speech based on multiple regression hidden semi-Markov model",
   "original": "i07_2285",
   "page_count": 4,
   "order": 620,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "This paper presents a technique for estimating the degree or intensity of emotional expressions and speaking styles appeared in speech. The key idea is based on a style control technique for speech synthesis using multiple regression hidden semi-Markov model (MRHSMM), and the proposed technique can be viewed as the inverse process of the style control. We derive an algorithm for estimating predictor variables of MRHSMM each of which represents a sort of emotion intensity or speaking style variability appeared in acoustic features based on an ML criterion. We also show preliminary experimental results to demonstrate an ability of the proposed technique for synthetic and acted speech samples with emotional expressions and speaking styles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-620"
  },
  "zhang07d_interspeech": {
   "authors": [
    [
     "Chi",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Analysis and classification of speech mode: whispered through shouted",
   "original": "i07_2289",
   "page_count": 4,
   "order": 621,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "Variation in vocal effort represents one of the most challenging problems in maintaining speech system performance for coding, speech and speaker recognition. Changes in vocal effort (or mode) result in a fundamental change in speech production which is not simply a change in volume. This is the first study to collectively consider the five speech modes: whispered, soft, neutral, loud and shouted. After corpus development, analysis is performed for i) sound intensity level, ii) duration and silence percentage, iii) frame energy distribution and iv) spectral tilt. The analysis shows vocal effort dependent traits which are used to investigate speaker recognition. Matched vocal mode conditions result in a closed-set speaker ID rate of 97.62%, with mismatch vocal conditions producing 54.02%. Finally, a speech mode classification system is developed, which has a range of classification rate from 44.5% to 98.5% confusing with adjacent vocal modes. These advancements can provide improved speech/speaker modeling information, as well as classified vocal mode knowledge to improve speech and language technology in real scenarios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-621"
  },
  "bettonitechio07_interspeech": {
   "authors": [
    [
     "Melissa",
     "Bettoni-Techio"
    ],
    [
     "Andréia S.",
     "Rauber"
    ],
    [
     "Rosana Denise",
     "Koerich"
    ]
   ],
   "title": "Perception and production of word-final alveolar stops by brazilian portuguese learners of English",
   "original": "i07_2293",
   "page_count": 4,
   "order": 622,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "This paper focuses on the perception and production of the English alveolar stops (/t/ and /d/) in syllable coda by Brazilian learners of English. In the production test, the participants read a list of English sentences containing alveolar stops in word-final position. The preceding and following phonological contexts were controlled, so that the effect of context on the production of the alveolar stops could be analyzed. The perception test consisted of an oddity discrimination task where the target obstruents were either unreleased, aspirated, palatalized or produced with a paragogic vowel. The results of the production investigation show that the learners tended to unrelease the obstruents, but several tokens were aspirated, palatalized or produced with a paragogic vowel. As regards the relationship between perception and production of word-final /t/ and /d/, a positive correlation was found between the discrimination and production rates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-622"
  },
  "kluge07_interspeech": {
   "authors": [
    [
     "Denise Cristina",
     "Kluge"
    ],
    [
     "Andréia S.",
     "Rauber"
    ],
    [
     "Mara Silvia",
     "Reis"
    ],
    [
     "Ricardo A. Hoffmann",
     "Bion"
    ]
   ],
   "title": "The relationship between the perception and production of English nasal codas by brazilian learners of English",
   "original": "i07_2297",
   "page_count": 4,
   "order": 623,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "This study aims at investigating the perception and production of the English nasals /m/ and /n/ in syllable-final position by 20 Brazilian EFL learners and 3 native speakers of American English. Perception was assessed by means of both a discrimination and an identification test. Production data was collected by means of a Sentence Reading Test. The results from the perception tests revealed that the Brazilian learners and the native speakers seemed to have difficulties in distinguishing between the coda nasals, although to different degrees. Interestingly, the context of the preceding vowel influenced the perception of both natives and non-natives. The production results show that the participants had difficulty to produce the coda nasals. Concerning the relationship between perception and production, a positive correlation was found between the results of the Brazilian learners in the two perception tests and in the production test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-623"
  },
  "utashiro07_interspeech": {
   "authors": [
    [
     "Takafumi",
     "Utashiro"
    ],
    [
     "Goh",
     "Kawai"
    ]
   ],
   "title": "CALL courseware for learning reactive tokens in face-to-face dialogs",
   "original": "i07_2301",
   "page_count": 4,
   "order": 624,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "We developed courseware for teaching reactive tokens. Reactive tokens are audio or visual signals that structure discourse, and are sent by discourse participants who do not have the floor to those who do. Cultures with much in common sometimes differ markedly in their reactive tokens. Non-native language learners need to learn how to interpret and convey reactive tokens in order to be perceived as attentive, intelligent and polite. We believe reactive tokens are best learned when examples can be readily observed, practiced and critiqued. Our courseware combines computer-aided language learning (CALL) material for independent study, peer-based learning for production practice, and instructor-led learning for lectures and coaching. Experimental results show that some reactive tokens can be learned and retained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-624"
  },
  "kiriyama07_interspeech": {
   "authors": [
    [
     "Shinya",
     "Kiriyama"
    ],
    [
     "Ryo",
     "Tsuji"
    ],
    [
     "Tomohiko",
     "Kasami"
    ],
    [
     "Shogo",
     "Ishikawa"
    ],
    [
     "Naofumi",
     "Otani"
    ],
    [
     "Hiroaki",
     "Horiuchi"
    ],
    [
     "Yoichi",
     "Takebayashi"
    ],
    [
     "Shigeyoshi",
     "Kitazawa"
    ]
   ],
   "title": "The developmental analysis of demonstrative expression skills utilizing a multimodal infant behavior corpus",
   "original": "i07_2305",
   "page_count": 4,
   "order": 625,
   "p1": "2305",
   "pn": "2308",
   "abstract": [
    "We have succeeded to obtain the valuable findings about the developmental processes of demonstrative expression skills, which concern the fundamental human commonsense knowledge, such as to get an object and to catch someone's attention. We have already developed a framework to record genuine spontaneous speech of infants. We are constructing a multimodal infant behavior corpus, which enables us to elucidate human commonsense knowledge and its acquisition mechanism. Based on the observation utilizing the corpus, we proposed a multimodal behavior description model for the effective observation of demonstrative expressions. We proved that the proposed model has the nearly 90% coverage in an open test of the behavior description task. The analysis results using the model produced many valuable findings from multimodal viewpoints; for example, the change of ‘line of sight’ from ‘object to person’ to ‘person to object’ means that the infant has obtained a better way to catch someone's attention. Furthermore, the results of intention-based analysis provided us with an infant behavior model which is possible to apply to construct a behavior simulation system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-625"
  },
  "lyakso07_interspeech": {
   "authors": [
    [
     "Elena E.",
     "Lyakso"
    ],
    [
     "Olga V.",
     "Frolova"
    ]
   ],
   "title": "Russian vowels system acoustic features development in ontogenesis",
   "original": "i07_2309",
   "page_count": 4,
   "order": 626,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "These data form a part of a longitudinal study of vowel system formation for Russian language. The goal of this investigation is to examine the process of vowels spectral characteristics dynamics from child speech to the corresponding values in normal Russian adult speech. The vowels from vocalizations and words of 5 Russian children from 3 to 60 months of age were analyzed. The spectral features of the vowels recognized by native speakers as corresponding Russian phonemes were taken into account. It was shown that the formants characteristics do not correspond to those in adults' speech yet, but they differ from vowel to vowel at 3-5 years of age. Many sounds at each age are impossible to describe by means of classical acoustic keys, but native speakers attributed these sounds to certain categories with high probability. To describe these sounds we use an additional system of distinctive features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-626"
  },
  "alphen07_interspeech": {
   "authors": [
    [
     "Petra van",
     "Alphen"
    ],
    [
     "Elise de",
     "Bree"
    ],
    [
     "Paula",
     "Fikkert"
    ],
    [
     "Frank",
     "Wijnen"
    ]
   ],
   "title": "The role of metrical stress in comprehension and production in dutch children at-risk of dyslexia",
   "original": "i07_2313",
   "page_count": 4,
   "order": 627,
   "p1": "2313",
   "pn": "2316",
   "abstract": [
    "The present study compared the role of metrical stress in comprehension and production of three-year-old children with a familial risk of dyslexia with that of normally developing children. A visual fixation task with stress (mis-)matches in bisyllabic words, as well as a non-word repetition task with bisyllabic targets were presented to the control and at-risk children. Results show that the at-risk group is less sensitive to stress mismatches in word recognition than the control group. Correct production of metrical stress patterns did not differ significantly between the groups, but the percentages of phonemes produced correctly were lower of the at-risk than the control group. The findings indicate that processing of metrical stress patterns is not impaired in at-risk children, but that the at-risk group cannot exploit metrical stress in word recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-627"
  },
  "nakagawa07b_interspeech": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Kei",
     "Ohta"
    ]
   ],
   "title": "A statistical method of evaluating pronunciation proficiency for presentation in English",
   "original": "i07_2317",
   "page_count": 4,
   "order": 628,
   "p1": "2317",
   "pn": "2320",
   "abstract": [
    "In this paper, we propose a statistical method of evaluating the pronunciation proficiency for presentation in English. We statistically analyze the utterances to find a combination that has a high correlation between an English teacher's score and some acoustic features. We found that ratio of the likelihoods of free phoneme recognition by using native English HMMs and non-native English HMMs was the best measure of pronunciation proficiency. The combination of likelihood ratio between native English HMMs and non-native English HMMs (concatenation of correct phone HMMs), likelihood ratio between native English HMMs and non-native English HMMs (phoneme recognition), phoneme recognition rate (correct rate) and word recognition rate (correct rate) are highly related to the English teacher's score. We obtained a correlation coefficient of 0.781 with closed data and 0.743 with open data for speaker at sentence level, respectively. The coefficient was near the correlation between human's scores; 0.691‘0.791.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-628"
  },
  "joto07_interspeech": {
   "authors": [
    [
     "Akiyo",
     "Joto"
    ],
    [
     "Yoshiki",
     "Nagase"
    ],
    [
     "Seiya",
     "Funatsu"
    ]
   ],
   "title": "The intelligibility and its relations to acoustic characteristics of English /s/ and /esh/ produced by native speakers of Japanese",
   "original": "i07_2321",
   "page_count": 4,
   "order": 629,
   "p1": "2321",
   "pn": "2324",
   "abstract": [
    "This paper examined the intelligibility of English /s/ and /esh/ followed by the vowel /ilengthmark/ produced by 20 native speakers of Japanese (JE /s/, JE /esh/), and the acoustic characteristics of JE /s/ and /esh/ according to the different levels of intelligibility. Five native English speakers evaluated the intelligibility of JE /s/ and /esh/ in the word-initial position of \"seat\" and \"sheet.\" The major energy peak locations were analyzed for /s/ and /esh/ produced by Japanese speakers and native English speakers, and for the Japanese sibilant /ctc/.\n",
    "It was found that the overall intelligibility was lower for JE /esh/ than for /s/ and that JE /s/ and /esh/ with low intelligibility were heard as /esh/ and /s/, respectively. When the intelligibility was lower, JE /s/ had energy peaks in the lower frequencies, and JE /esh/, in the higher frequencies, having characteristics similar to /ctc/. L1 transfer occurred in the pronunciation of English /s/ and /esh/ by Japanese speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-629"
  },
  "goudbeek07_interspeech": {
   "authors": [
    [
     "Martijn",
     "Goudbeek"
    ],
    [
     "Daniel",
     "Swingley"
    ],
    [
     "Keith R.",
     "Kluender"
    ]
   ],
   "title": "The limits of multidimensional category learning",
   "original": "i07_2325",
   "page_count": 4,
   "order": 630,
   "p1": "2325",
   "pn": "2328",
   "abstract": [
    "Distributional learning is almost certainly involved in the human acquisition of phonetic categories. Because speech is inherently a multidimensional signal, learning phonetic categories entails multidimensional learning. Yet previous studies of auditory category learning have shown poor maintenance of learned multidimensional categories. Two experiments explored ways to improve maintenance: by increasing the costs associated with applying a unidimensional strategy; by providing additional information about the category structures; and by giving explicit instructions on how to categorize. Only with explicit instructions were categorization strategies maintained in a maintenance phase without supervision or distributional information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-630"
  },
  "uther07_interspeech": {
   "authors": [
    [
     "Maria",
     "Uther"
    ],
    [
     "James",
     "Uther"
    ],
    [
     "Panos",
     "Athanasopoulos"
    ],
    [
     "Pushpendra",
     "Singh"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ]
   ],
   "title": "Mobile adaptive CALL (MAC): a lightweight speech-based intervention for mobile language learners",
   "original": "i07_2329",
   "page_count": 4,
   "order": 631,
   "p1": "2329",
   "pn": "2332",
   "abstract": [
    "A computer-assisted language learning software for mobile devices (MAC) is presented, that was aimed to helping speakers acquire speech contrasts not native to their own language. The software is based on the high variability phonetic training (HVPT) technique. An overview of the software is given, followed by results from an efficacy study. Two groups using slightly different variations of the MAC software. One group received software training that was equivalent to an ordinary HVPT training, but delivered on mobile devices. The second group received a version that used an adaptive algorithm to determine which words and speakers that the learner had most difficulty with, and gave them more practice on those. Results showed that both groups showed significant but equivalent improvements. These results also showed that the magnitude of improvement using mobile phones was similar to those obtained using fixed PCs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-631"
  },
  "best07_interspeech": {
   "authors": [
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Pierre A.",
     "Hallé"
    ],
    [
     "Jennifer S.",
     "Pardo"
    ]
   ],
   "title": "English and French speakers' perception of voicing distinctions in non-native lateral consonant syllable onsets",
   "original": "i07_2333",
   "page_count": 4,
   "order": 632,
   "p1": "2333",
   "pn": "2336",
   "abstract": [
    "English and French listeners were tested on discrimination and open-response categorization of laryngeal contrasts in three non-native syllable onsets differing in gestural complexity, in particular in the phasing between laryngeal and supralaryngeal articulations. All onsets involved a lateral, which was combined with a coronal stop in two contrasts. Results support the view that syllable onsets are perceived as holistic articulatory patterns, in which voicing is more difficult to perceive separately as gestural complexity of the onset increases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-632"
  },
  "lacerda07_interspeech": {
   "authors": [
    [
     "Francisco",
     "Lacerda"
    ],
    [
     "Lisa",
     "Gustavsson"
    ]
   ],
   "title": "Predicting the consequences of vocalizations in early infancy",
   "original": "i07_2337",
   "page_count": 4,
   "order": 633,
   "p1": "2337",
   "pn": "2340",
   "abstract": [
    "This paper describes a method to study the infant's ability to predict the consequences of its vocalizations and presents the first results of the on-going investigation. The research method uses a voice-controlled device, with which the infant may control the position of a figure on a screen, in combination with an eye-tracking system (Tobii) that simultaneously registers the infant's gaze fixations on the screen where the figure appears. The preliminary results indicate that 12.5 month-old infants seem to be able to predict the consequences of their vocalizations as indicated by the decrease in the mismatch between the infant's gaze position and the location at which the figure is displayed as a function of the infant's F0. [Work supported by grants from the Bank of Sweden Tercentenary Foundation (MILLE, K2003-0867) and EU NEST program (CONTACT, 5010).]\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-633"
  },
  "weenink07_interspeech": {
   "authors": [
    [
     "David",
     "Weenink"
    ],
    [
     "Guangqin",
     "Chen"
    ],
    [
     "Zongyan",
     "Chen"
    ],
    [
     "Stefan de",
     "Konink"
    ],
    [
     "Dennis",
     "Vierkant"
    ],
    [
     "Eveline van",
     "Hagen"
    ],
    [
     "R. J. J. H. van",
     "Son"
    ]
   ],
   "title": "Learning tone distinctions for Mandarin Chinese",
   "original": "i07_2341",
   "page_count": 4,
   "order": 634,
   "p1": "2341",
   "pn": "2344",
   "abstract": [
    "We describe the SpeakGoodChinese system that supports beginning students of Mandarin Chinese to produce tones correctly (http://speakgoodchinese.org/). Students pronounce a word spelled in pinyin notation and receive feedback from our system on their production of the tones. The novelty in our approach lies in the use of synthetic reference tone(s) produced from the pinyin notation. Preliminary results indicate a 6% rejection rate for six words, read multiple times, by three reference speakers and less than 15% acceptance rate on incorrectly produced tones on shadowed versions of these words by 8 speakers. With speech from 4 reference speakers collected with a fully functional test application, a rejection rate of less than 15% was achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-634"
  },
  "lai07_interspeech": {
   "authors": [
    [
     "Catherine",
     "Lai"
    ],
    [
     "Kyle",
     "Gorman"
    ],
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "Perception of disfluency: language differences and listener bias",
   "original": "i07_2345",
   "page_count": 4,
   "order": 635,
   "p1": "2345",
   "pn": "2348",
   "abstract": [
    "This paper describes a crosslinguistic disfluency perception experiment. We tested the recognizability of pause fillers and partial words in English, German and Mandarin. Subjects were speakers of English with no knowledge of Mandarin or German. We found that subjects could identify disfluent from fluent utterances at a level above chance. Pause fillers were easier to identify than partial words. Accuracy rates were highest for English, followed by German and then Mandarin. Although German accuracy rates were higher than those for Mandarin, discriminability analysis suggests that this is due to conservative bias towards false negatives rather than non-recognition of the acoustic material. The fact that subjects could identify disfluent speech in languages they did not know shows that there are real phonetic crosslinguistic cues to disfluency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-635"
  },
  "pigeon07_interspeech": {
   "authors": [
    [
     "Stephane",
     "Pigeon"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "Aaron",
     "Lawson"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Design and characterization of the non-native military air traffic communications database (nnMATC)",
   "original": "i07_2417",
   "page_count": 4,
   "order": 636,
   "p1": "2417",
   "pn": "2420",
   "abstract": [
    "This paper describes the speech database that has a central role in the Interspeech 2007 special session \"Novel techniques for the NATO non-native Air Traffic Communications database.\" The rationale for recording and distributing this common research object is given, and details about the acquisition and annotation are given, as well as some statistics. Further, a summary is given of potential uses of the database, in terms of evaluation measures and protocols.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-636"
  },
  "shen07b_interspeech": {
   "authors": [
    [
     "Wade",
     "Shen"
    ],
    [
     "Douglas",
     "Reynolds"
    ]
   ],
   "title": "A comparison of speaker clustering and speech recognition techniques for air situational awareness",
   "original": "i07_2421",
   "page_count": 4,
   "order": 637,
   "p1": "2421",
   "pn": "2424",
   "abstract": [
    "In this paper we compare speaker clustering and speech recognition techniques to the problem of understanding patterns of air traffic control communications. For a given radio transmission, our goal is to identify the talker and to whom he/she is speaking. This information, in combination with knowledge of the roles (i.e. takeoff, approach, hand-off, taxi, etc.) of different radio frequencies within an air traffic control region could allow tracking of pilots through various stages of flight, thus providing the potential to monitor the airspace in great detail. Both techniques must contend with degraded audio channels and significant non-native accents. We report results from experiments using the nn-MATC database [6] showing 9.3% and 32.6% clustering error for speaker clustering and ASR methods respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-637"
  },
  "dimitriadis07b_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Dimitriadis"
    ],
    [
     "Jose C.",
     "Segura"
    ],
    [
     "Luz",
     "Garcia"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Petros",
     "Maragos"
    ],
    [
     "Vassilis",
     "Pitsikalis"
    ]
   ],
   "title": "Advanced front-end for robust speech recognition in extremely adverse environments",
   "original": "i07_2425",
   "page_count": 4,
   "order": 638,
   "p1": "2425",
   "pn": "2428",
   "abstract": [
    "In this paper, a unified approach to speech enhancement, feature extraction and feature normalization for speech recognition in adverse recording conditions is presented. The proposed front-end system consists of several different, independent, processing modules. Each of the algorithms contained in these modules has been independently applied to the problem of speech recognition in noise, significantly improving the recognition rates. In this work, these algorithms are merged in a single front-end and their combined performance is demonstrated. Specifically, the proposed advanced front-end extracts noise-invariant features via the following modules: Wiener filtering, voice-activity detection, robust feature extraction (nonlinear modulation or fractal features), parameter equalization and frame-dropping. The advanced front-end is applied to extremely adverse environments where most feature extraction schemes fail. We show that by combining speech enhancement, robust feature extraction and feature normalization up to a fivefold error rate reduction can be achieved for certain tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-638"
  },
  "gemello07_interspeech": {
   "authors": [
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ],
    [
     "Stefano",
     "Scanzio"
    ]
   ],
   "title": "Experiments on hiwire database using denoising and adaptation with a hybrid HMM-ANN model",
   "original": "i07_2429",
   "page_count": 4,
   "order": 639,
   "p1": "2429",
   "pn": "2432",
   "abstract": [
    "This paper presents the results of a large number of experiments performed on the Hiwire cockpit database with a hybrid HMM-ANN speech recognition model1. The Hiwire database is a noisy and non-native English speech corpus for cockpit communication. The noisy component of the database has been used to test two noise reduction methods recently introduced, while the adaptation component is exploited to perform supervised and unsupervised adaptation of the HMM-ANN model with an innovative technology, both in multi-speaker and speaker dependent way. Baseline results are presented, and the improvements obtained with noise reduction and adaptations are reported, showing an error reduction of about 60%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-639"
  },
  "smolenski07_interspeech": {
   "authors": [
    [
     "Brett Y.",
     "Smolenski"
    ]
   ],
   "title": "Detection and removal of switching noise in push-to-talk and voice operated exchange communications systems",
   "original": "i07_2433",
   "page_count": 4,
   "order": 640,
   "p1": "2433",
   "pn": "2436",
   "abstract": [
    "This paper addresses the detection and removal of key clicks in the NATO non-native Air Traffic Control database. Key clicks are impulse-like noises generated in Push-to-Talk (PTT) and Voice Operated eXchange (VOX) communications systems. The removal of key clicks can improve the quality of the signal for both listening and machine processing. The detection of key clicks could also assist in other applications, such as speech segmentation on these types of channels. The approach taken was to first apply a Recursive Least Squares (RLS) whitening filter to augment the key clicks. Outlier detection based on order statistics was then applied to detect the candidate key click samples. Finally, sample values extending to the second zero crossing prior to a detected key click and to the third zero crossing following a detected key click were set to zero to remove the key clicks. With this approach 98.3% detection of the key clicks with only 0.19% false alarms was obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-640"
  },
  "buera07b_interspeech": {
   "authors": [
    [
     "Luis",
     "Buera"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Óscar",
     "Saz"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "Evaluation of the combined use of MEMLIN and MLLR on the non-native adaptation task of hiwire project database",
   "original": "i07_2437",
   "page_count": 4,
   "order": 641,
   "p1": "2437",
   "pn": "2440",
   "abstract": [
    "This paper describes the performance of the combination of Multi-Environment Model-based LInear Normalization, MEMLIN, which provides an estimation of the uncorrupted feature vector, with Maximum Likelihood Linear Regression, MLLR, for the collected database under the auspices of the IST-EU STREP project HIWIRE. In this work the results for the non-native adaptation task (NNA) are presented. The HIWIRE project database consist on command and control aeronautics application utterances pronounced by non-native speakers which are digitally corrupted with airplane cockpit noise. Thus, three noise conditions are defined: low, medium and high noise. In the proposed system, each MEMLIN-normalized feature vector is decoded using the MLLR-adapted acoustic models. The experiments show that an important improvement is reached combining MEMLIN and MLLR methods for all kinds of non-native speakers and noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-641"
  },
  "dechelotte07_interspeech": {
   "authors": [
    [
     "Daniel",
     "Déchelotte"
    ],
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Improved machine translation of speech-to-text outputs",
   "original": "i07_2441",
   "page_count": 4,
   "order": 642,
   "p1": "2441",
   "pn": "2444",
   "abstract": [
    "Combining automatic speech recognition and machine translation is frequent in current research programs. This paper first presents several pre-processing steps to limit the performance degradation observed when translating an automatic transcription (as opposed to a manual transcription). Indeed, automatically transcribed speech often differs significantly from the machine translation system's training material, with respect to caseing, punctuation and word normalization. The proposed system outperforms the best system at the 2007 TC-STAR evaluation by almost 2 points BLEU. The paper then attempts to determine a criteria characterizing how well an STT system can be translated, but the current experiments could only confirm that lower word error rates lead to better translations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-642"
  },
  "saleem07_interspeech": {
   "authors": [
    [
     "S.",
     "Saleem"
    ],
    [
     "K.",
     "Subramanian"
    ],
    [
     "R.",
     "Prasad"
    ],
    [
     "David",
     "Stallard"
    ],
    [
     "Chia-Lin",
     "Kao"
    ],
    [
     "P.",
     "Natarajan"
    ],
    [
     "R.",
     "Suleiman"
    ]
   ],
   "title": "Improvements in machine translation for English/iraqi speech translation",
   "original": "i07_2445",
   "page_count": 4,
   "order": 643,
   "p1": "2445",
   "pn": "2448",
   "abstract": [
    "In this paper, we describe techniques for improving machine translation quality in the context of speech-to-speech translation for significantly different language pairs. Specifically, we explore three broad approaches for improving translation from English to Iraqi and vice versa. First, we investigate normalization techniques which address the differences in spoken and written forms of both languages. Second, we incorporate additional knowledge sources into the translation process such as a bilingual lexicon and named entity detection. Third, we exploit the rich morphological structure of Iraqi Arabic using two different approaches. The first approach decomposes words in Iraqi Arabic whereas the second approach, a novel one inflects English by combining key phrases into words using the minimum descriptive length criterion. Significant gains in accuracy are observed, while translating from text as well as speech recognition output.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-643"
  },
  "matusov07_interspeech": {
   "authors": [
    [
     "Evgeny",
     "Matusov"
    ],
    [
     "Dustin",
     "Hillard"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improving speech translation with automatic boundary prediction",
   "original": "i07_2449",
   "page_count": 4,
   "order": 644,
   "p1": "2449",
   "pn": "2452",
   "abstract": [
    "This paper investigates the influence of automatic sentence boundary and sub-sentence punctuation prediction on machine translation (MT) of automatically recognized speech. We use prosodic and lexical cues to determine sentence boundaries, and successfully combine two complementary approaches to sentence boundary prediction. We also introduce a new feature for segmentation prediction that directly considers the assumptions of the phrase translation model. In addition, we show how automatically predicted commas can be used to constrain reordering in MT search. We evaluate the presented methods using a state-of-the-art phrase-based statistical MT system on two large vocabulary tasks. We find that careful optimization of the segmentation parameters directly for translation quality improves the translation results in comparison to independent optimization for segmentation quality of the predicted source language sentence boundaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-644"
  },
  "cattoni07_interspeech": {
   "authors": [
    [
     "Roldano",
     "Cattoni"
    ],
    [
     "Nicola",
     "Bertoldi"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Punctuating confusion networks for speech translation",
   "original": "i07_2453",
   "page_count": 4,
   "order": 645,
   "p1": "2453",
   "pn": "2456",
   "abstract": [
    "Translating from confusion networks (CNs) has been proven to be more effective than translating from single best hypotheses. Moreover, it is widely accepted that the availability of good punctuation marks in the input can improve translation quality. At present, no ASR systems can generate punctuation marks in the word graphs, therefore CNs miss punctuation. In this paper we investigate the problem of adding punctuation marks into confusion networks. We investigate different punctuation strategies and show that the use of multiple hypotheses improves translation quality in a large-vocabulary speech translation task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-645"
  },
  "reddy07_interspeech": {
   "authors": [
    [
     "Aarthi",
     "Reddy"
    ],
    [
     "Richard",
     "Rose"
    ],
    [
     "Alain",
     "Désilets"
    ]
   ],
   "title": "Integration of ASR and machine translation models in a document translation task",
   "original": "i07_2457",
   "page_count": 4,
   "order": 646,
   "p1": "2457",
   "pn": "2460",
   "abstract": [
    "This paper is concerned with the problem of machine aided human language translation. It addresses a translation scenario where a human translator dictates the spoken language translation of a source language text into an automatic speech dictation system. The source language text in this scenario is also presented to a statistical machine translation system (SMT). The techniques presented in the paper assume that the optimum target language word string which is produced by the dictation system is modeled using the combined SMT and ASR statistical models. These techniques were evaluated on a speech corpus involving human translators dictating English language translations of French language text obtained from transcriptions of the proceedings of the Canadian House of Commons. It will be shown in the paper that the combined ASR/SMT modeling techniques described in the paper were able to reduce ASR WER by 26.6 percent relative to the WER of an ASR system that did not incorporate SMT knowledge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-646"
  },
  "tam07_interspeech": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Bilingual LSA-based translation lexicon adaptation for spoken language translation",
   "original": "i07_2461",
   "page_count": 4,
   "order": 647,
   "p1": "2461",
   "pn": "2464",
   "abstract": [
    "We present a bilingual LSA (bLSA) framework for translation lexicon adaptation. The idea is to apply marginal adaptation on a translation lexicon so that the lexicon marginals match to in-domain marginals. In the framework of speech translation, the bLSA method transfers topic distributions from the source to the target side, such that the translation lexicon can be adapted before translation based on the source document. We evaluated the proposed approach on our Mandarin RT04 spoken language translation system. Results showed that the conditional likelihood on the test sentence pairs is improved significantly using an adapted translation lexicon compared to an unadapted baseline. The proposed approach showed improvement on BLEU-score in SMT. When both the target-side LM and the translation lexicon were adapted and applied simultaneously for SMT decoding, the gain on BLEU-score was more than additive compared to the scenarios when the adapted models were individually applied.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-647"
  },
  "stallard07_interspeech": {
   "authors": [
    [
     "David",
     "Stallard"
    ],
    [
     "Fred",
     "Choi"
    ],
    [
     "Chia-Lin",
     "Kao"
    ],
    [
     "Kriste",
     "Krstovski"
    ],
    [
     "P.",
     "Natarajan"
    ],
    [
     "R.",
     "Prasad"
    ],
    [
     "S.",
     "Saleem"
    ],
    [
     "K.",
     "Subramanian"
    ]
   ],
   "title": "The BBN 2007 displayless English/iraqi speech-to-speech translation system",
   "original": "i07_2817",
   "page_count": 4,
   "order": 648,
   "p1": "2817",
   "pn": "2820",
   "abstract": [
    "Spoken communication across a language barrier is of increasing importance in both civilian and military applications. In this paper, we present an English/Iraqi Arabic speech-to-speech translation system for the military force protection domain (checkpoints, municipal services surveys, basic descriptions of people, houses, vehicles, etc). The system combines statistical N-gram speech recognition, statistical machine translation, hand-crafted rules, and speech synthesis in order to translate between the two languages. The system is designed for \"eyes-free\", or \"displayless\" use. That is, it does not make use of a screen, mouse, or keyboard, but is instead operated by a handheld microphone with just two push buttons: one for English, and the other for Arabic.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-648"
  },
  "sarikaya07_interspeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Yonggang",
     "Deng"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Context dependent word modeling for statistical machine translation using part-of-speech tags",
   "original": "i07_2821",
   "page_count": 4,
   "order": 649,
   "p1": "2821",
   "pn": "2824",
   "abstract": [
    "Word based translation models in particular and phrase based translation models in general assume that a word in any context is equivalent to the same word in any other context. Yet, this is not always true. The words in a sentence are not generated independently. The usage of each word is strongly affected by its immediate neighboring words. The state-of-the-art machine translation (MT) methods use words and phrases as basic modeling units. This paper introduces Context Dependent Words (CDWs) as the new basic translation units. The context classes are defined using Part-of-Speech (POS) tags. Experimental results using CDW based language models demonstrate encouraging improvements in the translation quality for the translation of dialectal Arabic to English. Analysis of the results reveals that improvements are mainly in fluency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-649"
  },
  "appling07_interspeech": {
   "authors": [
    [
     "Darren Scott",
     "Appling"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Translating conversational speech to standard linguistic form",
   "original": "i07_2825",
   "page_count": 4,
   "order": 650,
   "p1": "2825",
   "pn": "2828",
   "abstract": [
    "This paper describes the so-called ill-formed nature of spontaneous conversational speech as observed from the study of a 1500-hour corpus of recorded dialogue speech. We note that the structure is quite different from that of more formal speech or writing and propose a Statistical Machine Translation approach for mapping between the spoken and written forms of the language as if they were two entirely separate languages. We further posit that the particular nature of the spoken language is especially well suited for the display of affective states, inter-speaker relationships and discourse management information. In summary, both modes of communication appear to be particularly suited to their pragmatic function, neither is ill-formed, and it appears possible to map automatically between the two. This mapping has applications in speech technology for the processing of conversational speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-650"
  },
  "lavecchia07_interspeech": {
   "authors": [
    [
     "Caroline",
     "Lavecchia"
    ],
    [
     "Kamel",
     "Smaïli"
    ],
    [
     "David",
     "Langlois"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Using inter-lingual triggers for machine translation",
   "original": "i07_2829",
   "page_count": 4,
   "order": 651,
   "p1": "2829",
   "pn": "2832",
   "abstract": [
    "In this paper, we present the idea of cross-lingual triggers. We exploit this formalism in order to build up a bilingual dictionary for machine translation. We describe the idea of cross-lingual triggers, the way to exploit and to make good use of them in order to produce a bilingual dictionary. We then compare it to ELRA and a free downloaded dictionaries. Finally, our dictionary is evaluated by comparing it to the one achieved by GIZA++ [1] (which is an extension of the program GIZA [2]) into an entire translation decoding process supplied by Pharaoh [3]. The experiments showed that the obtained dictionary is well constructed and is suitable for machine translation. The experiments have been conducted on a parallel corpus of 19 million French words and of 17 million English words.\n",
    "Finally, the encouraging results allow us to put forward the concept of cross-lingual triggers which could have so many applications in machine translation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-651"
  },
  "falavigna07_interspeech": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Nicola",
     "Bertoldi"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Roldano",
     "Cattoni"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Boxing",
     "Chen"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Deepa",
     "Gupta"
    ],
    [
     "Dino",
     "Seppi"
    ]
   ],
   "title": "The IRST English-Spanish translation system for european parliament speeches",
   "original": "i07_2833",
   "page_count": 4,
   "order": 652,
   "p1": "2833",
   "pn": "2836",
   "abstract": [
    "This paper presents a full-fledged spoken language translation system developed at IRST during the TC-STAR project. The system integrates automatic speech recognition with machine translation through the use of confusion networks, which permit to represent a huge number of transcription hypotheses generated by the speech recognizer. Confusion networks are efficiently decoded by a statistical machine translation system which computes the most probable translation in the target language. This paper presents the whole architecture developed for the translation of political speeches held at the European and Spanish parliaments from English to Spanish and vice versa.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-652"
  },
  "fugen07_interspeech": {
   "authors": [
    [
     "Christian",
     "Fügen"
    ],
    [
     "Muntsin",
     "Kolss"
    ]
   ],
   "title": "The influence of utterance chunking on machine translation performance",
   "original": "i07_2837",
   "page_count": 4,
   "order": 653,
   "p1": "2837",
   "pn": "2840",
   "abstract": [
    "Speech translation systems commonly couple automatic speech recognition (ASR) and machine translation (MT) components. Hereby the automatic segmentation of the ASR output for the subsequent MT is critical for the overall performance. In simultaneous translation systems, which require a continuous output with a low latency, chunking of the ASR output into translatable segments is even more critical. This paper addresses the question how utterance chunking influences machine translation performance in an empirical study. In addition, the machine translation performance is also set in relation to the segment length produced by the chunking strategy, which is important for simultaneous translation. Therefore, we compare different chunking/segmentation strategies on speech recognition hypotheses as well as on reference transcripts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-653"
  },
  "precoda07_interspeech": {
   "authors": [
    [
     "Kristin",
     "Precoda"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Colleen",
     "Richey"
    ],
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Sachin",
     "Kajarekar"
    ]
   ],
   "title": "Iraqcomm: a next generation translation system",
   "original": "i07_2841",
   "page_count": 4,
   "order": 654,
   "p1": "2841",
   "pn": "2844",
   "abstract": [
    "This paper describes the IraqComm translation system developed by SRI International, with components from Language Weaver, Inc., and Cepstral, LLC. IraqComm, supported by the DARPA Translation Systems for Tactical Use (TRANSTAC) program, mediates and translates spontaneous conversations between an English speaker and a speaker of colloquial Iraqi Arabic. It is trained to handle topics of tactical importance, including force protection, checkpoint operations, civil affairs, basic medical interviews, and training. Major components of the system include SRI's DynaSpeak speech recognizer, Gemini, SRInterp, and Language Weaver translation engines, Cepstral's Swift speech synthesis engine, and the user interface. The system runs on standard Windows computers and can be used in a variety of modes, including an eyes-free, nearly hands-free mode.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-654"
  },
  "rao07_interspeech": {
   "authors": [
    [
     "Sharath",
     "Rao"
    ],
    [
     "Ian",
     "Lane"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Optimizing sentence segmentation for spoken language translation",
   "original": "i07_2845",
   "page_count": 4,
   "order": 655,
   "p1": "2845",
   "pn": "2848",
   "abstract": [
    "The conventional approach in text-based machine translation (MT) is to translate complete sentences, which are conveniently indicated by sentence boundary markers. However, since such boundary markers are not available for speech, new methods are required that define an optimal unit for translation. Our experimental results show that with a segment length optimized for a particular MT system, intra-sentence segmentation can improve translation performance (measured in BLEU) by up to 11% for Arabic Broadcast Conversation (BC) and 6% for Arabic Broadcast News (BN). We show that acoustic segmentation that minimizes Word Error Rate (WER) may not give the best translation performance. We improve upon it by automatically resegmenting the ASR output in a way that is optimized for translation and argue that it might be necessary for different stages of a Spoken Language Translation (SLT) system to define their own optimal units.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-655"
  },
  "richmond07_interspeech": {
   "authors": [
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "A multitask learning perspective on acoustic-articulatory inversion",
   "original": "i07_2465",
   "page_count": 4,
   "order": 656,
   "p1": "2465",
   "pn": "2468",
   "abstract": [
    "This paper proposes the idea that by viewing an inversion mapping MLP from a Multitask Learning perspective, we may be able to relax two constraints which are inherent in using electromagnetic articulography as a source of articulatory information for speech technology purposes. As a first step to evaluating this idea, we perform an inversion mapping experiment in an attempt to ascertain whether the hidden layer of a \"multitask\" MLP can act beneficially as a hidden representation that is shared between inversion mapping subtasks for multiple articulatory targets. Our results in the case of the tongue dorsum x-coordinate indicate this is indeed the case and show good promise. Results for the tongue dorsum y-coordinate however are not so clear-cut, and will require further investigation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-656"
  },
  "qin07b_interspeech": {
   "authors": [
    [
     "Chao",
     "Qin"
    ],
    [
     "Miguel Á.",
     "Carreira-Perpiñán"
    ]
   ],
   "title": "A comparison of acoustic features for articulatory inversion",
   "original": "i07_2469",
   "page_count": 4,
   "order": 657,
   "p1": "2469",
   "pn": "2472",
   "abstract": [
    "We study empirically the best acoustic parameterization for articulatory inversion (the problem of recovering the sequence of vocal tract shapes that produce a given acoustic speech signal). We compare all combinations of the following factors: 1) popular acoustic features such as MFCC and PLP with and without dynamic features; 2) different short-time window lengths; 3) different levels of smoothing of the acoustic temporal trajectories. Experimental results on a real speech production database show consistent improvement when using features closely related to the vocal tract (in particular LSF), dynamic features, and large window length and smoothing (which reduce the jaggedness of the acoustic trajectory). Further improvements are obtained with a 15 ms time delay between acoustic and articulatory frames. However, the improvement attained over other combinations is very small (at most 0.3 mm RMSE).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-657"
  },
  "scharenborg07b_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Vincent",
     "Wan"
    ]
   ],
   "title": "Can unquantised articulatory feature continuums be modelled?",
   "original": "i07_2473",
   "page_count": 4,
   "order": 658,
   "p1": "2473",
   "pn": "2476",
   "abstract": [
    "Articulatory feature (AF) modelling of speech has received a considerable amount of attention in automatic speech recognition research. Although termed ‘articulatory’, previous definitions make certain assumptions that are invalid, for instance, that articulators ‘hop’ from one fixed position to the next. In this paper, we studied two methods, based on support vector classification (SVC) and regression (SVR), in which the articulation continuum is modelled without being restricted to using discrete AF value classes. A comparison with a baseline system trained on quantised values of the articulation continuum showed that both SVC and SVR outperform the baseline for two of the three investigated AFs, with improvements up to 5.6% absolute.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-658"
  },
  "shah07_interspeech": {
   "authors": [
    [
     "Milind S.",
     "Shah"
    ],
    [
     "Prem C.",
     "Pandey"
    ]
   ],
   "title": "Estimation of place of articulation in stop consonants for visual feedback",
   "original": "i07_2477",
   "page_count": 4,
   "order": 659,
   "p1": "2477",
   "pn": "2480",
   "abstract": [
    "Speech-training systems providing visual feedback of vocal tract shape are found to be useful for improving vowel articulation. Estimation of vocal tract shape, based on LPC and other analysis techniques, generally fails during stop closures, due to very low signal energy and unavailability of spectral information. Based on estimated area values and line spectrum pair (LSP) coefficients before and after stop closure in vowel-consonant-vowel (VCV) syllables, least-squares bivariate conic and cubic polynomial surfaces were generated and used for shape estimation during stop closure by performing 2D interpolation. Implementation of the technique with automated processing of /aCa/ syllables successfully estimated the place of articulation during closure segments of velar, alveolar, and bilabial stops. The technique can be used to provide visual feedback to improve production of stop consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-659"
  },
  "potard07_interspeech": {
   "authors": [
    [
     "Blaise",
     "Potard"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Compact representations of the articulatory-to-acoustic mapping",
   "original": "i07_2481",
   "page_count": 4,
   "order": 660,
   "p1": "2481",
   "pn": "2484",
   "abstract": [
    "Articulatory codebooks are very often used to represent the articulatory-to-acoustic mapping. They thus need to be compact while offering a very good acoustic precision. This paper presents a method of articulatory codebook construction more general than that of Ouni [1] in the sense that the articulatory-to-acoustic mapping is approximated by multivariable polynomials. The second major contribution concerns the subdivision process which finds out the most efficient subdivision, i.e. that which minimizes the size of the codebook while guarantying a very good acoustic precision.\n",
    "Experiments carried out show that the size of the codebook can be divided by a factor of 20, and simultaneously, the acoustic precision can improved by a factor of 2 by using second order polynomials together with this new construction strategy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-660"
  },
  "frankel07_interspeech": {
   "authors": [
    [
     "Joe",
     "Frankel"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Özgür",
     "Çetin"
    ]
   ],
   "title": "Articulatory feature classifiers trained on 2000 hours of telephone speech",
   "original": "i07_2485",
   "page_count": 4,
   "order": 661,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "This paper is intended to advertise the public availability of the articulatory feature (AF) classification multi-layer perceptrons (MLPs) which were used in the Johns Hopkins 2006 summer workshop. We describe the design choices, data preparation, AF label generation, and the training of MLPs for feature classification on close to 2000 hours of telephone speech. In addition, we present some analysis of the MLPs in terms of classification accuracy and confusions along with a brief summary of the results obtained during the workshop using the MLPs. We invite interested parties to make use of these MLPs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-661"
  },
  "noureldin07_interspeech": {
   "authors": [
    [
     "Amr H.",
     "Nour-Eldin"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Objective analysis of the effect of memory inclusion on bandwidth extension of narrowband speech",
   "original": "i07_2489",
   "page_count": 4,
   "order": 662,
   "p1": "2489",
   "pn": "2492",
   "abstract": [
    "For the purpose of improving Bandwidth Extension (BWE) of narrowband speech, we continue our recent work on the positive effect of exploiting the temporal correlation of speech on the dependence between speech frequency bands. We have shown that such memory inclusion into MFCC speech parametrization translates into higher highband certainty. In the work presented herein, we employ VQ to estimate highband discrete entropies, thus refining our analysis of the effect of memory inclusion on increasing highband certainty. Moreover, we extend our previous analysis to LSF parameters. We further construct a BWE system that exploits our memory inclusion technique, thus translating highband certainty gains into practical BWE performance improvement as measured by the objective quality of reconstructed speech. Results show that memory inclusion decreases the log- Spectral Distortion of the extended highband speech by as much as 1 dB corresponding to more than 14% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-662"
  },
  "geiser07_interspeech": {
   "authors": [
    [
     "Bernd",
     "Geiser"
    ],
    [
     "Hervé",
     "Taddei"
    ],
    [
     "Peter",
     "Vary"
    ]
   ],
   "title": "Artificial bandwidth extension without side information for ITU-t g.729.1",
   "original": "i07_2493",
   "page_count": 4,
   "order": 663,
   "p1": "2493",
   "pn": "2496",
   "abstract": [
    "This paper discusses a potential extension of the ITU-T G.729.1 speech and audio codec. The G.729.1 coder is hierarchically organized, i.e., the obtained quality increases with the amount of bits that is received for each frame. In particular, the bit rates of 8 and 12 kbit/s offer\n",
    "narrowband (50Hz - 4 kHz) speech transmission. With a received bit rate of at least 14 kbit/s, the output bandwidth is increased to the wideband frequency range (50Hz - 7 kHz). Here, we investigate efficient methods to provide the full wideband frequency bandwidth already for the lower bit rates of 8 and 12 kbit/s while maintaining interoperability with the standard implementation of G.729.1. These techniques are not necessarily limited to G.729.1 and thus may serve in other applications as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-663"
  },
  "pulakka07_interspeech": {
   "authors": [
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Laura",
     "Laaksonen"
    ],
    [
     "Päivi",
     "Valve"
    ]
   ],
   "title": "The effect of highband harmonic structure in the artificial bandwidth expansion of telephone speech",
   "original": "i07_2497",
   "page_count": 4,
   "order": 664,
   "p1": "2497",
   "pn": "2500",
   "abstract": [
    "The quality of narrowband telephone speech can be improved by artificial bandwidth expansion (ABE), which generates missing frequency components above the telephone bandwidth using only information from the narrowband speech signal. Straightforward bandwidth expansion methods do not reproduce the harmonic structure of voiced sounds properly, but a pitch-adaptive technique can be used to approximate the correct alignment of harmonic frequencies. In this study, pitch-adaptive highband alignment was implemented into an existing ABE method, and the quality of the modified method was studied with formal listening tests in Finnish and Mandarin Chinese. The effect of the highband harmonic structure was found unimportant for the perceived speech quality. Consequently, computationally expensive pitch adaptation was found to be unnecessary for the bandwidth expansion of telephone speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-664"
  },
  "kuroiwa07_interspeech": {
   "authors": [
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Masashi",
     "Takashina"
    ],
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Ren",
     "Fuji"
    ]
   ],
   "title": "Artificial bandwidth extension for speech signals using speech recogniton",
   "original": "i07_2501",
   "page_count": 4,
   "order": 665,
   "p1": "2501",
   "pn": "2504",
   "abstract": [
    "In this paper, we propose a non-realtime speech bandwidth extension method using HMM-based speech recognition and HMM-based speech synthesis. In the proposed method, first, the phoneme-state sequence is estimated from the bandlimited speech signals using the speech recognition technique. Next, for estimating spectrum envelopes of lost high-frequency components, an HMM-based speech synthesis technique generates a synthetic speech signal (spectrum sequence) according to the predicted phoneme-state sequence. Since both speech recognition and speech synthesis take into account dynamic feature vectors, we can obtain a smoothly varying spectrum sequence. For evaluating the proposed method, we conducted subjective and objective experiments. The experimental results show the effectiveness of the proposed method for bandwidth extension. However, the proposed method needs more improvement in speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-665"
  },
  "guerchi07_interspeech": {
   "authors": [
    [
     "Driss",
     "Guerchi"
    ],
    [
     "Tamer",
     "Rabie"
    ],
    [
     "Abdelrhani",
     "Louzi"
    ]
   ],
   "title": "Voicing-based codebook in low-rate wideband CELP coding",
   "original": "i07_2505",
   "page_count": 4,
   "order": 666,
   "p1": "2505",
   "pn": "2508",
   "abstract": [
    "In this paper we propose a new technique to quantize the spectral information in an Algebraic Code-Excited Linear Prediction (ACELP) wideband codec. The Voicing-Based Vector Quantization (VBVQ) presented in this paper, optimizes the search of the optimum vector while reducing the codebook entries by almost one third. In the VBVQ training phase, three codebooks are individually designed for voiced, unvoiced and transition speech. The proposed technique reduces the processing delay since it restricts the quantization of an input vector to only one of the three codebooks. For each speech frame, one codebook is selected based on the interframe correlation of the spectral information.\n",
    "The VBVQ was successfully implemented in an ACELP wideband coder. The objective and subjective performance are superior to that of the combination of the split vector quantization and multistage vector quantization after using the same database for training and testing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-666"
  },
  "duni07_interspeech": {
   "authors": [
    [
     "Ethan R.",
     "Duni"
    ],
    [
     "Bhaskar D.",
     "Rao"
    ]
   ],
   "title": "Performance of speaker-dependent wideband speech coding",
   "original": "i07_2509",
   "page_count": 4,
   "order": 667,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "This paper examines the performance gains available in wideband speech coding using speaker-dependent systems. It is shown that a performance gain of 4 bits per frame, in the rate-distortion sense, is achievable in the LSF coding. While variations are evident in the pitch lag statistics during voiced frames, there is no gain to be had in unvoiced frames or in the adaptive gains; thus, there is little benefit to speaker-dependent coding of adaptive codebook parameters. Lastly, it was shown that gains of 40-50 bits per frame are available in the fixed excitation. These performance boosts can be exploited in a number of ways, most simply by reducing the operating rate. Alternatively, the complexity of the coding systems can be reduced while maintaining the same performance of speaker-independent coding. It was shown that a reduction in complexity by a factor of 4 is achievable using speaker-dependent LSF quantization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-667"
  },
  "dreuw07_interspeech": {
   "authors": [
    [
     "Philippe",
     "Dreuw"
    ],
    [
     "David",
     "Rybach"
    ],
    [
     "Thomas",
     "Deselaers"
    ],
    [
     "Morteza",
     "Zahedi"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Speech recognition techniques for a sign language recognition system",
   "original": "i07_2513",
   "page_count": 4,
   "order": 668,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "One of the most significant differences between automatic sign language recognition (ASLR) and automatic speech recognition (ASR) is due to the computer vision problems, whereas the corresponding problems in speech signal processing have been solved due to intensive research in the last 30 years. We present our approach where we start from a large vocabulary speech recognition system to profit from the insights that have been obtained in ASR research.\n",
    "The system developed is able to recognize sentences of continuous sign language independent of the speaker. The features used are obtained from standard video cameras without any special data acquisition devices. In particular, we focus on feature and model combination techniques applied in ASR, and the usage of pronunciation and language models (LM) in sign language. These techniques can be used for all kind of sign language recognition systems, and for many video analysis problems where the temporal context is important, e.g. for action or gesture recognition.\n",
    "On a publicly available benchmark database consisting of 201 sentences and 3 signers, we can achieve a 17% WER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-668"
  },
  "nakamura07_interspeech": {
   "authors": [
    [
     "Keigo",
     "Nakamura"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Impact of various small sound source signals on voice conversion accuracy in speech communication aid for laryngectomees",
   "original": "i07_2517",
   "page_count": 4,
   "order": 669,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "We proposed a speaking aid system using statistical voice conversion for laryngectomees, whose vocal folds have been removed. This paper investigates the influence of various small sound sources on the voice conversion accuracy. Spectral envelopes and power of sound sources are controlled independently. In total 8 different kinds of sound source signals, e.g. pulse train, sierra wave and so on, are examined. Results of objective and subjective evaluations demonstrate that for voice conversion, sound sources with various spectral envelopes and power in a large degree are acceptable unless the power of them is comparable to that of silence parts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-669"
  },
  "cerva07_interspeech": {
   "authors": [
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Design and development of voice controlled aids for motor-handicapped persons",
   "original": "i07_2521",
   "page_count": 4,
   "order": 670,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "In this paper we present two voice-operated systems that have been designed for Czech motor-handicapped people to allow them full access to computers and computer based services. The programs, which are named MyVoice and MyDictate, are complementary in their functions. Both employ ASR engines developed in our lab. The former is used primarily as a mid-size-vocabulary (up to 10K words) voice commander for PC programs and PC-controlled home devices, the latter allows for very-large-vocabulary dictation (with more than 500K words). They are designed to cooperate and thus allow an entirely hands-free access to any computer application, including text typing, e-mail exchange, Internet browsing or handling telephone calls as well as for controlling external home devices such a TV/radio sets or air-conditioning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-670"
  },
  "katsurada07_interspeech": {
   "authors": [
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Yuji",
     "Okuma"
    ],
    [
     "Makoto",
     "Yano"
    ],
    [
     "Yurie",
     "Iribe"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Management of static/dynamic properties in a multimodal interaction system",
   "original": "i07_2525",
   "page_count": 4,
   "order": 671,
   "p1": "2525",
   "pn": "2528",
   "abstract": [
    "This paper provides a mechanism to deal with static/dynamic properties in a web-based Multi-Modal Interaction (MMI) system. The static/dynamic properties in this paper include user profile, user's facial expression, surrounding environment, and so on. By using these properties, the MMI system can make interaction more natural based on context or situation. To consolidate these properties into a module, we have designed and developed a static/dynamic property manager on our MMI system. We have also prototyped a user navigation system that introduced user profile, user's facial expression and GPS information as static/dynamic properties.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-671"
  },
  "sansegundo07_interspeech": {
   "authors": [
    [
     "R.",
     "San-Segundo"
    ],
    [
     "A.",
     "Pérez"
    ],
    [
     "D.",
     "Ortiz"
    ],
    [
     "L. F.",
     "D'Haro"
    ],
    [
     "M. Inés",
     "Torres"
    ],
    [
     "F.",
     "Casacuberta"
    ]
   ],
   "title": "Evaluation of alternatives on speech to sign language translation",
   "original": "i07_2529",
   "page_count": 4,
   "order": 672,
   "p1": "2529",
   "pn": "2532",
   "abstract": [
    "This paper evaluates different approaches on speech to sign language machine translation. The framework of the application focuses on assisting deaf people to apply for the passport or related information. In this context, the main aim is to automatically translate the spontaneous speech, uttered by an officer, into Spanish Sign Language (SSL).\n",
    "In order to get the best translation quality, three alternative techniques have been evaluated: a rule-based approach, a phrase-based statistical approach, and a approach that makes use of stochastic finite state transducers. The best speech translation experiments have reported a 32.0% SER (Sign Error Rate) and a 7.1 BLEU (BiLingual Evaluation Understudy) including speech recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-672"
  },
  "nemeth07b_interspeech": {
   "authors": [
    [
     "Géza",
     "Németh"
    ],
    [
     "Gábor",
     "Olaszy"
    ],
    [
     "Mátyás",
     "Bartalis"
    ],
    [
     "Géza",
     "Kiss"
    ],
    [
     "Csaba",
     "Zainkó"
    ],
    [
     "Péter",
     "Mihajlik"
    ]
   ],
   "title": "Speech based drug information system for aged and visually impaired persons",
   "original": "i07_2533",
   "page_count": 4,
   "order": 673,
   "p1": "2533",
   "pn": "2536",
   "abstract": [
    "Medicine Line (MLN) is an automatic telephone information system operating in Hungary since December 2006. It is primarily intended for visually handicapped persons and elderly people. In Hungary the National Institute of Pharmacy (NIP) coordinates the approval of new drugs and also their Patient Information Leaflets (PIL). Medicine Line reads this textual information chapter by chapter to the citizens. The number of different medicines used in Hungary is about 5000. New drugs come into practice regularly, and some are withdrawn after a certain time. The MLN system ensures 24 hour access to the information. The spoken dialogue input is processed by a specialized ASR module (the caller tells the name of the drug, the chapter title etc.). The output is given by a TTS synthesizer specialized to read drug names and medical Latin words correctly. The user can control the system by DTMF buttons too. In this article we will focus on features of speech based components.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-673"
  },
  "nogueira07_interspeech": {
   "authors": [
    [
     "Waldo",
     "Nogueira"
    ],
    [
     "Tamás",
     "Harczos"
    ],
    [
     "Bernd",
     "Edler"
    ],
    [
     "Jörn",
     "Ostermann"
    ],
    [
     "Andreas",
     "Büchner"
    ]
   ],
   "title": "Automatic speech recognition with a cochlear implant front-end",
   "original": "i07_2537",
   "page_count": 4,
   "order": 674,
   "p1": "2537",
   "pn": "2540",
   "abstract": [
    "Today, cochlear implants (CIs) are the treatment of choice in patients with profound hearing loss. However speech intelligibility with these devices is still limited. A factor that determines hearing performance is the processing method used in CIs. Therefore, research is focused on designing different speech processing methods. The evaluation of these strategies is subject to variability as it is usually performed with cochlear implant recipients. Hence, an objective method for the evaluation would give more robustness compared to the tests performed with CI patients.\n",
    "This paper proposes a method to evaluate signal processing strategies for CIs based on a hidden markov model speech recognizer. Two signal processing strategies for CIs, the Advanced Combinational Encoder (ACE) and the Psychoacoustic Advanced Combinational Encoder (PACE), have been compared in a phoneme recognition task. Results show that PACE obtained higher recognition scores than ACE as found with CI recipients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-674"
  },
  "suk07_interspeech": {
   "authors": [
    [
     "Soo-Young",
     "Suk"
    ],
    [
     "Hiroaki",
     "Kojima"
    ]
   ],
   "title": "Voice activated powered wheelchair with non-voice rejection algorithm",
   "original": "i07_2541",
   "page_count": 4,
   "order": 675,
   "p1": "2541",
   "pn": "2544",
   "abstract": [
    "In this paper, we introduce a non-voice rejection method to perform Voice/Non-Voice (V/NV) classification using a fundamental frequency (F0) estimator called YIN. Although current speech recognition technology has achieved high performance, it is insufficient for some applications where high reliability is required, such as voice control of powered wheelchairs for disabled persons. The non-voice rejection algorithm, which classifies V/NV in Voice Activity Detection (VAD) step, is helpful for realizing a highly reliable system. The proposed algorithm adopts the ratio of a reliable F0 contour to the whole input interval. To evaluate the performance of our proposed method, we used 1567 voice commands and 447 noises in powered wheelchair control in a real environment. These results indicate that the recall rate is 97% when the lowest threshold is selected for noise classification with 99% precision in VAD.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-675"
  },
  "sitbon07_interspeech": {
   "authors": [
    [
     "Laurianne",
     "Sitbon"
    ],
    [
     "Patrice",
     "Bellot"
    ],
    [
     "Philippe",
     "Blache"
    ]
   ],
   "title": "Phonetic based sentence level rewriting of questions typed by dyslexic spellers in an information retrieval context",
   "original": "i07_2545",
   "page_count": 4,
   "order": 676,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "This paper introduces a method combining spell checking and phonetic interpretation in order to automatically rewrite questions typed by dyslexic spellers. The method uses a finite state automata framework. Dysorthographics refers to incorrect word segmentation which usually causes classical spelling correctors fail. The specificities of the information retrieval context are that flexion errors have no impact since the sentences are lemmatised and filtered and that several hypothesis can be processed for one query. Our system is evaluated on questions collected with the help of an orthophonist. The word error rate on lemmatised sentences falls from 60% to 22% (falls to 0% on 43% of sentences).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-676"
  },
  "berton07_interspeech": {
   "authors": [
    [
     "André",
     "Berton"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ],
    [
     "Hans-Ulrich",
     "Block"
    ],
    [
     "Stefanie",
     "Schachtl"
    ],
    [
     "Manfred",
     "Gehrke"
    ]
   ],
   "title": "How to integrate speech-operated internet information dialogs into a car",
   "original": "i07_2549",
   "page_count": 4,
   "order": 677,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "Telematics and entertainment systems in cars usually contain audio, phone and navigation functions that rely mostly on static content. The Human Machine Interface (HMI), including the speech dialog system, reflects these static applications by their very restricted dialogs. In the future the driver would like to access current information from the internet to provide driving assistance and information of personal interest. The HMI should allow the user to request information by voice, in order to keep distraction minimal. This paper presents three architectural approaches to obtain current internet information in a car, link it to the existing applications, and make them available for speech input with as few hardware and software changes as possible to the existing system. We propose speech applications broadcast by digital radio, speech access to web services and a complete server-based processing architecture. The prototype dialog system for all three architectures was jointly developed with Siemens AG, the DFKI and Fraunhofer FIRST in the SmartWeb project.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-677"
  },
  "glass07_interspeech": {
   "authors": [
    [
     "James",
     "Glass"
    ],
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Scott",
     "Cyphers"
    ],
    [
     "Igor",
     "Malioutov"
    ],
    [
     "David",
     "Huynh"
    ],
    [
     "Regina",
     "Barzilay"
    ]
   ],
   "title": "Recent progress in the MIT spoken lecture processing project",
   "original": "i07_2553",
   "page_count": 4,
   "order": 678,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "In this paper we discuss our research activities in the area of spoken lecture processing. Our goal is to improve the access to on-line audio/visual recordings of academic lectures by developing tools for the processing, transcription, indexing, segmentation, summarization, retrieval and browsing of this media. In this paper, we provide an overview of the technology components and systems that have been developed as part of this project, present some experimental results, and discuss our ongoing and future research plans.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-678"
  },
  "fischer07_interspeech": {
   "authors": [
    [
     "Philipp",
     "Fischer"
    ],
    [
     "Andreas",
     "Österle"
    ],
    [
     "André",
     "Berton"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "How to personalize speech applications for web-based information in a car",
   "original": "i07_2557",
   "page_count": 4,
   "order": 679,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "We present a system for exploring and personalizing internet information in the car using natural language queries. Speech dialog applications are generated automatically from well-structured internet content, such as tables, and transferred to the car. In order to cope with the large variety of speech applications, we propose a hybrid content-based personalization approach. Speech applications are clustered into various topic areas by mapping them to a domain ontology. Applications are ranked according to explicit preferences of the driver, global profile data and an implicit user profile. This profile adapts itself while the user is interacting with the system and takes into account the selected application and the speech queries. The resulting list of ranked applications is displayed to the user. Global ratings are based on the preferred topics of 20 subjects that were also questioned about the prototype's usability and accuracy in a first user evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-679"
  },
  "ikeda07_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Ikeda"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Topic estimation with domain extensibility for guiding user's out-of-grammar utterances in multi-domain spoken dialogue systems",
   "original": "i07_2561",
   "page_count": 4,
   "order": 680,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "In a multi-domain spoken dialogue system, a user's utterances are more prone to be out-of-grammar, because this kind of system deals with more tasks than a single-domain system. We defined a topic as a domain about which users want to find more information, and we developed a method of recovering out-of-grammar utterances based on topic estimation, i.e., by providing a help message in the estimated domain. Moreover, the domain extensibility, that is, to facilitate adding new domains, should be inherently retained in multi-domain systems. We therefore collected documents from the Web as training data for topic estimation. Because the data contained not a few noises, we used Latent Semantic Mapping (LSM), which enables robust topic estimation by removing the effect of noise from the data. The experimental results based on using 272 utterances collected with a Woz-like method showed that our method increased the topic estimation accuracy by 23.1 points from the baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-680"
  },
  "nishimura07_interspeech": {
   "authors": [
    [
     "Ryota",
     "Nishimura"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Prosody change and response timing analysis in spontaneously spoken dialogs and their modeling in a spoken dialog system",
   "original": "i07_2565",
   "page_count": 4,
   "order": 681,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "If a dialog system were to respond to a user as naturally as a human, interaction would be smoother. Imitating the human prosodic behavior of utterances is important in computer-human natural conversations. In this paper, to develop a cooperative/ friendly spoken dialog system, we analyzed the correlations between F0 synchrony tendency or overlap frequency and subjective measures: \"liveliness,\" \"familiarity,\" and \"informality\" in human-human dialogs. We also modeled the properties of these features and implemented the model on our dialog system that generated the response timing of aizuchi (back-channel), turn-taking based on a decision tree in real time, and dynamical F0 changes to realize chat-like conversations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-681"
  },
  "tamura07_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Kunihiko",
     "Takamatsu"
    ],
    [
     "Shinji",
     "Ogura"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "GEMSIS - a novel application of speech recognition to emergency and disaster medicine",
   "original": "i07_2569",
   "page_count": 4,
   "order": 682,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "This paper reports an instance of novel application of speech recognition applied to emergency and disaster medicine. The emergency medical system named \"GEMSIS\" (Gifu Emergency Medical Supporting Intelligent System) including the speech recognition application is also introduced in this paper. Speech recognition plays an important role in this system; when a paramedic team is sent to a disaster or accident site, a lifesaving technician reports the situation using speech recognition in the site. The recognized information are shared by all hospitals and critical care centers. This system can solve the severe issue of the emergency medical care in which pre-hospital medical care is insufficient due to lack of information. A prototype application of speech recognition interface was constructed to evaluate a baseline performance and to make a discussion with medical doctors. Through this work, it is found that the applicable domain of speech processing technology can be extended.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-682"
  },
  "coulston07_interspeech": {
   "authors": [
    [
     "Rachel",
     "Coulston"
    ],
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Jacques de",
     "Villiers"
    ],
    [
     "John-Paul",
     "Hosom"
    ]
   ],
   "title": "Application of speech technology in a home based assessment kiosk for early detection of alzheimer's disease",
   "original": "i07_2573",
   "page_count": 4,
   "order": 683,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "Alzheimer's disease, a degenerative disease that affects an estimated 4.5 million people in the U.S., can be treated far more effectively when it is detected early. There are numerous challenges to early detection. One is objectivity, since caretakers are often emotionally invested in the health of the patients, who may be their family members. Consistency of administration can also be an issue, especially where longitudinal results from different examiners are compared. Finally, the frequency of testing can be adversely affected by scheduling or cost constraints for in-home psychometrician visits. The kiosk system described in this paper, currently deployed in homes around the country, uses speech technology to provide advantages that address these challenges.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-683"
  },
  "vybornova07_interspeech": {
   "authors": [
    [
     "Olga",
     "Vybornova"
    ],
    [
     "Monica",
     "Gemo"
    ],
    [
     "Ronald",
     "Moncarey"
    ],
    [
     "Benoit",
     "Macq"
    ]
   ],
   "title": "Ontology-based multimodal high level fusion involving natural language analysis for aged people home care application",
   "original": "i07_2577",
   "page_count": 4,
   "order": 684,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "This paper presents a knowledge-based method of early-stage high level multimodal fusion of data obtained from speech input and visual scene. The ultimate goal is to develop a human-computer multimodal interface to assist elderly people living alone at home to perform their daily activities, and to support their active ageing and social cohesion. Crucial for multimodal high level fusion and successful communication is the provision of extensive semantics and contextual information from spoken language understanding. To address this we propose to extract natural language semantic representations and map them onto the restricted domain ontology. This information is then processed for multimodal reference resolution together with visual scene input. To make our approach flexible and widely applicable, a priori situational knowledge, modalities and the fusion process are modelled in the ontology expressing the domain constraints. Here, we illustrate ontology-based multimodal fusion on an example scenario combining speech and visual scene analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-684"
  },
  "chan07_interspeech": {
   "authors": [
    [
     "Shing-kai",
     "Chan"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Modeling the statistical behavior of lexical chains to capture word cohesiveness for automatic story segmentation",
   "original": "i07_2581",
   "page_count": 4,
   "order": 685,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "We present a mathematically rigorous framework for modeling the statistical behavior of lexical chains for automatic story segmentation of broadcast news audio. Lexical chains were first proposed in [1] to connect related terms within a story, as an embodiment of lexical cohesion. The vocabulary within a story tends to be cohesive, while a change in the vocabulary distribution tends to signify a topic shift that occurs across a story boundary. Previous work focused on the concept and nature of lexical chains but performed story segmentation based on arbitrary thresholding. This work proposes the use of the lognormal distribution to capture the statistical behavior of lexical chains, together with data-driven parameter selection for lexical chain formation. Experimentation based on the TDT-2 Mandarin Corpus shows that the proposed statistical model leads to better story segmentation, where the F1-measure increased from 0.468 to 0.641.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-685"
  },
  "fung07_interspeech": {
   "authors": [
    [
     "James G.",
     "Fung"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Sébastien",
     "Cuendet"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Cross-linguistic analysis of prosodic features for sentence segmentation",
   "original": "i07_2585",
   "page_count": 4,
   "order": 686,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "In this paper, we perform a cross-linguistic study of prosodic features in sentence segmentation by using two different feature selection approaches: a forward search wrapper and feature filtering. Experiments in Arabic, English, and Mandarin show that prosodic features make significant contributions in all three languages. Feature selection results indicate that feature relevancy can vary greatly depending on the target language, and therefore the optimal feature subset varies considerably between languages. We observe patterns in the feature selection and the affinity of the different languages toward certain feature types, which gives us insight into future feature selection and feature design.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-686"
  },
  "rosenberg07b_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Mehrbod",
     "Sharifi"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Varying input segmentation for story boundary detection in English, Arabic and Mandarin broadcast news",
   "original": "i07_2589",
   "page_count": 4,
   "order": 687,
   "p1": "2589",
   "pn": "2592",
   "abstract": [
    "Story segmentation of news broadcasts has been shown to improve the accuracy of the subsequent processes such as question answering and information retrieval. In previous work, a decision tree trained on automatically extracted lexical and acoustic features was trained to predict story boundaries, using hypothesized sentence boundaries to define potential story boundaries. In this paper, we empirically evaluate several alternatives to this choice of input segmentation on three languages: English, Mandarin and Arabic. Our results suggest that the best performance can be achieved by using 250ms pause-based segmentation or sentence boundaries determined using a very low confidence score threshold.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-687"
  },
  "kolluru07b_interspeech": {
   "authors": [
    [
     "BalaKrishna",
     "Kolluru"
    ],
    [
     "Yoshihiko",
     "Gotoh"
    ]
   ],
   "title": "Speaker role based structural classification of broadcast news stories",
   "original": "i07_2593",
   "page_count": 4,
   "order": 688,
   "p1": "2593",
   "pn": "2596",
   "abstract": [
    "This paper is concerned with automatic classification of broadcast news stories based on speaker roles such as anchor, reporter and others. The story classification is the first step for many related tasks such as browsing, indexing, and summarising the news broadcast. We use broadcast news audio and its automatic speech recogniser transcripts to implement the classification system. It builds on speaker segmentation and identification, story segmentation and named entity identification. It has achieved 92% accuracy when individual stories were provided manually. The performance declined to 67% and 51%, of precision and recall related measures respectively, when combined with automatic story boundary segmentation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-688"
  },
  "jilka07_interspeech": {
   "authors": [
    [
     "Matthias",
     "Jilka"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "The influence of vowel quality features on peak alignment",
   "original": "i07_2621",
   "page_count": 4,
   "order": 689,
   "p1": "2621",
   "pn": "2624",
   "abstract": [
    "This study continues an approach that uses a unit selection corpus in order to investigate aspects of the phonetic realization of tonal categories. The focus lies on the peak position of German H*L pitch accents, specifically on the question of whether it is influenced by vowel quality. It is confirmed that vowel backness does not affect peak alignment at all. The distinction between tense and lax vowels initially promises to be relevant, as the H*L peaks seemingly occur significantly earlier in lax vowels. The effect is however demonstrated to be caused by the far greater number of lax vowels in the closed syllables found in the corpus. Finally, the feature of vowel height is revealed to be a significant factor (peaks are aligned latest in high vowels, earliest in low vowels). Various parameters (e.g., syllable structure, position in the phrase) are examined for interactions, but cannot account for the effect. While vowel height correlates with vowel duration, vowel duration itself does not influence peak position. The only possible explanation found involves peak height, which is intrinsically higher in high vowels, thus it may require more time to reach the peak.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-689"
  },
  "shue07_interspeech": {
   "authors": [
    [
     "Yen-Liang",
     "Shue"
    ],
    [
     "Markus",
     "Iseli"
    ],
    [
     "Nanette",
     "Veilleux"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Pitch accent versus lexical stress: quantifying acoustic measures related to the voice source",
   "original": "i07_2625",
   "page_count": 4,
   "order": 690,
   "p1": "2625",
   "pn": "2628",
   "abstract": [
    "In this paper, we explore acoustic correlates of pitch accent and main lexical stress in American English, and the interaction of these cues with other factors that affect prosody. In a controlled study, we varied presence or absence and type of pitch accent (L* vs H*), boundary-related tone sequence (L-L% vs. H-H%) and gender of the talker, for the sentence \"Dagada gave Bobby doodads\". The measures were duration, F0 (fundamental frequency),H*1-H*2 (related to open quotient), and H*1-A*3 (related to spectral tilt). Contour approximations were used to analyze time-course movements of these measures. For \"Dagada\" we found that, consistent with earlier literature, a) H* and L* pitch accents showed different F0 contours, b) pitch-accented syllables were longer than unaccented ones, c) stressed \"ga\" syllables had lower H*1-H*2 values than surrounding unstressed syllables, and for male talkers, lower H*1-A*3 values, indicating lesser spectral tilt. Unexpectedly, F0 maxima associated with an H* accent occurred most of the time later in the accented syllable than F0 minima associated with L*. The cues to lexical stress were consistent with or without pitch accent (e.g. lower H*1-H*2 ), but they sometimes interacted with gender and/or boundary tones: for example, lower H*1-A*3 in stressed \"ga\" syllables was only found for female talkers in unaccented cases, and some cues of both accent and stress were less pronounced in the final word \"doodads\", which also carried boundary-related tones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-690"
  },
  "benus07_interspeech": {
   "authors": [
    [
     "Stefan",
     "Benus"
    ],
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Prosody, emotions, and… ‘whatever’",
   "original": "i07_2629",
   "page_count": 4,
   "order": 691,
   "p1": "2629",
   "pn": "2632",
   "abstract": [
    "We examine the role of prosody in cueing a scale of negative meanings associated with the use of whatever. The analysis of a corpus of elicited examples shows that the more negative the token, the more likely it is to have an additional pitch accent, extended duration, and expanded pitch range on the first syllable. These findings are analyzed as a link between pragmatic meaning and the strength of the prosodic boundary between the first two syllables ( what#ever). The results of perception experiments show that the prosody of whatever itself is a systematic cue for the degree of negative connotation associated with the utterance in which whatever occurs. Potential applications of this result for spoken dialogue systems and synthesis of emotional speech are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-691"
  },
  "gu07b_interspeech": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Rerrario Shui-Ching",
     "Ho"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Modeling tones in hakka on the basis of the command-response model",
   "original": "i07_2633",
   "page_count": 4,
   "order": 692,
   "p1": "2633",
   "pn": "2636",
   "abstract": [
    "As one of the major Chinese dialects, Hakka typically has a tone system with six lexical tones. The traditional 5-level notation of tones in Hakka varies in previous references due to its subjective and relative nature. In order to overcome the limitations of the traditional approach, the command-response model for the process of F0 contour generation is employed to analyze quantitatively the tones in continuous speech of two varieties of Hakka, spoken in Meixian and in Shataukok, respectively. By providing both phonological descriptions to each tone type and quantitative approximations to continuous F0 contours, the model-based approach provides an efficient connection between phonetics and phonology of Hakka tones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-692"
  },
  "kentner07_interspeech": {
   "authors": [
    [
     "Gerrit",
     "Kentner"
    ]
   ],
   "title": "Length, ordering preference and intonational phrasing: evidence from pauses",
   "original": "i07_2637",
   "page_count": 4,
   "order": 693,
   "p1": "2637",
   "pn": "2640",
   "abstract": [
    "This paper reports a speech production experiment in which the effects of surrounding phrase lengths and head-argument distance on intra-sentential pause duration were tested. While the results confirm an effect of phrase length on pausing, this effect is found to be distinctly stronger for long phrases preceding the pause than for long upcoming phrases. The results are discussed with respect to intonational phrasing tendencies and ordering preferences for unequal-sized constituents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-693"
  },
  "peters07_interspeech": {
   "authors": [
    [
     "Jörg",
     "Peters"
    ],
    [
     "Judith",
     "Hanssen"
    ],
    [
     "Carlos",
     "Gussenhoven"
    ]
   ],
   "title": "Alignment of the second low target in dutch falling-rising pitch contours",
   "original": "i07_2641",
   "page_count": 4,
   "order": 694,
   "p1": "2641",
   "pn": "2644",
   "abstract": [
    "Two production experiments were conducted to establish the anchor point for the beginning of the final rise in Dutch falling-rising pitch contours. We systematically varied the prosodic structure of the post-nuclear words by including the stress level (primary or secondary) of the penultimate syllable and the distance of the last stressed syllable to the utterance end as factors. None of the syllable types provided an anchor point for the timing of the beginning of the rise, which appeared to be most constant relative to the utterance end or to the end of the rise. Our finding is not consistent with earlier experiments which found a tendency for the beginning of the rise to be attracted to the last stressed syllable. Additionally, we found that unaccented primary stressed syllables are somewhat longer than un-accented secondary stressed syllables confirming earlier findings for Dutch obtained on the basis of reiterant speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-694"
  },
  "moniz07_interspeech": {
   "authors": [
    [
     "Helena",
     "Moniz"
    ],
    [
     "Ana Isabel",
     "Mata"
    ],
    [
     "M. Céu",
     "Viana"
    ]
   ],
   "title": "On filled-pauses and prolongations in european portuguese",
   "original": "i07_2645",
   "page_count": 4,
   "order": 695,
   "p1": "2645",
   "pn": "2648",
   "abstract": [
    "This paper reports preliminary results from a study of disfluencies in European Portuguese, based on a corpus of prepared (non-scripted) and spontaneous oral presentations in high school context. We will focus on the contextual distribution and temporal patterns of filled pauses and segmental prolongations, as well as on the way those are rated by listeners.\n",
    "Results suggest that filled pauses and segmental prolongations behave alike, have similar functions and may be considered in complementary distribution, obeying general syntactic and prosodic constraints.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-695"
  },
  "olsberg07_interspeech": {
   "authors": [
    [
     "Michael",
     "Olsberg"
    ],
    [
     "Yi",
     "Xu"
    ],
    [
     "Jeremy",
     "Green"
    ]
   ],
   "title": "Dependence of tone perception on syllable perception",
   "original": "i07_2649",
   "page_count": 4,
   "order": 696,
   "p1": "2649",
   "pn": "2652",
   "abstract": [
    "We tested the hypothesis that, given the consistency of tone-syllable alignment found in recent research, accuracy of tone perception is dependent on the accuracy of syllable perception. In two experiments, subjects either judged the number of syllables or identified the tones in nonsense sentences that were spectrally intact, low-pass filtered at 300 Hz or converted to sustained schwa carrying the original F0. It was found that removing spectral information affected not only subjects' ability to judge the number of syllables in a sentence, but also their ability to identify the tones. The results thus confirm the dependence of tone perception on syllable perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-696"
  },
  "winkler07_interspeech": {
   "authors": [
    [
     "Ralf",
     "Winkler"
    ]
   ],
   "title": "Testing the relevance of speech rate, pitch and a glottal Chink for the perception of age in synthesized speech using formant synthesis",
   "original": "i07_2653",
   "page_count": 4,
   "order": 697,
   "p1": "2653",
   "pn": "2656",
   "abstract": [
    "Listeners are able to rate a speaker's age with reasonable accuracy. However, it is still controversial which features reliably signal a speaker's age. This paper presents results of a synthesis study, where speech rate, pitch, and a glottal chink were varied systematically over a range that effectively occurs in natural speech to shift the mean perceived age.\n",
    "The strongest impact on age judgements was found for (i) speech rate, followed by (ii) the glottal chink, while the impact of pitch was only marginal. Some interactions (iii) between the parameters were observed as well.\n",
    "Results regarding (i) and (ii) show, that formant synthesis is capable of producing speech considerably varying in its mean perceived age even if only a small number of features are manipulated. Regarding (iii), results indicate, that in the study of the impact of selected features their interactions should be considered too.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-697"
  },
  "bohm07_interspeech": {
   "authors": [
    [
     "Tamás",
     "Böhm"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "Utterance-final glottalization as a cue for familiar speaker recognition",
   "original": "i07_2657",
   "page_count": 4,
   "order": 698,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "Several studies have reported systematic differences across speakers in the rate and type of intermittent irregular vocal fold vibration (glottalization). Still, it remains an open question whether human listeners use this speaker-specific information as a cue for recognizing familiar voices. A perceptual experiment was conducted to investigate this issue, concentrating on irregularity in utterance-final position. A novel method was employed to manipulate the final voice quality (in our case, modal or glottalized). Listeners, who were familiar with the voices of the speakers, were presented pairs of speech samples: one with the original and another with manipulated final voice quality. When listeners were asked to select the member of the pair that was closer to the talker's voice, they chose the unmanipulated token in 63% of the trials. This result suggests that irregular pitch periods in utterance-final regions play a role in the recognition of individual speaker voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-698"
  },
  "huang07d_interspeech": {
   "authors": [
    [
     "Chun-Fang",
     "Huang"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A rule-based speech morphing for verifying a expressive speech perception model",
   "original": "i07_2661",
   "page_count": 4,
   "order": 699,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "This paper describes a rule-based approach for verifying a three-layer model that was proposed for modeling expressive speech perception. The three layers are expressive speech, semantic primitives, and acoustic features. In our previous work we built the model. In the current work, the built model is verified by creating rules with parameters that morph the acoustic characteristics of a neutral utterance to the perception of certain semantic primitives or expressive speech categories. There are two types of rules. Base rules verify the validity of the analytic results. Intensity rules verify the perceived intensity of expressive speech and semantic primitives. The experiments results show the significant relationships of expressive speech, semantic primitives, and acoustic features. This model will help to develop tools such as a synthesizer to produce utterances that could give listeners the perception of different categories and intensity-levels of expressive speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-699"
  },
  "helander07_interspeech": {
   "authors": [
    [
     "Elina E.",
     "Helander"
    ],
    [
     "Jani",
     "Nurminen"
    ]
   ],
   "title": "On the importance of pure prosody in the perception of speaker identity",
   "original": "i07_2665",
   "page_count": 4,
   "order": 700,
   "p1": "2665",
   "pn": "2668",
   "abstract": [
    "Many of the current techniques and systems that deal with speaker identity do not regard detailed prosody as a crucial source of speaker-dependent information. The reasoning behind this relates to the common assumption that the F0 level and the spectral data carry all or almost all of the speaker-dependent information. But is this assumption really valid? We have investigated the importance of prosodic information in the perception of speaker identity by conducting a test where the listeners tried to identify people they know after hearing only delexicalized pure prosody signals. The findings presented in this paper show that even a very rough prosodic representation consisting only of a single sinusoid can contain information on speaker identity, giving motivation for the development and wider usage of techniques that better exploit the prosodic aspects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-700"
  },
  "chen07b_interspeech": {
   "authors": [
    [
     "Shi-Han",
     "Chen"
    ],
    [
     "Chih-Chung",
     "Kuo"
    ]
   ],
   "title": "Perceptual relevance of pitch contours of Mandarin tones and its efficacy in prosody generation of speech synthesis",
   "original": "i07_2669",
   "page_count": 4,
   "order": 701,
   "p1": "2669",
   "pn": "2672",
   "abstract": [
    "Modeling Mandarin tones is one of the most important issues in speech synthesis. However, established knowledge is mainly focused on the \"production\" aspect. In this paper, we first characterized relative pitch levels of tones. Next, two perceptual experiments were designed to investigate \"perceptual\" relevance of pitch levels and shapes in Mandarin. Results showed that relative pitch levels of tones were perceptually more important than exactness of pitch shapes, and humans could not perceptually distinguish tonal variations in synthesized Chinese names.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-701"
  },
  "nishizaki07_interspeech": {
   "authors": [
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Mitsuhiro",
     "Sohmiya"
    ],
    [
     "Kenji",
     "Kobayashi"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ]
   ],
   "title": "The effect of filled pauses in a lecture speech on impressive evaluation of listeners",
   "original": "i07_2673",
   "page_count": 4,
   "order": 702,
   "p1": "2673",
   "pn": "2676",
   "abstract": [
    "This paper examines and reports on how \"filled pauses\" included when delivering speeches influence the understanding, and change the impression, of the speech as shown through the research and experiments we conducted on trial subjects. We conducted research about speeches and lectures given at classes at our university, and at academic meetings. A questionnaire related to filled pauses was given to audiences in university classrooms, and the speeches given where recorded. Then, we prepared a number of speeches that were manually altered to put emphasis on the frequency, position, and duration of filled pauses in the speeches. Comparing those speeches with the original speeches which were not processed in our listening experiments, we were able to estimate the effect of filled pauses in a lecture speech and how effective these were in altering the impressions of the audiences. We were able to find the best conditions related to the frequency, position, and duration of filled pauses, and how these conditions clearly changed a lecture or speech into a better one which is easy to understanding and listen to for the audience.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-702"
  },
  "li07j_interspeech": {
   "authors": [
    [
     "Yujia",
     "Li"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Perceptual equivalence of approximated Cantonese tone contours",
   "original": "i07_2677",
   "page_count": 4,
   "order": 703,
   "p1": "2677",
   "pn": "2680",
   "abstract": [
    "This paper describes a perceptual study on approximated Cantonese tone contours. We believe that the perception of tone contours relies mainly on the major trend of pitch movement, and is not sensitive to the exact F0 values at particular time instants. The tone contours of individual syllables and the transition between them are approximated as a small number of linear movements. The effect of such approximation is assessed by perceptual experiments. It is found that the six Cantonese tones can be represented by one or two linear movements, and the transition between tones can be represented by a single linear movement, without creating noticeable perceptual difference. Such simple approximations are desirable for perception-driven F0 modeling for text-to-speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-703"
  },
  "shahid07_interspeech": {
   "authors": [
    [
     "Suleman",
     "Shahid"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Audiovisual emotional speech of game playing children: effects of age and culture",
   "original": "i07_2681",
   "page_count": 4,
   "order": 704,
   "p1": "2681",
   "pn": "2684",
   "abstract": [
    "In this paper we study how children of different age groups (8 and 12 years old) and with different cultural backgrounds (Dutch and Pakistani) signal positive and negative emotions in audiovisual speech. Data was collected in an ethical way using a simple but surprisingly effective game in which pairs of participants have to guess whether an upcoming card will contain a higher or lower number than a reference card. The data thus collected was used in a series of cross-cultural perception studies, in which Dutch and Pakistani observers classified emotional expressions of Dutch and Pakistani speakers. Results show that classification accuracy is uniformly high for Pakistani children, but drops for older and for winning Dutch children.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-704"
  },
  "lemon07_interspeech": {
   "authors": [
    [
     "Oliver",
     "Lemon"
    ],
    [
     "Olivier",
     "Pietquin"
    ]
   ],
   "title": "Machine learning for spoken dialogue systems",
   "original": "i07_2685",
   "page_count": 4,
   "order": 705,
   "p1": "2685",
   "pn": "2688",
   "abstract": [
    "During the last decade, research in the field of Spoken Dialogue Systems (SDS) has experienced increasing growth. However, the design and optimization of SDS is not only about combining speech and language processing systems such as Automatic Speech Recognition (ASR), parsers, Natural Language Generation (NLG), and Text-to-Speech (TTS) synthesis systems. It also requires the development of dialogue strategies taking at least into account the performances of these subsystems (and others), the nature of the task (e.g. form filling, tutoring, robot control, or database search/browsing), and the user's behaviour (e.g. cooperativeness, expertise). Due to the great variability of these factors, reuse of previous hand-crafted designs is also made very difficult. For these reasons, statistical machine learning (ML) methods applied to automatic SDS optimization have been a leading research area for the last few years. In this paper, we provide a short review of the field and of recent advances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-705"
  },
  "rieser07_interspeech": {
   "authors": [
    [
     "Verena",
     "Rieser"
    ],
    [
     "Oliver",
     "Lemon"
    ]
   ],
   "title": "Learning dialogue strategies for interactive database search",
   "original": "i07_2689",
   "page_count": 4,
   "order": 706,
   "p1": "2689",
   "pn": "2692",
   "abstract": [
    "We show how to learn optimal dialogue policies for a wide range of database search applications, concerning how many database search results to present to the user, and when to present them. We use Reinforcement Learning methods for a wide spectrum of different database simulations, turn penalty conditions, and noise conditions. Our objective is to show that our policy learning framework covers this spectrum. We can show that even for challenging cases learning significantly outperforms hand-coded policies tailored to the different operating situations. The polices are adaptive/context-sensitive in respect of both the overall operating situation (e.g. noise) and the local context of the interaction (e.g. user's last move). The learned policies produce an average relative increase in reward of 25.7% over the corresponding threshold-based hand-coded baseline policies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-706"
  },
  "cuayahuitl07_interspeech": {
   "authors": [
    [
     "Heriberto",
     "Cuayáhuitl"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Oliver",
     "Lemon"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "Hierarchical dialogue optimization using semi-Markov decision processes",
   "original": "i07_2693",
   "page_count": 4,
   "order": 707,
   "p1": "2693",
   "pn": "2696",
   "abstract": [
    "This paper addresses the problem of dialogue optimization on large search spaces. For such a purpose, in this paper we propose to learn dialogue strategies using multiple Semi-Markov Decision Processes and hierarchical reinforcement learning. This approach factorizes state variables and actions in order to learn a hierarchy of policies. Our experiments are based on a simulated flight booking dialogue system and compare flat versus hierarchical reinforcement learning. Experimental results show that the proposed approach produced a dramatic search space reduction (99.36%), and converged four orders of magnitude faster than flat reinforcement learning with a very small loss in optimality (on average 0.3 system turns). Results also report that the learnt policies outperformed a hand-crafted one under three different conditions of ASR confidence levels. This approach is appealing to dialogue optimization due to faster learning, reusable subsolutions, and scalability to larger problems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-707"
  },
  "ai07b_interspeech": {
   "authors": [
    [
     "Hua",
     "Ai"
    ],
    [
     "Diane J.",
     "Litman"
    ]
   ],
   "title": "Knowledge consistent user simulations for dialog systems",
   "original": "i07_2697",
   "page_count": 4,
   "order": 708,
   "p1": "2697",
   "pn": "2700",
   "abstract": [
    "We propose a novel model to simulate user knowledge consistency in tutoring dialogs, where no clear user goal can be defined. We also propose a new evaluation measure of knowledge consistency based on learning curves. We compare our new simulation model to real users as well as to a previously used simulation model. We show that the new model performs similarly to the real students and to the previous model when evaluated on high-level dialog features. The new model outperforms the previous model when measured on knowledge consistency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-708"
  },
  "wu07g_interspeech": {
   "authors": [
    [
     "Hsu-Chih",
     "Wu"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Reducing recognition error rate based on context relationships among dialogue turns",
   "original": "i07_2701",
   "page_count": 4,
   "order": 709,
   "p1": "2701",
   "pn": "2704",
   "abstract": [
    "We have recently been conducting research on developing spoken dialogue systems to provide conversational practice for a learner of a foreign language. One of the most critical aspects of such a system is speech recognition errors, since they often take the dialogue thread down a wrong turn that is very confusing to the student and may be irrecoverable. In this paper we report on a machine learning technique to assist the process of selection from a list of N-best candidates based on a high-level description of the semantics of the preceding dialogue. In a user simulation experiment, we show that a significant reduction in sentence error rate can be achieved, from 29.2% to 23.6%. We have not yet verified that our techniques hold for real user data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-709"
  },
  "misu07_interspeech": {
   "authors": [
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Bayes risk-based optimization of dialogue management for document retrieval system with speech interface",
   "original": "i07_2705",
   "page_count": 4,
   "order": 710,
   "p1": "2705",
   "pn": "2708",
   "abstract": [
    "We propose an efficient dialogue management for an information navigation system based on a document knowledge base. It is expected that incorporation of appropriate N-best candidates of ASR and contextual information will improve the system performance. The system also has several choices in generating responses or confirmations. In this paper, this selection is optimized as minimization of Bayes risk based on reward for correct information presentation and penalty for redundant turns. We have evaluated this strategy with our spoken dialogue system \"Dialogue Navigator for Kyoto City\", which also has question-answering capability. Effectiveness of the proposed framework was confirmed in the success rate of retrieval and the average number of turns for information access.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-710"
  },
  "ulbrich07_interspeech": {
   "authors": [
    [
     "Christiane",
     "Ulbrich"
    ],
    [
     "Horst",
     "Ulbrich"
    ]
   ],
   "title": "Realisations and alternations in German /r/-realisation",
   "original": "i07_2733",
   "page_count": 4,
   "order": 711,
   "p1": "2733",
   "pn": "2736",
   "abstract": [
    "Rhotics, generally believed to be phonetically heterogeneous - are usually classified into one group of sounds due to their similar phonological behaviour and their diachronic and synchronic alternation [13, 12, 31, and 1]. Previous research has shown considerable segmental variation, especially in the realisation of /r/ in German which lead some authors to conclude that a positive description of the German phoneme /r/ does not make sense [11].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-711"
  },
  "doty07_interspeech": {
   "authors": [
    [
     "Christopher S.",
     "Doty"
    ],
    [
     "Kaori",
     "Idemaru"
    ],
    [
     "Susan G.",
     "Guion"
    ]
   ],
   "title": "Singleton and geminate stops in Finnish - acoustic correlates",
   "original": "i07_2737",
   "page_count": 4,
   "order": 712,
   "p1": "2737",
   "pn": "2740",
   "abstract": [
    "The present study examined a variety of acoustic correlates to the stop length contrast in Finnish beyond the duration of the consonant itself. Of interest were the durations of surrounding vowels, the duration of voice onset time (VOT), and the amplitude of the release burst and the following vowel. Results indicated that for geminate stops, VOT is shorter and the amplitude of both the following vowel and the release burst are higher than for singleton stops. Further, long vowels preceding geminate stops are shorter than those preceding singleton stops, although no difference was found for short vowels. Post-consonantal vowel duration does not vary as a function of consonant length, but is affected by the length of the first-syllable vowel. These results agree with data from other languages in some respects, but not in others. It is proposed that this discrepancy arises from the fact that Finnish, despite being stress- or syllable-timed, also has mora-like length features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-712"
  },
  "bael07_interspeech": {
   "authors": [
    [
     "Christophe Van",
     "Bael"
    ],
    [
     "Harald",
     "Baayen"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Segment deletion in spontaneous speech: a corpus study using mixed effects models with crossed random effects",
   "original": "i07_2741",
   "page_count": 4,
   "order": 713,
   "p1": "2741",
   "pn": "2744",
   "abstract": [
    "We studied the frequencies of phone and syllable deletions in spontaneous Dutch, and the extent to which such deletions are influenced by the various linguistic and sociolinguistic factors represented in the transcriptions, word segmentations and metadata of the Spoken Dutch Corpus. In addition to providing insight into the frequencies of phone and syllable deletions and the factors influencing them, our study illustrates the new opportunities for analysing rich and therefore complex corpus data offered by a recently developed statistical modelling technique: the possibility to model the effects of random factors as crossed instead of nested with generalised linear mixed effects models.\n",
    "We observed average phone and syllable deletion rates of 7.57% and 5.46% respectively. 20.32% of the words had at least one phone missing, and 6.89% of the words had at least one syllable deleted. The mixed effects models for phone and syllable deletion had several effects in common, which implies that both types of deletion are to a large extent influenced by the same factors. The strongest factors across both models were lexical stress, word duration and the segmental context of the syllable onset of the following word.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-713"
  },
  "zheng07b_interspeech": {
   "authors": [
    [
     "Hongying",
     "Zheng"
    ],
    [
     "Peter W. M.",
     "Tsang"
    ],
    [
     "William S. -Y.",
     "Wang"
    ]
   ],
   "title": "Categorical perception of Cantonese tones in context: a cross-linguistic study",
   "original": "i07_2745",
   "page_count": 4,
   "order": 714,
   "p1": "2745",
   "pn": "2748",
   "abstract": [
    "When human beings perceive speech sounds, they categorize the sounds into one or another phonemic category. The mechanism which is responsible for this phenomenon remains unknown. Is it influenced by listeners' long term language experience or does it reflect some general psychoacoustic aspects of processing? Previous study shows Cantonese level tones are perceived continuously in citation forms [1]. However, they are perceived categorically in the presence of context [6]. The work in [6] does not provide enough evidence to support the hypothesis of long term language influence. In this study, we compare Mandarin and Cantonese speaker's perception on Cantonese level tones in context, and Cantonese speaker's perception on speech and nonspeech (analogous complex harmonics) in the same context as well. Results show evidence of categorical perception on speech stimuli for Cantonese speakers only. These findings support the hypothesis of long term language influence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-714"
  },
  "chen07c_interspeech": {
   "authors": [
    [
     "Yiya",
     "Chen"
    ],
    [
     "Jiahong",
     "Yuan"
    ]
   ],
   "title": "A corpus study of the 3<sup>rd</sup> tone sandhi in standard Chinese",
   "original": "i07_2749",
   "page_count": 4,
   "order": 715,
   "p1": "2749",
   "pn": "2752",
   "abstract": [
    "In Standard Chinese, a Low tone (Tone3) is often realized with a rising F0 contour before another Low tone, known as the 3rd tone Sandhi. This study investigates the acoustic characteristics of the 3rd tone Sandhi in Standard Chinese using a large telephone conversation speech corpus. Sandhi Rising was found to be different from the underlying Rising tone (Tone2) in bi-syllabic words in two measures: the magnitude of the F0 rising and the time span of the F0 rising. We also found different effects of word frequency on Sandhi Rising and the underlying Rising tones. Finally, for tri-syllabic constituents with Low tone only, constituent boundary showed interesting but puzzling effects on the 3rd tone Sandhi.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-715"
  },
  "harrington07_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Harrington"
    ],
    [
     "Sallyanne",
     "Palethorpe"
    ],
    [
     "Catherine I.",
     "Watson"
    ]
   ],
   "title": "Age-related changes in fundamental frequency and formants: a longitudinal study of four speakers",
   "original": "i07_2753",
   "page_count": 4,
   "order": 716,
   "p1": "2753",
   "pn": "2756",
   "abstract": [
    "The study is concerned with a longitudinal acoustic analysis of two sets of recordings from the same four speakers over an interval of between 29 and 50 years. The aim was to determine whether there is any evidence for age-related acoustic changes. Our analysis showed that the same speakers have lower f0, a lower F1, a marginally lower F2, and an unchanging or sometimes higher F3 in their later recordings. There is some suggestion from these data that the change in F1-f0 in Bark from earlier to late recordings is proportional to the change in F3-F2 in Bark. This suggests that there is shift in the speaker space roughly along a diagonal in the phonetic height x backness plane with increasing age.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-716"
  },
  "zhang07e_interspeech": {
   "authors": [
    [
     "Jian",
     "Zhang"
    ],
    [
     "Ho Yin",
     "Chan"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Lu",
     "Cao"
    ]
   ],
   "title": "A comparative study on speech summarization of broadcast news and lecture speech",
   "original": "i07_2781",
   "page_count": 4,
   "order": 717,
   "p1": "2781",
   "pn": "2784",
   "abstract": [
    "We carry out a comprehensive study of acoustic/prosodic, linguistic and structural features for speech summarization, contrasting two genres of speech, namely Broadcast News and Lecture Speech. We find that acoustic and structural features are more important for Broadcast News summarization due to the speaking styles of anchors and reporters, as well as typical news story flow. Due to the relatively small contribution of lexical features, Broadcast News summarization does not depend heavily on ASR accuracies. We use SVM based summarizer to select the best features for extractive summarization, and obtain state-of-the-art performances: ROUGE-L F-measure of 0.64 for Mandarin Broadcast News, and 0.65 for Mandarin Lecture Speech. In the case of Lecture Speech summarization where lexical features are more important, we make the surprising discovery that summarization performance is very high (0.63 ROUGE-L F-measure) even when the ASR accuracy is low (21% CER).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-717"
  },
  "murray07_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Murray"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Towards online speech summarization",
   "original": "i07_2785",
   "page_count": 4,
   "order": 718,
   "p1": "2785",
   "pn": "2788",
   "abstract": [
    "The majority of speech summarization research has focused on extracting the most informative dialogue acts from recorded, archived data. However, a potential use case for speech summarization in the meetings domain is to facilitate a meeting in progress by providing the participants - whether they are attending in-person or remotely - with an indication of the most important parts of the discussion so far. This requires being able to determine whether a dialogue act is extract-worthy before the global meeting context is available. This paper introduces a novel method for weighting dialogue acts using only very limited local context, and shows that high summary precision is possible even when information about the meeting as a whole is lacking. A new evaluation framework consisting of weighted precision, recall and f-score is detailed, and the novel online summarization method is shown to significantly increase recall and f-score compared with a method using no contextual information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-718"
  },
  "yamagata07_interspeech": {
   "authors": [
    [
     "Tomoyuki",
     "Yamagata"
    ],
    [
     "Atsushi",
     "Sako"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "System request detection in conversation based on acoustic and speaker alternation features",
   "original": "i07_2789",
   "page_count": 4,
   "order": 719,
   "p1": "2789",
   "pn": "2792",
   "abstract": [
    "For a hands-free speech interface, it is important to detect commands in spontaneous utterances. To discriminate commands from human-human conversations by acoustic features, it is efficient to consider the head and the tail of an utterance. The different characteristics of system requests and spontaneous utterances appear on these parts of an utterance. Experiment shows that by separating the head and the tail of an utterance, the accuracy of detection was improved. And also, considering the alternation of speakers using two channel microphones improved the performance. Although detecting system requests using linguistic features shows high accuracy, combining acoustic and turn-taking features lift up the performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-719"
  },
  "levit07_interspeech": {
   "authors": [
    [
     "Michael",
     "Levit"
    ],
    [
     "Elizabeth",
     "Boschee"
    ],
    [
     "Marjorie",
     "Freedman"
    ]
   ],
   "title": "Selecting on-topic sentences from natural language corpora",
   "original": "i07_2793",
   "page_count": 4,
   "order": 720,
   "p1": "2793",
   "pn": "2796",
   "abstract": [
    "We describe a system that examines input sentences with respect to arbitrary topics formulated as natural language expressions. It extracts predicate-argument structures from text intervals and links them into semantically organized proposition trees. By instantiating trees constructed for topic descriptions in trees representing input sentences or parts thereof, we are able to assess degree of \"topicality\" for each sentence. The presented strategy was used in the BBN distillation system for the GALE Year 1 evaluation and achieved outstanding results compared to other systems and human participants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-720"
  },
  "kim07d_interspeech": {
   "authors": [
    [
     "Seokhwan",
     "Kim"
    ],
    [
     "Minwoo",
     "Jeong"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "A semi-supervised method for efficient construction of statistical spoken language understanding resources",
   "original": "i07_2797",
   "page_count": 4,
   "order": 721,
   "p1": "2797",
   "pn": "2800",
   "abstract": [
    "We present a semi-supervised framework to construct spoken language understanding resources with very low cost. We generate context patterns with a few seed entities and a large amount of unlabeled utterances. Using these context patterns, we extract new entities from the unlabeled utterances. The extracted entities are appended to the seed entities, and we can obtain the extended entity list by repeating these steps. Our method is based on an utterance alignment algorithm which is a variant of the biological sequence alignment algorithm. Using this method, we can obtain precise entity lists with high coverage, which is of help to reduce the cost of building resources for statistical spoken language understanding systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-721"
  },
  "fujii07_interspeech": {
   "authors": [
    [
     "Yasuhisa",
     "Fujii"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Automatic extraction of cue phrases for important sentences in lecture speech and automatic lecture speech summarization",
   "original": "i07_2801",
   "page_count": 4,
   "order": 722,
   "p1": "2801",
   "pn": "2804",
   "abstract": [
    "We automatically extract the summaries of spoken class lectures. This paper presents a novel method for sentence extraction-based automatic speech summarization.\n",
    "We propose a technique that extracts \"cue phrases for important sentences (CPs)\" that often appear in important sentences. We formulate CP extraction as a labeling problem of word sequences and use Conditional Random Fields (CRF) [1] for labeling. Automatic summarization using CP extraction results as features yields precisions of 0.603 and 0.556 when using manual transcriptions and Automatic Speech Recognition (ASR) results, respectively.\n",
    "Combining the features derived from the CPs and traditional features (including repeated words, words repeated in a slide text, and term frequency (tf), which are surface linguistic information, and speech power and duration, which are prosodic features) [2, 3], we obtained better summarization performance with a κ-value of 0.380, a F-measure of 0.539, and a Rouge-4 of 0.709.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-722"
  },
  "chen07d_interspeech": {
   "authors": [
    [
     "Yi-Ting",
     "Chen"
    ],
    [
     "Hsuan-Sheng",
     "Chiu"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "A unified probabilistic generative framework for extractive spoken document summarization",
   "original": "i07_2805",
   "page_count": 4,
   "order": 723,
   "p1": "2805",
   "pn": "2808",
   "abstract": [
    "In this paper, we consider extractive summarization of Chinese broadcast news speech. A unified probabilistic generative framework that combined the sentence generative probability and the sentence prior probability for sentence ranking was proposed. Each sentence of a spoken document to be summarized was treated as a probabilistic generative model for predicting the document. Two different matching strategies, i.e., literal term matching and concept matching, were extensively investigated. We explored the use of the hidden Markov model (HMM) and relevance model (RM) for literal term matching, while the word topical mixture model (WTMM) for concept matching. On the other hand, the confidence scores, structural features, and a set of prosodic features were properly incorporated together using the whole sentence maximum entropy model (WSME) for the estimation of the sentence prior probability. The experiments were performed on the Chinese broadcast news collected in Taiwan. Very promising and encouraging results were initially obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-723"
  },
  "hebert07_interspeech": {
   "authors": [
    [
     "Matthieu",
     "Hébert"
    ]
   ],
   "title": "Generic class-based statistical language models for robust speech understanding in directed dialog applications",
   "original": "i07_2809",
   "page_count": 4,
   "order": 724,
   "p1": "2809",
   "pn": "2812",
   "abstract": [
    "We investigate the usage of class-based statistical language models (SLMs) for robust speech understanding. Generic class-based SLMs are built using data from several applications and then tested on data from a distinct target application to benchmark their portability. The results show that these generic class-based SLMs perform as well as those trained on data from the target testing application. This leads us to conclude that, for directed dialog applications, words that do not fall within a rule (class) are generic across applications. Also, the generic class-based SLMs can be used to automatically transcribe utterances from the target application with high accuracy. These transcriptions are then used to train a word-based SLM; the resulting word-based SLM outperforms the class-based ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-724"
  },
  "seltzer07_interspeech": {
   "authors": [
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Ivan",
     "Tashev"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Robust location understanding in spoken dialog systems using intersections",
   "original": "i07_2813",
   "page_count": 4,
   "order": 725,
   "p1": "2813",
   "pn": "2816",
   "abstract": [
    "The availability of digital maps and mapping software has led to significant growth in location-based software and services. To safely use these applications in mobile and automotive scenarios, users must be able to input precise locations using speech. In this paper, we propose a novel method for location understanding based on spoken intersections. The proposed approach utilizes a rich, automatically-generated grammar for street names that maps all street name variations into a single canonical semantic representation. This representation is then transformed to a sequence of position-dependent subword units. This sequence is used by a classifier based on the vector space model to reliably recognize spoken intersections in the presence of recognition errors and incomplete street names. The efficacy of the proposed approach is demonstrated using data collected from users of a deployed spoken dialog system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-725"
  },
  "markaki07_interspeech": {
   "authors": [
    [
     "Maria",
     "Markaki"
    ],
    [
     "Michael",
     "Wohlmayr"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Speech-nonspeech discrimination using the information bottleneck method and spectro-temporal modulation index",
   "original": "i07_2913",
   "page_count": 4,
   "order": 726,
   "p1": "2913",
   "pn": "2916",
   "abstract": [
    "In this work, we adopt an information theoretic approach - the Information Bottleneck method - to extract the relevant spectro-temporal modulations for the task of speech / non-speech discrimination - non-speech events include music, noise and animal vocalizations. A compact representation (a \"cluster prototype\") is built for each class consisting of the maximally informative features with respect to the classification task. We assess the similarity of a sound to each representative cluster using the spectro-temporal modulation index (STMI) adapted to handle the contribution of different frequency bands. A simple threshold check is then used for discriminating speech from non-speech events. Conducted experiments have shown that the proposed method has low complexity and high accuracy of discrimination in low SNR conditions compared to recently proposed methods for the same task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-726"
  },
  "jang07_interspeech": {
   "authors": [
    [
     "Keun Won",
     "Jang"
    ],
    [
     "Dong Kook",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "A uniformly most powerful test for statistical model-based voice activity detection",
   "original": "i07_2917",
   "page_count": 4,
   "order": 727,
   "p1": "2917",
   "pn": "2920",
   "abstract": [
    "This paper presents a new voice activity detection (VAD) method using the Gaussian distribution and a uniformly most powerful (UMP) test. The UMP test is employed to derive the new decision rule based on likelihood ratio test (LRT). This method requires the Rayleigh distribution for the magnitude of the noisy spectral component and the adaptive threshold estimated from the noise statistics. Experimental results show that the proposed VAD algorithm based on UMP test outperforms the conventional scheme.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-727"
  },
  "dines07_interspeech": {
   "authors": [
    [
     "John",
     "Dines"
    ],
    [
     "Jithendra",
     "Vepa"
    ]
   ],
   "title": "Direct optimisation of a multilayer perceptron for the estimation of cepstral mean and variance statistics",
   "original": "i07_2921",
   "page_count": 4,
   "order": 728,
   "p1": "2921",
   "pn": "2924",
   "abstract": [
    "We propose an alternative means of training a multilayer perceptron for the task of speech activity detection based on a criterion to minimise the error in the estimation of mean and variance statistics for speech cepstrum based features using the Kullback-Leibler divergence. We present our baseline and proposed speech activity detection approaches for multi-channel meeting room recordings and demonstrate the effectiveness of the new criterion by comparing the two approaches when used to carry out cepstrum mean and variance normalisation of features used in our meeting ASR system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-728"
  },
  "huijbregts07b_interspeech": {
   "authors": [
    [
     "Marijn",
     "Huijbregts"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Roeland",
     "Ordelman"
    ]
   ],
   "title": "Filtering the unknown: speech activity detection in heterogeneous video collections",
   "original": "i07_2925",
   "page_count": 4,
   "order": 729,
   "p1": "2925",
   "pn": "2928",
   "abstract": [
    "In this paper we discuss the speech activity detection system that we used for detecting speech regions in the Dutch TRECVID video collection. The system is designed to filter non-speech like music or sound effects out of the signal without the use of predefined non-speech models. Because the system trains its models on-line, it is robust for handling out-of-domain data. The speech activity error rate on an out-of-domain test set, recordings of English conference meetings, was 4.4%. The overall error rate on twelve randomly selected five minute TRECVID fragments was 11.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-729"
  },
  "sangwan07_interspeech": {
   "authors": [
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "Nitish",
     "Krishnamurthy"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Environmentally aware voice activity detector",
   "original": "i07_2929",
   "page_count": 4,
   "order": 730,
   "p1": "2929",
   "pn": "2932",
   "abstract": [
    "Traditional voice activity detectors (VADs) tend to be deaf to the acoustical background noise, as they (i) utilize a single operating point for all SNRs (signal-to-noise ratios) and noise types, and (ii) attempt to learn the background noise model online from finite data length. In this paper, we address the aforementioned issues by designing an environmentally aware (EA) VAD. The EA VAD scheme builds prior offline knowledge of commonly encountered acoustical backgrounds, and also combines the recently proposed competitive Neyman-Pearson (CNP) VAD with a SVM (support vector machine) based noise classifier. In operation, the EA VAD obtains accurate noise models of the acoustical background by employing the noise classifier and its prior knowledge of the noise type, and thereafter uses this information to set the best operating point and initialization parameters for the CNP VAD. The superior performance of the EA VAD scheme over the standard AMR (adaptive multi-rate) VADs in low SNR is confirmed in a simulation study, where speech and noise data were drawn from the SWITCHBOARD and NOISEX databases. We report an absolute improvement of 10-15% in detection rates over AMR VADs in low SNR for different noise types.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-730"
  },
  "fujimoto07_interspeech": {
   "authors": [
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Kentaro",
     "Ishizuka"
    ]
   ],
   "title": "Noise robust voice activity detection based on switching kalman filter",
   "original": "i07_2933",
   "page_count": 4,
   "order": 731,
   "p1": "2933",
   "pn": "2936",
   "abstract": [
    "This paper addresses the problem of voice activity detection (VAD) in noisy environments. The VAD method proposed in this paper is based on a statistical model approach, and estimates statistical models sequentially without a priori knowledge of noise. Namely, the proposed method constructs a clean speech / silence state transition model beforehand, and sequentially adapts the model to the noisy environment by using a switching Kalman filter when a signal is observed. The evaluation is carried out by using a VAD evaluation framework, CENSREC-1-C. The evaluation results revealed that the proposed method significantly outperforms the baseline results of CENSREC-1-C as regards VAD accuracy in real environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-731"
  },
  "jo07_interspeech": {
   "authors": [
    [
     "Q-Haing",
     "Jo"
    ],
    [
     "Yun-Sik",
     "Park"
    ],
    [
     "Kye-Hwan",
     "Lee"
    ],
    [
     "Ji-Hyun",
     "Song"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Voice activity detection based on support vector machine using effective feature vectors",
   "original": "i07_2937",
   "page_count": 4,
   "order": 732,
   "p1": "2937",
   "pn": "2940",
   "abstract": [
    "In this paper, we propose effective feature vectors to improve the performance of voice activity detection (VAD) employing a support vector machine (SVM), which is known to incorporate an optimized nonlinear decision over two different classes. To extract the effective feature vectors, we present a novel scheme combining the a posteriori SNR,\n",
    "a priori SNR, and predicted SNR, widely adopted in conventional statistical model-based VAD. Based on the results of experiments, the performance of the SVM-based VAD using novel feature vectors is found to be better than that of ITU-T G.729B and other recently reported methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-732"
  },
  "murty07_interspeech": {
   "authors": [
    [
     "K Sri Rama",
     "Murty"
    ],
    [
     "B",
     "Yegnanarayana"
    ],
    [
     "S",
     "Guruprasad"
    ]
   ],
   "title": "Voice activity detection in degraded speech using excitation source information",
   "original": "i07_2941",
   "page_count": 4,
   "order": 733,
   "p1": "2941",
   "pn": "2944",
   "abstract": [
    "This paper proposes a method for detection of voiced regions from speech signals collected in noisy environment. The proposed method is based on the characteristics of excitation source of speech production. The degraded speech signal is processed by linear prediction analysis for deriving the linear prediction residual. Hilbert envelope of the linear prediction residual is processed using covariance analysis to obtain coherently-added covariance signal. The periodicity property of the coherently added covariance signal is exploited to detect the voiced regions using autocorrelation analysis. The performance of the proposed voice activity detection algorithm is evaluated under different noise environments and at different levels of degradation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-733"
  },
  "cournapeau07_interspeech": {
   "authors": [
    [
     "David",
     "Cournapeau"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Evaluation of real-time voice activity detection based on high order statistics",
   "original": "i07_2945",
   "page_count": 4,
   "order": 734,
   "p1": "2945",
   "pn": "2948",
   "abstract": [
    "We have proposed a method for real-time, unsupervised voice activity detection (VAD). In this paper, problems of feature selection and classification scheme are addressed. The feature is based on High Order Statistics (HOS) to discriminate close and far-field talk, enhanced by a feature derived from the normalized autocorrelation. Comparative effectiveness on several HOS is shown. The classification is done in real-time with a recursive, online EM algorithm. The algorithm is evaluated on the CENSREC-1-C database, which is used for VAD evaluation for automatic speech recognition (ASR) [1], and the proposed method is confirmed to significantly outperform the baseline energy-based method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-734"
  },
  "guo07_interspeech": {
   "authors": [
    [
     "Yanmeng",
     "Guo"
    ],
    [
     "Qian",
     "Qian"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Robust voice activity detection based on adaptive sub-band energy sequence analysis and harmonic detection",
   "original": "i07_2949",
   "page_count": 4,
   "order": 735,
   "p1": "2949",
   "pn": "2952",
   "abstract": [
    "Voice activity detection (VAD) in real-world noise is a very challenging task. In this paper, a two-step methodology is proposed to solve the problem. First, segments with non-stationary components, including speech and dynamic noise, are located using sub-band energy sequence analysis (SESA). Secondly, voice is detected within the selected segments employing the proposed method concerning its harmonic structure. Therefore, speech segments can be accurately detected by this rule-based framework. This algorithm is evaluated in several databases in terms of speech/non-speech discrimination and in terms of word accuracy rate when it is used as the front-end of automatic speech recognition (ASR) system. It provides a more reliable performance over the commonly used standard methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-735"
  },
  "fredouille07_interspeech": {
   "authors": [
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "The influence of speech activity detection and overlap on speaker diarization for meeting room recordings",
   "original": "i07_2953",
   "page_count": 4,
   "order": 736,
   "p1": "2953",
   "pn": "2956",
   "abstract": [
    "This paper addresses the problem of speaker diarization in the specific context of meeting room recordings which often involve a high degree of spontaneous speech with large overlapped speech segments, speaker noise (laughs, whispers, coughs, etc.) and very short speaker turns. A large variability in signal quality has brought an additional level of complexity. This paper investigates the effects of speech activity detection and overlapped speech through speaker diarization experiments conducted on the NIST RT'05 and RT'06 data sets. Results indicate that our system is highly sensitive to the shape of the initial segmentation and that, perhaps surprisingly, perfect references can even degrade performance. Finally we propose a direction for future research to incorporate confidence values according to acoustic attributes in order to unify what is currently a somewhat disjointed approach to speaker diarization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-736"
  },
  "kim07e_interspeech": {
   "authors": [
    [
     "Gibak",
     "Kim"
    ],
    [
     "Nam Ik",
     "Cho"
    ]
   ],
   "title": "Voice activity detection using the phase vector in microphone array",
   "original": "i07_2957",
   "page_count": 4,
   "order": 737,
   "p1": "2957",
   "pn": "2960",
   "abstract": [
    "If desired speech source is located at different position from interference, it is possible to exploit spatial selectivity for reliable speech detection. In this paper, we propose a voice activity detector (VAD) for the microphone array system, using spatial information obtained by the eigendecomposition of multi-channel correlation matrix. We use the phase vector as a measure for VAD, which is derived from the principal eigenvector. Voice activity is detected by the log likelihood ratio test under the assumption that phase vectors of speech absent and present signals have complex Gaussian distributions. The proposed algorithm is tested with the real data recorded by 8 microphones and the result shows that it performs better than GSC-based method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-737"
  },
  "flego07_interspeech": {
   "authors": [
    [
     "Federico",
     "Flego"
    ],
    [
     "Christian",
     "Zieger"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Adaptive weighting of microphone arrays for distant-talking F0 and voiced/unvoiced estimation",
   "original": "i07_2961",
   "page_count": 4,
   "order": 738,
   "p1": "2961",
   "pn": "2964",
   "abstract": [
    "This paper introduces a new technique of multi-microphone processing which aims to provide features for the extraction of fundamental frequency and for the classification of voiced/unvoiced segments in distant-talking speech. A multi-channel periodicity function (MPF) is derived from an adaptive weighting of normalized and compressed magnitude spectra. This function highlights periodic clues of the given speech signals, even under noisy and reverberant conditions. The resulting MPF features are then exploited for voiced/unvoiced classification based on Hidden Markov Models. Experiments, conducted both on simulated data and on real seminar recordings based on a network of reversed T-shaped arrays, showed the robustness of the proposed technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-738"
  },
  "murthy07_interspeech": {
   "authors": [
    [
     "A. Sreenivasa",
     "Murthy"
    ],
    [
     "S. Chandra",
     "Sekhar"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Robust and high-resolution voiced/unvoiced classification in noisy speech using a signal smoothness criterion",
   "original": "i07_2965",
   "page_count": 4,
   "order": 739,
   "p1": "2965",
   "pn": "2968",
   "abstract": [
    "We propose a novel technique for robust voiced/unvoiced segment detection in noisy speech, based on local polynomial regression. The local polynomial model is well-suited for voiced segments in speech. The unvoiced segments are noise-like and do not exhibit any smooth structure. This property of smoothness is used for devising a new metric called the variance ratio metric, which, after thresholding, indicates the voiced/unvoiced boundaries with 75% accuracy for 0dB global signal-to-noise ratio (SNR). A novelty of our algorithm is that it processes the signal continuously, sample-by-sample rather than frame-by-frame. Simulation results on TIMIT speech database (downsampled to 8kHz) for various SNRs are presented to illustrate the performance of the new algorithm. Results indicate that the algorithm is robust even in high noise levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-739"
  },
  "sainath07_interspeech": {
   "authors": [
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Victor",
     "Zue"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ]
   ],
   "title": "Audio classification using extended baum-welch transformations",
   "original": "i07_2969",
   "page_count": 4,
   "order": 740,
   "p1": "2969",
   "pn": "2972",
   "abstract": [
    "Audio classification has applications in a variety of contexts, such as automatic sound analysis, supervised audio segmentation and in audio information search and retrieval. Extended Baum-Welch (EBW) transformations are most commonly used as a discriminative technique for estimating parameters of Gaussian mixtures, though recently they have been applied in unsupervised audio segmentation. In this paper, we extend the use of these transformations to derive an audio classification algorithm. We find that our method outperforms both the Support Vector Machine (SVM) and Gaussian Mixture Model (GMM) likelihood classification methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-740"
  },
  "knox07_interspeech": {
   "authors": [
    [
     "Mary Tai",
     "Knox"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Automatic laughter detection using neural networks",
   "original": "i07_2973",
   "page_count": 4,
   "order": 741,
   "p1": "2973",
   "pn": "2976",
   "abstract": [
    "Laughter recognition is an underexplored area of research. Our goal in this work was to develop an accurate and efficient method to recognize laughter segments, ultimately for the purpose of speaker recognition. Previous work has classified pre-segmented data as to the presence of laughter using SVMs, GMMs, and HMMs. In this work, we have extended the state-of-the-art in laughter recognition by eliminating the need to presegment the data, while attaining high precision, as well as yielding higher resolution for labeling start and end times. In our experiments, we found neural networks to be a particularly good fit for this problem and the score level combination of the MFCC, AC PEAK, and F0 features to be optimal. We achieved an equal error rate (EER) of 7.9% for laughter recognition, thereby establishing the first results for non-presegmented frame-by-frame laughter recognition on the ICSI Meetings database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-741"
  },
  "peng07_interspeech": {
   "authors": [
    [
     "Gang",
     "Peng"
    ],
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Automatic acoustic segmentation for speech recognition on broadcast recordings",
   "original": "i07_2977",
   "page_count": 4,
   "order": 742,
   "p1": "2977",
   "pn": "2980",
   "abstract": [
    "This paper investigates the issue of automatic segmentation of speech recordings for broadcast news (BN) and broadcast conversation (BC) speech recognition. Our previous segmentation algorithm often exhibited high deletion errors, where some speech segments were misclassified as non-speech and thus were never passed on to the recognizer. In contrast with our previous segmentation models, which only differentiated between speech and non-speech segments, phonetic knowledge is applied to represent speech by using multiple models for different types of speech segments. Moreover, the \"pronunciation\" of the speech segment has been modified to loosen the minimum duration constraint. This method makes use of language specific knowledge, while keeping the number of models low to achieve fast segmentation. Experimental results show that the new segmenter outperforms our previous segmenter significantly, particularly in reducing deletion errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-742"
  },
  "birkholz07b_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Articulatory synthesis of singing",
   "original": "i07_4001",
   "page_count": 4,
   "order": 743,
   "p1": "4001",
   "pn": "4004",
   "abstract": [
    "Germany A system for the synthesis of singing on the basis of an articulatory speech synthesizer is presented. To enable the synthesis of singing, the speech synthesizer was extended in many respects. Most importantly, a rule-based transformation of a musical score into a gestural score for articulatory gestures was developed. Furthermore, a pitch-dependent articulation of vowels was implemented. The results of these extensions are demonstrated by the synthesis of the canon Dona nobis pacem. The two voices in the canon were generated with the same underlying articulatory models and the same musical score, the only difference being that their pitches differ by one octave.\n",
    "Index Terms: Articulatory singing synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-743"
  },
  "saitou07_interspeech": {
   "authors": [
    [
     "Takeshi",
     "Saitou"
    ],
    [
     "Masataka",
     "Goto"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Vocal conversion from speaking voice to singing voice using STRAIGHT",
   "original": "i07_4005",
   "page_count": 2,
   "order": 744,
   "p1": "4005",
   "pn": "4006",
   "abstract": [
    "A vocal conversion system that can synthesize a singing voice given a speaking voice and a musical score is proposed. It is based on the speech manipulation system STRAIGHT [1], and comprises three models controlling three acoustic features unique to singing voices: the F0, duration, and spectral envelope. Given the musical score and its tempo, the F0 control model generates the F0 contour of the singing voice by controlling four F0 fluctuations: overshoot, vibrato, preparation, and fine fluctuation. The duration control model lengthens the duration of each phoneme in the speaking voice by considering the duration of its musical note. The spectral control model converts the spectral envelope of the speaking voice into that of the singing voice by controlling both the singing formant and the amplitude modulation of formants in synchronization with vibrato. Experimental results showed that the proposed system could convert speaking voices into singing voices whose quality resembles that of actual singing voices.\n",
    ""
   ]
  },
  "roebel07_interspeech": {
   "authors": [
    [
     "Axel",
     "Roebel"
    ],
    [
     "Joshua",
     "Fineberg"
    ]
   ],
   "title": "Speech to chant transformation with the phase vocoder",
   "original": "i07_4007",
   "page_count": 2,
   "order": 745,
   "p1": "4007",
   "pn": "4008",
   "abstract": [
    "The technique used for this composition is a semi automatic system for speech to chant conversion. The transformation is performed using an implementation of shapeinvariant signal modifications in the phase vocoder and a recent technique for envelope estimation that is denoted as True Envelope estimation. We first describe the compositional idea and give an overview of the preprocessing steps that were required to identify the parts of the speech signal that can be used to carry the singing voice. Furthermore we describe the envelope processing that was used to be able to continusously transform the orginal voice of the actor into different female singing voices.\n",
    "Index Terms: envelope estimation, speech transformation, chant synthesis.\n",
    ""
   ]
  },
  "kenmochi07_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kenmochi"
    ],
    [
     "Hayato",
     "Ohshita"
    ]
   ],
   "title": "VOCALOID - commercial singing synthesizer based on sample concatenation",
   "original": "i07_4009",
   "page_count": 2,
   "order": 746,
   "p1": "4009",
   "pn": "4010",
   "abstract": [
    "The song submitted here to the \"Synthesis of Singing Challenge\" is synthesized by the latest version of the singing synthesizer \"Vocaloid\", which is commercially available now. In this paper, we would like to present the overview of Vocaloid, its product lineups, description of each component, and the synthesis technique used in Vocaloid.\n",
    ""
   ]
  },
  "dalessandro07b_interspeech": {
   "authors": [
    [
     "Nicolas",
     "D’Alessandro"
    ],
    [
     "Thierry",
     "Dutoit"
    ]
   ],
   "title": "RAMCESS/handsketch : a multi-representation framework for realtime and expressive singing synthesis",
   "original": "i07_4011",
   "page_count": 2,
   "order": 747,
   "p1": "4011",
   "pn": "4012",
   "abstract": [
    "In this paper we describe the different investigations that are part of the development of a new singing digital musical instrument, adapted to real-time performance. It concerns improvement of low-level synthesis modules, mapping strategies underlying the development of a coherent and expressive control space, and the building of a concrete bi-manual controller.\n",
    ""
   ]
  },
  "ternstrom07_interspeech": {
   "authors": [
    [
     "Sten",
     "Ternström"
    ],
    [
     "Johan",
     "Sundberg"
    ]
   ],
   "title": "Formant-based synthesis of singing",
   "original": "i07_4013",
   "page_count": 2,
   "order": 748,
   "p1": "4013",
   "pn": "4014",
   "abstract": [
    "Rule-driven formant synthesis is a legacy technique that still has certain advantages over currently prevailing methods. The memory footprint is small and the flexibility is high. Using a modular, interactive synthesis engine, it is easy to test the perceptual effect of different source waveform and formant filter configurations. The rule system allows the investigation of how different styles and singer voices are represented in the low-level acoustic features, without changing the score. It remains difficult to achieve natural-sounding consonants and to integrate the higher abstraction levels of musical expression.\n",
    ""
   ]
  },
  "sloetjes07_interspeech": {
   "authors": [
    [
     "Han",
     "Sloetjes"
    ],
    [
     "Albert",
     "Russel"
    ],
    [
     "Alexander",
     "Klassmann"
    ]
   ],
   "title": "ELAN: a free and open-source multimedia annotation tool",
   "original": "i07_4015",
   "page_count": 2,
   "order": 749,
   "p1": "4015",
   "pn": "4016",
   "abstract": [
    "In this demo we will show the main features and capabilities of ELAN. ELAN is a multipurpose, multimedia annotation tool, available for multiple platforms and it is being developed at the Max Planck Institute for Psycholinguistics\n",
    ""
   ]
  },
  "szakos07_interspeech": {
   "authors": [
    [
     "Jozsef",
     "Szakos"
    ],
    [
     "Ulrike",
     "Glavitsch"
    ]
   ],
   "title": "Speechindexer in action: managing endangered Formosan languages",
   "original": "i07_4017",
   "page_count": 3,
   "order": 750,
   "p1": "4017",
   "pn": "4019",
   "abstract": [
    "Among the most endangered languages of the World, Formosan Austronesian vernaculars occupy a sadly prominent place. Two decades spent in language documentation would only weakly repair the broken transmission chain between the generations of speakers, if there had been no chance to develop SpeechIndexer, a novel software combining portability, adaptability for processing large amount of speech data and its first transcription. This presentation introduces the situation of the languages under investigation and shows examples of SpeechIndexer used for marking up and retrieving spoken corpus data, archived for preservation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2007-744"
  },
  "ifukube07_interspeech": {
   "authors": [
    [
     "Tohru",
     "Ifukube"
    ],
    [
     "Yasuyuki",
     "Shimizu"
    ]
   ],
   "title": "A portable record player for wax cylinders using a laser-beam reflection method",
   "original": "i07_4020",
   "page_count": 1,
   "order": 751,
   "p1": "4020",
   "pn": "",
   "abstract": [
    "The wax phonograph cylinder invented by Thomas Edison in 1885 was the medium for recording sound until about 1930. In around 1900, using the Edison-type phonograph, a Polish anthropologist (B. Pilsudski) recorded the songs of the Ainu people in the most northern Japan on 65 wax cylinders. The wax cylinders were accidentally discovered in Poland and we were asked to reproduce them in 1984. Most of them, however, changed in quality by re-crystallization and had many cracks on their surfaces. The Pilsudski's wax cylinders were successfully reproduced by using a laser-beam reflection as well as a light stylus which we developed.\n",
    "Although a lot of wax cylinders, which may be historically valuable, have been preserved all over the world, most of them would change in quality like the Pilsudski.s wax cylinders. We have developed a portable record player having both laser-beam reflection and light stylus methods. Our record player is light and small enough to be carried by a hand and it can reproduce sounds in real time from damaged wax cylinders as well as the undamaged. The portable record player is commercially available in Japan.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes 1-4",
   "papers": [
    "zue07_interspeech",
    "scott07_interspeech",
    "waibel07_interspeech",
    "oudeyer07_interspeech"
   ]
  },
  {
   "title": "Discriminative and Large Margin Techniques in Acoustic Modeling",
   "papers": [
    "li07_interspeech",
    "yin07_interspeech",
    "li07b_interspeech",
    "valente07_interspeech",
    "olsen07_interspeech",
    "wu07_interspeech"
   ]
  },
  {
   "title": "Speech Production I, II",
   "papers": [
    "zhou07_interspeech",
    "martins07_interspeech",
    "takano07_interspeech",
    "torres07_interspeech",
    "singampalli07_interspeech",
    "qin07_interspeech",
    "dusan07_interspeech",
    "miki07_interspeech",
    "toutios07_interspeech",
    "kaburagi07_interspeech",
    "seeber07_interspeech",
    "cisonni07_interspeech",
    "nomura07_interspeech",
    "schneider07_interspeech",
    "aron07_interspeech",
    "robert07_interspeech",
    "turkmani07_interspeech",
    "airas07_interspeech",
    "knoll07_interspeech",
    "araujo07_interspeech",
    "khatiwada07_interspeech",
    "lamoureux07_interspeech"
   ]
  },
  {
   "title": "Phonetic Segmentation and Classification I, II",
   "papers": [
    "karsmakers07_interspeech",
    "park07_interspeech",
    "jansen07_interspeech",
    "parate07_interspeech",
    "huda07_interspeech",
    "goldman07_interspeech",
    "niu07_interspeech",
    "pruthi07_interspeech",
    "hou07_interspeech",
    "golipour07_interspeech",
    "stouten07_interspeech",
    "kalinli07_interspeech",
    "an07_interspeech",
    "tanaka07_interspeech",
    "scharenborg07_interspeech",
    "errity07_interspeech"
   ]
  },
  {
   "title": "Discourse, Dialog and Conversation",
   "papers": [
    "mori07_interspeech",
    "yang07_interspeech",
    "tseng07_interspeech",
    "crocco07_interspeech",
    "watanabe07_interspeech"
   ]
  },
  {
   "title": "Spoken Dialog Systems I, II",
   "papers": [
    "wootton07_interspeech",
    "schooten07_interspeech",
    "kim07_interspeech",
    "moller07_interspeech",
    "mann07_interspeech",
    "komatani07_interspeech",
    "sherwani07_interspeech",
    "rangarajan07_interspeech",
    "ai07_interspeech",
    "chu07_interspeech",
    "quindere07_interspeech",
    "yu07_interspeech",
    "zweig07_interspeech",
    "winterboer07_interspeech",
    "wang07_interspeech",
    "higashinaka07_interspeech",
    "kuo07_interspeech"
   ]
  },
  {
   "title": "Accent and Language Identification I, II",
   "papers": [
    "bauer07_interspeech",
    "sim07_interspeech",
    "wang07b_interspeech",
    "yin07b_interspeech",
    "timoshenko07_interspeech",
    "wong07_interspeech",
    "rosner07_interspeech",
    "toledano07_interspeech",
    "leeuwen07_interspeech",
    "yang07b_interspeech",
    "castaldo07_interspeech",
    "li07c_interspeech",
    "cordoba07_interspeech",
    "shen07_interspeech"
   ]
  },
  {
   "title": "Education and Training",
   "papers": [
    "bolanos07_interspeech",
    "pan07_interspeech",
    "black07_interspeech",
    "minematsu07_interspeech",
    "samir07_interspeech",
    "wet07_interspeech"
   ]
  },
  {
   "title": "Robust ASR I, II",
   "papers": [
    "denda07_interspeech",
    "alvarez07_interspeech",
    "ishizuka07_interspeech",
    "toh07_interspeech",
    "gibson07_interspeech",
    "hsieh07_interspeech",
    "dimitriadis07_interspeech",
    "sasou07_interspeech",
    "hu07_interspeech",
    "buera07_interspeech",
    "tsao07_interspeech",
    "lin07_interspeech",
    "martinez07_interspeech",
    "chien07_interspeech",
    "ma07_interspeech",
    "xiao07_interspeech",
    "boril07_interspeech",
    "jitsuhiro07_interspeech",
    "nishiura07_interspeech",
    "windmann07_interspeech",
    "hung07_interspeech",
    "petrick07_interspeech",
    "liao07_interspeech",
    "nasersharif07_interspeech"
   ]
  },
  {
   "title": "Adaptation in ASR I, II",
   "papers": [
    "tang07_interspeech",
    "teng07_interspeech",
    "gomez07_interspeech",
    "mak07_interspeech",
    "pylkkonen07_interspeech",
    "liu07_interspeech",
    "nishida07_interspeech",
    "karafiat07_interspeech",
    "levy07_interspeech",
    "loof07_interspeech",
    "omar07_interspeech",
    "morales07_interspeech",
    "huo07_interspeech",
    "zheng07_interspeech",
    "hazen07_interspeech"
   ]
  },
  {
   "title": "Speaker Verification &amp; Identification I-IV",
   "papers": [
    "karam07_interspeech",
    "lee07_interspeech",
    "aronowitz07_interspeech",
    "dehak07_interspeech",
    "lopezmoreno07_interspeech",
    "longworth07_interspeech",
    "calvo07_interspeech",
    "ferrer07_interspeech",
    "charlet07_interspeech",
    "lei07_interspeech",
    "tsai07_interspeech",
    "preti07_interspeech",
    "bao07_interspeech",
    "zhang07_interspeech",
    "yingthawornsuk07_interspeech",
    "garreton07_interspeech",
    "zhao07_interspeech",
    "farrus07_interspeech",
    "shan07_interspeech",
    "tseng07b_interspeech",
    "mclaren07_interspeech",
    "fauve07_interspeech",
    "shriberg07_interspeech",
    "hannani07_interspeech",
    "dehak07b_interspeech",
    "vair07_interspeech",
    "matrouf07_interspeech",
    "hazen07b_interspeech",
    "gerber07_interspeech",
    "solewicz07_interspeech",
    "chao07_interspeech",
    "nakagawa07_interspeech",
    "manocha07_interspeech",
    "yang07c_interspeech",
    "okamoto07_interspeech",
    "lu07_interspeech",
    "lindh07_interspeech",
    "prakash07_interspeech",
    "morris07_interspeech",
    "pham07_interspeech",
    "huenupan07_interspeech",
    "chetty07_interspeech",
    "tur07_interspeech",
    "bonastre07_interspeech"
   ]
  },
  {
   "title": "Spoken Data Retrieval I, II",
   "papers": [
    "miller07_interspeech",
    "pan07b_interspeech",
    "merkel07_interspeech",
    "akiba07_interspeech",
    "hakkanitur07_interspeech",
    "thambiratnam07_interspeech",
    "wallace07_interspeech",
    "itoh07_interspeech",
    "vergyri07_interspeech",
    "goto07_interspeech",
    "camelin07_interspeech",
    "shao07_interspeech",
    "kim07b_interspeech",
    "favre07_interspeech"
   ]
  },
  {
   "title": "Speech Perception I, II",
   "papers": [
    "yip07_interspeech",
    "wolters07_interspeech",
    "kusumoto07_interspeech",
    "dufour07_interspeech",
    "irino07_interspeech",
    "toth07_interspeech",
    "yang07d_interspeech",
    "wu07b_interspeech",
    "bagou07_interspeech",
    "stadler07_interspeech",
    "wade07_interspeech",
    "maniwa07_interspeech",
    "jurgens07_interspeech",
    "ikeno07_interspeech",
    "goy07_interspeech",
    "bunnell07_interspeech",
    "wang07c_interspeech",
    "gilbert07_interspeech",
    "brungart07_interspeech",
    "cutler07_interspeech",
    "le07_interspeech",
    "meyer07_interspeech",
    "jesse07_interspeech",
    "mitterer07_interspeech"
   ]
  },
  {
   "title": "Prosody: Prosodic Structure",
   "papers": [
    "igarashi07_interspeech",
    "nesterenko07_interspeech",
    "volin07_interspeech",
    "cho07_interspeech",
    "dreyer07_interspeech",
    "shi07_interspeech",
    "yang07e_interspeech"
   ]
  },
  {
   "title": "Prosodic Modeling I, II",
   "papers": [
    "yu07b_interspeech",
    "tamm07_interspeech",
    "lin07b_interspeech",
    "nemeth07_interspeech",
    "lolive07_interspeech",
    "read07_interspeech",
    "ni07_interspeech",
    "inanoglu07_interspeech",
    "chiang07_interspeech",
    "hirose07_interspeech",
    "tian07_interspeech",
    "strom07_interspeech",
    "takada07_interspeech",
    "stocksmeier07_interspeech",
    "li07d_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "crammer07_interspeech",
    "nilsson07_interspeech",
    "ezzat07_interspeech",
    "dekens07_interspeech",
    "falk07_interspeech"
   ]
  },
  {
   "title": "Spectral Analysis, Formants and Vocal Tract Models",
   "papers": [
    "waterschoot07_interspeech",
    "magi07_interspeech",
    "rudoy07_interspeech",
    "schnell07_interspeech",
    "thiemann07_interspeech",
    "nguyen07_interspeech",
    "darch07_interspeech",
    "deng07_interspeech",
    "kalgaonkar07_interspeech",
    "guruprasad07_interspeech",
    "sturmel07_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Processing for Intelligent Environments",
   "papers": [
    "harma07_interspeech",
    "obuchi07_interspeech",
    "schmalenstroeer07_interspeech",
    "lu07b_interspeech",
    "borgstrom07_interspeech",
    "wolfel07_interspeech",
    "wyatt07_interspeech",
    "abad07_interspeech",
    "ion07_interspeech"
   ]
  },
  {
   "title": "Language Modeling I, II",
   "papers": [
    "su07_interspeech",
    "akita07_interspeech",
    "sako07_interspeech",
    "pan07c_interspeech",
    "wu07c_interspeech",
    "wang07d_interspeech",
    "yamazaki07_interspeech",
    "munteanu07_interspeech",
    "alumae07_interspeech",
    "heidel07_interspeech",
    "yaman07_interspeech",
    "martins07b_interspeech",
    "bach07_interspeech",
    "justo07_interspeech",
    "arsoy07_interspeech"
   ]
  },
  {
   "title": "Prosody Production and Perception",
   "papers": [
    "calhoun07_interspeech",
    "bulut07_interspeech",
    "liu07b_interspeech",
    "rochetcapellan07_interspeech",
    "hide07_interspeech",
    "niebuhr07_interspeech"
   ]
  },
  {
   "title": "Multimodal Speech Recognition",
   "papers": [
    "aboutabit07_interspeech",
    "lucey07_interspeech",
    "seymour07_interspeech",
    "hueber07_interspeech",
    "zhu07_interspeech",
    "dean07_interspeech"
   ]
  },
  {
   "title": "Speech and Other Modalities",
   "papers": [
    "ishi07_interspeech",
    "larsen07_interspeech",
    "campr07_interspeech",
    "edlund07_interspeech",
    "wand07_interspeech",
    "ferre07_interspeech",
    "ouni07_interspeech",
    "burnham07_interspeech"
   ]
  },
  {
   "title": "Multimodal/Multimedia Signal Processing",
   "papers": [
    "kjellstrom07_interspeech",
    "grauwinkel07_interspeech",
    "zhou07b_interspeech",
    "wu07d_interspeech",
    "lee07b_interspeech",
    "hofer07_interspeech",
    "denda07b_interspeech",
    "campbell07_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "wojcicki07_interspeech",
    "li07e_interspeech",
    "das07_interspeech",
    "deger07_interspeech",
    "borowicz07_interspeech",
    "fingscheidt07_interspeech",
    "aicha07_interspeech",
    "mauler07_interspeech",
    "hendriks07_interspeech",
    "krishnamurthy07_interspeech",
    "essebbar07_interspeech",
    "warsitz07_interspeech",
    "koldovsky07_interspeech",
    "lee07c_interspeech",
    "kinoshita07_interspeech",
    "lee07d_interspeech",
    "huang07_interspeech"
   ]
  },
  {
   "title": "Structure-based and Template-based Automatic Speech Recognition",
   "papers": [
    "maier07_interspeech",
    "coro07_interspeech",
    "wachter07_interspeech",
    "han07_interspeech",
    "hamalainen07_interspeech",
    "espywilson07_interspeech",
    "asakawa07_interspeech",
    "togneri07_interspeech",
    "deng07b_interspeech",
    "grangier07_interspeech",
    "yu07c_interspeech"
   ]
  },
  {
   "title": "Robust ASR Against Noise and Reverberation",
   "papers": [
    "segbroeck07_interspeech",
    "demange07_interspeech",
    "kuhne07_interspeech",
    "laidler07_interspeech",
    "stern07_interspeech",
    "morales07b_interspeech"
   ]
  },
  {
   "title": "Language Resources and Tools",
   "papers": [
    "foslerlussier07_interspeech",
    "barroso07_interspeech",
    "schwartz07_interspeech",
    "abdennadher07_interspeech",
    "cieri07_interspeech",
    "heeman07_interspeech"
   ]
  },
  {
   "title": "Single-channel Speech Enhancement",
   "papers": [
    "radfar07_interspeech",
    "suhadi07_interspeech",
    "tat07_interspeech",
    "falk07b_interspeech",
    "abolhassani07_interspeech",
    "shin07_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology",
   "papers": [
    "rathcke07_interspeech",
    "john07_interspeech",
    "demol07_interspeech",
    "gibbon07_interspeech",
    "kalimeris07_interspeech",
    "wissing07_interspeech",
    "woehrling07_interspeech",
    "barry07_interspeech",
    "christodoulou07_interspeech",
    "williams07_interspeech",
    "chow07_interspeech",
    "burki07_interspeech",
    "hu07b_interspeech",
    "martin07_interspeech",
    "ho07_interspeech"
   ]
  },
  {
   "title": "Features for ASR",
   "papers": [
    "hsu07_interspeech",
    "sakai07_interspeech",
    "tyagi07_interspeech",
    "macho07_interspeech",
    "li07f_interspeech",
    "li07g_interspeech",
    "kubo07_interspeech",
    "zahorian07_interspeech",
    "sanand07_interspeech",
    "alencar07_interspeech",
    "sato07_interspeech",
    "matsumasa07_interspeech",
    "valente07b_interspeech",
    "singhmiller07_interspeech",
    "su07b_interspeech",
    "prasanna07_interspeech"
   ]
  },
  {
   "title": "Objective Assessment of Voice and Speech Quality",
   "papers": [
    "bruckl07_interspeech",
    "cnockaert07_interspeech",
    "alpan07_interspeech",
    "laukkanen07_interspeech",
    "leeuw07_interspeech",
    "bruijn07_interspeech",
    "bonastre07b_interspeech",
    "pouchoulin07_interspeech",
    "boucher07_interspeech",
    "maier07b_interspeech",
    "duchateau07_interspeech",
    "ferrer07b_interspeech",
    "fraile07_interspeech",
    "manfredi07_interspeech"
   ]
  },
  {
   "title": "Discourse, Dialog and Emotion Expression",
   "papers": [
    "house07_interspeech",
    "wesseling07_interspeech",
    "laskowski07_interspeech",
    "barkhuysen07_interspeech",
    "schlangen07_interspeech",
    "dalessandro07_interspeech"
   ]
  },
  {
   "title": "Resource Acquisition and Preparation; Resource and System Evaluation",
   "papers": [
    "habernal07_interspeech",
    "nagino07_interspeech",
    "rutten07_interspeech",
    "orr07_interspeech",
    "selmini07_interspeech",
    "lecouteux07_interspeech",
    "paek07_interspeech",
    "kun07_interspeech",
    "zhang07b_interspeech",
    "angkititrakul07_interspeech",
    "kolluru07_interspeech",
    "takezawa07_interspeech",
    "hooijdonk07_interspeech",
    "li07h_interspeech",
    "kessens07_interspeech",
    "holter07_interspeech",
    "boonsuk07_interspeech"
   ]
  },
  {
   "title": "ASR: New Paradigms",
   "papers": [
    "tan07_interspeech",
    "zhao07b_interspeech",
    "markov07_interspeech",
    "breslin07_interspeech",
    "nanjo07_interspeech",
    "bouselmi07_interspeech",
    "emori07_interspeech",
    "stoimenov07_interspeech",
    "pylypenko07_interspeech",
    "hetherington07_interspeech",
    "cincarek07_interspeech",
    "ma07b_interspeech",
    "colthurst07_interspeech",
    "bosch07_interspeech",
    "meyer07b_interspeech",
    "ohta07_interspeech",
    "kumaran07_interspeech"
   ]
  },
  {
   "title": "Speech and Language Technology for Less-resourced Languages",
   "papers": [
    "mihajlik07_interspeech",
    "yang07f_interspeech",
    "huyssteen07_interspeech",
    "draxler07_interspeech",
    "martinovic07_interspeech",
    "pauw07_interspeech",
    "gros07_interspeech",
    "paulo07_interspeech",
    "ringersma07_interspeech",
    "loftsson07_interspeech",
    "peche07_interspeech",
    "abate07_interspeech",
    "nimaan07_interspeech",
    "siivola07_interspeech",
    "jongtaveesataporn07_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "raymond07_interspeech",
    "iosif07_interspeech",
    "gravano07_interspeech",
    "minescu07_interspeech",
    "kolar07_interspeech",
    "albalate07_interspeech"
   ]
  },
  {
   "title": "Pitch Extraction I, II",
   "papers": [
    "wohlmayr07_interspeech",
    "kim07c_interspeech",
    "pelle07_interspeech",
    "legat07_interspeech",
    "molla07_interspeech",
    "hirst07_interspeech",
    "hussein07_interspeech",
    "droppo07_interspeech",
    "ghosh07_interspeech",
    "heckmann07_interspeech",
    "christensen07_interspeech",
    "lienard07_interspeech",
    "rosenberg07_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Transmission",
   "papers": [
    "chatterjee07_interspeech",
    "zhang07c_interspeech",
    "ismail07_interspeech",
    "ismail07b_interspeech",
    "wu07e_interspeech",
    "zopf07_interspeech",
    "agiomyrgiannakis07_interspeech",
    "agiomyrgiannakis07b_interspeech",
    "yuan07_interspeech",
    "raake07_interspeech",
    "lee07e_interspeech",
    "oshikiri07_interspeech"
   ]
  },
  {
   "title": "Topics in Acoustic Modeling",
   "papers": [
    "wielgat07_interspeech",
    "yu07d_interspeech",
    "wu07f_interspeech",
    "oikonomidis07_interspeech",
    "heigold07_interspeech",
    "scanzio07_interspeech",
    "liu07c_interspeech"
   ]
  },
  {
   "title": "Confidence Measures (and Related Topics)",
   "papers": [
    "molina07_interspeech",
    "lin07c_interspeech",
    "huet07_interspeech",
    "li07i_interspeech",
    "allauzen07_interspeech",
    "oba07_interspeech",
    "ketabdar07_interspeech"
   ]
  },
  {
   "title": "Grapheme-to-Phoneme Conversion",
   "papers": [
    "braga07_interspeech",
    "choueiter07_interspeech",
    "thomas07_interspeech",
    "heuvel07_interspeech",
    "thangthai07_interspeech",
    "werner07_interspeech",
    "tsourakis07_interspeech",
    "singh07_interspeech"
   ]
  },
  {
   "title": "Lexical and Prosodic Modeling",
   "papers": [
    "astrov07_interspeech",
    "pellegrini07_interspeech",
    "qiang07_interspeech",
    "seppi07_interspeech",
    "tamburini07_interspeech",
    "ananthakrishnan07_interspeech",
    "pinto07_interspeech",
    "liu07d_interspeech"
   ]
  },
  {
   "title": "Speech Recognition by Automatic Attribute Transcription",
   "papers": [
    "lee07f_interspeech",
    "bromberg07_interspeech",
    "lin07d_interspeech",
    "strik07_interspeech",
    "krajewski07_interspeech",
    "ore07_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization",
   "papers": [
    "otterson07_interspeech",
    "han07b_interspeech",
    "huijbregts07_interspeech",
    "aronowitz07b_interspeech",
    "huang07b_interspeech",
    "le07b_interspeech"
   ]
  },
  {
   "title": "First and Second Language Learning",
   "papers": [
    "lee07g_interspeech",
    "xu07_interspeech",
    "ko07_interspeech",
    "hirano07_interspeech",
    "escudero07_interspeech",
    "broersma07_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis I, II",
   "papers": [
    "krstulovic07_interspeech",
    "gu07_interspeech",
    "liu07e_interspeech",
    "maia07_interspeech",
    "steigner07_interspeech",
    "tachibana07_interspeech",
    "chomphan07_interspeech",
    "bonardo07_interspeech",
    "schmid07_interspeech",
    "xu07b_interspeech",
    "birkholz07_interspeech",
    "nishizawa07_interspeech",
    "strecha07_interspeech",
    "cahill07_interspeech",
    "krul07_interspeech",
    "tsai07b_interspeech",
    "kirkpatrick07_interspeech",
    "langner07_interspeech",
    "schnell07b_interspeech",
    "prahallad07_interspeech",
    "gallardoantolin07_interspeech",
    "lin07e_interspeech"
   ]
  },
  {
   "title": "Voice Conversion and Modification",
   "papers": [
    "hanzlicek07_interspeech",
    "erro07_interspeech",
    "erro07b_interspeech",
    "nurminen07_interspeech",
    "percybrooks07_interspeech",
    "ohtani07_interspeech",
    "petkov07_interspeech",
    "mesbahi07_interspeech"
   ]
  },
  {
   "title": "Improved Acoustic Modeling for ASR",
   "papers": [
    "kuo07b_interspeech",
    "shinozaki07_interspeech",
    "zen07_interspeech",
    "ketabdar07b_interspeech",
    "qian07_interspeech",
    "lamel07_interspeech",
    "mcdermott07_interspeech",
    "kang07_interspeech",
    "chen07_interspeech",
    "deselaers07_interspeech",
    "teunen07_interspeech",
    "gollan07_interspeech",
    "omar07b_interspeech",
    "yu07e_interspeech",
    "bell07_interspeech",
    "sakti07_interspeech",
    "varjokallio07_interspeech"
   ]
  },
  {
   "title": "Multilingualism in Speech and Language Processing",
   "papers": [
    "schultz07_interspeech",
    "deprez07_interspeech",
    "stouten07b_interspeech",
    "cordoba07b_interspeech",
    "solar07_interspeech"
   ]
  },
  {
   "title": "Systems for LVCSR and Rich Transcription I, II",
   "papers": [
    "loof07b_interspeech",
    "koh07_interspeech",
    "batista07_interspeech",
    "yeh07_interspeech",
    "huang07c_interspeech",
    "hazen07c_interspeech",
    "guz07_interspeech",
    "esteve07_interspeech",
    "fu07_interspeech",
    "stuker07_interspeech",
    "hwang07_interspeech",
    "ogata07_interspeech"
   ]
  },
  {
   "title": "Language Learning and Assessment",
   "papers": [
    "tepperman07_interspeech",
    "lee07h_interspeech",
    "waple07_interspeech",
    "cucchiarini07_interspeech",
    "tepperman07b_interspeech"
   ]
  },
  {
   "title": "Multimodal Interaction: Analysis and Technology",
   "papers": [
    "holzapfel07_interspeech",
    "turunen07_interspeech",
    "theune07_interspeech",
    "gandhe07_interspeech",
    "hui07_interspeech",
    "bell07b_interspeech"
   ]
  },
  {
   "title": "Emotion",
   "papers": [
    "kawatsu07_interspeech",
    "arimoto07_interspeech",
    "biadsy07_interspeech",
    "busso07_interspeech",
    "satoh07_interspeech",
    "barra07_interspeech",
    "dupuis07_interspeech",
    "gupta07_interspeech",
    "grichkovtsova07_interspeech",
    "vlasenko07_interspeech"
   ]
  },
  {
   "title": "Speakers: Expression, Emotion and Personality Recognition",
   "papers": [
    "schuller07_interspeech",
    "quang07_interspeech",
    "tachibana07b_interspeech",
    "truong07_interspeech",
    "hu07c_interspeech",
    "sethu07_interspeech",
    "muller07_interspeech",
    "enos07_interspeech",
    "nose07_interspeech",
    "zhang07d_interspeech"
   ]
  },
  {
   "title": "First Language, Second Language, Cross-language",
   "papers": [
    "bettonitechio07_interspeech",
    "kluge07_interspeech",
    "utashiro07_interspeech",
    "kiriyama07_interspeech",
    "lyakso07_interspeech",
    "alphen07_interspeech",
    "nakagawa07b_interspeech",
    "joto07_interspeech",
    "goudbeek07_interspeech",
    "uther07_interspeech",
    "best07_interspeech",
    "lacerda07_interspeech",
    "weenink07_interspeech",
    "lai07_interspeech"
   ]
  },
  {
   "title": "Novel Techniques for the NATO Non-native Air-traffic Control and HIWIRE Cockpit Databases",
   "papers": [
    "pigeon07_interspeech",
    "shen07b_interspeech",
    "dimitriadis07b_interspeech",
    "gemello07_interspeech",
    "smolenski07_interspeech",
    "buera07b_interspeech"
   ]
  },
  {
   "title": "Systems for Spoken Language Translation I, II",
   "papers": [
    "dechelotte07_interspeech",
    "saleem07_interspeech",
    "matusov07_interspeech",
    "cattoni07_interspeech",
    "reddy07_interspeech",
    "tam07_interspeech",
    "stallard07_interspeech",
    "sarikaya07_interspeech",
    "appling07_interspeech",
    "lavecchia07_interspeech",
    "falavigna07_interspeech",
    "fugen07_interspeech",
    "precoda07_interspeech",
    "rao07_interspeech"
   ]
  },
  {
   "title": "Articulatory Features",
   "papers": [
    "richmond07_interspeech",
    "qin07b_interspeech",
    "scharenborg07b_interspeech",
    "shah07_interspeech",
    "potard07_interspeech",
    "frankel07_interspeech"
   ]
  },
  {
   "title": "Wideband Speech Processing",
   "papers": [
    "noureldin07_interspeech",
    "geiser07_interspeech",
    "pulakka07_interspeech",
    "kuroiwa07_interspeech",
    "guerchi07_interspeech",
    "duni07_interspeech"
   ]
  },
  {
   "title": "Accessibility Issues",
   "papers": [
    "dreuw07_interspeech",
    "nakamura07_interspeech",
    "cerva07_interspeech",
    "katsurada07_interspeech",
    "sansegundo07_interspeech",
    "nemeth07b_interspeech",
    "nogueira07_interspeech",
    "suk07_interspeech",
    "sitbon07_interspeech"
   ]
  },
  {
   "title": "New Application Areas",
   "papers": [
    "berton07_interspeech",
    "glass07_interspeech",
    "fischer07_interspeech",
    "ikeda07_interspeech",
    "nishimura07_interspeech",
    "tamura07_interspeech",
    "coulston07_interspeech",
    "vybornova07_interspeech"
   ]
  },
  {
   "title": "Story Segmentation",
   "papers": [
    "chan07_interspeech",
    "fung07_interspeech",
    "rosenberg07b_interspeech",
    "kolluru07b_interspeech"
   ]
  },
  {
   "title": "Prosody: Production",
   "papers": [
    "jilka07_interspeech",
    "shue07_interspeech",
    "benus07_interspeech",
    "gu07b_interspeech",
    "kentner07_interspeech",
    "peters07_interspeech",
    "moniz07_interspeech"
   ]
  },
  {
   "title": "Prosody: Perception",
   "papers": [
    "olsberg07_interspeech",
    "winkler07_interspeech",
    "bohm07_interspeech",
    "huang07d_interspeech",
    "helander07_interspeech",
    "chen07b_interspeech",
    "nishizaki07_interspeech",
    "li07j_interspeech",
    "shahid07_interspeech"
   ]
  },
  {
   "title": "Machine Learning for Spoken Dialog Systems",
   "papers": [
    "lemon07_interspeech",
    "rieser07_interspeech",
    "cuayahuitl07_interspeech",
    "ai07b_interspeech",
    "wu07g_interspeech",
    "misu07_interspeech"
   ]
  },
  {
   "title": "Phonetics",
   "papers": [
    "ulbrich07_interspeech",
    "doty07_interspeech",
    "bael07_interspeech",
    "zheng07b_interspeech",
    "chen07c_interspeech",
    "harrington07_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding and Summarization",
   "papers": [
    "zhang07e_interspeech",
    "murray07_interspeech",
    "yamagata07_interspeech",
    "levit07_interspeech",
    "kim07d_interspeech",
    "fujii07_interspeech",
    "chen07d_interspeech",
    "hebert07_interspeech",
    "seltzer07_interspeech"
   ]
  },
  {
   "title": "Voice Activity Detection and Sound Classification",
   "papers": [
    "markaki07_interspeech",
    "jang07_interspeech",
    "dines07_interspeech",
    "huijbregts07b_interspeech",
    "sangwan07_interspeech",
    "fujimoto07_interspeech",
    "jo07_interspeech",
    "murty07_interspeech",
    "cournapeau07_interspeech",
    "guo07_interspeech",
    "fredouille07_interspeech",
    "kim07e_interspeech",
    "flego07_interspeech",
    "murthy07_interspeech",
    "sainath07_interspeech",
    "knox07_interspeech",
    "peng07_interspeech"
   ]
  },
  {
   "title": "Unreviewed Papers for Special Sessions",
   "papers": [
    "birkholz07b_interspeech",
    "saitou07_interspeech",
    "roebel07_interspeech",
    "kenmochi07_interspeech",
    "dalessandro07b_interspeech",
    "ternstrom07_interspeech",
    "sloetjes07_interspeech",
    "szakos07_interspeech",
    "ifukube07_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2007"
}