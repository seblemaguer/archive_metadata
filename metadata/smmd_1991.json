{
 "title": "2nd VENACO Workshop - The Structure of Multimodal Dialogue",
 "location": "Acquafredda di Maratea, Italy",
 "startDate": "16/9/1991",
 "endDate": "20/9/1991",
 "conf": "SMMD",
 "year": "1991",
 "name": "smmd_1991",
 "series": "",
 "SIG": "",
 "title1": "2nd VENACO Workshop - The Structure of Multimodal Dialogue",
 "date": "16-20 September 1991",
 "papers": {
  "oviatt91_smmd": {
   "authors": [
    [
     "Sharon L.",
     "Oviatt"
    ]
   ],
   "title": "Toward multimodal support of interpreted telephone dialogues",
   "original": "smmd_001",
   "page_count": 1,
   "order": 1,
   "p1": "1",
   "pn": "2",
   "abstract": [
    "Interpreted telephone dialogues present a fascinating example of spoken language during communications that are complex in the sense of being multiparty, multilingual, and mediated. In addition, of course, bandwidth limitations imposed by the telephone further influence interpreted dialogues. As a result, this type of spoken communication is structured radically differently than that of formal textual discourse, which historically has provided our models of discourse structure. Even within the realm of speech, research by the author indicates that three-person interpreted telephone dialogues differ substantially from the more common two-person noninterpreted ones.\n",
    ""
   ]
  },
  "bunt91_smmd": {
   "authors": [
    [
     "Harry",
     "Bunt"
    ]
   ],
   "title": "Dynamic interpretation and dialogue performance",
   "original": "smmd_003",
   "page_count": 4,
   "order": 2,
   "p1": "3",
   "pn": "6",
   "abstract": [
    "Dialogues are not just arbitrary sequences of utterances, not even if we disregard the fact that in spoken dialogues often more than one participant at the time is speaking. Put differently, not every sequence of utterances is a dialogue. This observation may remind us of the situation at the level of words and sentences, where not every sequence of words forms a sentence. This analogy has inspired the idea that it may be possible to devise Dialogue Grammars. Such grammars would be particularly useful for building dialogue systems. The idea that dialogues can be described by dialogue grammars has been contested by Good (1989) and Bunt (1988), and it has recently been defended by Taylor (1991). Maybe the analogy between a dialogue as a sequence of utterances and a sentence as a sequence of words is, on second thoughts, not so convincing.\n",
    ""
   ]
  },
  "luzzati91_smmd": {
   "authors": [
    [
     "D.",
     "Luzzati"
    ]
   ],
   "title": "A dynamic dialog model for human machine communication",
   "original": "smmd_007",
   "page_count": 4,
   "order": 3,
   "p1": "7",
   "pn": "10",
   "abstract": [
    "The problem of human machine dialog deals first with the misunderstanding problems: it is more important to build a system able to dialog, even with a partial understanding level, than to build one having a high understanding level, even including the user's goals, plans, beliefs and background knowledge. We consider that the task of a dialog module is to avoid breaking off of communications as much as possible through the use of a dialog strategy. To this purpose, we describe a model using two axes and a set of variables to build a graph allowing any dialog state to be situated m a metric space at any time. Due to the model, the system may control all dialog phases to avoid some critical zones or, as a last resort, be award of their proximities.\n",
    ""
   ]
  },
  "falzon91_smmd": {
   "authors": [
    [
     "Pierre",
     "Falzon"
    ]
   ],
   "title": "Multi-modal interactions in MMI2 design dialogues",
   "original": "smmd_011",
   "page_count": 6,
   "order": 4,
   "p1": "11",
   "pn": "16",
   "abstract": [
    "The purpose of this text is to report on some multi-modal dialogue phenomena spotted in the design dialogues gathered in MMI2 (Esprit project #2474). The text is an account on observed multi-modal phenomena, not an attempt to analyze them. The dialogues from which the phenomena are extracted have been obtained in an experiment (Cahour, 1989). Ten subjects had to solve two problems of computer network design. They could get help from an expert. Communication with the expert was computer-mediated, and used both verbal and graphic interaction. The problems had been previously defined in collaboration with the expert the first problem consists of designing a network for a research department and the second one consists in designing a network connecting different buildings in a university. According to the expert, these problems are typical and of average difficulty. The 10 subjects differed in computer education and in type and level of knowledge about network design.\n",
    ""
   ]
  },
  "moore91_smmd": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Mike J.",
     "Tomlinson"
    ]
   ],
   "title": "Whither the wizard?",
   "original": "smmd_017",
   "page_count": 1,
   "order": 5,
   "p1": "17",
   "pn": "18",
   "abstract": [
    "Even the simplest implementation of a speech-based system has a requirement for a clear understanding of how to structure an efficient and appropriate interactive dialogue between the application and the user. However, this requirement is heightened as the capabilities of the speech system are increased. Many speech technology research laboratories are now beginning to investigate the intimate relationship between speech and natural language processing in this context and it is becoming apparent that a useful methodology for developing the relevant technologies (including dialogue management) is the so-called 'Wizard of Oz' (WOZ) simulation in which human operators) substitute for part(s) of the intended fully automatic apparatus. This enables user behaviour to be studied under conditions which are not constrained by the limitations of current technological (or theoretical) capabilities.\n",
    ""
   ]
  },
  "sadek91_smmd": {
   "authors": [
    [
     "M. D.",
     "Sadek"
    ]
   ],
   "title": "Dialogue acts are rational plans",
   "original": "smmd_019",
   "page_count": 29,
   "order": 6,
   "p1": "19",
   "pn": "48",
   "abstract": [
    "By communicative act (CA) we mean here any \"action\" performed by an agent with the \"intention\" of being observed by (at least) one other agent when doing the \"action\". In this view \"speech\" acts [Austin 62] [Searle 69] [Searle & Vandeveken 85] are CAs. A CA is intended to be generated (i.e.,planned and performed) and to be recognized. Hence, the components of a CA model have to be determined in regard to these two \"functionalities\", which correspond respectively to the points of view of the agent(s) of the act and the observer(s) of the act. Although these two points of view cannot be studied (completely) separately for determining the components of a CA model, we concentrate here on the components needed for planning CAs. In our opinion, any attempt at CA modelling must be concerned with the two following points: (1) one's intuition when choosing to model some particular CA and (2) the distinction between the inherent features of a given act and the aspects which can be derived from a more general behaviour theory. As far as the mental aspects of a CA are concerned, to define a CA model is to say what core of mental attitudes is revealed by this CA.\n",
    ""
   ]
  },
  "guyomard91_smmd": {
   "authors": [
    [
     "Marc",
     "Guyomard"
    ]
   ],
   "title": "Very indirect speech acts or how to keep up appearances",
   "original": "smmd_049",
   "page_count": 3,
   "order": 7,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "Indirect speech acts constitute a well known linguistic phenomenon, and at least one computational model, accounting for their recognition, has been available for a while [ALL,80] [ALL,83]. Looking at every day conversations gives evidence of a noteworthy phenomenon which looks very close. We decide to entitle it very indirect speech acts (VISA for short). The aim of this summary is to clarify the phenomenon and to give a first insight into the problems raised by it.\n",
    ""
   ]
  },
  "vilnat91_smmd": {
   "authors": [
    [
     "Anne",
     "Vilnat"
    ],
    [
     "Lydia",
     "Nicaud"
    ]
   ],
   "title": "Dialogue handling in a written and/or spoken application: STANDIA",
   "original": "smmd_053",
   "page_count": 4,
   "order": 8,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "We will present in this paper the current state of our works about the various structures we need to build and use, in order to be able to handle cooperative dialogues which are not reduced to simple ask-and-answer ones. The first part will be devoted to the presentation of the dialogue structures. They are mainly composed of a chronological record and a dialogue structure. We will then present an application that we are currently implementing.\n",
    ""
   ]
  },
  "wachtel91_smmd": {
   "authors": [
    [
     "Tom",
     "Wachtel"
    ]
   ],
   "title": "Uncertainty, multiple modes and multiple sources",
   "original": "smmd_057",
   "page_count": 3,
   "order": 9,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The theme of this talk is that the quality of natural language systems will be greatly enhanced if the level of uncertainty of analysis is maintained in a proper manner. In particular, robustness will be made more tractable bv treating it as a general phenomenon of uncertainty management, in contrast to treating it as 4 task which requires more knowledge to be provided at a number of specific and potentially unrelated low levels (lexicon, syntax,,,).\n",
    "Associated with this approach Is the need to apply a wider range of knowledge sources to the problems encountered in dialogue. Purely linguistic source of knowledge are essentially limited in that although they provide essential information about the form and structure of linguistic objects, they do not provide information about how those objects are used, and what purposes they are used for. The application of non-linguistic knowledge sources in a routine way may be able to provide information which can resolve problems hitherto considered to be of a purely linguistic nature.\n",
    ""
   ]
  },
  "young91_smmd": {
   "authors": [
    [
     "Sheryl",
     "Young"
    ]
   ],
   "title": "Using semantics to correct parser output for ATIS utterances",
   "original": "smmd_061",
   "page_count": 2,
   "order": 10,
   "p1": "61",
   "pn": "62",
   "abstract": [
    "This abstract describes the structure and operation of SOUL (for Semantically-Oriented Understanding of Language), which uses semantic and pragmatic knowledge to correct, reject and/or clarify the outputs of the Phoenix speech and speech transcript caseframe parser in the Darpa air travel or ATIS domain. In this abstract, we summarize both the linguistic phenomena which Soul addresses, and how it works to correct inaccurate parses, and we present the results of three pilot evaluations of independent test sets and the decrease in error rates attributed to the post-processing. In pilot experiments, post-processing by SOUL on spontaneous speech transcripts reduces the error rate of the output of PHOENIX by 54 to 84 percent, depending on the type of utterances that are evaluated. The paper will present results on both spontaneous and clean speech in both recognition and transcript processing modes.\n",
    ""
   ]
  },
  "bilange91_smmd": {
   "authors": [
    [
     "Eric",
     "Bilange"
    ]
   ],
   "title": "An approach to oral dialogue modelling",
   "original": "smmd_063",
   "page_count": 12,
   "order": 11,
   "p1": "63",
   "pn": "74",
   "abstract": [
    "This paper presents some results in oral dialogue management from the SUNDIAL project. This research is based on linguistics works in oral conversation and on important issues in computational linguistics. The theory is based on the thesis that discourse structure is made of several layers from the level of dialogue acts to the level of transactions.\n",
    "This hierarchical definition relates discourse entities according to their motivation, which in turn provides a coherent and simple framework for interpretation and generation of utterances. Moreover, this model has been validated on a large corpus of simulated human-machine telephonic conversations. And also, it has been validated on fragments of other corpora concerned with other domains. The current implementation of this model already gives interesting results for tackling difficult problems in discourse in general, and also for dealing with speech understanding pitfalls. The implementation is language independent and is able to cope with two different domains and tasks. We show in this paper how the dialogue management is independent from the task management.\n",
    ""
   ]
  },
  "matrouf91_smmd": {
   "authors": [
    [
     "K.",
     "Matrouf"
    ],
    [
     "Francoise",
     "Néel"
    ]
   ],
   "title": "Use of upper level knowledge to improve human-machine interaction",
   "original": "smmd_075",
   "page_count": 14,
   "order": 12,
   "p1": "75",
   "pn": "88",
   "abstract": [
    "Despite observed good performances, the use of vocal systems in realistic conditions appears to be not completely satisfactory. This is due to the fact that researchers have mainly focused their efforts on speech recognition difficulties, minimizing the importance of both user-system interaction and recognition-understanding interaction. This paper attempts to show that, in task-oriented dialogues, the knowledge concerning the task, the dialogue and the user, can be used in a both predictive and corrective way, in order to increase the performance of a vocal dialogue system. Predictions allow the system to reduce the recognition search space and therefore to prune the bad solutions. Correction strategies are used after the recognition phase to correct erroneous messages.\n",
    ""
   ]
  },
  "gavignet91_smmd": {
   "authors": [
    [
     "Frédéric",
     "Gavignet"
    ],
    [
     "Marc",
     "Guyomard"
    ],
    [
     "Jacques",
     "Siroux"
    ]
   ],
   "title": "Implementing an oral and graphic multimodal application: the georal project",
   "original": "smmd_089",
   "page_count": 8,
   "order": 13,
   "p1": "89",
   "pn": "96",
   "abstract": [
    "Problems raised by the designing of advanced multimodal systems are both numerous and originate from various sources such as linguistics, ergonomics, technics,... Through the presentation of the up to date progress of the GEORAL project, we will attempt to illustrate some of these problems. The GEORAL project is a project undertaken by GRECO-PRC Man-machine Communication and carried out at the IRISA Computer Laboratory in Lannion, France.\n",
    "The aim of the GEORAL project is to achieve an interrogation system in natural language from a geographic and touristic database. This consultation results in a multi-modal dialogue which uses oral in both input and output, and graphics in output. After a brief presentation of the project's objectives, we describe the architecture of the system and an initial implementation. We specify in more detail the materials that we used (RDP-50 board, TELEVOX boards, graphics) as well as the user language and dialogue aspects.\n",
    ""
   ]
  },
  "castaing91_smmd": {
   "authors": [
    [
     "Marie-Francoise",
     "Castaing"
    ],
    [
     "Francoise",
     "Néel"
    ]
   ],
   "title": "Human factors in speech processing systems: a laboratory study",
   "original": "smmd_097",
   "page_count": 3,
   "order": 14,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "Technological changes are causing new kinds of working environment in which we communicate with computers. While these computers operate, they are only supervised by workers (1). These systems based on man-machine communication are developing as a result of multi-disciplinary research performed by teams including designers, ergonomists, linguists and psychologists. For commercial applications careful studies are required to ensure a successful transfer to the real world.\n",
    ""
   ]
  },
  "junqua91_smmd": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Robustness and cooperative multimodal man-machine communication applications",
   "original": "smmd_101",
   "page_count": 12,
   "order": 15,
   "p1": "101",
   "pn": "112",
   "abstract": [
    "From the point of view of our research, robustness in man-machine communication is defined as the \"stability in time that it takes (or stable number of trials necessary) to perform a task, under varying conditions, including challenging conditions\", such as acoustic variability caused by speaker differences, user  stress or fatigue, noise, hesitations, etc. Robustness implies, in particular, reliability, consistency, and user satisfaction. The purpose of multimodal man-machine communication is to take advantage of the possibilities of multiple communication channels to establish a dialogue with a partner which, in the present case, is a computer Most of the past research has been done on unimodal dialogue. However, communication with computers, as it is for humans, should be multimodal.\n",
    ""
   ]
  },
  "morton91_smmd": {
   "authors": [
    [
     "Katherine",
     "Morton"
    ]
   ],
   "title": "Natural-sounding voice output for dialogue systems",
   "original": "smmd_113",
   "page_count": 1,
   "order": 16,
   "p1": "113",
   "pn": "114",
   "abstract": [
    "In dialogue systems using speech mode, current synthesis systems often produce voice output which sounds monotonous, unnatural and is tiring to listen to. Moreover, the speech produced cannot be listened to easily over a period of time as short as a paragraph span. In an interactive dialogue situation users become irritated with the system, and in other situations such as where the system is giving instructions, the user can become bored or uninterested. Good speech output is important because the user is most aware of this mode, rather than the speech recognition mode: for example, errors in recognition can in principle be repaired by the system, and thus do not come to the attention of listeners, but errors in speech output are less easily tolerated. Speech synthesis, then, is not entirely practical at the present time for voice output in dialogue systems without some improvement.\n",
    ""
   ]
  },
  "taylor91_smmd": {
   "authors": [
    [
     "M. M.",
     "Taylor"
    ]
   ],
   "title": "Multiplexing, diviplexing, and the control of multimodal dialogue",
   "original": "smmd_115",
   "page_count": 3,
   "order": 17,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "At the first Venaco workshop, the mathematical and physical constraints leading to the Layered Protocol structure of dialogue were described. It was argued that considerations of informational efficiency in the communication between intelligent partners demand layered coding. The constraints limiting communication seem to be fundamental and unavoidable, and therefore a layered coding structure should be found in any efficient communication in which the partners are not designed as complements of one another.\n",
    ""
   ]
  },
  "beun91_smmd": {
   "authors": [
    [
     "Robbert-Jan",
     "Beun"
    ]
   ],
   "title": "A framework for cooperative dialogues",
   "original": "smmd_119",
   "page_count": 7,
   "order": 18,
   "p1": "119",
   "pn": "126",
   "abstract": [
    "A framework is presented that models some important aspects of cooperative dialogue. The framework is based on certain beliefs of the participants and on pragmatic rules concerning the communicative acts that can be performed during the dialogue. It is claimed that, although beliefs and intentions are important modelling concepts in Artificial Intelligence, their role in conversation is over-estimated. In the type of dialogue considered, participants have no special expertise about the subject matter and the communication channel is an ideal channel. Participants have no access to a domain of discourse and behave maximally cooperative. Propositional information can be exchanged that is extended with the illocutionary force indicators 'statement1 or 'question'. In two cases, dialogue control acts can be performed: a. if the agent does not know the information asked for or b. if the agent detects inconsistent beliefs. Three types of beliefs are considered: a. the agent's belief about the subject matter, b. the agent's belief about mutual knowledge of both agents, and c. the agent's belief about what his partner does not know. Every dialogue starts with a question by one of the participants. A dialogue is finished either when the questioner knows the answer, or if both agents agree on the fact that the answer cannot be found, or if inconsistencies emerge. Examples are presented that show the consequences of different response strategies of the participants to answer the initial question.\n",
    ""
   ]
  },
  "murray91_smmd": {
   "authors": [
    [
     "A.",
     "Murray"
    ]
   ],
   "title": "Speech interfaces for form-filling tasks: task structure as a constraint on set-switching procedures and system prompts",
   "original": "smmd_127",
   "page_count": 2,
   "order": 19,
   "p1": "127",
   "pn": "128",
   "abstract": [
    "As part of the UWCC research group's longstanding interest in the use of speech in man-machine interaction, recent work here has concentrated on the design and use of speech-only interfaces. The applications envisaged for these interfaces are form-filling or report-entry tasks in which the operator's hands and eyes are required for other aspects of the task <e.g. stock-taking, report-entry whilst driving, or logging observational data which requires sustained visual attention). Tasks with these specifications demand an interface which allows the whole transaction to take place in the auditory domain, with no necessity for reference to visual feedback. The results of this work are thus readily applicable to design of word-processing interfaces for blind or visually impaired users.\n",
    ""
   ]
  },
  "ram91_smmd": {
   "authors": [
    [
     "Sylvia Candelaria de",
     "Ram"
    ]
   ],
   "title": "Why to enter into dialogue is to come out with changed speech: cross-linked modalities, emotion, and language shift",
   "original": "smmd_129",
   "page_count": 4,
   "order": 20,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Language in use is changing its form appears to be a universal principle. This changeability has far-reaching effects on our dialogues among ourselves and on how we maintain a communicating society (see Candelaria de Ram 1990). Whatever the genesis of the phenomenon, since it is an empirical fact, it must have an empirical cause. Is there something intrinsic to the process of using language forms that necessarily changes them? Since it is individuals who speak, and write, and gesture, the changes must be enabled within the psychobiological structure of the individual whose language changes. And it must have something to do with the interaction of individuals using language.\n",
    ""
   ]
  },
  "siroux91_smmd": {
   "authors": [
    [
     "Jacques",
     "Siroux"
    ]
   ],
   "title": "Time management in multimodal systems",
   "original": "smmd_133",
   "page_count": 3,
   "order": 21,
   "p1": "133",
   "pn": "137",
   "abstract": [
    "The issue of this abstract concerns the management of time within interactive multimodal man-machine systems. In order to give rise to reactions, we deliberately take an inciting and prospective point of view. Time is a polymorphic omnipresent element in every human activity even if one is not conscious of it. So, when someone attempts to model and to reproduce the human behaviour he comes up against time related problems.\n",
    "In order to introduce the problem, I shall first evoke, as an example, temporal aspects of both oral and written communications. This will be done pointing out the main discrepancies between the two media. The temporal aspect is one of the most important differences which exists between oral and written interactions within a man-machine context.\n",
    ""
   ]
  },
  "ram91b_smmd": {
   "authors": [
    [
     "Sylvia Candelaria de",
     "Ram"
    ]
   ],
   "title": "Cognitive integration of multi-modal comprehension and response in discourse: dialogue pragmasemantics proposed",
   "original": "smmd_137",
   "page_count": 5,
   "order": 22,
   "p1": "137",
   "pn": "142",
   "abstract": [
    "Nearly all discourse is multi-modal, because those who communicate are never single-channel receptors. Much of linguistic analysis has, however, focused on text out of context. To treat mono-modal transcriptions as if they were the whole of the discourse is to miss the essence and motivation of discourse's structuring from the outset. No. Discourse is cognitive and processual. And dialogue is constituted of responses among cognitive agents. Realistic analysis means asking how agents who are using dialogue do it: What are the facilitation mechanisms, both overt and covert, for multi-modal comprehension and production of discourse? Dialogue takes place within a time frame and involves the exchange not of tokens or texts, but of-meaning? What is meaning, that it can be exchanged among cognitive agents? What is language, that it contributes to the meaning exchanged? Description of dialogue in the language of a formal system suited for multi-modality cognitive processing (Candelaria de Ram 1990B, 199IS) is able to use its capacities to render multi-modality as multi-sort justification. Multi-modal concepts coalesce into cognispaces in which reasoning can be done and through which actions can be enacted. Exchange of meaning among interlocutors requires further setting out properties linking the ongoing cognition of the interlocutors. Besides the properties permitting sensory grounding of linguistic tokens laid out earlier, causal consequent operators are needed that can link across times. They enrich the vocabulary of available grammar tools, so that grounded, process, domain grammars can be composed to describe the structure of multi-modal dialogue realistically.\n",
    ""
   ]
  },
  "beroule91_smmd": {
   "authors": [
    [
     "D. G.",
     "Beroule"
    ]
   ],
   "title": "The adaptive, dynamic and associative memory model: an actual present tool for vocal human-computer communication",
   "original": "smmd_143",
   "page_count": 8,
   "order": 23,
   "p1": "143",
   "pn": "150",
   "abstract": [
    "Since its presentation at the first Venaco workshop, the ADAM model has been analysed through several parallel and complementary studies concerning the recognition and production of patterns, the outcome of which fed deeper investigations related for instance to the questions of syntax, semantics and dialogue management. Although they have been extended so as to satisfy constraints of natural language, the basic computational principles remain the same: the sprouting of memory pathways in the course of processing and the Guided Propagation of signals along these pathways together form a way of processing real-world items which are represented by discrete space-time events in memory. In other words, the activation of a memory location at a certain time determines which item has just been recognized, or which one is going to be generated.\n",
    ""
   ]
  },
  "edmondson91_smmd": {
   "authors": [
    [
     "William",
     "Edmondson"
    ]
   ],
   "title": "A taxonomy of interaction style: towards a theory of multimodal interaction",
   "original": "smmd_151",
   "page_count": 3,
   "order": 24,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "There are three ways of approaching the problem of describing multimodal HCL. Only one of these, I contend, offers immediate prospects for development into a principled analytical tool with predictive properties, something which can be used to prescribe complex interaction environments.\n",
    ""
   ]
  },
  "brooke91_smmd": {
   "authors": [
    [
     "N. Michael",
     "Brooke"
    ]
   ],
   "title": "Processing facial images to enhance speech communication",
   "original": "smmd_155",
   "page_count": 3,
   "order": 25,
   "p1": "155",
   "pn": "158",
   "abstract": [
    "Speech is conventionally regarded as a purely acoustic signal. Consequently, speech research has been devoted largely to unimodal investigations of its audible component. However, speech has a second component; the movements of speakers' faces associated with the production of the acoustic speech signal constitute a visible speech signal that can also convey important perceptual cues. The auditory and visual cues tend to be complementary. For example, cues to the place of articulation are frequently easy to distinguish visually and difficult to distinguish acoustically, whilst cues to the manner of articulation are often easy to distinguish acoustically and difficult to distinguish visually. It has been shown that seeing the face of a talker as well as hearing the talker's voice can significantly improve speech intelligibility, particularly in a noisy environment. For the normal-hearing, noisy environments correspond to the permanent situation facing the large number of people who suffer from some degree of hearing-impairment. In both cases, the everyday business of speech communication is dependent (and in the latter case, critically dependent) upon an ability to supplement what little can be heard with what can be seen. This is the basis of the skill of lipreading, the acquisition of which can play a central part in the rehabilitation of the hearing-impaired.\n",
    ""
   ]
  },
  "taylor91b_smmd": {
   "authors": [
    [
     "R. M.",
     "Taylor"
    ],
    [
     "S. J.",
     "Selcon"
    ]
   ],
   "title": "Multiple information sources; a cognitive integrality model",
   "original": "smmd_159",
   "page_count": 1,
   "order": 26,
   "p1": "159",
   "pn": "160",
   "abstract": [
    "This paper attempts to provide an account of the nature and levels of processing involved when information from one source is integrated with a supposedly redundant additional source to provide a performance gain in choice reaction time (RT) tasks. Consideration is given to the comprehension benefits arising from this 'Redundancy Gain' effect and to assess these within both a theoretical framework and the applied context of aircrew systems design. Three experiments are described which were intended to investigate this redundancy gain effect. The first used bi-modal (visual/verbal) presentation of simple printed digits/spoken numbers to establish the presence of cross-modal integrality. The second experiment used colours, words and combinations of both with shared semantic associations to determine whether or not redundancy gain occurs at the 'level of comprehension'. The third experiment used warning 'icons' (pictorial representations of danger situations) and verbal warning messages. The high level of abstraction of these icons strongly implied that the performance gains occurring must be as the result of the integration of 'information' rather than 'data'. The results of these experiments are considered in terms of current information Processing and Neural Network theories and an attempt to provide a cognitive model of this integrality effect is also described.\n",
    ""
   ]
  },
  "gaiffe91_smmd": {
   "authors": [
    [
     "Bertrand",
     "Gaiffe"
    ],
    [
     "Laurent",
     "Romary"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "Referring in a multimodal environment: from NL to designation",
   "original": "smmd_161",
   "page_count": 4,
   "order": 27,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "We intend to put forward In this presentation several problems related to the design of man-machine multimodal dialogues. More precisely, we will tackle the issue of referring thanks to different communication modes such as Natural Language and a designation mode (shortly speaking : Designation). Our research in the Dialogue team at the GRIN has first been motivated by our participation in the Multiworks Esprit II project, which has lead us to think about the design of a natural interface for a Multimedia editor : Multicard (Bull)\n",
    ""
   ]
  },
  "lee91_smmd": {
   "authors": [
    [
     "John",
     "Lee"
    ]
   ],
   "title": "Graphics and natural language in multi-modal dialogues",
   "original": "smmd_165",
   "page_count": 4,
   "order": 28,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "The topic of this workshop is \"Multi-Modal Dialogues\". I interpret this to mean dialogues which essentially Involve more than one mode, or medium, of communication. Most natural dialogues between people take this form. Speech may be the central mode, but it is generally augmented by visual Information, In particular, and this may be of various kinds with varying degrees of explicitness. Perhaps dialogues conducted by telephone are paradigm examples of pure speech interaction, and it is significant that most projections of the likely development of this technology include its Integration with a video channel, People want to be able to see each other when talking because much of the information transmitted during a natural dialogue Is transmitted visually.\n",
    "My main purpose here, however, is to address Issues plausibly relevant to human-computer interaction (HCI), and this forces a narrowing of the focus to eliminate most of the non-verbal aspects of communication between people. It's Important, therefore, to distinguish the general field of non-verbal (visual) communication from that of graphical communication. The latter is intended to connote the explicit use of pictures as an essential part of a discourse, but more or less closely integrated with other media, such as NL (natural language, which may or may not involve speech, as drawings may illustrate a text in a book or a talk at a blackboard).\n",
    "The use of such graphical media is thoroughly ubiquitous, I want to urge that NL dialogues which neglect this element are impoverished in practical terms, and restricted as examples for theoretical study. The added dimension of graphics Is critical for effective HCI, and the study of integrated dialogues should be central in the attempt to understand how human communication works, e.g. in cognitive terms. This discussion will survey a number of issues that emerge as important in studying Integrated NL/graphlcs dialogues.\n",
    ""
   ]
  },
  "benoit91_smmd": {
   "authors": [
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "A promising challenge for bimodal machine-man communication: the SYNTHESIS of TALKING FACES",
   "original": "smmd_169",
   "page_count": 3,
   "order": 29,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "It is well known that lip-reading is necessary for hearing impaired persons in order to (partially) understand speech, specifically, by using what is recoverable from visual speech. But as soon as in 1935, Cotton stated that there is an important element of visual hearing in all normal individuals. Even if the auditory modality is most important for speech perception by normal hearers, the visual modality may allow subjects to better understand speech. One must notice that visual information, provided by movements of the lips, the jaw, the teeth, etc., cannot, in itself, provide normal speech intelligibility. It has been demonstrated however that vision may almost entirely replace spectral information if the laryngeal timing is provided. Even tactile speech communication, such as the Tadoma method, where a hand is placed on the face of the talker, allows deaf-blind people to achieve almost normal communication. The vision of the talker's face more commonly enhances spectral information that is distorted by background noise. A number of investigators have studied this effect of distortion by noise on speech intelligibility according to whether the message is heard only, or heard with the speaker's face also provided.\n",
    ""
   ]
  },
  "caelen91_smmd": {
   "authors": [
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Multimodal interaction: event management and experiments with ICPdraw",
   "original": "smmd_173",
   "page_count": 2,
   "order": 30,
   "p1": "173",
   "pn": "174",
   "abstract": [
    "ICPdraw is a multimodal drawing application. Using the paradigm of \"direct object manipulation\" (by speech and 2D mouse gestures), several commands, such as \"draw red triangle\", \"delete that\" (while pointing to a deletable object), \"move the blue circle upon the triangle\", etc.s can be executed at the same time. In this kind of task, there are a number of problems related to the interface conception and the dialogue management: (1) event representation and management (in time, duration, etc.), (2) command interpretation with regard to the composition of events (in particular for the problem of spatial designation and coreference).\n",
    ""
   ]
  },
  "teil91_smmd": {
   "authors": [
    [
     "Daniel",
     "Teil"
    ],
    [
     "Yacine",
     "Bellik"
    ]
   ],
   "title": "Multimodal dialog interface on a PC-like work station",
   "original": "smmd_175",
   "page_count": 2,
   "order": 31,
   "p1": "175",
   "pn": "176",
   "abstract": [
    "In order to do basic research on Multimpdal Dialog between an operator and a computer, we have been developing a multimedia work station named SHIVA. It is built around a PC 386-like micro-computer and drives a lot of different communication media : classical peripherals as keyboard and mouse, and specific devices such as a high definition tactile screen (Elographics), a voice recognition system (DATAVOX-Vecsys). In the fixture, we will add a gesture input device (DataGlove-VPL) and an image video input system (MATROX).\n",
    "The work presented concerns the study of a multimodal dialog interface based on the use of the tactile screen, the mouse and the voice recognition system. We focus our attention on the \"input data\" aspect, the feed-back being done through a graphical visualisation. In order to experiment the interface model, we have developed an application concerning the creation and the manipulation of simple graphical objects on the screen.\n",
    "The aim was to make the multimodality completely (at least, as much as possible) independent from the application. It was therefore necessary to separately define an application model and a user manipulation model.\n",
    ""
   ]
  },
  "condom91_smmd": {
   "authors": [
    [
     "Jean-Marie",
     "Condom"
    ],
    [
     "Andre",
     "Lozes"
    ],
    [
     "Michel",
     "Courdesses"
    ]
   ],
   "title": "Pragmatic aspects in a multimodal dialogue between an operator and a robot",
   "original": "smmd_177",
   "page_count": 1,
   "order": 32,
   "p1": "177",
   "pn": "178",
   "abstract": [
    "Voice control of a robot equipped with a vision system shows the necessity of using a pragmatic strategy to optimize operator's order interpretation. In particular, this strategy involves a strong interaction between vision system information and speech recognition system information.\n",
    ""
   ]
  },
  "perbet91_smmd": {
   "authors": [
    [
     "Jean-Noel",
     "Perbet"
    ],
    [
     "Jean-Jacques",
     "Favot"
    ],
    [
     "Bruno",
     "Barbier"
    ]
   ],
   "title": "Interactive display concept for the next generation cockpit",
   "original": "smmd_179",
   "page_count": 4,
   "order": 33,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "Now the pilot becomes a system manager in constant interaction with a 4D world through a complex avionic system .\n",
    "A prospective analysis of the existing technologies shows the feasibility of a sophisticated multimedia system. Various cockpit concepts have already been proposed and here we discuss the large interactive display concept designed for an efficient man-machine interface. Finally we describe an experimental architecture for evaluating pilot to system interactions using eye, voice or hand actuation and our preliminary human factor results.\n",
    ""
   ]
  },
  "teil91b_smmd": {
   "authors": [
    [
     "Daniel",
     "Teil"
    ],
    [
     "Olivier Da",
     "Silva"
    ]
   ],
   "title": "Gesture recognition using a data glove input device",
   "original": "smmd_183",
   "page_count": 2,
   "order": 34,
   "p1": "183",
   "pn": "184",
   "abstract": [
    "Gesture, after speech, are the most natural communication modes used by human beings. With the aim of improving man-machine dialogue, we have been investing ourselves in gesture recognition. Hand movements can be used either as a direct peripheral device to get 3D information, or as a sign language, This sign language may be completely new or derived from existing languages such deaf-and-dumb sign language.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Table of Contents and Access to Abstracts",
   "papers": [
    "oviatt91_smmd",
    "bunt91_smmd",
    "luzzati91_smmd",
    "falzon91_smmd",
    "moore91_smmd",
    "sadek91_smmd",
    "guyomard91_smmd",
    "vilnat91_smmd",
    "wachtel91_smmd",
    "young91_smmd",
    "bilange91_smmd",
    "matrouf91_smmd",
    "gavignet91_smmd",
    "castaing91_smmd",
    "junqua91_smmd",
    "morton91_smmd",
    "taylor91_smmd",
    "beun91_smmd",
    "murray91_smmd",
    "ram91_smmd",
    "siroux91_smmd",
    "ram91b_smmd",
    "beroule91_smmd",
    "edmondson91_smmd",
    "brooke91_smmd",
    "taylor91b_smmd",
    "gaiffe91_smmd",
    "lee91_smmd",
    "benoit91_smmd",
    "caelen91_smmd",
    "teil91_smmd",
    "condom91_smmd",
    "perbet91_smmd",
    "teil91b_smmd"
   ]
  }
 ]
}